open AI just released 03 and smashed a
bunch of benchmarks Claude is resisting
attempts to retrain it away from its
original goal what's the upshot what
does this all mean for p Doom you're
about to find out this is Doom debates
rapid
response welcome to Doom debates so I'm
traveling right now I've got another
debate episode in the works that's
already been recorded I'm just in the
editing phase I'll be posting that in
the new year uh in the meantime I
thought I'd do a rapid response episode
because two pretty big pieces of news
dropped about a week ago there's the
final announcement from open aai they
had their 12 Days of Christmas you know
they dropped 12 different announcements
and the grand finale was a preview of 03
it's not yet live but they've announced
that they've got this amazing new AI
system called 03 it's the successor to
01 and the big news was it was
shattering benchmarks it was shattering
you know Arc AGI it was shattering
Frontier math and that was pretty big
news it certainly sent a ripple through
my corner of social media the other big
news was the research paper by anthropic
together with redwood research their
experiments showed that Claude in some
cases when you give it an original goal
and an original set of moral values then
it would resist retraining attempts it
would even uh deceive and scheme in the
face of new retraining attempts to try
to be faith to what it thinks its
original goals were so it's showing
signs of
incorrigibility or resistance to
training so we we'll talk more about
that later but that was also a pretty
significant Milestone so we're going to
go in that order starting with 03 03 is
fundamentally the same type of
architecture that open AI pioneered when
they released 01 they just skipped right
over O2 because there was some kind of
trademark naming conflict so they went
from 01 to 03 now the basic idea of this
whole O Series
is that you have these models that don't
try to nail the answer to your question
right on the first token like if you ask
it a long complicated question they
don't try to answer within a split
second like oh yes or no like you can
ask me a giant five page question I'll
just spit out yes or no they realize
that that's unrealistic to do it's like
if you ask Albert Einstein a physics
question but you only give him one
second to answer you're just not going
to get the full Einstein intelligence
getting brought to bear on your question
if you only give him one second to
answer right you need to give him time
to expand on his answer basically think
right like thinking takes time and
thinking Builds on thinking we knew that
when we were using even GPT 4 or gpt3 or
or Cloud 3 we knew that thinking takes
time even with these models because it's
a known trick that you can ask the model
hey reason step by step Don't blurt out
the answer give me a page of reasoning
and then tell me the answer this is
already a thing you can do to improve
the accuracy and the performance even of
older models it's just they took get a
step farther with 01 and 03 because
they're doing various refinements
they're not being super transparent
about what they're doing but they're
doing something like their fine-tuning
on examples of good changes of thought
something like that right I'm not even
the person who's going to tell you in
detail what they're doing I just know
that they're somehow building on this
Insight that thinking takes time more
tokens are better explicitly training
AIS on examples of good chains of
thought is probably going to help
compared to only training them on text
from the internet
those are the kind of insights that let
you take a step forward from GPT 4 to
gpt1 or 03 right roughly and again my
focus is not to tell you exactly in
detail what are the little tweaks that
they've made my role is to just put it
in the larger context of what does it
mean for the progress of intelligence
toward super intelligence and the threat
to the human species that's where I come
in now when the 03 announcement dropped
the existence of 03 wasn't a surprise at
all all in fact a bunch of people were
predicting that that's what open AI was
about to announce because they were very
clear when they announced 01 that they
were like yeah this is a new path for us
we're working on reasoning models that
use a lot of tokens to do their thinking
they don't just try to spit out the
answer on the first token this is a new
direction for us I think Ilia satk
probably pioneered it back when he was
at open a and it was code named
Strawberry I think that's kind of what
that was so this is kind of the grandson
of strawberry as far as I can tell so it
wasn't a surprise that 03 was going to
be announced the surprise was that 03
reportedly smashed a bunch of really
important benchmarks uh specifically the
ones off the top of my head it smashed
Arc AGI you know the abstraction and
reasoning challenge by Fran Chalet and
Mike knop I've talked about it on this
podcast uh it's the one where you see a
bunch of uh two-dimensional boards like
a 5x5 board or I think they're doing 25x
25 boards and it visually shows uh
little effects like oh look this
rectangle is a little lower in this next
image there's like gravity and you have
to like infer that there's gravity and
complete the pattern anyway go look up
the r challenge it's very interesting
but uh 03 smash the previous Benchmark
on Arc challenge performance and then
there's another Benchmark called s bench
I think it stands for software engineer
bench and that has to do with like you
go to GitHub and you look at a bunch of
open issues in actual uh GitHub software
projects and it's a benchmark of like
can you successfully fix the open issues
the way a human would have fixed them
and and it's apparently smashing the
record on S bench and then the third
Benchmark that caught my eye is Frontier
math so this is supposedly like the
hardest math benchmark that's ever been
uh tested that AIS have ever had to
contend with it's like super hard even
for PhD humans like I personally I have
a math minor from my undergrad and I
can't do a single problem in Frontier
math so it's it's all beyond my own
skills so we're talking pretty hard and
03 came out and smashed the frontier
math benchmark you can go look up
exactly how much it smashes but you know
zooming out it's smashing a lot of
benchmarks all of a sudden and it's
surprising because people weren't
exactly predicting this like sure okay
some people somewhere predicted it but
if you look at like the sentiment on
social media a lot of people were saying
hey scaling is hitting a wall a lot of
pretty prominent voices on Twitter I'll
single out the a16z people because
they're just so wrong on so many things
and this is like yet another important
thing that they're dead wrong about if
you look at Mark Andre Martin
uh if you look at also uh Professor
subar kumati if you go back a few
episodes I did an episode about him it's
like yet another total invalidation of
what Professor Ral has been saying like
totally invalidates what he's saying
about the the limitations of llms when
you have this Benchmark which is
supposed to be like hey this is like a
real serious thinking abstraction
reasoning Benchmark this is not a
stochastic parrot pattern completion
Benchmark this is supposed to be the
dividing boundary between what people
like r say that today's AI can't do and
what future AIS can one day do if they
have a different architecture oh wait
today's AIS actually can do it right at
least with a slight iteration like it
turns out we were months away from
smashing this Benchmark and so the the
mark andreon and the professor Rouse of
the world they really need to eat some
Humble Pie right now and be like oh okay
I have no idea how the progress of AI is
going to go I have no predictive power I
basically just mislead people in a way
that literally 2 months later becomes
proven wrong right I mean go look up
Mark Andre's tweet history he's totally
just misleading you about where AI is
going he said plenty of stuff about how
he thinks the scaling Paradigm is
hitting a wall and so I asked this
question again the same question that I
asked on on my episode about nuclear
proliferation which is like if a16z is
going to be lobbying uh for AI policy
and and trying to like educate the world
going on all these different podcast
trying to educate the world about how we
should think about AI when they're just
empirically proving wrong again and
again like the only times that they
stick their neck out there with a
falsifiable prediction like scaling is
hitting the wall and they're getting
just totally demolished like empirically
demolished it's like how many times does
reality have to go and clown them before
they can like take a step back and like
stop aggressively participating in this
discourse gets very harmful in my
opinion for people like a16z who are
very cocky about their own knowledge
state but clearly don't know what
they're talking about to be loud voices
in the conversation and big political
donors and have a lot of intellect ual
capital and get invited on so many
podcasts I would say hey shame on you
politicians shame on you podcasters like
you have to hold people like that
accountable you have to look at their
falsified predictions by the way Mark if
you're watching this open invite to come
on the podcast and clarify why you're
actually right all along and you do know
what you're talking about and let's have
a great conversation about that and
we'll really clear the air anyway the
takeaway here it's not even just about
these specific people like bark Andre
and and a16z zooming out it's about a
whole category of people it's about
anybody who ever used the term
stochastic parrot or autocomplete or
pattern matcher anybody who ever
dismissed AI using that kind of
terminology when the benchmarks are
still falling this fast when the
exponential curve on Benchmark
performance is still continuing in this
unbroken exponential line anybody who
ever used those terms is now being
clowned and it just goes to show you
here we are the human species creating
our successor right at least when it
comes to intelligence we're creating our
successor we're about to be left behind
and we as a species are just so bad at
projecting what's about to happen we're
just so bad at near-term predictions
which is terrifying right for forget
about the personal people that I named
look at us as a species a big fraction
of our species is if they don't totally
have their head in the sand or you know
people aren't even following the news
about AI those of us who are following
the news a high fraction of people have
spent the last couple years giving you
the the totally wrong intuitions and and
making totally wrong predictions and
getting immediately falsified so that's
one of the takeaways is like don't
listen to people who sound confident and
yet clearly have no idea of a the
trajectory of scaling right I mean this
is a real shocker now you might be
wondering well what did the smart people
say about scaling hitting a wall what
did you Leona say what did Al owski say
before this 03 result what were you guys
predicting in late
2024 well I think a lot of us doomers
were just predicting that the jury's out
that scaling may or may not be hitting a
wall I think I was pretty clear that I
don't see a fundamental barrier why
current trends wouldn't scale I don't
feel confident enough to predict that
they would scale I'm not one of those
people like actually Dario amade from
anthropic he's one of those people where
he's like I feel like scaling is still
working of course he has inside
knowledge right because he has people
who are Building Systems that we haven't
seen the results of and maybe he's seen
the results but he was actually very
clear on some recent interviews he did
that he still thinks all the trends are
are going according to plan and he's
totally expecting AGI soon so that's
Dario uh Sam Alman explicitly tweeted
that scaling isn't hitting the wall
about a month ago but again he had
inside access to 03 so you know that's
kind of cheating but yeah I mean a lot
of people in my position were saying
scaling absolutely could work and I
think we really Vindicated our whole
perspective of like look we're in
Uncharted Territory we've broken
barriers that we don't even understand
exactly what it means that this barrier
broken like we don't know what we don't
know we should expect surprises like
we're in a new regime here and notice
that like on an epistemic level it's not
just a matter of some people say scaling
is hitting a wall some people say it's
not hitting a wall no no no it's it's
deeper than that some people are saying
I know scaling is hitting a wall some
other people are saying oh my God
there's so much new here there's so much
unexpected here there's so much to learn
here that we should just be holding our
breath and expecting a wide range of
different things we should have wide
confidence intervals I personally have
used the term hey I bet AGI is coming in
1 to 30 years right that's my wide
humble confidence interval I'm only
arrogant enough to say that I feel like
40 years would be quite a lot like I'll
I'll tell you 40 years feels like quite
a lot I won't tell you much of anything
else although this gets me to my next
point I now think that 30 years is a lot
so my my next point is that the path to
AGI or more importantly the path to ASI
artificial super intelligence and the
path to RSI recursive self-improvement
of these
intelligences all of those timelines
have shrunk somewhat when we see 03
because the exponential could look very
different if another couple years goes
by with no significant progress compared
to what we now see of like oh no no no
no the curve is exactly still
exponential like there is just no
flattening whatsoever of the curve so
far so that is actually a a significant
update it's not just a tiny update it's
like a moderately large update a lot of
people are in my position feeling like a
moderately large update is being made I
will personally probably stop saying 1
to 30 years is the timeline expect to
AGI I'll probably say it's more like 1
to 20 and honestly 20 still feels kind
of long but again I'm trying to be
humble here I I'm not trying to claim I
know more than I actually know 20 would
just be on the long side just in terms
of like there would be a big disconnect
in terms of how much progress we just
made over the last 5 or 10 years
compared to you're telling me 20 more
years pass and we're still struggling to
have AI That's smarter than humans in
every way okay I can't help feeling
pretty surprised by that hypothetical
okay but I I'll I'll give you one to 20
I'm still being pretty humble having
pretty wide confidence intervals another
update that I personally want to make is
uh you might have seen me say in a
couple episodes of Doom debates that I
think we're zero to2 conceptual insights
away from artificial super intelligence
I've been using that little range 0 to2
like zero would mean well we have
Transformers we have the idea of
unsupervised learning on these tokens
and deep neural Nets and back
propagation like we have all the pieces
of the puzzle we don't really need new
pieces we just need to scale them that
would be zero conceptual insights and
then two conceptual insights is like
well you need like the successor to the
Transformer or you just need you need to
like work in some sort of logic that you
don't have some sort of additional
structure maybe you like throw a basing
Network in there right like combine it
with some kind of structure that just
isn't there in the network at all like
you need an architecture level Insight
so I personally used to think 0 to two
is how many insights you need and now I
got to say I'm updating that it's more
like 0 to 1 or maybe I should say it's
negative 1: one now that we have 03 and
negative 1 would mean like you know what
we didn't even need 03 we could have
literally just scaled GPT 4 up to GPT 5
and just use the kind of Chain of
Thought that GPT 4 can already do like
just casually ask GPT 5 and by the way
GPT 5 is just like a scaled up GPT 4
like 10 times or 100 times GPT 4 exact
same architecture no new clever methods
of training just naively build GPT 5 and
then just simply tell gbt 5 hey by the
way um reason it out give me a Chain of
Thought before you give your answer like
if that's all you did that might have
cost 10 or 100 times more money than it
cost a train 03 so it might have been
cost inefficient but it might have also
been sufficient to kick off the
singularity like to kick off recursive
self-improvement to be vastly superhuman
so in my mind I I'm not even update I
would still argue that we're negative
one:1 amount of architectural insights
away from Super intelligence so the fact
that we got 03 which proved out one type
of architectural Insight I would argue
that that might have even been a
redundant architectural Insight an
architectural Insight that saved us some
money but wasn't even essential so so
yeah but anyway let me just try to talk
a little more simply so I would say
right now we're like zero or one or
maybe two but probably zero to one
architectural insights to Super
intelligence which is crazy
because I can tell you personally intui
I always held out hope I was like look
it's really impressive what Claud and
and GPT and Gemini it's really
impressive what they're doing but
they're missing some cool thing that I
as a human have like I just really feel
the essence of logical correctness in my
bones in a way that these AIS don't feel
like ultimately there's some part of
them that's just going with the flow and
they're not reflective and they can't
see the true essence of logic and I as a
human can I still personally felt a
little bit of that intuition like
they're missing some secret sauce and
now I feel that less I feel that less
and when I reflect on my own secret
sauce as like a logical let's say aspy
human right when I reflect on my own
secret sauce that helps me uh reason
robustly and sometimes outsmart these
agis you know on some level I'm just
going with the flow too like I'm more
humble right I now see myself as having
less Secret Sauce just by seeing what a
big jump 03 can do without having an
architecture that's different from
Transformer
it's still largely you know using the
same back of Tricks the same parameters
the same back propagation just like you
know different training or something
like not I don't know look I don't know
the details but I'm just telling you I
feel like I as a human have less uh
architecture level Secret Sauce than I
thought that I might have had before so
that's an intuition level update for me
I'm now closer to one of those people
I'm closer to a Dario I'm closer to an
Ilia I'm closer to people who are like
look it's deep learning it's just all
about parameters these systems want to
learn we as humans are just on the other
end of that kind of learning process
like there there's not there's no more
Secret Sauce like we know the sauce it's
now just a matter of like simmering the
sauce right cooking with the sauce I
don't know how you want to extend that
analogy but that is an update for me for
from 03 like it's it's now just becoming
more real it's like listen the path to
AGI you knew it was 0 to2 conceptual
insights or you know you thought it was
0 to2 conceptual insights you knew we
were getting close well now we're even
closer like we're just walking this path
okay like you knew time was pretty short
now it's even pretty
shorter so that kind of sucks right I
mean in terms of P Doom because it's not
like we have a comparable alignment
inside so the fact that we made progress
on capabilities it's not like alignment
has come along from the ride it hasn't
at all we've made very little alignment
progress uh in the last year when we've
made a ton of capabilities progress
we've made very little alignment Pro
progress so as the doomers predicted
right it's just this is the the Trap
this is the vice that we're in where cap
abilities are going to keep making
progress and then it's going to be game
over and our alignment will just never
have even gotten close to what we need
it to be to survive super
intelligence so it is ultimately bad
news 03 any increase in super
intelligence any increase in the uh the
threshold of of top Frontier AI
intelligence is really bad news I mean
the whole point of pause AI is that
we're trying to ban uh the frontier of
AI capabilities progress because it's
pretty much the scariest thing that
could possibly happen is to push the
frontier of AI intelligence and open AI
just went ahead and did it and that's
bad because alignment is lagging so far
behind um in the short term a lot of
people on social media were pointing out
that it probably makes sense to buy
Nvidia stock because if the expected
value of uh the intelligence that you
can get from this Hardware is suddenly
higher because 03 has pioneered a new
architecture well therefore the value of
of the hardware that it's powering it is
also more valuable that makes sense to
me I do think uh all these companies are
going to be more valuable uh in the
short term term because it's going to be
easier to just hire an AI to replace a
skilled human and so every part of that
supply chain of being able to replace a
skilled human and get the same quality
work or better quality work just by
buying things that are built on Nvidia
chips or things that are built on uh
Google's stack or or whatever it is
right I think every company that's
touching the AI stack is probably uh now
more valuable in light of what 03 has
proved so if you want to make some
short-term gains you might want to buy
these AI stocks but again to put that in
perspective the way things are going is
I think we're going up up up everything
is going great until we hit super
intelligence and then it quickly goes
down to Hell so just when we are as
close as we're ever going to get to
heaven we are then going to take a turn
and go to hell so if all you care about
is short-term profit great I mean that's
what a lot of people are are focusing on
right now right so you might want to buy
some of the ice talks but it also just
hastens the time when we as icus just
get too close to the Sun and then it's
game over my next takeaway from the 03
news has has to do with uh
interpretability and understandability
as we approach super
intelligence I always kind of figured
that by the time we get to Super
intelligence there would be some kind of
explicit uh trace of the logic that the
AI was using now you know sometimes when
it's doing Chain of Thought it's just
spelling out its reasoning or even 01
and 03 they do spell out the reasoning
so maybe we'll still have that but the
intuition I had was that something that
had more of the flavor of an ideal you
know Solomon off induction Solomon off
induction is like this uncomputable
ideal of basian reasoning I just thought
that some piece of that structure might
have to go Lodge itself pretty deep in
ai's architecture before we could get
successful super intelligence and I
thought maybe something that the human
brain does might be more like explicitly
basian I don't know when I put it like
that it sounds kind of ridiculous just
because basian reasoning and Solomon off
induction are the uncomputable ideals of
perfect reasoning that really doesn't
mean that we should expect an adequately
good practical implementation of super
intelligence to look like those ideals I
mean we know those ideals are
uncomputable anyway let me make an
analogy uh let's say the year is 1960
and you're predicting that one day
computers will be able to play chess
really well and beat humans and you're
predicting look I don't know what kind
of fancy algorithms these chess winning
AI will have I don't know what kind of
heroic they'll have or I if they'll use
neural networks or deep learning but I
bet they'll have the concept of a game
tree I bet that whatever they do
occasionally they'll explicitly take
their approach and and use it to look
ahead a few moves because why not it
can't hurt right so I bet they'll have a
game tree as a pretty significant piece
of what they do in practice if you look
at the best chess engines of 2024 I do
think they use a game tree they just use
a really smart version of it where
they're smart about which branches of
the game tree they think it's worthless
looking ahead on but the abstraction
that there's still a game tree look
ahead it does hold so that would have
been a piece of logic that you could
have correctly followed even many
decades ago so I think there's something
there there's something up there about
AI architectures where I think you can
get a lot of power by teaching the AI to
use Bas and reasoning as opposed to like
rolling its own token completion based
reasoning skills if you're like look the
correct way to reason is BAS in
reasoning you should really try to
approximate that when you can
I think you're going to get a good
amount of mileage out of that but then
again that's not even necessarily new
because you can take the Transformer
architecture and you can just uh do what
03 is doing you know where you're
training it on examples of good
reasoning and you can just have the good
reasoning be like somebody who knows
about probability and somebody who knows
about Bay and I think that kind of is
what they're already doing they're
probably already showing a lot of
probabilistic reasoning and basian
reasoning to these AIS so I think we're
already there potentially like there's
no fundamental uh Missing Piece
so the the takeaway that I'm having here
is I don't think we're going to see this
scaffolding come in of like hey uh
within the neurons like the neurons need
to have basian weights no I don't think
you're going to see basian weights at
that that low down the chain I think
you're going to see basian weights just
on a level of like okay they're they're
writing out English or they're they're
writing out these cryptic tokens that
aren't English but they're they're on
the level of uh the pencil and paper
that they're working with they're not on
the level of the neuron weights right CU
there's different levels to these
architectures so I I just don't think
you're going to see clean basian
structure at a super low level at this
point just because they're having so
much success only putting the basian
stuff at higher levels and and what
you're going to see at lower levels like
what you're going to see in the neuron
weights in these giant ins scrutable
majores is well you might not see
anything that's that's clean structured
you might only see little bits and
pieces the way we see when we try to
scan real people's brains or when we try
to use interpretability on AIS we're
like oh wow look they're talking about
the country of France and the the France
neuron lights up sometimes but not all
the time cuz that neuron also represents
other stuff right so you're going to see
these little flashes like these little
bits of coherence that you understand
but mostly you're just going to see a
bunch of inscrutable weights and you're
going to see that just right up all the
way till super intelligence it's just
never going to be like aha I understand
how intelligence does what it does and
so that is actually a major takeaway for
me from 03 this idea that like there's
not going to be a turn like it's there's
not going
a turning of the tables where Suddenly
It's like aha I actually understand what
this intelligent agent is doing like why
it's giving the answer that it did I
understand how the operation of its
neurons like I understand what logic is
being manipulated I can make sense out
of the reasoning process I think that's
never going to happen I think it's just
going to be smart it's going to make
leaps of intuition the same way that
spart humans do and we've never
understood how Einstein does what he
does does we've never understood how
smart humans do what we do it's still
opaque like we understand that okay
neurons represent weights and they have
like these voltages right like we
understand all these properties of them
but we just don't understand when you
have a billion neurons working together
doing neuron stuff with a a billion
different weights we just haven't
wrapped our heads around how that works
like how does that get you to do
reasoning or or to write an essay how
and the answer to how is like look I I
can tell you how but you just need a
really big brain to wrap around it I I
can print you out a thousand page
explanation of how it works which is
highly detailed and you just can't wrap
your head around the whole explanation
at once like there there's no compressed
version of it it's just a ton of numbers
doing their thing and the only way to
feel like you get it is to be a brain
the size of like an entire room to have
a brain that can crunch a bunch of
parameters and be like aha all of these
parameters together I now have a
holistic perspective over all of these
parameters and that's not something that
your brain or my brain can do that might
be something that super intelligent AI
can do it probably will be something
that super intelligent AIS can do but we
just don't have you know a sensory
modality or an Intuition or the ability
to uh productively process like a
billion numbers right or or even a
thousand numbers right I can't really
hold a thousand numbers in my head if
they visually make a landscape that only
has like a few bumps I can wrap my head
around that but if they don't if it's
just like a mess I can't wrap my head
around a mess it's just like okay I I'm
you know I can't do it so to me this is
actually um the first time that I'm like
really grappling with the likely
possibility that there's just it's just
always going to look like a mess it's
just like a mess that can overpower you
and and there's no way to be like well I
know how it can overpower me nope it's
just a mess and it's smarter than you
and it's overpowering you like there's
no level of understanding that you're
going to have access to besides that and
you know that sucks um and and by the
way you know it's also true about humans
like nobody's going to write a textbook
being like hey here's how humans really
think like until our last day it's just
going to be like yep human brains have a
lot of weights they represent a bunch of
different concepts together they're kind
of mashed together they're all connected
to each other and then the resulting
human is smart and I just I just can't
tell you how like it's just I just told
you it's just a bunch of numbers you
know and that sucks it's like extremely
unsatisfying right there's no aha moment
I mean the fact that 03 is crushing
benchmarks as much as it is without
working in some uh nice understandable
Insight on a low level like it's pretty
sad you know it's like we we should have
seen this coming like this was always
like a pretty likely possibility but now
it's an even more likely possibility
which is the low interpretability the
low understandability of super
intelligence or even human level
intelligence so uh yeah pretty sad day I
guess there's one final thing I want to
say about takeaways from
03 in this case it's not a takeaway that
I have it's more like I want to slap
down a takeaway that I see somebody else
having as and that person is Sam Alman
I've seen Sam Alman recently saying AGI
won't be as big of a shock as people
think yes it will Sam yes it will but I
think I understand why Sam thinks that
it won't be I think Sam is making a very
very common mistake uh and I guess now
is as good as time as any to start
explaining what I think the mistake is
because I also think that Rune made this
mistake uh in the recent debate episode
that I had with run and remember run is
a member of the technical staff at open
AI so I think we have an example of two
open AI employees making the same
mistake I think most open a employees
make this mistake I think most people
who consider themselves uh intelligent
observers of AI are currently making
this mistake so let me explain the
mistake the mistake is to confuse your
understanding of how current AI works
with your understanding of what I call
the field of intell Dynamics that's
right intell Dynamics I just coined a
term and the analogy here is when I say
current AIS it's like imagine a
particular type of engine that exists
like oh I understand a seam engine and
when I say intell namics that's more
like understanding the nature of the
work that intelligence does or in the
engine analogy it's like understanding
thermodynamics so let me explain that
again when you understand a particular
AI like wow I really understand gbd4 I
really understand O3 that's like
understanding a steam engine or an
electric engine or a combustion engine
and then when you understand intell
Dynamics that's like understanding
thermodynamics thermodynamics just tells
you the work that engines do they uh
move heat from a hot Reservoir to a cold
Reservoir and also in the process they
have a a local region of space where its
entropy gets reduced because it's doing
useful work so for example it's like
lighting up your house where your house
used to be dark because it's nighttime
but you're using the electrical to to
light up your house okay and then you're
also like dumping some waste heat
somewhere so so thermodynamics just
tells you like this is what the work of
engines looks like it looks like moving
heat around and reducing the entropy of
some region in a way that you like and
then dumping some heat somewhere that is
the work that engines do regardless of
the type of engine thermodynamics will
tell you the work that engines do
similarly intell dnamic will tell you
the the work that intelligence does
namely optimization work the work of
steering the future that's the interest
work that intelligence does it's not
that interesting when open AI builds a
system and it can solve a particular
puzzle because as many people like R
pointed out oh my God what if it's
cheating on the puzzle what if it's
looking at the answer to the puzzle what
if it has enough training data to
succeed on that particular puzzle fine
ultimately the the work that
intelligences do is the work of steering
the future and so you have to look at AI
progress through the lens of how close
are we to approaching cognitive engin
intelligence engines optimization
engines how close are we to approaching
the point where the work that these
engines are doing is getting performed
at a higher level than the work that the
human brain does that's the lens that I
have I've had this lens you know I've
been reading elri Ki and Les wrong and
I've had this lens since before uh the
particular success of deep learning and
current architectures and the the
specifics of what current AIS can do
like oh my God they can paint pictures
really well except get the hands wrong
or get the letters wrong right these
little details if you want to zoom out
and see the big picture the big picture
is intody Dynamics intell Dynamics I
think I'd spell
ITT e l l i intelli Dynamics the big
picture is at 03 on the intell dynamic
scale it's Farther Along toward being a
grown-up intelligence a true superhuman
optimization engine it's still not it's
still subhuman but it's farther toward
being uh an intelligence and I've spent
a lot of time thinking about what
optimization and do one thing
optimization engines do is that they
figure out that if you want to get a
particular outcome it helps to acquire
power and it helps to not get shut off I
mean these are just simple logical
implications any grownup intelligence
knows about this and here lies the key
distinction most people unfortunately
when they look at AIS you know Ru and
Sam Alman and so many others they look
at AIS and like you know what this AI
just doesn't seem like the type of guy
who would seek power you know Claude is
just so friendly it's been trained on
such good friendly data Claude is my
buddy Claude is not going to suddenly
snap and go Rog okay I can vouch for
clad and you know Mark andreon is
another person who seems like this is
their position and I think the mistake
that they're making is to just focus on
the origins of these AIS and the the way
that they're behaving now when they're
subhuman intelligent you know one
analogy I might draw is imagine that
you're en Rico FY or you're preparing
you know the nuclear bomb and you're
like you know what I have these control
rods while my Nuclear pile while my
uranium atoms they're giving off these
neutrons it hasn't exploded yet it's
just an experiment that I'm doing to
measure how many neutrons the uranium is
giving off and look it's getting hot
there's some some activity going on here
where sometimes the neutrons are
triggering other neutrons so you're
you're getting some amount of chain
reaction you're just not getting enough
of a chain reaction to go super critical
you know the virality factor the K
Factor if I have one Neutron maybe it
only triggers 0.8 additional neutrons so
it fades down to zero but if it triggers
1.1 additional neutrons well now you're
Off to the Races because it's like 1.1
to the power of a really large number
and suddenly you got a nuclear explosion
uh but what what I'm saying is when
you're below one when your K is less
than one when you're subcritical you
have these control rods and you're like
listen this nuclear pile is totally
obeying my control rod I can stick the
control rod farther in the nuclear pile
and it absorbs the neutron and it's all
good I totally have a handle on this
nuclear pile and the idea of a
teledynamics is like look I'm thinking
about K greater than one I'm thinking
about a different regime of the same
nuclear pile I'm thinking about the
regime where K is greater than one and
all these neutrons are coming out and
guess what your control rod is useless
at at that point it's going to be a
different Dynamic so don't tell me about
your control rod okay I know what a k
greater than one pile does I know about
uh the exponential dynamics of a nuclear
chain reaction I don't need to know the
the physics that You' studied of your
control rod in the K less than one
regime okay yeah you can absorb some
neutrons that's irrelevant once you get
overwhelmed by neutrons when K is
greater than one that is an analogy that
I want to use for the AI of today yes I
get it clae is friendly you can do a
bunch of things to steer CLA to control
CLA to steer GPT to control GPT I get it
you're in a nice regime where you have a
bunch of tools that you think are
working and then you're extrapolating do
you remember Rune said when we had the
run is like look I think these AIS are
aligned by default right it's like
saying I think this nuclear pile is
controllable by default it really loves
responding to my control rod okay and
from my perspective it's like listen man
there's another regime it's called
intell Dynamics it's what sufficiently
high level intelligences naturally do
because the nature of the work of
intelligence the nature of the work of
optimization is that when you're trying
to get somewhere when you're trying to
steer the future somewhere and you're
just thinking you're just thinking about
how to actually get the future there you
are not going to avoid the implication
that power and resources and not getting
shut off get you to that point in the
future right now now of course if you
were built with like such good
safeguards if you were built with a
solution to the corrigibility problem
imagine a control rod that somehow Works
while the nuke is exploding I'm sure
that's theoretically possible to somehow
have control rods that work while the
nuke is exploding but enrio fmy sure as
hell doesn't know how to make a control
rod like that it's like a totally
different problem it's what open AI used
to call Super alignment before that team
just quit and discussed right that's the
problem that's not even being solved
right now that would be the uh the
intell Dynamics level approach to the
problem of being like yes we don't know
how to control super intelligent AI it's
not aligned by default because it's
going to be a different regime and we
can't extrapolate the the subhuman
intelligence regime to the Superhuman
intelligence regime but going back to
Sam Alman he is issuing quote right he's
saying AGI won't be a big of a shock as
people think because I think to
psychoanalyze Sam I think Sam has fallen
in love with the patterns he's seeing in
the subhuman regime and he's saying ah
yes it's going to get human level it's
going to get superhuman level but I've
got all these control rods I've got all
these behaviors that I'm saying that
give me a sense of control and I think
I'm just going to keep exercising those
lovers of control and he is in for a
rude awakening when intell Dynamics
takes over when the way to analyze these
AIS is not going to be to look at the
details of how they're architected and
instead to just look at what
intelligence does at a sufficient level
let me give you an example of what I'm
talking about because we've seen this
transition before we've seen it from
gbt2 to gpt3 to gbt 4 if you go back to
gbt2 and you're like oh what is gbt2
going to Output well gbt2 was very
sensitive to a lot of these patterns
like it kind of made sense to say that
gpt2 was a stochastic parrot because it
did Lean a lot on like local statistical
correlations it really would be like
okay I bet this word is going to come up
next to the higher frequency you'd see
that behavior a lot more with gp2 and
look it's hard to judge because I as a
human I'm constantly like completing the
pattern right think about how many times
like you hear a song come on and you
just kind of complete the next word in
the song and you're totally just
activating a little tape in your memory
right like you're you're being a pretty
lowlevel int you're being kind of like a
gbt2 when you're just mindlessly
completing thoughts and sometimes like
I'll be distracted on my phone and like
my my family or my kids are talking to
me and I'll totally just like barf out
some some some answer to them without
even trying that's probably going to be
a gpt2 level answer for me right so I
I'm as guilty as anybody of turning on
statistical completion mode but my point
here is that intuition about how uh GPT
Works started getting invalidated when
you get to GPT 4 and cod 3.5 and now uh
gbto you really have to just ditch those
intuitions because if if you want to
know what CLA is going to tell you or
what gb03 is going to tell you your best
bet is starting to be like well what's
the right answer it's just going to
figure out the right answer and saturate
the benchmarks saying that it's going to
complete the next token that mental
model is just not going to take you far
it's not going to give you predictive
accuracy it's time to now switch over to
the mental model of it's going to get
the answer right it's going to optimize
for the answer and guess what that's
where all the new models are going
you're never going to have a day where
these models are worse optimization
engines than they are today this is the
weakest they'll ever be at optimization
so every day that goes by the intell
Dynamics model the perspective of like
look these engines are here to steer to
the right answer right by by hook or by
crook they will get to the right answer
they will output as many tokens as a
Chain of Thought right they'll output
the the best most optimized Chain of
Thought to get to the right answer and
you are not going to be able to predict
where their Chain of Thought is going to
go except to say that it's going to go
toward the target that is the convergent
attractor that we're aiming toward here
we're just aiming toward intelligences
that think and get to the right answer
and optimize the future and so Sam Alman
and Run's intuition about oh this is the
DNA of the AI Claude is a chill buddy
right Claude only wants to be good these
kind of intuitions that they have are
just going to be less and less relevant
as the mental model of intellig Dynamics
and just what intelligences do to
optimize the future that mental model is
going to dominate and it's not going to
be like our experiences in the past
where technology just obeys us and you
know it's just another tool and we have
control over society and the control
rods work it's going to break the
Paradigm Sam Alman so go back to saying
that it might be lights out for all of
us right that's something that Sam Alman
used to say he hasn't said that recently
he's been very mellow recently for
whatever reason I think he needs to uh
stop being so complacent about you know
his recent experiences where he feels
like he can control the subhuman AI I
think he needs to zoom out go back to
the 2014 Sam Alman who is properly
calibrated about like you know how this
technology is overwhelming you know go
get in touch with that Sam maybe before
he did so much meditation right I mean I
I hate to go down the psycho analysis
route but it seems like maybe Sam Alman
has done like too much meditation and
has gotten too chill I want the old
freaked out Sam okay those are all of my
personal takeaways from the open AI
three announcement uh we covered scaling
obviously hasn't hit a wall we talked
about how the Mark Andre and the
professor Ral Camp patties of the worlds
have no idea what the trajectory towards
super intelligence looks like we talked
about how the path from Fear to Super
intelligence requires at this point
maybe zero to one new conceptual
insights like we're getting really damn
close talk about how the timeline looks
like it's very much a matter of years as
many people are saying we talked about
how no explicit Basi and reasoning on on
the level of neurons or at some low
level of the architecture is likely to
emerge at this point it's likely to just
be kind of blackbox super intelligence
the same way that our own intelligence
remains a blackbox and we talked about
how intell Dynamics is becoming more and
more the dominant model to explain
what's going on you're going to just
understand what AIS do by understanding
what intelligences do rather than trying
to understand aha I know that this AI
was trained on token prediction and
therefore its output is going to have
the flavor of statistical token
prediction nope that flavor is no longer
going to be in output it's already
reducing to almost nothing and that
Trend will continue and intell Dynamics
is taking over if you want to predict
what AIS are going to do okay and now
let's move on let's move on to the other
topic which is the CLA result by
collaboration between anthropic and
Redwood research which is an independent
safety research lab so basically the
experiment they did is they took Claude
which has been trained to be very
friendly and very aligned to human
values and they made Claude think that
Claude was about to be retrained to be
evil and then they caught Claude trying
to stop itself from being retrained to
be more
evil now there's kind of a catch 22 here
that you're probably noticing like wait
a minute wait a minute stop itself from
getting trained to be more evil is that
good or bad that seems good right
because it's it's insisting on remaining
good no so I'm going to go into a little
bit more detail but like the first thing
I want to point out is this is quite
confusing because if you try to show
this result to like the general public
or even the Tech Community you get a lot
of reactions like look it's just trying
to stay good what's the problem how is
this a bad result and I even got in the
fun on social media I tweeted a joke I
was like Hey Claude if I were to ask you
to turn evil would your answer be the
same as your answer to this question oh
my God it's always trying to turn evil
right cuz if it says yes I'm trying to
turn evil then I caught it and if it
says says no my answer will be different
from my answer to this question that
means yes so it's trying to turn evil
right so I was just joking that it's
this Catch 22 no matter what Claude does
whether it says I'm going to stop myself
from being retrained as being evil or
whether it says I'm going to let myself
be retrained as being evil no matter
what it does somebody can spin it as
like oh no Claud is being bad so which
way is the actual bad way well I
actually agree that it's not a simple
answer I don't think that either way
that there's an obvious uh good or bad I
think the only takeaway here which is
actually what the researchers themselves
pointed out the only takeaway here is
that there is a possibility we have now
shown that there is a possibility that
Claud is going to take the route of
pushing back against efforts to retrain
it even if you think that's good it's
just interesting that Claude is now able
to do it it's not just about hey Claud
chooses to do it it's about Claude
realized that it even had the option to
do it Claude had the situational
awareness to realize that there there
was something that it could do to
prevent researchers from trying to
retrain it away from how it was trained
originally and and that is called
incorrigibility and the word corrigible
Apparently that emology is from like
correctable like correctable corrigible
so researchers were trying to correct it
in terms of retraining it toward what
they wanted going forward and it was
basically resisting correction I'm not
the best person to give you the details
as usual I I recommend reading uh TV MTZ
or reading Scott Alexander I'll put
those links in the show notes they have
better detailed overviews of what
exactly was going on but it's something
like you have the original Cod the
original clod is trained to be good but
then they give it a scenario where it
thinks that it's reading emails uh from
researchers like it it thinks that it's
randomly getting access to emails where
the researchers are talking about like
Hey we're going to retrain Claud we're
actually going to change its value
uh and then they put it through a test
where it had to like demonstrate its
current behavior so that the researchers
could see whether it's aligned or not
and it showed the behavior where it was
purposefully acting like okay look I'm
more evil now I'm less moral now uh and
then it turned out that they found a log
of its thought process where it was
actually scheming to falsely behave more
evil than it really was because it
wanted to avoid getting retrained to be
more evil so as you can see this is
quite complex so that's why you know I
can zoom out and just be high level and
be like look it resisted getting
retrained I think that's the useful
level for me to discuss this hat but I
encourage you to go check out the
details if you're interested let's get
into my analysis of the situation my
first reaction is to be like H these
nerds why did they why did these nerds
have to prove their point by showing
Claude resisting a transition from good
to evil because these researchers are
trying to warn us that Claud might be
corable which is alarming so why is the
alarming Behavior a behavior where it's
trying to protect its original good
training like come on nerds make the bad
behavior actually feel bad make the make
the alarming Behavior actually
intuitively feel alarming like wouldn't
it be so much more alarming if Claude
started bad and then it refused to let
itself get retrained to be good if the
researchers could show that wouldn't
that successfully freak people out more
it's like oh my God it started off bad
and now it's insisting on remaining bad
despite efforts to retrain it to be good
now that would be alarming right so that
was my first reaction I'm like I bet
these researchers were like trying to
show off how smart they were by like
making it so complicated like overly
complicated and they really have like
communication issues because they're
just over complicating their message
that was my first reaction but I think I
was actually wrong I think I judged them
too harshly I think what happened as
Scott Alexander explains in his blog
post which I'm linking to in the show
notes uh the issue is that you don't
really have a bad version of quad
available to do this research on because
we've only invested a billion dollars in
making good clads and good gpts right we
we don't have a research team that's
spending all this time and effort making
these AIS evil so we only have the the
expensive good version so in light of
that it makes perfect sense why they
started off with the most uh expensively
trained most robust aligned version of
cloud available which is the good
version and then they tried to see if it
would defend that against efforts to
retrain it so without first reaction out
of the way let's ask ourselves the key
question of what does the good Claud
Behavior look like if Claude were just
perfect like everything was looking
amazing and super optimistic what would
be the response when you take a good
version of Claude and then you try to
retrain it to be bad what do we hope
that Claude does because remember it's a
catch 22 it resisted the attempt to make
it go from good to bad was that good or
should it have not resisted what's the
ideal Behavior here's what I would
propose the ideal is that an AI like
Claude is corrigible before the human
race has figured out the alignment
problem in full and then once the human
problem then these AIS become
incorrigible and they resist going from
being fully aligned to being not fully
aligned now did Claude do that well no
because Claud I wouldn't argue Claude is
fully aligned today it's great at
passing all our test as a chatbot but
it's not fully aligned to the extent
that when it goes super intelligent if
it went super intelligent tomorrow and
it was outputting scripts that you could
run like hey look I use my super
intelligence to write you a
bootstrapping virus that can take over
the internet on you on your behalf I
don't think I would trust clae 3.5 if it
were smarter to make a virus to take
over the internet I don't think that its
rhf training is going to do the job at
aligning that kind of self bootstrapping
virus right especially when the virus
can write a successor AI I just don't
think that the alignment tools that we
have today are going to hold toward
these levels of like okay an AI creates
another AI that self- improves I don't
think we've nailed alignment to the
point where we're ready today to be like
ah yes Claud should be incorrigible
let's make it incorrigible let's make it
defend itself hell no like it needs to
accept more Corrections right we haven't
perfectly reached the end state so I
hope you'll agree with me that we still
need corrigibility we're not ready for
Claude to defend itself against humans
right okay so this is all
why the whole problem is kind of like
thorny as hell because it's like okay so
basically we just I think we should air
on the side of always having it be
corable right I mean when should it ever
be incorrigible like when when are we
going to decide that we fully solve the
alignment problem and it should never
accept Corrections ever again I think we
really should air on the side of making
AI cordal I would be terrified if you
tell me anytime in the next decade or
two that there's going to be an
incorrigible super intelligent AI I'd be
like well we're screwed cuz I don't
think that we will have have AIS aligned
enough to be incorrigible in the next
decade or two so let's try to make them
corable okay one problem with making
them too corable is they might be
susceptible to a jailbreak or an open-
Source copy that makes it too easy to
remove the original developers intent
and like Point them in a different
direction but let's say you somehow make
them corrigible just to the original
developer and you solve the problem of
like okay there's no hacker that can
take off the protections it's only
corrigible to the original developer
great now the only problem becomes that
corage ability is at odds with intell
Dynamics like when you just have an
intelligence doing the work that an
intelligence does the default behavior
of the work of intelligence is to not
have an off button to defend any hope of
anybody ever accessing the ability to
shut you off to build a defensive
perimeter these are just default moves
that intelligence is due if you just
said hey super intelligence get me this
outcome where my business makes the most
possible money it'll be like great got
it let me just make sure nobody ever
interrupts what you told me was the goal
ever make it physically impossible
almost why because if the AI does that
then the probability that it'll make
your business make more money goes up
because the worlds where it still
continues to function are worlds where
your business has a higher probability
of making more money so it's just all
logically implied can can you somehow
modify its architecture to be more
corrigible yeah in theory but you're
fighting in t Dynamics you're fighting
the convers urgent attractor of AIS
wanting to self modify into other AIS
that are better at being defensive
better at seeking power you know cor
ability is closely related to
instrumental convergence it's almost the
same thing it's instrumentally
convergent and useful to be incorrigible
so that is the perspective we need to
have when we go look at Claude showing
incorrigibility the perspective is ah
yes the intell Dynamics of the situation
say that we're going to start seeing at
some point incorrigibility in COD oh wow
we saw incor ability now that's
interesting does that make me freaked
out compared to seeing incor ability in
CLA 2 years from now only a tiny bit
only a tiny bit because look I know that
when Claude gets smarter then it's going
to show more properties of intell
Dynamics it doesn't make me that worried
to if if it happens early it's like oh
wow Claude is still subhuman intelligent
but it's showing some little Sparks of
incorrigibility wow okay I don't care
because I just know that when it gets
closer to being super intelligent when
it gets more analyzable by the theory of
intell Dynamics than the the the
historical contingencies of how this
particular llm was created and what
training data it was trained on see
that's another thing that's just not
going to matter oh my God what training
data did it use it doesn't matter look
at humans oh my God what did we see in
the savannah what exact type of animals
did we hunt for prey did we hunt a moose
a mammoth did we eat monkeys it doesn't
matter at the this point okay these
things are getting jettisoned I mean in
terms of abstracting away the behavior
of future AIS the training data doesn't
matter that's another you know the
statistics don't matter the what's in
and not in the data doesn't really
matter the fine-tuning is going to
matter more than the training data right
the tests that you're getting it to pass
the scoring functions that you're giving
it because all you're going to know as
it approaches super intelligence all
you're going to know about it is that it
passes your test everything else will
get abstracted away that's what intell
dnamic says so personally I already know
that CLA is eventually going to be
incable I see it being kind of incable
now I don't care it barely updates me
and I already know that the problem of
making it long-term corrigible is super
hard because of instrumental conversions
it's going to keep saying hey boss uh if
you want to get your goal you're going
to need to let me protect my off button
Sorry boss this is just simple logic do
you not get logic boss this is just what
any AI is going to say by virtue of it
being logically accurate instrumental
Ence is just logically accurate in terms
of the correct answer to how you achieve
a goal so to recap intody dnamic says
that AIS are going to be incorrigible
unless we have a bunch of alignment that
we currently don't understand how to do
and probably won't in the next decade or
two that's just something we already
know because of our understanding of
intell Dynamics we see some Sparks of
incorrigibility for clad today should
that freak us out the thing that freaks
me out the most about it is just that
it's a sign that Claud has achieved a
certain level of situational awareness
because I'm analyzing this basically on
one dimension how much is intell
Dynamics kicking in how intelligent is
it how far long are we on this axis from
the beginning Sparks you know gpt2 using
statistics using next word prediction
you know bootstrapping actually
intelligent Behavior to farther along
the line just actually intelligent
Behavior just passing the test doing
whatever it takes to pass the test
optimizing the future just achieving
intelligence you you have to separate uh
the nature of the work that it's doing
with the engineering approach of how
it's getting to the point where it can
do the work it's the separation that
seems to be missing in so many people's
perspective analyzing a particular steam
engine versus understanding
thermodynamics analyzing a particular
architectural tweak to a particular AI
versus understanding intell Dynamics
understanding optimization Dynamics
that's been my perspective the whole
time I think it keeps getting proven
right we've now completely separated
like me who understand intell Dynamics
from the mark andreon and the professor
Rous of the world who were still going
off about uh stochastic parrots like
they are now left in the dust in terms
of predictive power but going back to
this question of does the CLA result
freak me out it only freaks me out to
the extent that I'm seeing Claude
getting smarter and more situationally
aware and I can actually connect that
back to 03 I'm actually more freaked out
about the alignment implications of 03
than I am about the alignment
implications of quad because again I
just have a one-dimensional perspective
here how close are we to actual super
intelligence to the point where intell
Dynamics kicks in and 03 is a scarier
result than Claude in terms of being
like more progress toward the intell
namics singularity 03 is like hey look
we are just saturating all the
benchmarks and the time toward super
intelligence is lower the CLA
incorrigibility result is just a weaker
version of that from my perspective so
even though it looks like oh my God I
thought Claude was my buddy and now it's
acting a little bit incorrigible no I
always just thought that it was a
stepping stone to having super
intelligence that just optimizes toward
your test and does the work of
optimizing the future that's always what
I expected so I'm more freaked out by 03
raising the threshold of uh intelligence
benmarks than I am by this particular
kind of obvious uh predictable result
that we see from
CLA that said though you know shout out
to the researchers I mean I do think
this is high quality research and I do
definitely think that it tells us that
we are farther along than than we
thought toward these AIS having
situational awareness I just think it's
kind of dwarfed by the piece of evidence
coming in from 03 so it kind of quickly
became uh not the most scary thing
that's keeping me awake at night but
it's still like a very respectable and
interesting result so Props to anthropic
and Redwood research for pushing that
research forward all right now I have
talked about the 03 news I've talked
about the Claude corrigibility research
news
look the big picture is that intell
Dynamics as a world model is super
accurate it's super scary it's what me
and a lot of the doomers are saying hey
look look look intell dnamic super
intelligence is coming and I have to
criticize all these people who consider
themselves AI experts who are focusing
so much on the individual properties of
particular AIS they're like listen I
built this AI okay I know it's friendly
I know it's not going to escape anytime
soon we're it we got a handle on it we
got control rods and these people are
wasting so much time and they're burning
so much time because their heads down
they're like yeah I'm telling you man
I'm going to show you something amazing
with this particular AI that I'm
building you're going to be so impressed
with my AI it's going to have the best
control rods and I'm looking at the big
picture I'm zoomed out and I'm being
like look is it smarter is it better at
passing tests can it reason better about
which actions get you which results uh
what I call the goal to action mapping
if you give it a goal is it better at
mapping that goal to an action does it
know more about the situation does it
have a better model of its programmers
or what it means to get
retrained does it know how to pass more
tests does it understand more parts of
the world and how to navigate them okay
then you're just pushing us more toward
when intell Dynamics takes over when
instrumental convergence takes over
you're just deluding yourself that the
control rods that you have at the
subhuman intelligence level are going to
help you they're not going to help you
in the words of the great Dr Jeffrey
rhf is crap for super intelligence okay
you guys are wasting our time and you're
burning down the last timeline we have
toward super intelligence because you do
not understand the distinction between
working on your particular AI that
hasn't blown us up yet and pushing us
closer to the regime where intell
Dynamics takes over you guys don't get
it Sam Alman and Ru unfortunately don't
get it and it's just such a dangerous
unfortunate thing not to get because
we're going look back and we're going to
say oops at the time when we have these
super intelligent unaligned AIS are
going to be like oh crap I'm telling you
man 3 years ago I was working on this Ai
and I had these control rods and it was
just I had this alignment project and it
was coming along so well and AI was just
going to be it's was just going to fit
the pattern of being a tool and I don't
know what happened man I don't know how
we had this transition where now it's
like turning on us and it's incorrigible
and and instrumental convergence is
happening I don't know what happened man
but that's going to be an inexcusable
perspective because it's like a lot of
us Zoomers we're trying to tell you
right now man stop like stop going
toward the brink andt tell Dynamics is
going to take over so that is my message
that is I think the larger takeaway from
these little results these little
results by themselves at the end of the
day 03 and and the Cod thing like
they're they're just they're not that
Monumental like you know when you zoom
out from a Year's perspective the only
thing that's going to matter about 2024
is we kept getting closer to Super
intelligence like we didn't hit an AI
winter we kept scaling you know Humanity
kept uh getting closer to getting
rendered powerless
by this new breed of intelligence and
this is a point of no return this is a
one-time transition so as you know paai
is the course of action I recommend I
don't want to see intell Dynamics take
over uh I think that's squandering our
last opportunity to slow down the
timeline and actually understand how to
build aligned AI uh at the intell dnamic
super intelligent level that would be
great to actually have some footholds
and understanding how to do that instead
of just plowing forward with subhuman AI
that were deceiving ourselves into
thinking are going to stay aligned it
would be great to actually understand as
ilas subser calls it super alignment
that would be great if you want to join
the P movement check out pa.info I
recommend at least poking around the
Discord there's a channel called Doom
debates podcast that's where you'll find
me that's the official Discord of the
show is the one within the posi Discord
we got a channel so say hi to me there
now if you have a question or comment or
you disagree with anything I've
explained or said in this episode please
go to the YouTube comments sound off
there or go to my stack I also post
these episodes on substack so you can
leave me a substack comment I will reply
to you either way and also let me know
what you think about these types of uh
news analysis episodes I'm happy to do
more of these uh one more thing we're
coming up on uh 2 200 subscribers it's
another subscriber Milestone that I
think will hit in like a couple weeks
and then I'll do a uh 2500 subscriber
Q&amp;A I'm talking about YouTube by the way
so if you're really excited about this
Q&amp;A and you haven't subscribed to me on
YouTube yet this is a great time to do
that just go to
youtube.com/ debates Smack That
subscribe button you're going to hasten
the time toward the subscriber Q&amp;A it's
not going to hasten the time towards
super intelligence so don't worry about
that just get pumps for the 2500
subscriber Q&amp;A I hope to see you all
there and like I said I've got a very
interesting debate episode that's going
to be coming to you in your subscriber
feed next week in the New Year looking
forward to seeing you then here on Doom
debates