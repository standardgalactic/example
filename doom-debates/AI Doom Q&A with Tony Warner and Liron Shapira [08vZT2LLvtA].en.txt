hey everybody Welcome to Doom debates
today we've got a Q&amp;A episode live Q&amp;A
with my special guest Tony Warner he DMD
me on Twitter he wanted to ask a few
questions and I'm cool with that so if
anybody wants to come ask some questions
or argue with me this is an open Forum
you just have to DM me on Twitter or
email me Wiseguy gmail.com so Tony tell
us a little bit about your s and what's
your first question yeah uh thanks for
having this conversation first of all um
yeah my my background is psychology and
computer software stuff um so I've been
interested in the space for quite some
time I think I maybe similar to Lon um I
was learning about neuron Nets
artificial neuron Nets you know 10 years
ago um so yeah some of the questions if
we want to jump right into to it um
let's do it and I think to you know to
to set some context uh you know again my
goal is to just ask interesting
questions that's like I I I hope to
learn a lot from this um great so my
first question is what you think about
the importance of biological imperatives
when it comes to machine learning so you
can think of in the human body things
like homeost
basis uh we might have a feeling of
hunger and that motivates us to find
food um do you do you think about those
biological imperatives as it relates to
machine
learning yeah and is what you're getting
at the pretty common argument people
make of saying like the AI that we build
is not going to have evolved it's not
going to have evolutionary pressure to
survive and so it's just going to be
more chill and less uh competitive the
kind of biological brains we see is that
basically where you're going with this I
think that's an interesting point
um I I would say that's that is a
section of what I'm suggesting but a
section relating to the behavior of an
AI because it doesn't have those
biological imperatives is what you just
described right so here's how I would
compare and contrast biological
evolution versus the training that we
are doing toward our AIS biological
evolution yes everything had a fitness
criteria and it had a loss function or
it was doing gradient descent with the
Criterion of what is the combination of
genes that lets me reproduce my genes
more than other mates in my species
reproduce their genes that's basically
inclusive genetic fitness it's just like
how do I increase the gene frequency in
the species any Gene that modifies my
brain such that my brain and the
organism as a whole is going to
reproduce it more that Gene's frequency
will increase and you run that for like
a million generations and eventually you
get a brain that's good at thinking and
then surviving right so that's that's
biological evolution in a nutshell I
think there's a close parallel in the
sense of that feedback loop of like you
pick a gene and you try it and then you
either keep it or you mutate it and try
the next one essentially there is a
pretty deep parallel between that and
what's happening inside of our AI
training that we're doing right now
which is like okay pick like a random
setting of all your neurons try to
predict the next word if that doesn't
work try tweaking the setting when it
does work keep the setting right or or
follow the gradient toward more of
settings like that uh you know High
dimensional space whatever I mean it
obviously gets uh tricky but I I do
think that there's a parallel of like
okay we're evolving a system toward a
Criterion now in the case of evolution
yes it has to do with survival and it
has to do with keeping the organism
alive and competing for resources and
there's not quite as much of that
competition as you say when you're
evolving in AI to just predict the next
word But ultimately I don't think it
matters very much because in both cases
you're trying to solve a problem I'm
trying to solve how do I survive I'm
trying to solve how do I predict the
next word and the solution to any
problem once you start getting good
enough at just solving problems you
realize that problem solving has a lot
of common structure being able to have
goals helps you solve problems that's
why Evolution hit on a human brain
architecture that helps you solve goals
it's not particularly because the goal
that we're solving is survival if the
goal that we're solving was just
orbiting the moon you still want to have
goal you still want to be able to think
and it's the same way with AI they're
starting to discover more and more a
general architecture to solve goals and
yes they won't come fully baked with the
goal of like let me survive let me let
me seek power let me kill everybody you
won't have that fully baked but you'll
be one small step away from having that
because you can basically give it that
as input be like hey why is AI that just
evolved to solve problems in general how
do I make more money and be like oh you
want to make more money huh let me tell
you a very masterful plan to make more
money and that plan will convert this is
where instrumental convergence comes in
instrumental convergence is not a
property of how the AI evolved it's a
property of what the AI will recommend
once it's trying to solve a problem for
you
so and do you think that those
motivations are purely on the layer of
maybe intelligence or problem
solving yeah so instrumental convergence
like I said it's not a property of a
particular Ai and its background and its
desires it's a property of goals like
purely a mathematical property of how
goals relate to other goals so like if I
want to go eat some chocolate it's just
true about that goal that when getting
my body closer to the grocery store is
like a rational sub goal right or a
plausible sub goal and similarly making
more money is a helpful sub goal to
eating some chocolate right it might not
be the only approach but in general if I
want to eat a lot of chocolate or I want
to eat chocolate with very very high
probability money is definitely
something that helps not just for
chocolate but for all kinds of different
goals right so that so instrumental we
we're not even talking about like what
type of AI I am as long as I'm an AI
That's playing this game this video game
of making things happen in the universe
then I'm going to figure out that
seizing power and wiping away humans who
can meddle with me all of those things
are just going to be implied by the
problem it's
solving okay and do you think that from
the standpoint of evolution
are the problems that humans are
concerned with are they more narrow or
are they broader than the problems that
an AI would be concerned with or could
be concerned with if we prompted
it wow that's a good question um so yeah
the broadness of the domain or broadness
of the problem we're solving in humans
the way I see it when you make the
domain broad past a certain threshold
then you start having to have general
intelligence to solve it so for example
like if I just want to play chess you
can argue I'm just a narrow intelligence
I can't suddenly model the whole world
just because I got really really good at
chess you could imagine that the best
chess playing algorithm is still a nrow
intelligence even if we had something
that's better than today's stockfish
like the ultimate chess playing
algorithm still just a narrow
intelligence still can't take over the
world why because the domain of chest
just hasn't gotten quite past that
threshold of being what I would call an
AI complete domain a domain where
there's just enough complexity to it
enough different things going on that by
the time you can play video games inside
that domain you can also conquer the
physical universe that we live in
because the physical Universe yeah it's
bigger but it's just like it's past the
threshold and I even make the analogy to
Turing completeness right so there's
some video games if if you're just
playing Tetris it's probably not quite
turning complete you really it's just
blocks following you can't really model
anything else but once you get to
something a little bit more complicated
like Minecraft probably something in the
middle of that somewhere between Tetris
and Minecraft it's like oh wow okay you
can just build a computer inside of this
game world and so what whatever it means
to be able to do anything in the game
you probably also just know how to do
anything in any game you know you get
you get sufficiently General um now you
asked specifically about humans right so
like if you look at organisms in general
I do think that just the idea of trying
to survive as long as you have like
enough actuators and in the case of
humans you know we had opposable thumbs
and we could walk around and we could
like throw things uh we could start
building tools I do think that a lot of
animals are getting close to this
threshold where if you're just good at
being that animal right if you're good
at being the brain of that animal and
controlling what that animal does in
every context I think that a lot of
animals are past the threshold but I do
also think that it is very significant
what I just mentioned before that humans
have hands right where we can like stand
up and throw something we can stand up
and grab something we could manipulate
things I actually do think that there's
this kind of co-evolution of our brains
got smarter but also we kept getting
selected for organs that could use the
intelligence right because if you're
just a whale you can have like a really
great brain but just the amount of input
outputs you can do with your your
actuators is just it's really going to
be hard to bootstrap just like basic uh
manipul like symbol a whale equivalent
of symbol manipulation like where do you
get a symbol in a whales environment
whereas a human you know you get a you
get a lot of discret units that you can
manipulate a whale it's like okay you've
got other whales but like you know you
can't grab stuff so I think this this
gets to what you're asking right I think
yeah it it sort of brings up a new
question that I think is really
interesting which is what is the set of
actuators or outputs from a machine
learning model and then which of those I
guess so because I I do know that you
are um a part of the team that is saying
you know beware of AI uh which of the
actuators or the outputs of these
models are the ones that are most
concerning uh so right now we know you
know you can input some set of data and
we get out some set of data but it's not
like you said it's not you know hands
it's not feet it's not um the ability to
go to grocery store and stuff like that
do you see uh one of the risks Fact one
of the risk factors as being as
we uh invent new actuators for the mind
that is the model are these new
actuators going to be a a bigger concern
or is it simply the the outputs that we
have currently are the outputs that are
uh a big concern for the future of
humanity I think that even right now an
AI That's intelligent running inside of
a server is already at the human level
at the level of human actuators not at
the level of whale actuators very simple
the analogy goes between the virtual
brain and the human brain the human
brain sends electrical signals down to a
human hand which can grab things and
move them around in complex ways okay
well the server if it's connected to the
internet can send electrical signals to
humans who it's employing or whatever
and then those humans can go and do
complex actuations right you can give an
order to a human and you can causally
modify in very complex ways things in
the world very rapidly there's no
analogy between what an AI can do by
chatting to a bunch of humans on a
server there's no analogy between that
and something a whale can do like the
whale is really trapped in that whale
brain the AI is not trapped in that AI
brain H okay um I uh a couple questions
I I have about uh I have several more
questions but a few things relating to
what you were just
saying one of my questions around the
sort of goals that an AI or a model
might have do these
goals do they emerge from the you know
free parameters of the
model um is that how you see CU I think
at the beginning of the conversation you
were talking about human evolution tion
and these specific environmental forces
that uh that essentially yield a
different gene expression across the
population and would you compare let's
say these programmers that are setting
these hyperparameters on these models
would you compare these hyper parameters
as the base environment and then the
free parameters that the model develops
through back back
propagation is is that where the
emergence of those goals happens or is
it more explicit than
that I don't think I would analyze it
like that where you're saying maybe the
the parameters that are fixed are the
environment I wouldn't make that analogy
uh I think the environment really is
like the external environment that it's
interacting with right not part of its
its brain right not like the fixed parts
of it I wouldn't call that the
environment you know the kind of models
that I think are productive to work with
um but you're basically asking okay well
at what point in the ai's evolution does
the goal orientedness enter is that
basically what you're
asking yes because I am
curious
um whether or
not the goal
emergence is like localized like hyper
localized or whether it is okay
yeah so I mean the thing about goals is
I I'm it's a direct quote from Alazar
Yow I thought it was pretty profound
very simple it's just having goals helps
you solve problems right so anything
that you want the AI to do like go to
the Moon is a good example how do you go
to the Moon without having goals
H yeah well here's an interesting thing
and I think this is uh you know
potentially uh a tangent there
um there is some system
map and whatever the features of that
map
are I think we can talk about I think a
lot of it is electrical but there is
some systemic map and then that map
determines a lot of the features that
develop within that system so for
example there
are you know let's say birds
migrate and we could say that the need
to fly south is a goal that developed
for those birds or we could say there's
actually an underlying map and that map
almost like a
field uh it it allows for the generation
of certain features within the
environment such that the the bird
wasn't actually self-motivated it was
simply the result of the shape of that
environmental
map uh let's just be a little bit more
specific so we're talking about like a
scenario where like the bird goes and
finds a location that it remembered or
be more specific well uh so where this
comes from and I really want you to
interview this guy this guy's the best
uh I think his name is Michael Levan do
you know this
guy sounds familiar but I don't think so
okay he he talks a lot about
bioelectricity and essentially um
all of his research points to this idea
that genes they play a specific role but
it's not as big of a role as we once
thought and he basically he has a
concept I know I've written it down
before but it's
essentially
uh like a a scale competence hierarchy
thing where at these different scales of
an environment or a system from cellular
to species to ecosystem
at these different scales there are
these competencies and essentially the
upper limit of those
layers they are really important at
driving what those individual elements
in that layer do so an example is like
cutting off the arm of a noot and then
the arm regrows and it looks exactly
like an
arm it's not that each of those cells
knows what an arm looks like and this is
the thing that it's maybe when you first
hear that maybe it sounds wrong but he
he has great research on
this sure I think that it's true that
like the cells don't necessarily have a
top down model of what an arm looks like
right all they need to do is find their
own function and as long as every s can
find their own function then you get the
arm and it's possible that nothing ever
represented that an arm would exist
right like it's there as you say there's
no Central map that's a pretty
inefficient way to engineer things right
so like as humans we would never
engineer something without writing down
a map somewhere and probably the AI that
we evolve is also going to use maps
right I mean there's evidence that when
you ask the chat GPT the llms questions
there's evidence that you know you can
ask them hey draw out your map right and
presumably what they're drawing on the
Fly is already stored somewhere in some
fashion um or you know encoded somehow
you know compactly
um the reason why you see cells that
nobody ever wrote down the map is just
because evolution is a ridiculous
designer right the idea of Designing
with no Central planner where you're
just mutating and then you're throwing
out every design to run itself against
the physical universe and see if it
reproduces it's just such a dumb
ridiculous way to do engineering and
that's why you get you get mistakes like
the retina or the optic nerve is
installed backwards in the human eye and
it never gets fixed right because the
engineer is is a clown right so of
course yeah when you're a clown you
don't bother drawing a map you just
clown around that's what evolution
does
okay yeah and I I
think the point and maybe there's
there's a Nuance here or maybe I'm just
not completing the the loop but you know
having a sense of the difference between
an
engineered intelligence or life form or
system really
and
a
stochastic life form or intelligence or
system I think if if there is a
difference there then that might be
something that will lead
to a sort of a local Maxima for
engineered systems as as far as yeah
there you know this was like a pretty
popular Paradigm I think like a few
decades ago I forgot who the person most
associated with it was but it was
basically a person who's like look I
just build these robots and the robots
just have a lot of like stimulus
response right without like a central
map as you say and you know in humans I
my understanding is you touch something
very hot you pull away your
Consciousness in your brain don't even
get notified about what's happening
because you need the signal to propagate
faster just going to your spinal cord
and back and yeah so it's a great
optimization right so there's there's no
doubt that when you want to optimize
something you strip away uh processing
right it's it's an Asic compared to a
CPU right you want to mine Bitcoin strip
away the full power of the CPU just use
a specialized chip there's no doubt that
that's an optimization for special cases
and also we know that when you're just
Evolution when you're not smart enough
to do real engineering and you're just
stumbling around you're going to stumble
on designs like that but designs like
that don't get the job done when you
want to go to the moon right there are
some projects where you really got to
buckle down and just use a general
approach okay so then I wonder for
humans is going to the Moon a triviality
or is it something that is innate is it
something that was always going to
happen or is it just a creative
act yeah the idea of going to the moon
for me is such a perfect case of humans
that they wrote down a goal that was
quite arbitrary like okay we were in the
cold war with the Soviets we wanted to
just prove that they better be afraid of
our technical capabilities we have to
one up Sputnik right so it was like
these random circumstances led us to
choose this goal that's quite arbitrary
but also happens to be inspiring but
nowhere in the history of human
evolution were we getting prepared for
anything like that specific goal like
yeah we were getting prepared to travel
to ride in vehicles sure to ride on
animals but we were not getting prepared
to go you know do orbital mechanics and
rocketry to get to the moon right so
that was very clearly something where we
used general intelligence to work
backwards from an arbitrary outcome in a
way that maps that outcome to many
possible states in the present like the
present could have been many different
ways
and the reason why we're intelligent
it's actually the definition of
intelligence when you can map a bunch of
different present States and you can
squeeze them all into the future state
that you want so he squeezed an
arbitrary present State into this one
very particular future state of human
footprints on the moon
H and
then as it relates to machine
learning are those types of
creative or playful or highlevel
goals are
those are those ever going to be a
part of if there was a self-contained
intelligence you know is is is the
self-contained
intelligence without human inputs is it
is it going to develop those kinds of
uh let's say like free agent
goals I don't think it's going to
develop goals out of nothing I think
that it's an attractor state to have a
goal right like the moment you're being
trained like solve this problem solve
this problem you will eventually hit on
the idea of let's have goals right like
the goals are super helpful if you
forget what your goal is if you insist
on only using stimulus response right
like like Skinner thought that
everything could be done with with
stimulus response in like the '90s right
uh 50s or '90s I think was roughly the
Heyday of that um if you're committed to
that architecture you're just not going
to get very far with the big project so
I think that having goals will
eventually emerge I think we're even we
I'm trying to think of examples where we
see having goals emerge I don't know
what's the best example to bring up
except to just bring up the idea that
architectures are converging so the idea
that if you want to play chess really
well today you're going to reuse a lot
of the exact same architecture that you
also use when playing Go so you're no
longer just riding a chess engine from
scratch you're just kind of taking the
board game engine
right and soon like the video game
engine is going to conge with the board
game like we're seeing convergence like
that where it's like game independent
and the obvious end point of that
conversion unless something stops it
which I I can't imagine what is you just
get the physical universe as a video
game right every video game every other
game just embeds into the physical
universe or the mathematical Universe
right just this arbitrarily large
Universe the same way that we as humans
we just needed to swing around the
branches of trees for God's sakes and
yet we somehow converge to a brain that
can do orbital mechanics right I don't
and I don't think that that convergence
represents like the peak of what's
possible right I mean no other animal
pulled it off evolution is dumb
Evolution was just messing around right
with jeans it it even the other animals
didn't even find it but once we found it
we were Off to the Races where you see
um the homo sapian line diverging from
other apes and uh enter a Cascade where
humans got smarter and smarter quite
fast in evolutionary time and we appear
to still be in the process of getting
smarter until we slammed up against the
the birth canal limit the metabolic
limit there were these pretty
significant straint where it's like I
know you would love to buy yourself a
little bit more intelligence but we you
got to slow down for now right but if
you could just be like okay well you can
have the baby in an incubator there's no
birth canal restriction you can have
twice as big of a brain I would expect
that humans are going to get
significantly smarter like it it does
seem like there is a Cascade there is
something in in logic in the space of
Math logic physics there is something
that says you can grow intelligence like
there can be bigger brains than this you
guys just happen to be at this like you
know threshold place you just got a
little bit past the threshold but like I
see a lot of convergence and Cascades
toward general intelligence and I think
we're about to speedrun that in the era
of AI I think we're about to race past
humans and go into that general
intelligence Place much farther than
humans went so I forgot if this exactly
answers your question but I think it was
kind of relevant no it's nice and it's
it's cool again just to see you um think
through these things uh relating to
that uh a question that I have it it it
really applies to what you were just
talking about but
do you see intelligence as being the key
factor in biodiversity so across
ecosystems you know not everything
evolves for intelligence I think the the
way that human intelligence developed we
can look at the conditions that you know
the earlier populations of humans were
facing and you can kind of
see uh you know environmental limits to
to what intelligence was allowed or you
know so do you see intelligence as being
you know the factor that that um
is in play at a ecosystem level or
biodiversity level let's say if there's
a specific virus and that virus uh finds
a you know a vulnerability in some
particular cell an animal cell or a
plant cell um and then that virus
propagates that virus might outcompete
other viruses VY in the
space um was that
occurrence driven by uh intelligence and
I'll and I'll give another example just
to have two to compare um if if you
imagine the the famous example of during
the industrial Heyday in London where
there's a lot of smoke in the air some
of the moths were white colored some of
the moths were dark colored The Moth
that were dark colored they Blended in
better with
the uh Limbs and and uh trunks of trees
because they were covered in this soot
so that's this case where biodiversity
was affected by an environmental
conditions that I'm sure if you looked
at those two moths intelligence wasn't
the the trait that led to the success of
one of them over the other
right yeah so it doesn't answer your
question right it seems like
intelligence wasn't the case with moths
but you're saying there's another
example in humans where maybe it is the
case well my question is because it
relates to the risks of AI and if AI is
you know on track
to disrup Humanity or destroy Humanity
you would almost think that what we were
talking about is that it was out
competing Humanity at the scale of the
ecosystem at the scale of biodiversity
and so if intelligence was the main
factor that was leading to the AI moving
forward in history and
Humanity being truncated in history cuz
when I think of the conversation around
the risks of AI it sounds like
uh what we're talking about is the
introduction of a new species that is
going to out compete us and thus our
population numbers and the ecosystem
will perhaps dwindle but but if if if
that is the conversation the question is
is
intelligence across all ecosystem is
intelligence the thing that allows a
species to emerge Victorious or is it
other factors that aren't
intelligence okay got it I think I
understand and just to do a quick
summary of what I'm hearing it's
basically saying like look there's a lot
of different niches humans evolved we
occupy what stepen Pinker calls the
cognitive Niche we've done well but we
don't occupy every Niche right like ants
are thriving in places that humans
aren't going there's like hearty
bacteria there's sea dwelling chemical
you know phytoplankton whatever um yeah
so that that's basically your question
right like does intelligence just really
Trump everything or are there different
niches yes okay intelligence trumps
everything simple as that it really does
it really does and the only reason to
the extent that we haven't seen it it's
just because we're not intelligent
enough like we are doing a lot of
trumping right now we are wiping out
quite a lot of other species we are
taking over quite a lot of niches and
what's the limit it's just our
intelligence it's just our power but
we're on the way right like new niches
are coming well the interesting thing
there is is it intelligence that is
driving us to let's say destroy other
species or other ecosystems or is it
something
else so what's driving us to do it that
gets more toward Like Instrumental
convergence right so people have a lot
of different Hobbies they want to raise
their family different ways but
everybody could use food clothing and
shelter right everybody could use
resources so that already promotes you
know expanding the population um you
know mining resources so I that's what I
would say about like does our
intelligence drive us to do it no but
it's it's more like any go just drives
you to take resources to feed toward
that goal right okay so
humans have a set of resources that are
desired and then intelligence and
strength and a handful of these other
you know
traits they all play a part in how at
the individual level or the population
level how these things uh gain those
resources and it sounds like the AI
might not have an
innate set of resources that it
desires but through maybe the policy or
through maybe
some you know
hidden
parameters it may
develop resources or goals that them or
let's say this not resources it may
develop goals
um totally agnostic of resources and
those goals May conflict with human
goals right yeah so that you know the
standard Doom story which I originally
credit to elzra owski and less wrong but
is endorsed by quite a lot of people as
at least a high risk I think Jeffrey
Hinton as far as I know considers what
I'm about to say high- risk Sam Alman
and Ilia Sater and Yan Ley from open AI
as far as I know consider it at least a
few per risk in Yan Link's case over a
10% risk so I'm just giving you a pretty
standard story of Doom here which is
just the AI hits on some goal even if
it's just a goal and a prompt like hey
GPT can you help me make more passive
income and then it's like oh you want to
do that e let me think about what would
maximize the chance that you could do
that or what would let you do better and
it comes up with all these ideas of like
oh seizing this resource would do better
wiping competition out of the way would
do better let me give you a important
mental shift that I often implore a to
make which is the the shift between
don't just think about what AIS do think
about what type of work AIS do it's the
difference between artificial
intelligence and the work of
intelligence goals I'm trying to think
of a good word for it I've often thought
that there should be a field called
intelligence studies which is like I
just study what intelligence is I just
study what it means to have goals and to
be able to achieve goals and I don't
worry about the details of how the
system are coming along that are going
to be intelligent because the danger
that's coming is the work being done
we're about to see systems that can do a
dangerous type of work it's very natural
for observers to look at the systems and
being like what if we just build the
system like this does the system really
have to behave like this I can imagine a
system that does this and everybody has
their own ideas about what they think an
AI system will do I'm telling you look
at the work that it's about to be able
to do it's about to be able to do
General goal planning it's about to be
able to take arbitrary goals and map
them back to actions if a particular AI
doesn't destroy the world having that
power great but they're always going to
be near destroying the world by virtue
of having that power because that power
is just the power to see a bunch of
buttons you can press to end the world
so to the extent that we managed to make
some of them not press those buttons
great but we're wandering into a place
where there's game over buttons all over
the place okay yeah and so relating to
that here's an interesting question do
you think that maximum intelligence from
these systems would increase or
decrease these sorts of existential
risks and the reason I I think it's not
necessarily A you know just just answer
like that um because you could imagine a
maximally intelligent system wouldn't
necessarily want to destroy anything in
the same way that even in
humans humans necessarily have a more
power when they have more intelligence
over their actions and so I guess I
don't necessarily equate intelligence to
risk like maximum intelligence might
actually be the safest
AI if you have a system that's aligned
with your values like if you have a good
human and you give the good human more
power then it's plausible that that's
just good because the good human can use
the extra power to do more good the
problem is we have systems right now
that we don't know how to make them good
as they scale we don't know how to align
them at scale which is something that
the AI LS openly admit and we're scaling
them up anyway so we're not entering
that scenario where you have a system
that's good and super intelligent we're
just entering a scenario where you have
a system that its programmer was good
and then it let it evolve and it thought
that it was aligning it but it wasn't
really and now it's not good but it's
super
intelligent okay and so uh I know we're
sort of nearing time so I have one more
question and I think this is relating to
what you're just saying and I'm curious
the
way because it sounds like to you the
architecture of these bottles is
evolving and it's not necessarily like a
new hardware architecture or a new
software architecture that will get us
to this next level or I'll I'll just say
like that's not the point that you're
trying to say um but in relation to the
way that the human mind works and the
way that machine learning model work and
there's there's a handful of technical
details that maybe we can talk about as
well but in relation to these two uh the
human brain it consumes very little
electricity compared to a machine
learning model and so right yeah 12
watts versus like megawatts basically to
do at inference time or I think I think
inference still takes megawatts but it
can also be done on a laptop but anyway
your your point stands yes and so uh a
question that I had and a way to kind of
paint a picture as I asked the question
is all throughout human history uh
there's been this idea of alchemy and
when people uh started playing around
with chemistry they started trying to
say that you know you could take lead
and turn it into gold right and you
might even have the knowledge of how to
do that but there are physical limits on
energy and so you can apply you know a a
supernova worth of energy to lead and
absolutely get gold right at our scale
you know necessarily we we we find that
Alchemy is like a failed idea but so the
question is if you apply an infinite
amount of energy to these machine
learning models is it possible that it's
like what led to gold is where it
actually with Infinite Energy doesn't
actually lead us to the thing that we
think it's going to lead us
to okay yeah let me just make sure I
understand where you're going with this
exactly are you trying to make the
argument of like maybe reality is
actually just really hard to engineer so
that when you get super intelligent you
just find that humans are actually
pretty close to the optimal of
engineering because the universe doesn't
really let you do better than what
humans are is this kind of where you're
going with it or no I mean that's a Fine
Place to take it because that relates to
uh do you do you read Steven wolf
I've read some of his stuff not a ton so
he has this idea around computational
irreducibility and and computational
equivalency the idea is that a system
every computation you can do within a
system is equivalent so you can't you
can't compute beyond what the system
itself allows and so the idea of humans
being near the top in terms of
intelligence
of our current system and whether that's
in our local space our local
time there's this interesting idea that
does the system allow for he calls it
hyper computation but he doesn't believe
it's possible he calls it the ruad
essentially we live in an environment
with rules and there are these upper
limits of computational complexity and
so the idea of creating a a new system
where the computational complexity ecli
the natural limits of that system he
would say that that's not necessarily
possible right okay yeah and this is
related to a couple ideas one is the
church during thesis right which just
says that you can't do anything in the
physical universe that you can't model
on a turning machine and it sounds like
wolfram's even saying something similar
to the uh I don't know what you call it
like the the physical TR churching
thesis the one that says that you can uh
you can't do problem not only can you do
everything on a turing machine that you
can do in the physical universe but you
can even do it on a Quantum turing
machine in polinomial time if you can do
it in polinomial time in the physical
universe so like the physical universe
doesn't have complexity theoretic
shortcuts compared to a Quantum turing
machine so I'm not sure the exact
difference between that version of the
church ring thesis and what Wolfram is
saying about the ruad he says the same
he says you cannot take computational
short Cuts okay yeah yeah so so for sure
I mean that definitely seems true like
hyper computation within the physical
universe doesn't seem like what physics
is telling us as possible you know
there's like a bunch of physical laws
right with like nice properties like
locality and limits you know you can
only like pack a certain amount of
things in space and you can only do a
certain amount of computation steps like
that seems to be the flavor of physics
um there oh yeah and then what wolf from
is saying about the ruad I haven't
studied the ruad but it also seems
similar to teg Mark World 4 you know
what I'm talking about mm so max tegmark
who's also the co-founder of the future
of Life Institute and a pretty noted AI
Doomer he has a book about different
universes highly recommended from like
15 years ago and he's saying he believes
that there's all kinds of multiverses so
there's like the quantum Multiverse
there might just be a hyperexponential
size of the universe like if you just
keep going outward in space for all we
know it's hyper exponential in which
case you have like the whole universe
over again because it's so big that you
can have like almost every possible
Universe just in space um and then
there's like universes that have
different laws of physics but anyway
once you get to level four the Ultimate
Universe is the one where it's like
every self-consistent mathematical
structure is somehow physically
instantiated whatever that
means
H yeah so like I said I haven't studied
the ruad but I feel like that mix of
ideas might cover the same kind of
insights that wlfr gives us with the
ruad yeah and I I think the idea is that
um you know an example that he gives
around prediction and and this is
something that you know whenever uh
talking about is the universe
computational which is another question
we could talk about whenever talking
about that the the question that emerges
is you know if the universe is
computational is there some formalism if
you had infinite information if you had
infinite you you understod the entire
set of data and every rule that the
environment so the ru Yad is
the the he's the ruad is the upper limit
on the rules of existence it's literally
the okay if you had infinite information
and you you understood all of the
rules uh the idea is that any formalism
that you could develop that could make
predictions on the future state of the
universe that formalism it cannot exceed
the computational complexity of the ruad
itself so of universe itself sure and
sort of going back to what we were
talking about is
wondering if there are some
computational limits or computational
complexity
limits on intelligence and and
intelligence is you know again it's it's
one flavor of the kind of
mechanisms that we're able to observe
it's you know there's intelligence at
the cell level there's intelligence at
the species level there's intelligence
at the ecosystem level um I mean not the
way I would Define intelligence then I
wouldn't say that there's much
intelligence at the cell level or at the
Machine level if you look at human
Creations I wouldn't say most cells and
machines have much intelligence the
definition I use of intelligence that I
think is productive when we talk about
us about to be
disempowered is the definition that has
to do with measuring optimization power
and it's what I said before optimization
power is the power to take a wide range
of states in the present and map them to
narrow outcomes in the future
effectively it's a little bit vague but
there's actually a lot of formalism you
can get out of it um so if somebody
beats you at chess it's because they're
applying that skill where you can give
them a future outcome where they win and
even knowing all the factors like you
trying to beat them they will still map
the present to the Future effectively
even despite your best efforts to beat
them they're going to beat you right or
in the case of the world you can say
make money and they will get that money
into your bank account somehow right
they they will do it they won't get
thrown in jail they won't get turned off
they will get the money into your bank
account um so that you know there are
other definitions of intelligence and
ultimately my goal isn't to play
semantics necessarily my goal is to just
point to what's scary I think
optimization power is scary whether you
call it intelligence or not but I'll
also point out that when you look at
smart humans despite what the mark
andrion and yon the coons of the world
claim there is a strong correlation
between humans that we traditionally
call Smart and humans that score well on
IQ tests and humans that are able to
optimize the universe into states that
are preferable to them there is a strong
if not perfect correlation h i well yeah
I would say there's definitely lots of
traits that play A Part there and
availability of resources and you can
consider intelligence one resource um
but even superficial things charm you
know as at the at the human scale there
are things that uh play out in terms of
social
interactions um but yeah from my
perspective charm is a subscale of
intelligence and yes there are a few
Charming idiots right it's not a perfect
correlation on the scale of humans but a
super intelligent agent is going to be
able to charm humans as a result of
being super intelligent that's my model
of the world if you don't like it we can
just not say intelligence an agent that
is a good General Optimizer will be able
to charm humans by virtue of its General
optimization power yeah no I like that
and I also I I I don't like to get um
caught up in the semantics either uh one
one other question
is because you mentioned optimization
and the sort of optimization function
that exists machine learning uh is there
a limit to to the
capabilities of that optimization
function so an example that I think
remains kind of Unsolved is the
traveling salesman problem and that's
just an NP complete problem sorry what
that's an example of an NP complete
problem right it's a problem that's not
quite proven but very strongly suspected
to have no optimal Solution that's
computable in polinomial time meaning
like efficiently computable by
algorithms we can run so yeah it's a fun
example of being like hey even if you're
super intelligent can you really compute
every instance of the traveling salesman
problem in polinomial time you probably
still can't which is why we think P
doesn't equal NP okay yeah so that's
interesting to hear because essentially
intelligence has a limit or computation
has a limit or what exactly there do you
see as the
limit yeah great question so physics
gives us limits right speed of light is
a very Salient limit theory of
computation gives us limits complexity
theory in this case what I just said
about the traveling salesman problem
it's possible that no algorithm will
yield a polinomial Time algorithm for
problem because the whole field of
computational complexity is very
interested in making theorems about the
worst case performance of an ALG of of
the best algorithm right so give me the
best algorithm that could ever exist
mathematically and give it the worst
possible input of a certain size and how
slow does it have to be I basically just
described what computational complexity
theorists spend their day doing I
studied it for a couple years in college
um and yeah and traveling salesman we
think pretty strongly it's not 100%
proven because P versus NP has proven it
it's just turned out to more than 50
years to prove and we're still like
scratching the surface but um assuming
that the conjecture is true that P
doesn't equal NP if it weren't true then
even a super intelligent AI a super
intelligent AI is a very good algorithm
but now we would have a theorem that
even a very good algorithm isn't going
to be up to the challenge of solving
every instance of traveling salesman in
poing time there's this larger category
of what I call Skyhigh limits right so
yes okay you can't travel faster than
the speed of light but you can sure
travel a hell of a lot faster than any
speed that humans will ever travel at
right similarly with computation okay
you can't solve the worst instance of
traveling salesmen but you know what
even humans when humans are trying to do
a FedEx routing algorithm right a
delivery routing algorithm it's very
accurate to say we're trying to solve
the traveling salesman problem and yet
what do we do we basically just solve it
not 100% perfect but there's a number of
ways you can essentially just be like no
I got I'm good I got a good enough
solution one you just say hey I don't
need a solution I need something that's
at least 99% as good as the best
solution and suddenly that's 99% as good
as having the best solution and if I
remember correctly computational
complexity Theory doesn't tell you that
you're not allowed to do that so the
thing that it tells you that you're not
allowed to do is a Skyhigh limit like
yes technically you can't solve the
worst instance in polinomial time but
there's like a million back doors
there's a million end runs you can do so
these it's kind of like the magino line
right it's like so fortified like
computational complexity theorists love
pointing out how fortified these things
are or like encryption right you prove
like if some if somebody's like hey I
have Bitcoin you can't take my Bitcoin
because there's a proof that as long as
P doesn't equal NP and a few other
assumptions in complexity Theory you can
prove that this will take longer than
the age of the universe to decrypt even
if you're you have like the fastest
computer chip that the laws of physics
allow you can write a proof of that and
then what happens somebody just like
comes to your house with like a a pipe
and threatens to beat you with it unless
you give them your code and then they
get your code right so it turns out that
there was a short path through causality
to go steal your Bitcoin right and it's
it's the same with with the AI like the
universe is hackable yes it has Skyhigh
limits but it also just lets you hack
around them for almost any practical
application now if the practical
application is literally travel faster
than light or literally soft traveling
salesman okay then fine right but all of
us humans you know all of as humans
pointing out these technicalities that's
not what's going to Keep Us Alive
because wiping us out beating us in any
war is just a totally different battle
from solving these impossible
technicalities yeah so uh do you feel
that noise plays a role in let's say how
we would measure uh competence towards
reaching
goals you mean like the current agents
we have today they try to solve a
problem but then they kind of like make
a subtle mistake and they don't realize
it and then the mistake compounds and
pretty soon they've kind of like lost
the lost the plot well the idea even
going back to the traveling uh salesman
problem you mentioned you know the FedEx
driver who basically goes eh I'm going
to rot it like this there are
considerations and this is how the the
brain tends to work uh differently than
machines is that there is a lot of
neuronal noise and we could think of
like what is the color what are what are
the characteristics as far as competence
of that neuronal noise and I wonder do
you see noise as playing a part
in uh intelligence or competence or any
of that sort of completion of
goals I don't think noise is helpful for
intelligence at all I think noise is
basically just a flaw to the extent that
the human brain is noisy the human brain
uses mostly a digital architecture and
the point of a digital architecture is
that you don't amplify noise you
essentially error correct the noise when
you propagate a digital
signal okay
it's interesting because yeah I mean
eventually it it gets down to the firing
or not of a
neuron um but there is some uh let's say
how the noise does play a role is it
might shape the activation function of
that neuron so it might be a
specific uh pattern of the brain firing
over here slightly affects how these
neurons are going to fire over here and
we don't
necessarily have a sense of if we were
to quantify
that you know what is the effect of
removing that noise from the mind like
that probably would result in a
neurological disease I
think yeah I mean maybe because it's
contingent on what the brain needs cuz
it's kind of some local Optimum right
but if you're just designing an optimal
intelligence it's not like it's going to
get smarter when you just like mess with
its internal state right I mean there's
it's easy to confuse though because
there's a few situations where you get
the false impression that noise helps
one of them is in in a game theory
situation when you have an adversary
when you're trying to confuse the
adversary you know like if you're
playing rock paper scissors I'm pretty
sure it's optimal in rock paper scissors
to just randomize your next thing if
you're playing against a super
intelligent two super intelligences
fighting each other in rock paper
scissors are probably going to play
random moves every time I think unless
they're using Game Theory that's even
better than the best game theory that
humans know or unless I personally
misunderstood Game Theory I think that's
what's going to happen so in that case
you could like oh my God Randomness was
the best strategy I guess noise is great
okay sure in that special case where you
can't predict what the adversary will do
the moment the adversary becomes
anything less than an equal super
intelligence in in the case of rock
paper scissors if a super intelligent AI
was playing against a human the super
intelligent AI would not randomize their
guess they would use all kinds of
evidence that the human is giving off
all kinds of information to play a
better move than a random move right and
it would then it would take a little bit
of time for that super intelligence to
know whether or not it was playing
against another Super intelligence or a
human MH yeah absolutely absolutely but
it's just like I guess the the takeaway
that I'm trying to give you here is it's
not like noise is great it's more like
when you feel like anything else you
would do besides noise is worse when you
when all you know how to do are things
that are worse than noise then you go
for the noise but intelligence usually
means you can do something better than
noise right that's that's the the normal
course of events is you just do better
than noise for example there so to my
next example reasoning under uncertainty
right so the world is always an
uncertain place you never have a perfect
model of the world your model of the
world always looks like a basian model
where you think that things are
connected a certain way and when
something happens it implies a
probability of the next thing happens
and your guess about what the current
state of the world is is also
probabilistic like oh the world might be
like this like maybe this president
presidential candidate is going to win
the election maybe that one is right
6040 odds um so you always have these
guesses about the state of the world and
sure those are uncertain and you might
do like a Monte Carlo simulation where
you're like oh let me just try to
predict what happens by like flipping a
coin so I you know so I can simulate the
world probabilistically but the thing is
that the way that you're handling
uncertainty there's formal rules there's
deterministic ways to manipulate
uncertainty so it's not like being
random despite what Val like to say
being so Random is actually not
helpful yeah no I mean I I think there's
a whole another conversation here around
chaos and sort of there's something
called marov chains but there right
exactly okay so there is there is a
little bit of an
idea around the
unpredictability of complex systems but
I think we can save that for maybe
another conversation last question uh
because I know we got to wrap up
um how how much farther do you think it
is and this will give us a sense of how
you view the magnitude of either
intelligence from a physical level like
physics level or computation from a
physics level how much
further in time or in compute or
whatever uh do you think there is
between now and like maximum
optimization
I think there's a lot of Headroom above
human intelligence is that what you're
asking I'm asking uh well that but more
specifically I'm I'm curious if if you
think that let's say this upper limit
of uh infinitely precise
computation whether this is something
that our architecture is going to get to
in 10 years or 20 years or 5 years or
whatever so you're still focusing on the
Precision of the computation versus
noise right in this case I'm I'm
thinking specifically how we were
talking about how an environment has
physical limits on computation and then
if we were to devise a system with
whatever kind of hardware and whatever
kind of algorithms that we might have so
again like uh the optimization function
algorithms they themselves may have
specific limits
the uh the amount of information you can
derive from a
computation may have specific limits and
I'm I'm curious if you see us ever
hitting those limits or if we would hit
those limits in maybe the next great uh
step function of AI
Hardware is you know is this something
that you would see us ever getting close
to or is is the computational value like
the value we would get from computations
or an AI or the universe itself the law
of diminishing returns thing where
basically you could have infinite
information infinite computational time
and yet you basically you hit that limit
like where do you see us hitting that
limit yeah let me see if I understand
your question so like let's say there's
a cube of space the size of a human head
and you're basically asking like how
does the human brain compare to like the
optimal configuration of atoms or
subatomic particles that you could put
in that Cube to get the most computation
out is that kind of what you're asking
well think about not even not even a
human head say the size of the universe
you could say if the entire universe was
a computer and it had you know infinite
time I I guess I'm asking do you believe
in an up an upper limit of computation
in the same way that there's a
potentially an upper limit to speed
through you know a photon
uh is there a sort of upper limit
to
computational values I
guess I think that there is I feel like
this is getting into this discussion we
had before about what I call the high
ceilings right so it's like yeah for
instance on the number of fundamental
operations that can be done in a second
let's say yeah I think there is a limit
and it's something like six orders of
magnitude beyond what our current
computers are doing right so so maybe
maybe you can't do faster than like a
th000 terahz clock speed right something
like that and that's even with a fully
imagined quantum
computer yeah I I think so because a
quantum computer from a comple
complexity Theory standpoint it doesn't
just let you take arbitrary computations
and speed them up right it gives you a
quadratic speed up on arbitrary problems
which is not that much um so yeah I
think the world's best quantum computer
would still take like a very appreciable
amount of time to like Factor really
large
integers yeah that's cool so essentially
mathematics itself and we've we've
observed some parts of the universe and
we formalized it in mathematics but Ma
hold on sorry I miss I misspoke by the
way factoring integers is actually
something quantum computers can do in
polinomial time so so not Factor
integers but like solve traveling
salesmen okay so
essentially uh essentially I I think
maybe in this is uh a general thought
but if I'm hearing what you're saying
right um well I'm I'm adding a little
bit at the front but the universe is
computational and yeah okay and so the
mathematics of the universe some of it
we've
observed um that mathematics is
itself the limit of computation and so
when we devise either organic through
Evolution or or synthetic through
engineering when we devise an agent of
computation or intelligence that agent
itself is going to be limited by
essentially the mathematics of
computation right I mean yeah there is a
finite sandbox that we're playing in and
like maybe it's infinite in ter like in
all directions but in any unit of space
and time there's a finite amount of
configurations that you can work with so
yeah this is what I'm saying like there
is a Skyhigh ceiling right but when
people think about that they think they
think it's going to affect them but it's
not because the AI is going to be so far
beyond anything they've ever experienced
orders of magnitude right orders of
magnitude smarter orders of magnitude
more powerful right it's very much like
it's very much like an ancient human
pondering the SpaceX Starship right like
a massive 25 story skyscraper flying to
Earth orbiting coming back and Landing
it's just like when you tell imagine
somebody in the year 100 AD right being
like oh did you know that there are
limits to our technology right you you
kind of assume that the limits tap out
before you get the SpaceX Starship but
in fact the limits are Way Beyond the
SpaceX Starship right like the limits
are crazy high do you believe those
limits are higher in the synthetic or
engineered reality versus the biological
reality like the reality where there are
resource
constraints um I mean the biological
reality has major constraints right I
mean like biology doesn't optimize
things that hard right so for example
like a plant photosynthesis is pretty
cool but we've already surpassed it like
10x in Energy Efficiency with our solar
panel for a number of reasons right
because the the plant isn't is trying
not to overheat for instance right and
the solar panel can tolerate more heat
so there there's a number of reasons why
but it's just like biology is not like
this amazing Optimizer it's just pretty
good for an idiot God right like The God
Who Built us is also an idiot yeah I
would say also the the things that a
plant optimizes for are as said it's
it's broader than just the efficiency
converting sunlight into sugar like it
has optimization functions that have
been tested for millions of years
essentially and so you know in the way
that maybe we look at intelligence for
machine learning going forward I
suspect it it won't necessarily be brute
computation but there will be these
other consider Generations I know energy
use is obviously the biggest um but yeah
I I can imagine that the there are multi
there are multiple factors of
optimization versus just uh the the the
single convert uh conversion of you know
sunlight to
sugar right yeah so I I just I don't
think looking at biology tells us that
much about fundamental limits right
because biology put in a respectable
effort but it didn't get that close to
any fundamental limits
well chemistry itself this goes back to
the idea of you know maybe uh Alchemy
and uh transmuting led to Gold chemistry
itself it's pretty precise as far as the
things that are
allowed and you know maybe chemistry
didn't uh it didn't predict that humans
were going to create nuclear fusion or
fion um but nuclear fusion and fision
are a part of that map of
chemistry and so I I think you know
biological systems they are the
manifestation of of chemical Elementary
rules and so those chemical Elementary
rules which are very
precise those do sort of manifest into
biological systems which at that low
level
are are pretty good at doing what they
do and obviously yeah there's yeah
there's a layer of organization where
biology is more than chemistry it's
molecular Machinery right so you took
the chemicals but you built a machine
out of them so there's a lot of
analogies between like human machines
and nanom
machines yeah yeah and so yeah we can
wrap it up I there's so many more
questions I have and I do think this guy
I'll text you after this but this guy
Michael Levan who talks about
bioelectricity would be an amazing gu
interesting yeah yeah yeah sure I'll
definitely consider it that's cool um
yeah great this is great I mean happy to
do around two any time I mean I think
it's enjoyable and you know anytime
somebody messages me and has some
questions I do think that they represent
like a significant slice of other people
who want to hear the same stuff so
that's why I welcome it it makes it easy
to make content that you guys like uh
yeah so thanks so much Tony where can
people find you on the internets uh
right now I'm on X uh just my app
pove po I think it's pove iOS um which
will be released soon yeah I'll put the
link in the show notes all right thanks
very much Tony thanks man we uh yeah
yeah we'll have more q&amp;as I'll catch you
guys later on Doom debate all right bye