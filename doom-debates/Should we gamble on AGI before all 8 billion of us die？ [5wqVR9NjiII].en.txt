yes I think P Doom is high but again if
we're only considering people that are
alive today our P Doom without AGI is
100% hey everybody Welcome to Doom
debates today I'm here with Chase man he
is an employee on Twitter on the
business side he says he's non-technical
and he has stepped up to the challenge
of debating me about why my policy
position of causing AI isn't the right
position and he isn't even that worried
about AI Doom is that right Chase uh
that's kind of right I I actually mostly
agree with you and owski and all of them
that we are probably
doomed uh I just kind of disagree on
basically your prescription as to
whether or not we should move forward
with that in consideration great tell us
a little bit about yourself uh I'm Chase
M like you said I work at X not Twitter
how dare you fair enough but yeah on the
business side not technical uh not an
engineer or anything so uh that's why I
agreed to have the debate with you more
on the philosophical side because
there's definitely nothing I can really
say on the technical side other than
that the arguments that I've heard for
the Dooms side sound plausible like I
generally agree that uh it seems very
unlikely that version 1.0 of an ASI will
be aligned to human values now and kind
of uh permanently into the future that
seems like a dubious proposition so I'm
kind of on board with that uh I Just
Disagree that uh that that's the worst
thing coming for us right now totally
and it sounds like you're pretty humble
about this like you're willing to change
your mind you don't necessarily think
you've thought about it super deeply is
that fair to say yeah that's fair to say
okay but you've been digging through
like the different arguments like have
you read a bunch of less wrong for
instance I haven't read a bunch of it no
a little bit here and there but no not
much Okay cool so you told me a little
bit about your position when we were
talking on X and I think my goal for
this debate is going to be to just show
you that there's internal
inconsistencies like it's just not
really a workable position um it's just
yeah like the moment you try to take
that stance there's just so many
contradictions that it's like okay you
might just want to rethink your own
position like I think you will actually
agree on your own accord entirely
possible sweet okay great and I also
think that there may be other people who
are thinking along the same lines as you
who can from these kind of discussions
which is why we do them sure okay so
essentially I agree that it is very prob
that we are doomed I'm not sure what my
P Doom would be exactly but it's
definitely like more than 1% less than
99 uh somewhere in there
and basically my argument here is that
we are already in a doom scenario we are
already as it stands all 8 billion
people alive right now very definitely
going to die we all have a a terminal
condition called inessence and it's
going to kill us all in less than 80
years and really it seems to me that our
one chance because I mean I don't think
we're making very uh fast progress on
the solving the cence problem with on
our own uh maybe that's that's one area
where I could be convinced otherwise but
assuming that's not the case it seems to
me that AGI is our one chance to not
have all 80 billion people alive right
now die so basically just kind of comes
down to AGI will probably kill us uh but
without AGI we're all definitely going
to die so would you rather probably die
or definitely die uh and then there's
also questions we can get into as far as
like future humans and other people that
aren't currently alive right now to
consider but before we get into that I
just want to kind of give your pulse
check there like if we don't consider
any future people do you agree with
everything so far or is there something
you want to quible with there got it
okay yeah we can Factor the whole debate
into what's the best way to help the
people who are alive today and what's
the best way to create a good long-term
future for Humanity including
potentially after 80 years from now
right those are like two
questions right and you asked me what if
we only care about today what would I
say if we only cared about today because
my P Doom is pretty high roughly 50%
then I would say I think it's Reckless
to make an AI as a hill Mary to Keep Us
Alive I think the obvious way to Keep Us
Alive is to do cryonics and then just go
work on building safe AI over 50 years
or however long that takes so that might
work my concern there is one uh cryic as
it currently stands as far as I
understand doesn't really work like uh
there's ice crystals and it's not really
reversible and there's lots of Damage
Done the process so I I think it's
dubious that even an ASI would ever be
able to revive people in chronics as it
currently stands although I did just
hear that they've made some progress
recently in like freezing a single cell
and like without damaging it so that's
pretty cool yeah um so last time I
checked the vitrification process where
you can pump a certain type of liquid a
cryoprotectant into somebody's brain and
then cool it down with liquid nitrogen
and just keep it in that state the
expert estimates that I've seen just say
that that looks like it probably does
preserve a vast majority of the critical
information about how neurons are
connected up in the brain such that a
sufficiently good AI scanner can take a
look and be like okay I know how to
upload this and restore or I know how to
do an operation to take exact neurons
and reanimate them like that seems
pretty good theoretically I mean I'm not
saying I'm necessarily more than 50%
confident that it'll work but there's no
fundamental reason it shouldn't work is
that fair to say yeah I agree there's no
fundamental reason it shouldn't work uh
I guess there there's two problems there
one yeah like if if your P Doom is 50%
and your percentage chance your your
probability on this working is less than
50% that should still make you go go
towards uh AGI I think but any even if
we think that it is more likely the
other issue is that only a small amount
of like relatively wealthy people even
have that as an option at all like the
vast majority of the 8 billion people
alive today there's just there's no way
they're going to be chronically
preserved I don't see any like PA to
that real we might be able to agree on
the Crux of the disagreement which is I
think the Crux of the disagreement is
which one has a higher probability of
working building AGI really fast or
cryic right and whichever one has the
higher probability of working if we just
care about present people we should
probably just prioritize that and not
the other right well with a further
stipulation that if you think it's
actually even if chronics works it's
also plausible to get some critical
number of the 80 billion people
currently alive into
that right exactly yeah yeah yeah um so
just to handle that I actually think
cryic is not expensive at all actually
so anybody who has any sort of job in
the US anybody who's going to make a few
million dollars over the course of their
career it's actually straightforward to
buy life insurance and assign that over
to the CICS and you're just looking at
payments on the order of maybe a th000
bucks a year for that guarantee that
they'll go shell out like $80,000 when
you die to get you cryo preserved so
it's definitely it's not going to break
the bank to for an average person to do
cryonics today well so maybe for like
the average American right now but you
know there's 8 billion and most those AR
in America but even with that like that
will help you if you die like when
you're 45 but like uh as you get you
know if you try to get life insurance
when you're 80 it gets a lot more
expensive around then if even if you
want to like let me ask you this okay
before AI looked like it was short
timelines right it's like two years ago
or whatever were you running around
telling everybody to sign up for
cryonics who could afford $1,000 do a
year is that a priority for you uh no
but I just I didn't really know about it
back then okay like it back then I well
maybe two years ago but sometime before
all of this AI stuff started I never
really even considered uh the
possibility of living beyond our 80-year
lifespan so it just didn't cons like
occur to me one way or the other but now
it's like your primary consideration I
don't know if I'd say primary because I
still think I I still think it's a very
low likelihood of working either way
like I don't like uh my all my energy on
isn't on this because I still think I I
still am very convinced that I'm
probably going to die so I I wouldn't
say primary but yeah I think there may
also be a contradiction in your revealed
preference or your revealed beliefs
because like let's say your ideal
scenario that you seem to be banking
everything on is we're going to rush to
AGI because we need to make it happen in
our lifetime and you get lucky and it's
it's coming in 10 years right let's say
your your Holy Grail is coming are you
signed up for cryonics
now uh I'm not but I agree I probably
should be okay all right just pointing
that out because if you get hit by a
truck right sometime in the next 10
years that's unfortunate because you
just needed to be cryopreserved for a
few more years and then the Holy Grail
of good AI would have saved you right I
I well two things one I like we already
said before I'm not sure that cionic is
that currently stands really works but I
agree that regardless of that you know
the mass says I should sign up for it
either way so uh this is kind of a do as
I say not as I do situation I should be
doing that for sure okay I mean it's
just like it's fine but it's just you
really are betting big on this premise
philosophical premise that it's all
about maximizing the current people who
are alive today their chance at
immortality right it's like that's
that's what you're focusing everything
on and you're going to make some serious
trade-offs like you're going to say
let's do a reckless project that doesn't
look like it's going to work because
it's so important for the people alive
today not to die well it's only
Reckless if uh we think that like
potential future humans are real in some
sense in the sense that they have like
they're worthy of moral consideration if
we don't if we're not considering them
then I would I'd say it's not Reckless
at all it's kind of like no no fair and
I'm not even I'm not even getting to the
argument of whether we should value
future people's lives I'm just saying
you care let's say future people don't
matter but you care so much about
extending the life of present people um
that you're willing to bank it all on an
AI project that looks like it's you know
pretty high PE Doom like friendly AI
doesn't look super likely but you're
saying you know damn the torpedoes right
like let's just go Full Speed Ahead um
yeah and it seems to that seems to imply
that you just really care about the
current people not dying and I just that
particular value that you claim to have
just doesn't seem consistent with like
other choices you've made well it's kind
of like imagine you had terminal cancer
and you were given six months to live
and you're like hey we have this super
experimental treatment that's probably
going to kill you in three months
instead of the six that you had before
uh do you want to take it I think I my
answer would be yeah let's go for it
yeah yeah but you're signing up for this
super risky treatment and it's not
really like a daily thing on your mind
of like hey we got to make sure we got
to promote cryonics right like you're
not really out there advocating for
cryonics despite it being an ambulance
ride to a good AI future well that's
just because I'm a private citizen don't
really advocate for anything we to go
advocate for things that I probably be
advocating for cryonics yeah so it
sounds like on a logical level it's it
sounds like I believe I'm successfully
convincing you of like in addition to
advocating for let's build AGI as fast
as we can you should also be advocating
for like let's bring cryic to the masses
let's lower down the price let's get
Universal CICS yes I agree and if it was
Poss I would even say that if it's if
that seemed like a possible scenario
that we could get it cheap enough to
where the vast majority of people could
get it I would if that seemed like a a
very likely possible future I would say
okay yeah pause AGI let's do that
instead that's we probably get cryic
down to five maybe $10,000 per person
total I mean putting somebody in a tank
of of liquid nitrogen after pumping
vitrifying liquid in their brain it's
just not that hard right like
5,000 we have to we have to keep all of
those uh icicles around for I don't know
how long but just long long-term
Refrigeration is not that expensive I
mean you it's you know you just pack
them in and you when you have a bunch of
really cold stuff packed together you
can keep the whole Cube cold pretty
easily I mean you could just like put
them underground like cryic is actually
fundamentally a very easy
problem yeah but I just I don't think
practically it work even if you there
there's another problem which is I guess
a bit more controversial but it's just
the idea that even if we did kind of get
it cheap I think the vast majority of
people alive wouldn't go for it uh I I
just don't think most people would most
people like don't even know what we're
talking about they wouldn't be into this
but you know if AGI came along and said
hey we can cure aging I think that's
people can kind of get behind that like
oh I can just take this pill or whatever
and stop aging like sure it's as opposed
to like hey do you want to sign to be
like frozen forever when you're when you
when you die I don't know it seems like
a harder cell but I say that's more
controversial because now we're talking
about like saving people against their
consent or something so that's a
different conversation okay so I think I
might be able to give you some amount of
concession and say something like if P
Doom is let's say only 1% if we all
agree yeah there's a 1% chance that
building AGI in the next decade is going
to kill everybody and and burn the
future if that chance is only 1% and
then there's all this upside then that's
pretty tempting to me right I can I can
imagine being like yeah let's race I
mean if there's going to be such a great
future so I do think that it's a
loadbearing claim I think it's part of
the Crux of our disagreement of whether
P Doom is more like 1% or more like 10
to 90% would you give me a little bit
more would you say that the argument
holds as long as your uh probability of
cronics working and being scalable being
a lower chance than AGI succeeding or a
higher chance than AGI succeeding no
sorry the thing it's well I think
there's some correlation here right
because it if you tell me does kics work
or not work like I think the key
question of whether kics works is
whether we're going to have AGI that can
read off these preserved neurons right I
feel like that's the hardest part of
getting chics to work like just the idea
of making sure to preserve the neurons
and keeping them cold is like much
easier now of course there could be a
part I'm missing like maybe the we
preserve the neurons but it turned out
that the quantum entanglement inside the
neuron was the key and we didn't
preserve that at all right so there's
always like a few percent chance of that
kind of thing um but I I really think
that the biggest point to failure of
cryonics is the the restore part of the
operation and if you're telling me that
what's the probability of getting to
good AGI I feel like you get restorable
cryonics out of that equation like that
that comes for free most likely um so I
agree I agree that if it's possible to
do like that you know AGI or ASI will
eventually be able to do it it just you
know but if if we do it in a way that's
just really not reversible like if at
the M like you said like the quantum
level or the micro even just the
microscopic level like the cells are
just broken and there's really no way
even a insanely smart AI could go in
there and connect everything back
together in the way that it was and a
way that restores the original
Consciousness another problem being we
don't even really know what that is or
how that works um then yeah then that's
really the Crux the Crux of the problem
is are we today uh making it possible
for even a super smart thing to do it
and you're like basically How likely do
you think it is we're doing at least
right enough for something to work out
all of the the terrible job we did
eventually right I mean if you let me
assume in a particular case that like
let's say I die but my brain gets frozen
by the current standards of cryonics um
you know it's not like smashed b or
whatever like it's successfully
cryopreserved um and also you grant me
that the coolant stays cool and you
grant me that we get super intelligent
AI if you grant me all of those things
then conditioned on all of that my
personal probability of cryic
successfully working becomes like I
don't know 75% at least right I mean
like I guess and a lot of the extra 25%
could just be like oh we didn't know
which part of the brain needed to be
preserved and we preserved it wrong
right so that that can go in the last
25% but I feel like my P CICS working
becomes quite High what do you think I
think that's fair so I think if that is
the case then I think it's rational for
you like selfishly to prefer the crry
ixs route so my only addition to that
would be uh that I that I guess you need
to factor into your your probability if
you wanted to expand that Beyond as
yourself is do you think that uh like
basically how much does it lower your
Credence if we add the additional task
of we need to get everyone or at least
the majority of the world into these
cryonic tubes uh within you know I don't
know within how long but less than 80
years right right right right yeah um
and that's that's a good humanitarian
mission right to think can we save every
um and if you ask me what does it take
to save everybody with chronics I think
it's very simple answer of just the will
right the political will the ground
swell because if everybody woke up
tomorrow and says we all need to have
cryonics I think with within a year
maybe two at most I think we we just
make it happen like there's no big
bottom L besides people wanting to do it
yeah I think that's probably true but
like that's a lot harder problem than it
sounds like to spread that course in
countries that are also that have famine
right those the same political issues
are going to of course prevent CICS kics
is probably harder than AGI on the other
hand you don't need all of this uh this
massive organization around the world
this is like one you know open AI can do
this by them and save the world alone
they don't need to get everyone bought
in like you you need for
cryic Okay cool so I think we're we're
kind of agreeing that cionic is a viable
ambulance ride to the Future where AGI
goes well right yeah so I guess my key
point is just like okay great so why
don't we just make sure that that P Doom
is low before we take the gamble on AGI
in other words like why don't we let
safety progress instead of being like we
need it as fast as possible because we
might
uh well we might all be dead by then
okay so yes I agree that in the world
where we can get like U cooperation
around the world and get everyone
rolling on cryonics then I would say
yeah let's let's pause AGI for as long
as it takes to to get that right I just
don't think that that world is likely I
think it's much more likely I basically
I think there's a it is more likely that
we save everyone with AGI because AGI is
hard of a problem that it is doesn't
require the Buy in the cryic does and
then you add that to the the fact
that going to work anyway summarizing
your current position which sounds like
you may still be refining it but current
position is like look we could try to
get everybody signed up for cronics and
then go slower to get to AGI but it's
going to be a better chance of success
if we just go as fast as possible to AGI
and that way we don't have to get the
ground swell for cronics even though we
might get a bad AI right like that's
basically your position yeah essentially
like I I just I don't think we're g to
get that buy in globally I don't I don't
think that so I mean maybe there's like
one key number that that is the cor
disagreement which is basically it's
basically P Doom from AI I think right
so like if you're pdom from from
Reckless AI right P Doom from going at
the current Pace toward AGI and at the
current pace Pace towards safety which
is a slow pace um my PD would be
something like 50% probably even more
maybe 70% condition because the reason I
normally quote my the reason I normally
quote my pum as 50% is because I'm also
factoring in successful Pai movements
right and and everybody getting as
scared as I am when I so um so if you
say nope there's no movement like we
just get Reckless AGI then my pdom
becomes let's say 70% by 240 or by
whenever I predict that the AGI is
coming whereas if your pdom was more
like 1% or 7% % I think that's just the
Crux of our disagreement because I can
imagine if my pom becomes 1% then I'm
like okay yeah forget about cryic just
like take the 1% gamble let's just go
build AGI and we have a 99% chance of
heaven so let's do it right is that
basically the Crux yeah I'd say that's
fair it just depends on what your your
overall number is for cionic plus like
World cooperation to actually get most
people into
cic yeah well I think you know we could
probably give a similar estimate on that
like I agree that that's like a key
number of can we get the people in
ambulance ride to the future I guess my
probability of like if if if Elites if
people like us who know how to do a good
debate and change our mind if we could
all get convinced that cryic Universal
cryonics is the way to go to save
Humanity I feel like the probability is
like at least 30% that we could like
mobilize most of Humanity on chronics
what do you
think um I I honestly don't know I have
no idea what number to put on that uh
okay
it's 30% like that's not an insane
number I guess if everyone like yeah
like if everyone with the highest levels
of government was like yeah let's this
makes sense let's do this it's
interesting that in your view convincing
people to do CICS for their own good is
like such a key bottleneck that we now
have to go build AGI for their own good
because they're too stupid to go sign up
for konics for their own good right is
that basically your
worldview um I don't know if I put it
like that like uh it's it's not
necessarily that they're like too stupid
it's just
that this is a you have to get people to
believe a lot of things to convince them
that this is the best way to do it add
that to the fact of chonic just being
hard period like why you have to
convince them that AGI is a thing
because a lot of people don't even think
like they just think this is a silly
notion you have to convince them it's
true you have to convince them that Doom
is very real you have to convince them
that cryonics is not this crazy cult
thing and that you you can actually do
it and it's okay then it's not against
religion you can do this and you have to
convince like the governments and then
the world to kind of like unite on this
common goal and then you have to
actually practically do it uh which is
still going to be very hard and also has
some chance of failure even if we did it
so there like factors there the
interesting thing about cronics though
is it's not a coordination problem per
se not in the game theoretic sense
because any individual who goes and
makes preparations for a cranic gets a
personal benefit from their own effort
right uh uh yeah that that sounds very
rational but right so if that's what I'm
saying if everybody woke up tomorrow and
said I want to be cryo preserve right I
don't care about anybody else I want to
be Crow preserved because that's good
for me then the coordination would work
itself out like capitalism is actually
perfectly well suited for this
particular problem right like maybe
capitalism is not necessarily suited for
things like climate change or avoiding
AI Doom but it is suited for having
everybody go purchase the kronic that
they want if they understood how good
kic
was yeah I mean maybe it seems seems I I
just it's really hard for me to imagine
a world in the near-term future where
everyone's just like going out and doing
this and talking about this like it's a
normal thing but I agree that that is
like potentially possible and if that
were possible um and it was more likely
than AGI going well that that should be
the route we take okay great and um and
I'll also note the reason we're even
focusing so much on CICS which is
something that's more relevant to the
people alive today is because I know
you're passionate about saving the
people Al life today and you have a
separate philosophical point we can into
in a second which is like you just don't
want to put moral weight on people who
may be alive in the future and so that's
why I'm humoring you and I'm like great
let's just only care about the people
today you and me and our current
families forget about their kids for now
um so I've just been humoring you which
is fine like because I think that it
most of my argument most of our
difference in beliefs I think does still
apply in asking just how to save the
people alive today and that's why I'm
I'm I keep narrowing it down to P AI
Doom I really still think that's the
Croc right and and I was going to say to
to summarize my recommendation I think
that if your P Doom is only 1% you
should go for the Hail Mary right just
step on the gas develop AGI because look
people are dying let's let's go take the
99% gamble of like saving the world
building Heaven um I think somewhere in
between 1% and 30% to put a broad
interval by the time your P Doom is up
to 30% I then I think it becomes crazy
to to say okay let's do it cuz we have a
70% chance of succeeding like I I don't
think 70% is high enough given you can
slow down lower that P Doom from 30% to
5% or whatever and then just do cryic so
it's like it's not that it's not a that
big of a problem to buy time so you can
lower your P Doom so so then I guess I
just have to ask the question like do
you have any idea if your P Doom is more
like 1% or
50% I'd say it's closer to like I don't
know somewhere around 50% it's not 1%
for sure yeah it might even be higher
it's just that I couldn't
even that's pretty fun high percentage
chance on Doom I just put an even higher
chance on uh that coordination problem
and Tech technological problem with
cardionics working out for
everyone okay um so so that's
interesting because I my point just now
was like Hey if your P Doom is really
high like mine is then you shouldn't
think that building AI recklessly makes
sense because cryic is a failable option
and now and you basically have two
arguments you're saying you're really
pessimistic about people waking up to
how much they need to sign up for konics
and number two you really don't care
about the future world so you want to go
all out on the current generation right
those are your two
arguments yeah that's essentially right
I just I like I would just to summarize
there I would say my p uh cryonics
failing for either technical or
coordination reasons is something like
uh failing is is something like 80%
while AI Doom is something like 50% it's
like a good way to summarize that let me
just ask you one more time this a little
red but just condition on this condition
on a world where every time people come
on the Doom debates podcast anytime
intellectuals discuss the matter they
all agree P Doom from building AI
recklessly is 50% so like the Yan Leons
of the world come out and say oh I was
wrong it's not .1% it's 50% this is
super Reckless so everybody gets on the
same page as like Jeff Hinton or elazer
owski and there's Mass consensus on that
and that consensus starts propagating
into like other policies and stuff even
in that world then you still think cryic
is is not going to happen so if we're in
that world uh then I think that it has a
if we actually do find ourselves in that
world and uh we progress down that
railroad track I think that that
definitely increases my Credence and
cryonics working out so really this is
all on you on right well that I mean you
know the reason I'm bringing up that
hypothal you you might uh convince me
over I mean yeah it's it's funny because
normally when two people are talking on
the street they're like yeah people are
frozen in their beliefs but the point of
this podcast Doom debates the point of
this forum is to be Upstream of people's
beliefs right we're trying to write down
what the correct beliefs are so if if
we're saying all it takes is for people
to have the correct beliefs and to see
reason and that will propagate to all
these great actions like get everybody
on the ambulance ride that is cryonics
let's do that let's start propagating
right we actually have a productive
thing we can do here in this Dune debate
so like you coming out of this Dune
debate saying like I was convinced that
cronics will work and we should you know
we should have a high PE doom and act
accordingly right like that that would
be productive I agree yeah good best of
luck on all of your future efforts
because sounds it's all on your
shoulders right now but yeah if you if
you do move the conversation or you know
obviously not just you but if the
conversation does move in that direction
it becomes much more likely that the
coordination problem won't be an issue
then yeah that would change my position
I'd be like all right cool let's pause
AGI then okay great great great and also
it's it's funny because most of the
debates on this podcast most of the
stops on what I call the Doom train are
going to be debates about what P Doom is
right whether it's 1% whether it's 50%
and so it's funny that you're like no
I've I've already followed you on all
these stops where I think pom is super
high but I just think we're so screwed
that it's just one crazy Hil Mar or the
other and and I pick the crazy Hilary of
building AGI that seems like it's going
to kill us yes I'm getting off on a very
late stop on the do train yeah or or
potential yeah a very late stop but also
I don't know maybe even a very early
stop which is like I don't care what P
Doom is so don't yeah I don't even know
where I'd order the stops uh okay great
so I think we we talked about like the
present generation and CICS and what the
Crux is um I think the the last thing
that we want to talk about is the way
that you're not even considering the
moral weight of the entire future where
whereas many people would say that even
just like the years 2100 to 2200 like
the medium-term future right not forget
about the year like 1 billion just like
the next Century would likely have like
a trillion sentient Minds on it just in
our local region of the Galaxy and
arguably be worth a lot more than like
the entire moral weight of all of the
past and present and you're just saying
like no we got to treat it as a zero
right that's your position yeah right
okay so yeah maybe summarize your
position yeah let me let me summarize
that a little bit so
essentially uh I just don't think like
future potential beings just they they
don't exist they're not real
and it doesn't really make sense to
consider them as if they are real like
it it leads you to all sorts of I think
pretty absurd like hyper pronatalist
conclusions if you consider Morley so
actually before I say that let me be
clear I'm not like a psychopath I do
really want like I have a very strong
aesthetic preference for there to be a
future with lot with billions and
trillions of happy humans spread across
the Galaxy like I want that I would be
happy in my deathbed if I knew that was
coming I'm not like an anti-natalist or
something like that I definitely want
all that to happen I just think that um
that's an aesthetic preference and it
comes second to ethical uh decisions
that we we need to make because like for
one example I'll give like if I said hey
Lon uh push this kid into this volcano
and I will guarantee that 10 kids are
born somewhere in America that would not
have otherwise been born I I think most
people's intuitions would agree that
like if you push that kid you're just
you're a monster like that like because
that kid is a real person that I think
we have some type of moral obligation to
even if it's at Fair leash it's to not
harm them whereas I think we have zero
moral obligation to people that don't
exist in the sense I don't think we have
any obligation to bring non-existent
beings into existence uh because I think
we kind of make this error when we're
thinking about potential beings or we're
like in some sense considering them as
if they already exist in some way and
like they're just kind of like sitting
in a waiting room somewhere waiting to
get their shot at existence and if we
fail to bring them in then they're like
they're missing out on something but in
reality no there there is no being
that's being deprived of anything if we
fail to bring them into existence so I I
I agree that there is like a strong
point of the argument where you say like
if somebody's alive today we should care
about them more than somebody that's not
even alive yet right like there's surely
some sense in which we should care about
them more I mean I agree that that's
like a strong intuition and I think you
made a strong case of like look would
you stick a knife into somebody right
now if if that would create two more
babies I would not right that that
doesn't seem like a good way to go um so
like I agree there's like something to
your argument um I think the type of
argument you're making is there's a big
class of arguments like this which is
basically saying like boy it's so hard
to do utilitarian reasoning so like
let's just throw it all out let's throw
out utilitarian reason because like look
at these like tough dilemmas that
utilitarian reasoning gives us so
instead here's like this huge rule of
thumb you can use instead right this
like blunt rule of thumb you can use
instead but the problem is that the
blunt rule of thumb also gives you like
a million thorny edge cases right so
it's not as simple as being like I don't
like utilitarianism or I don't like long
termism so here's a blunt rule of thumb
because I can equally well nitpick your
blunt rule of thumb so like let let me
take a stab at nitpicking your rule of
thumb of like ignore future people um a
couple a couple things that come to mind
so number one is just that like you know
you let's let's say uh so I have my kids
today and what if my kids really want to
have kids so shouldn't I like plan on
maximizing the welfare of my kids kids
should I plan ahead toward that so I
would what I would say to that is it's
it's different if you are like planning
to bring someone into existence like and
like an good example of this I would say
is like uh
if you're planning to have an abortion
like I'm pro-choice up to 20 weeks so I
I think like if if you're planning to
have an abortion that's fine uh if you
are planning to have an abortion at 10
weeks and you like start drinking
heavily at five weeks I don't think
you're doing anything wrong there but if
you start drinking heavily at five weeks
and you are not going to have an
abortion you're going to bring that baby
to term then you are harming a future
person like that person is going to be
born and you will have harm of them so I
think that's kind of like the difference
there like if you are planning to bring
someone into existence then yes you you
should plan accordingly and try to make
their existence happy but if not it's
different because like if I having if
I'm planning to bring this person to
existence then I am depriving them or
hurting them if I do something bad but
if I don't that's very different because
there is no one to have been deprived if
I never even bring them into ex okay but
if you look at like the United States
government aren't we planning ahead
toward the population being in a certain
Ballpark and making preparations for
that so should those count now as moral
patients because of that
planning and like they should count in
so far as we should plan that if they
are going to exist they should have good
lives but that doesn't um like make it
so that you have to that doesn't commit
you to bringing them to existence just
because we planned to bring them into
existence it it commits you to giving
them a good life if they if they do come
into existence let's say there's some
resource like a school that for some
reason is really slow to build like it's
going to take me 20 years to build this
school and it's going to be for a future
generation should we just not build any
such schools cuz we're investing in
future and the future is worth zero no
no you should you should invest in that
school because you were planning to have
people there that are going to live and
they're going to need education but if
you're not planning to yeah but aren't
aren't we as a species planning to
colonize the
Galaxy
yeah okay so it's but it sound you made
it a big precondition like you need to
have a plan once you have a plan then we
get to treat future people in the plan
as moral patients so shouldn't we just
start treating it and future humans as
moral patients the Crux isn't that you
need to have a plan the Crux is that
um it it's kind of like the the morality
here is like based on intentions it's
like if if you fail to do this then
you're like kind of like guilty of
conspiracy to to harm like if if I am
planning to have this person exist and I
am setting them up for failure I'm
setting them up to have a bad life then
I'm doing something wrong but if I just
decide like if you decide to have a kid
with your wife and you decide like Hey
we're going to like beat the crap out
this kid once they're born that's
terrible but if you guys at the same
that I just decide hey actually you know
you were planning to have a kid last
night but let's just not do that I don't
think you've done anything wrong by
deciding not to have the kid so just to
recap so it sounds like you're kind of
departing from utilitarianism that kind
of naively says future lives are
probably worth something you're trying
to depart from that and you're getting
into your own world that you're trying
you're trying to make it up you're
trying to lay down some rules and one of
the rules you're trying to lay down is
like sins of omission versus sins of
commission right no no I'm trying to say
that it doesn't like whether the S
Omission or commission doesn't matter
just my point is that you can only harm
beings that exist if they're going to
exist in the future and you do something
to set them up for harm sure that's bad
but if they're not going to exist or if
you prevent their existence that's just
a different you're you're not harming
anyone you're not uh right but it sounds
like deiving anyone you keep referring
back to this concept of like do we know
in the present that they're going to
exist and if the answer is yes then even
if they don't exist we have to treat
them as well as if they do is am I
describing your framework accurately
here uh I wouldn't say that you have to
say like you know like it uh because you
know you can be you could be guilty of
you know
like murder or something yeah like if if
you're planning or
if like and if it turns out that they
don't actually exist like if you plan
for them to Sy and they don't then you
can say that you didn't actually do
anything bad it would be kind of it
would be similar to like um attempted
assault like oh I was or conspiracy to
like punch someone like I was planning
to punch someone but they never they
weren't actually there on the street
corner when I plan to beat them up so
you can still say like yeah was wrong to
plan to do that but you didn't actually
end up doing something wrong I guess
because you didn't you know they weren't
there to be hurt so if you think yeah if
you think ahead to like the year 2035
like how many humans are currently being
planned for right now in your
view um that that I don't know that the
billions probably like ballpark yeah I
know so there's eight billion alive
today so in your moral framework the
rough number of people that we should
treat as future moral patients is 8
billion as opposed to like 80 billion or
or 1 billion like you just think the
correct number to treat as moral
patients is like still 8
billion no I'm trying to think how to
describe this a different way because
that's it's not that like once you've
decided to like if you've decided hey uh
we're going to have this
person just making that decision does
not create make them a moral agent in
the sense that now I owe them existence
now like I've planned for it so now I
have to bring them into existence that's
not what I'm saying I'm saying that if
they are going to exist and you do
something now that is going to hurt them
later that's bad but just deciding not
to make them exist is not bad because if
they don't exist now or in the future
they cannot be a moral agent they cannot
be relevant being I think I get that
part right but ultimately the reason
we're talking about this is just because
like we're it gets back to the idea of
should we build AGI recklessly because
in addition to potentially killing the
current generation we're killing way
more future humans than ever have lived
right I we're talking about not that's
yeah just to just to finish the you know
my side of the argument or like the you
know Steel Man the other side it's like
the number of future consciousnesses
that we are potentially giving up if we
recklessly build AI is we're it's it's
got to be at least 10 to the 20 it's
probably more than 10 to the 30 compared
to like 10 to the 10 humans who have
ever existed right so we're we're
talking about massive orders of
magnitude difference in what we have on
the table that we're about to
potentially WIP out compared to you know
what what's in our experience I mean
this is crazy right this is like a tiny
the the numbers are mindbending and it's
like no let's just cut off that whole
future right so to me that seems uh like
very scary and unwise well see okay so
that's right disagree because you're
saying that we're like wiping out or
killing these future beings that's why
I'm trying to say like no you're not
because they don't exist so maybe like a
good way to put it is you can hurt a
future being in a sense like if they do
exist in the future and you did
something now that's going to set them
up to be hurt in the future that is
something you can do and is wrong but
you cannot kill a a future being before
they exist because they hurt to the
future but they can't be killed before
they like if they never exist a
different category you're explaining a
pretty common stance right so like I I
get your clarifications right I I think
most people kind of know where you're
coming from um on that view but what I'm
getting at my line of argument is just
saying like Okay so today you value 8
billion beings in your moral framework
and I'm just trying to get a sense of
like okay so you basically think that
you're going to value about 8 billion
beings in 10 years but what if we what
if we have plans to colonize the Galaxy
right like we what if we had a concrete
plan like Elon Musk is leading like yep
there's going to be like billions of
people on Mars billions of people on
Saturn whatever there's like a concrete
plan to expand throughout the Galaxy and
I'm just telling you there's a milestone
on the road map that like in the year
2100 there's going to be a trillion
humans okay that's on the road map
there's a lot of plans for it in that
case you're just like H well we should
still treat it as zero yes but I'm
saying like if they do if like if I
found myself in that future I would
about those trillions people like I
guess maybe like to make make this more
concrete like if you and your wife
decide to have a kid and you change your
mind do you think that you have killed
that future person no I I think it's
fine for people to change their mind
about future humans but like just I I'm
just clarifying your claim like you
think in a hypothetical scenario where
we're just planning to colonize the
Galaxy and we're planning to have
trillions of humans you still think that
making a decision whose downside is to
cut off that future is just we should
just wait that equally bad as only
killing the current
generation as only killing the current
generation like the current generation
say is 8 billion so imagine a decision
where you kill all 8 billion but you
still leave the future and you get a
trillion eventually compared to a
decision where you just cut off the
entire future and you're like yeah
they're both equally bad see that that
gets weird because now like from that
you're considering that from like this
outside perspective where in this
picture you've painted it's essentially
like those billion people that like
scenario b or those sorry those
trillions of people scenario B like
already exist in this equation but
that's from our standpoint that's just
not the case like those people in the
future like it it's weird to even like
refer to them because there's not
actually like a refence there they just
they don't exist unless and until we
create them and so I don't I just for
the same reason I don't think it's wrong
to decide not to have a kid when you
were decided to yesterday with your wife
I don't think it's wrong to decide not
to create any number of trillions of
people that we were planning to
yesterday but it is wrong to plan to
create them and then plan to hurt them
once they created because that that's a
very different scenario going on there
okay so let me give you this thought
experiment so what if um I I'll give you
five bucks if you press a button that
applies a bunch of radiation to
everybody's balls so that they're
sterile right so you just made a billion
people impotent by pressing this button
uh but you got five bucks would you do
it I I would be frustrating a great deal
of currently existing people's
preferences by doing that okay but now
your argument might be becoming a little
circular because I'm somebody who values
future people so why are you frustrating
me in this
conversation
uh so yes you do okay so I you have a
preference for trillions of people in
the future I also have a preference for
trillions of people in the future okay
you're frustrating yourself yeah so I
would agree that in if uh by cutting off
that future you are in a sense doing
something bad by frustrating frustrating
current people's ability I don't think
frustrating preferences is as bad as
like an 8 billion people large genocide
though yeah no look I I agree right I'm
sympathetic to the idea that like I I
don't go around knifing people or or you
know it's and but this gets into the
category of arguments of like
utilitarianism is full of tricky edge
cases right like Pascal's mugging for
example right is a tricky Edge case like
there there are a lot of tricky edge
cases don't get me wrong it's just that
utilitarianism also gets a lot of things
right and when in doubt you can always
Retreat to like really there's like the
good side of utilitarianism which is
like hey if I have a limited budget
should I donate it to like build an
inspiring mural or like feed a bunch of
people probably feed a bunch of people
right like stuff like that where it's
like you sometimes utilitarianism like
really dovetails with common sense in a
very strong way and I think giving some
consideration to trillions of future
people like a significant amount of cons
you know a sizable amount of
consideration but not so much that you
go on a stabbing spree like if I
literally add a button where everybody
today dies but then a trillion future
people live that one would be really
tough I don't think it's super obvious
that you have to press it I think that
gets into the thorny edge cases you know
what I'm saying because of a number of
reasons like it's the sin of commission
like do I really want to be the murderer
that violates a deontological principle
so like I agree that gets into like the
tricky Zone but I think you are so Keen
to run away from the tricky Zone that
you've like really gone to a weird place
well it's just like I don't even think
it's really a tricky Zone I think
there's all this comes up just like all
the time like for example just like a uh
an abortion and also just like the case
I mentioned just like deciding not to
have a kid like like would you just say
that because of your view are you
entailed to think that if you and your
wife decided not to have a kid tomorrow
that you've done something at least a
little tiny bit wrong like is at least a
smidgen wrong in that case I don't think
it's wrong right it's it's quote unquote
wrong in the sense of like hey I didn't
max out my charity donation today right
so like there's all these things where
utilitarianism kind of points you in the
right direction and to the extent you're
not maximizing going in that direction
you're wrong in a sense right but I'm
not somebody who hyper optimizes
everything in my life for like maximum
utility and of course there's all kinds
of tradeoffs like you know should I
let's say I think eating meat is morally
wrong but I think eating meat energizes
me and makes me 10% more productive so I
can do more charity right so all of this
stuff is in the tricky Zone and I'm not
here trying to be like here's how to
navigate the tricky zone I'm just saying
that a button where you build AGI
recklessly and cut off trillions of
people in the future I'm very scared of
that button I don't think that we should
just like feel comfortable pressing that
button because future people don't
matter I mean I hear you I just I don't
know like I think it's like there's a
very big difference between Aesthetics
and like ethical respon like more
responsibility I just don't think we
have any ethical responsibility to
create trillions of people like I don't
think that there's any like we have a
moral responsibility to like fill up
Mars with people okay we don't have a
moral responsibility but you want to do
it I want to do it most people want to
do it so why not just grant this as like
a moral thing because I want to eat well
then you should then you should eat pie
and you should create a trillion
humans but I don't think should make it
an a moral obligation for like you it's
not a moral obligation but we all want
to do it and now see so you're basically
admitting you want to do it we I think
we probably agree most people want to do
most people want to colonize the galaxy
in the future and yet you're also out
here saying like look if I press a
button and that never happens that's
fine no it's not fine if it never
happens we want to do it so okay well I
will agree with that what I'll say is
this if most people and I'm not sure
what the majority would need to be but
like if a extremely significant majority
of people would be willing to say hey I
am willing to give up uh my potential
immortality for this aesthetic
preference for billions of humans
existing in the future if most people
agree to that then I'd be like yes all
right let's let's do it and I'm I think
I would probably even agree to that
myself probably and even but if if
everyone agreed to that I'd be like yeah
okay cool pause AI if we've all agreed
we took the vote pause AI but that's a
pretty big ask to give like to ask
everyone a lot all eight billion people
to give up their literal immortality for
the sake of this like aesthetic
preference for a future filled with a
lot of people
right so like I I agree that you have a
point which is saying like hey if if
people alive today had to agree to all
go six feet under in order to enable
this amazing future with trillions or 10
to the 20 like like I agree again tricky
zone right but the reason why I saved
this whole part of the conversation till
the end where we talk about whether
future lives have value the reason I
saved to the end is because I still
think even if you don't think future
lives have value even if you do think
future lives have value I still think
that the policy decision about whether
we let a Labs build AGI today the policy
decision still just all comes back to P
Doom right which you said yours is 50%
right but I still think okay let's say
future lives are worth trillions or 10
of 20 okay great should we build AGI I
still think it's like if your P Doom is
5 to 10% or more probably not but if
your P Doom is like 1% you can probably
go for it and it's fine I don't know I
mean if we're really because if we take
that seriously I do think you kind of
get like you know Pascal mugged like I I
think even like 01 % is not okay to risk
if we're really considering the
trillions and trillions of future but
you notice I'm carefully avoiding
Pascal's mugging because like yes there
is a Pascal's mugging that you could
make or or potentially a Pascal's wager
um as a technical distinction I think it
only counts as Pascal's mugging if you
use like hyper exponential quantities
otherwise it's merely Pascal's wager so
you know 10 to the 20th is merely an
exponential it's not a hyper exponential
it's not 10 to the power of 10 to the
20th that would be hyperare exponential
but so anyway so Pascal's wager would
say that
even if your P Doom is a mere 0 1% you
should still never build AGI until you
can make it super safe because after all
0 1% times10 20th of lives lost is so
bad right so that would be a Pascal's
wager style of argument and a Pascal's
wager style of argument is just
characterized by me trying to override a
super low probability by multiplying it
by a crazy high exponential utility and
I'm not doing that I'm explicitly
conceding like look if your P Doom is 1%
that's okay that going to kill a
quadrillion lives that's fine just go
for it anyway I'm conceding that I I
only take positions that I think are
very strong and the position of like we
should stop AI progress even if P Doom
is less than 1% I don't think that's a
strong position I think that's a
plausible position but it's not a strong
position are you saying you think you
have a good reason to escape Pascal's
wager in this situation like I want to
consider these trillions of humans but I
want to ignore the implications that
that has for our reasoning process uh no
okay let me be very precise so if I were
to try to convince you that P Doom of.
1% is still worth pausing AI for then
the form of my argument would be a
Pascal's wager because it would involve
a small probability multiplied by a
large utility that's not what I'm doing
in this conversation my claim my
original claim in this conversation is
that because pdom is higher than 5% and
a pretty high pdom multiplied by a high
utility is high therefore we should stop
people get confused because they see
really high utilities and they're like
aha a High utility means Pascal's wager
no a high utility that's just a bonus
that just makes my argument stronger but
I'm not multiplying it by a low P Doom
I'm multiplying it by a very sizable pom
the fact that I can use 10 to 20th on
the Utility side is just a bonus but I
would argue for it even if it's only the
8 billion people alive today gotcha so
you're saying that you would be in
Pascal's wager situation if you had a
very low P Doom but you don't so you're
not trying to use like a Pascal's wager
form of argument but it brings us into
this situation where the Pascal's wager
becomes possible and that's like not a
fun bullet bite but you're just saying
I'm willing to bite that bullet and
ignore it because that's not my position
no no no but I'm not biting a bullet
right cuz it's like I agree some people
out there have a low P doom and they're
coming in and they're saying pause AI
anyway which funny enough seems like the
mirror image of your position which is
you have a high P doom and you want to
continue
AI sure so maybe if I can find somebody
like you maybe I can go find one of
these Pascal's Wagers but I'm coming in
and I'm not saying anything about what a
low pom implies I'm just saying pom is
high and that's why we have to stop AI
right no I I I think that what you're
saying there makes sense my my only
condtion is I'm trying to suggest that
it this is another reason that we should
not be considering these trillions of
future beings because it would lead us
to it could lead us uh to this weird
pasal situation I don't think this is a
problem for your
specific sure well I I agree that if I
were to try to bust out the you know the
hyperexponential if I were to say you
know what which is a plausible argument
you know what AI could discover
wormholes and different laws of physics
you know the Multiverse is probably a
real thing and the concept of
hyperexponential
senen is not czy right there could be Hy
exponential senen absolutely but I'm not
even going there because again once you
get Doom of 5% plus a mere 8 Billion
Lives is already I would consider pretty
high stakes yeah I agree I'm just saying
that this is just a further reason that
I don't think we should be considering
trillions of people because it does lead
us to these to these weird uh issues
with rationality yep fair enough okay
great so I I think we we pretty much hit
on the key points that we came here to
discuss so synthesize it and give me
your closing
statement okay sure um so essentially I
think that if I had to summarize uh I
would say my P Doom for AGI going wrong
is somewhere around
50% uh my P Doom for us failing to
coordinate on and also have the tacal
side of cryonics workout is something
higher like I don't know
75% uh therefore are the best chance for
people live right now is to go full
steam ahead with AGI or if not full
steam ahead pretty fast um and then the
other side of my argument I would say to
summarize is that we should not be
considering future people because future
people don't exist you if you try to say
that they exist you get all sorts of
problems with even just like telling
your wife that you you don't you're not
in the mood one night or you could be
convinced to throw a kid in a volcano to
have 10 kids born Elsewhere for all
those sorts of arguments I would say
that we can have Aesthetics preferences
for future possible hum but we shouldn't
have moral obligations for future
possible humans okay I'll say my pece uh
so we both agree P Doom is quite High
yet my position is the obvious one of
like okay let's try to slow down AI
development the thing that we both agree
could extinct everybody and let's try to
figure out how to proceed with caution
or even not proceed at all since again
we both agree we might extinct everybody
whereas the alternative of saying like
no let's proceed because everybody is
too dumb and incapable of signing up for
CICS and waiting but like we we just
have to proceed for their own good and
we need it in 10 years because it's
available now right like all of a sudden
because we're so close we have to like
get to the end we have to do the Last 5
Years now we have to press the button
now even though it's a button that might
be a 50% button of ending the whole
future like let's just press it that
just seems like a very weak position to
me like I really think you've kind of
lost the battle the moment you admit P
Doom from AI is crazy high like I don't
think you're going to get a 50% P Doom
from any other
path
um here I I'll give you another minute
if you want to take the floor because I
want you to have the last word oh thanks
uh that's fair well I would just say
that I don't think it's because
everyone's dumb I think it's just a
larger coordination problem going on uh
and I my only other like closing remark
would be yes I think P Doom is high but
again if we're only considering people
that are alive today our P Doom without
AI is 100% it gets a little bit lower if
you consider the possibility of
widespread cryonics but I don't think it
gets as low as the P Doom for AGI so
that's why I think we should go that
route okay great so it also sounds like
in the course of this debate your
position hasn't moved much or has it
moved
significantly h no I wouldn't say it's
moved much okay well I wouldn't say mine
has moved much either but we found a
point where we agree which is that if
both of our P Doom is let's say 10% from
Reckless AGI and also both of us are
skeptical that we can make CICS work
even pulling out all the stops so let's
say probability of cryic working is only
10% and let's say we agree that future
Generations don't matter and it's all
about just rescuing the 8 billion people
who are on the way toward dying today if
you assume all of these preconditions at
that point then I think I agree okay
screw it go for reckless AGI except I
would still argue let's try to take more
like 20 years with it let's not try to
do it in five or 10 years because let's
try to maximize the chance of AI Safety
Fair yeah I think that's fair and I will
also say that uh thinking about it more
I will I do think that I have one point
of concession that I've changed my mind
on a little bit it's I do after this
conversation think that CICS is a bit
more of a viable option a viable path to
actually try to pursue uh than I did
before even like at at a a global scale
maybe that is something we could work on
and the only thing I will say uh to what
you just other than that is that I would
say that it's not just if um P Doom for
AGI going bad is low it could even be
high as as long as the P Doom of cryic
not working out is even higher so like
if our P Doom for AGI is 60% but our P
Doom of like cryic just not working out
for new one is 75 we should still do AGI
in that case if we again if if if you're
right if we don't care about the the
future people in that
scenario if you thought it was 70%
likely that after this conversation we
can rally the world to get most of
Humanity on cryic and also cryic had a
high chance of working then would you
just be like okay great let's pause AI
for now yes I would wow okay great so
more more agreement actually than it
seemed initially yeah um great yeah I'm
glad we came around to that okay great
that's probably going to be one of the
higher levels of agreement on Doom
debates probably oh yeah man okay great
yeah thanks so much for doing this and
uh thanks all of you for watching Doom
debates thanks again
[Music]