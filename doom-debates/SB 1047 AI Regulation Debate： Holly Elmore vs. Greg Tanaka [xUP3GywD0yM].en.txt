welcome to Doom debates today I've got
two special guests debating Pro versus
anti the SB 1047 bill on the pro side
we've got Holly Elmore who represents
the Pai organization which I'm actually
part of myself I support p and on the
other side debating anti sb147 we've got
Paulo Alto city council member Greg
Tanaka welcome guys thank you Lauren
let's kick it off with opening
statements both of you I want to hear a
little background so introduce yourself
and give us an overview of your position
on SB 1047 let's start with Greg Sean
yeah so I've been serving on the Paulo
city council for about seven and a half
years I've also been a technolog space
for some time I'm actually in AI space
so I'm entrepreneur in the space and I
think SB 1047 has a lot of good
intentions but I think it's
fundamentally flawed I think it's it's
basically a a solution looking for a
problem there's been a lot of sci-fi out
there with cator and things like that
and and I think that's been the
inspiration for this bill is all the
Sci-Fi fantasies that that people like
have nightm about but the reality is is
that
sb147 is trying to solve a problem that
doesn't exist my dad when he retired he
he was a Hollywood extra and he was an
ER and Austin Powers and a bunch of
different movies and TV shows and one
one of the movies he was in was a movie
called Minority Report and basically
1047 is like The Minority Report where
basically in this movie you just haven't
seen it the movie about precox where
basically these people were predict
future crimes and you get arrested
before you even committed the crime and
the FB 1047 like that it's forecasting a
Terminator like like Arnold sh or
Terminator like future where you know
all these people in Hollywood who have
no clue about AI dreamed up possible
nightmare scenarios so that people go
buy tickets and and it's like a precog
moment where where they're basically
trying to predict that AI is going to
cause a reparable damage to society and
so it needs to be nipped in the bud even
before it's done anything and so it's a
total disaster so train wreck it's
something that could be stopped
immediately and I'm shock that in the
senate pass I think what 33 341 or
something like that it flew through the
Senate it's probably to pass through the
assembly as well unfortunately but I'm
hoping that cooling Minds will prevail
and better legislation will be set
for great okay Holly your background and
opening statement in favor of
sb147 uh so I'm pulie eler and I'm the
executive director of the Grassroots
organization
paius and we're concerned about harms
from super intelligent Ai and even the
potential for extinction from super
intelligent AI so the scenarios that
you're talking about that's what we're
concerned about but it's not just us Sam
Alman Jeffrey Hinton Yan musk hundreds
of other industry leaders signed a
letter last April asking for a shorter
pause we're asking for an indefinite
pause because they recognize this danger
and we're concerned that we're not going
to have the chance to wait until AI has
done something to do something about AI
so we are very proactive and want we're
strongly in support of s SP 1047 as a
start to the sort of Regulation that we
need to make sure that AI Innovation
happens
safely great thanks Holly to kick it off
I think we can get a shared reality on
just the basic facts so let's go over
the basic facts I've copied a
summarization from SV MTZ who writes a
really good weekly overview of all the
AI news so we're going to go Point by
point and uh I'll just read it out and
explain it and then I'll just check with
both of you that I'm accurately
representing what's even in the bill
that we're here debating the first point
that is maybe the most important is just
this idea that the bill is trying to
have a relatively light touch and only
go after the larger players specifically
it's saying if you don't train a model
with $100 million worth of compute and
you don't fine-tune a $100 million model
with 10 million inut if you're not
spending vast amounts of money on
training or fine-tuning then this law
doesn't apply to you would you
agree yes I think the issue though it
there's this arbitrary limit of 10 to 26
and there's been I think the last
amendment was August 22nd right maybe
this later one I don't know but I think
August 22nd is the one I was looking at
and and I don't know if you guys know
but a lot of these models are derivation
of each other so like for instance like
llama 3 3.1 I think was 15 trillion
tokens and a bunch of I forgot how much
but what happens people do a lot of fine
tuning on top of it does that that 10 to
26 does it start with the foundation
model does ites and there's multiple
fine tunes that people do on these
models so where where like so for like
my we we do find tuning too so let's say
we use a model that is at 1020 26 and we
find tune on top of it does that mean
we're also liable we also be worried
about that if you're just a startup and
you're just trying to fine-tune and
you're doing a few rounds and you've
raised a couple million dollars it
should be impossible for you to go under
this bill if I understand correctly
right because the bill is very clear you
have to spend 10 million plus on your
fine tuning project for this to even
apply my last start was an all company
we didn't raise a lot of money but if
you look at the cost to find tun it's
pretty expensive right you have to buy
you have to rent a b of h100s and my
last startup we easily half a million
dollar bill a month is Not Unusual right
to fine tune it is definitely within
reason so I mean you look at all the
startups out there you know they could
spend
easily you spend in a year perhaps even
more $10 million right and over the
lifetime start up doesn't have much
funding they can spend $10
million talk about not but even then you
know right now 10 to 2016 seems like a
lot way back when maybe 20 years ago or
so there used to be this idea that
encryption is Munitions I'm not sure you
guys heard about this but so if you had
strong encryption it was basically
equivalent to a bomb and and it's was
illegal to have powerful encryption but
thankfully they got rid of it because if
they kept it basically what would happen
is you know in this conversation which
happening through encryption Gmail your
bank account everything you use on the
internet would be impossible and and and
people thought oh this encryption limit
is something that no one ever able to do
but I'll tell you my my wristwatch does
encryption more powerful than that law
so that law set back I think it was in 7
80s when it was set people said oh
there's no way people would have to hit
only only state actors would be able to
hit this encryption limit but today my
ristl has encryption more powerful than
that and and people if you said oh yeah
this encryption is munition people would
we think you're bongers right makes no
sense so 1026 today seems like a lot but
if you look at the curves GPU power is
going up exponentially so it doesn't
take much more to hit that 1026 I'm
talking about 5 to 10 years that 1026 so
I just checked the bill summary we we
can have the argument about flops but it
looks like it's a minimum requirement
that you have to spend 10 million so
even if it becomes comes really cheap to
have a lot of flops it's you know it's a
essential requirement so I inflation
like since Co right I think I think the
dollar is worth like 30 40 less so yeah
$10 million seem like a lot now but with
the rate inflation be $5 million in a
few years in a few more years it's maybe
$2 million and a few more years it's
worth $1 million
right with more slops so maybe there
should be we should be looking at
machines that powerful like it's
arbitrary it's not something that just
continues to be harder for business only
it also because they're making more
powerful technology and guys before we
debate too much I actually just want to
go back to just establishing the basic
fact so what I originally went over was
um that this bill requires you to have a
$100 million in compute spent
essentially being a large company if you
want to if to be eligible for being
regulated as somebody who's created a
non-derivative model so that's pretty
high threshold there $100 million and
then there was also a $10 million
minimum threshold if you become eligible
for having responsibility for
fine-tuning a model just going back to
an encryption example if you had strong
encryption it's Munitions it's illegal
right and you could be set to goog for
having that too strong right and it was
such a ridiculous bill or or law that
basically by the early 90s everyone
violate the law and pretty much like
every single electronic device like PS
anything with s for HTP uses encryption
stronger than that 10 10226 seems like a
lot but it's not a lot in a few years so
so that that encryption
bill I think it was only like seven
years or eight years after it got
enacted pretty much every device started
receiving it
because because you guys heard of mors
law right it goes exponential it's not a
it's not a linear the harm of it is that
let's say if we didn't have encryption
basically you couldn't Buy stuff on
Amazon so what happens as a result is
that it could also be used for political
purposes if they don't like you then
they come down hard on you but basically
if you use powerful encryption which is
like HPS then basically you could be
locked up you could be fine you could be
you could be taken down and so it's a
really that's not the that's not the
America is supposed to be the country of
the free right it's not supposed to be
the country the state controls
everything right so I've skimm through
speed sumary again and I I think I have
some ground truth here on what happened
so originally we were talking about 10
to the 26 flops we were trying to We The
Regulators were trying to find a
threshold over how they're going to make
the bill apply and 10 to the 26 was
controversial part and Greg spells out
some reasons why it's controversial so
what happened was they layered on the
additional requirement the $100 million
requirement so yes the 10 to the 26 is
still in there but you have to also
spend 10 to 20 so if you think that's if
you don't like 10 to the 26 it doesn't
really matter because most people agree
that 100 million is the real the main
threshold that's in the bill right now
how about we agree on all the facts and
then I'll call on you and you can talk
about inflation and make the point you
want to make okay all right to summarize
the bill it only applies to companies
that spend 100 million plus to train an
AI model or to companies that spend $10
million plus to fine-tune one of these
large models companies that don't spend
that much money to build or fine-tune a
model they won't even be covered by this
bill do you guys agree that's what the
bill says there's a lot of here right
because fine tuning there's lots of
steps involved is it does it say fine
tuning though or so what does fine
tuning mean so what does fine tuning
mean because fine tuning doesn't mean
you're acquiring the data does that mean
that you clean the data does that mean
you're verifying the data does that mean
you're sing up all the computer
verification environment does it mean
like what is fine tun and ENT it's a
good question but the bill talks in
terms of fine tuning I think I know but
but it's very ambiguous as what fine
tuning means do we agree it talks in
terms of fine tuning we're just
establishing what we do agree that it
says and then we can have these
conversations even within my own company
we talk about fine tun I think it's not
so clear where does that fine tun begin
and end and because it's unclear it's
hard to know when do you hit that
109 yeah it's a good point but like
Holly saying I just want to separate
going over what the bill says with going
over why you think it's unclear and why
you're objecting to it okay because a
big cost of fine tuning is actually
acquiring the data getting the data said
that's like a a huge amount of the cost
is actually just getting the data right
probably 90% of their budget is actually
spent getting that data and then you
spend a bunch of more money right so the
whole fine tuning process is actually
pretty expensive so where so I why
shouldn't you include the data I think
this isn't we should just be focusing on
the facts here but why the data makes it
a more powerful why it makes it better
right it increases the abilities of the
model I would argue that could limit
here could already be hit today by law
of companies he start depending on how
fine tuning is
Define right but we agree that the bill
says that the limit is $10 million spent
on something we call fine-tuning and 10
10 to 25 in or FL in or FL Point
operations but in operation there's like
a plus b and then there's J integral
those are both operations so which one
are we talking about here are we talking
about convolution or talking about n
right so that's also very
ambiguous all laws tend to requires some
interpretation and we do manage to to do
that if you look at some operations they
could be a thousand times different
right three Ora different easily right
or more okay let's let's go over the
other points all right so in addition to
the 100 million and10 million thresholds
there's language about uh critical harm
defining it as 500 million plus in
Damages from related incidents or mass
casualties so the bill is going to be
regulating these kinds of critical Harms
specifically it says if you train a
model with $100 million or more in
compute you need to have a reasonable
written plan you need to have a Safety
and Security protocol for preventing
unreasonable risk of critical harms
which is remember 500 million in Damages
or mass catties you need to have this
written plan for preventing unre
follow it and publish it with redactions
and your safety test results so that
that's what the bill says
correct roughly yeah so by the way I
looked up I looked up here if the
definition of fine tuning was in here
they actually have a definition of fine
tuning but when I look at it it's
incredibly ambiguous right it's the
scope it says okay the fine tuning is is
literally just one sentence right if you
talk to anyone that does fine tuning
it's hard to find in one sentence
because it means so many different
things to so many different people it
says fine tuning means adjusting the
model weight of a covered model or
covered model dtive by exposing to
additional data my God where does that
begin and end You could argue everything
is involved is a problem that it's
unclear who's covered by the law or is
it that you don't want people to be
covered by the law do it seem like two
different issues right is it would you
be happy if the threshold were higher
would you be happy were clearer or it
seems like you don't like the you don't
want there to be this the problem is La
so I'm I'm a lawmaker myself Council and
what's really important when we make
laws is that they're clear because if
the laws aren't clear like we have this
crazy law in Paul Alto about you can't
UT all your car do you know what me
times we've actually enforced the law of
not idling a car zal times it's been
around for six years now we've never
ever enforced it once because it's such
a freaking bad law like can't enforce it
right and so we have this thing here on
on Section J here for fine tuning that
this definition of fine tuning is crazy
ambiguous nobody's know you can never
really enforce it you don't know what it
is all right let me just get the other
facts out I'm almost done okay just tell
saying what the bill says okay it says
if you fail to take reasonable care
meaning a serious effort at writing and
following one of these Safety and
Security protocols and you're one of
these $100 million plus companies
training this large model then there may
be an injunction to stop you and if you
furthermore cause critical harms 500
million plus or mass casualties then
there's also a fine okay is that a good
characterization of what the bill says
roughly
yeah okay great almost done next thing
whistleblower did they get rid of the
criminal liability I think I saw it or
is it still there they don't require you
to they don't require the compliance
officer to swear under oath that the
test that the safety plan is thorough or
something like that now they just have
to affirm it yeah that's what I thought
that was the only possible criminal
penalty that people could have been
referring to yeah all right last couple
points almost done whistleblowers get
protections if they expose a potentially
dangerous model even before it causes
critical harm employers can't block
employes from reporting to the attorney
general or the Labor Commissioner so for
these these kind of whistleblower
scenarios and finally compute clusters
must do know your customer kyc on
sufficiently large customers okay that's
all I got in my high level summary does
anybody think that my high level summary
is factually inaccurate or can we start
going into objections I think there's a
lot of and one of the biggest issues
with this bill is that it's incredibly
ambiguous so from the definitions from
what is a plan I I think it it's
throwing mud on the windshield can't see
where you're going right it it's it's
ritten by someone that doesn't really
know what they're doing that's the
problem okay let me ask you guys this to
just to understand your viewpoints
better what is your dream scenario for
s1047 and any subsequent regulations
like how do you see a good regulation
road mapap I'll start with Holly my
dream is that this is the first layer
this is the this is very light touch
regulation very high ceiling for harm
that it's trying to prevent and then
from here we're able to get the level of
Regulation that we need to move forward
safely with AI I want to see AGI and
Powerful AI continue to be developed but
I want to do it only if we know it's
safe and we can only do that with real
external regulation not the AI companies
grading their own homework but external
regulators that they are accountable to
coming in and monitoring their progress
at having recourse if they're doing
something dangerous or if something
happens real full participation of both
the government and the AI companies in
building these models safely that would
be my dream how about you Greg ideal
regulation road map yeah I mean my ideal
uh regulation road map for SP 1047 is
that it goes down in flames that it
doesn't pass the assembly or probably
hopefully the governor Hees it I kind of
view spb147 like a paded cell for a
sedated toddler right it's basically
going to kill all innovation in
California and California unfortunately
tends to leave the country so probably
the rest of stat is going to adopt
California and what it's going to do is
going to push out all the AI companies
like mine and others somewhere else
would you leave yeah I want to know what
you would do we would have to move
you'll leave California yeah we would
have to move you stop being a councilman
oh I'm almost change your residency or
have change profession yeah be there'll
be significant liability yes have
to okay that's a strong
statement it's a disaster so if you look
at who's opposing they have this uh
ledge tracker thing right you look at
all the people post it's something like
nine out of 10 people if you look
at on the ledge tracker webs I can send
it to you later wait which people oppose
it which it talks about who's opposing
and who's supporting and opposing 77% of
Californians are in favor of actually
the strong form of the bill from before
the Amendments I think I think most
Californians have no idea what's going
on but if you when you give them an
explainer of what's in the bill 77% in
favor of it although 55% strongly in
favor they haven't written in then but
if you look at people WR who wrote in
vastly
opposed like
9% people who have written in where is
this tracker of people who have written
in oh it's it's where I could send it I
don't have right not s it to but
basically has all the people who opposed
it I'm on there too so you can you see
all the people whove supported and
opposed it and B that R so these are
like people just constituents or
constituents yeah
okay and Greg campaigns against it but
when you pull just Californians 77% are
in favor 55 I think 77% of Californians
were think that they don't want
Terminator to happen but anyone that's
involved this technology fact even Tech
workers were majority in favor in that
poll I I think if I could just get Tech
WT to properly edit an email i' be happy
right let alone take over the world it's
so far from taking over the world but
there's this Hollywood thing going on
this H Hollywood trope that's going take
up the world and kill everyone so your
argument is that AI is just not that
good it's like it's barely getting
started right now it has there's so many
different things that we're trying to
get it to do and it's barely getting
started and yet people it's it's like
this the right Brothers just got the
airplane going and then we're worried
about someone's going to not enough not
enough oxygen in space we're like we're
you think there are genuine safety
concerns about flight so should we kill
the Aerospace industry when the right
Brothers started flying did it kill the
Aerospace
industry most successful example regul
agency there was nothing crazy like
s1047 s1047 is basically the precox
basically saying that because one day an
airplane fly to their Twin Towers we
should ban airplanes that's crazy
there's no law like that 1047 I want to
clarify is there any regulatory road map
that you would support or you just very
much kill all regulation in the whole
Space well I I think if it's useful harm
that's one thing with the problem with f
FB 1047 is that it's Hollywood inspired
inspired by Minority Report it's
inspired by Terminator it's inspired by
things that are totally fanciful and are
not rooted in reality it's basically
trying to have predict the future if you
get the future wrong you're and that's
andu they are just basing their opinion
on SFI you think what' you say all right
Jeffrey Hanson and yosua Benjo that you
just think they're basing their on
sci-fi basically yeah it's Fierce right
it's Fierce that there's been no proven
there's been no proven harm so let's
think about this they're the most
respected experts in machine learning
there's a lot of respected experts but
there's a lot of respected experts and
on
who going on right now is so we met when
the printing press was invented the
automate ire thought it was not good
because it Bic SP knowledge and a bunch
of people who know how to write they
call them CLE out of business and so
they banned PR press and what that led
to was in in Europe the P went
everywhere and that led to Renaissance
and an otoman Empire nobody's heard of
them anymore right they went out of the
G of way and there's never been a time
in humanity when more intelligence more
knowledge was evil anytime there's been
more knowledge more intelligence the
productivity of people increased the
freedom of people increased the living
standards increased there's never been a
time except for now when they're trying
to ban this is trying to basically allow
people to become smarter with these
better models this is basically trying
to push us into Dark Ages right this
part is trying to turn us California
United States into the Ottoman Empire it
is craziness right on on a Hollywood
fear that's that's not proven in reality
at all so there totally
fkers I'm curious if you think there's
any technology or any path that we could
take that would be dangerous to us when
there's a danger then let's address it
there's already laws against speak to
people there really laws against using
for harm let's just use those laws this
one here is exceptional law because this
law is basically forcing someone to
predict the horror right it's it's a pre
you have to be a pre Minority Report is
a fictional film that my dad was to in
it is not reality is not
possibley Dr down more into your
nightmare scenario here because yeah
you're you're using strong language like
this is bad for Innovation it's put
Califoria so I just want to drill down
nightmare scenario and specifically if I
heard correctly you mentioned that you
would literally move out of the state of
California not just me of people move
out right so Elon must remove XI out I
would bet you right now I'd be happy to
put down money on whether you'll leave
California if this bill passes so just
do you want to reiterate that you will
literally move out of California if this
passes it's that bad and if so then can
you just go into detail of what you're
expecting well I I I think people would
have to change professions that's what
it means right it means that if you
didn't want to leave California you
would have to to you would have to do
something different right you would have
to not train models right you would have
to not you think that AI training would
not continue in California no no it
would not the employees live here their
families are here you don't think that
would be a greater cost you think the
cost will exceed the cost of changing
states this is Extreme enough bill that
I think thei Industry California would
be toast right I think anthropic open a
meta dropping supports the bill mildly I
think didn't opening air also oppos it I
think I think openi came out against it
yeah a couple days ago and and and braic
said that they thought that it would be
a feasible burden for them yeah I I
think so this is what they call also
called regulatory captur the leaders
they they have to want more regulation
because basically they put a wall around
their business and they could have arm
explain open AI then or open don't want
to want regulatory capture maybe I'll
talk you talk I can't question when I'm
talking okay go ahead Greg okay so uh so
you know so I think that by by pushing
atopic first they more leaders right now
by pushing for this regulation they
basically put up a wall they put a
remote around their business they rais
billions of dollars I don't know 50
billions some crazy amount of money and
so for them to hire Army of lawyers to
do a bunch of paperwork no big sweat but
little startups forget about it right if
you're afer forget about it right you
don't have time to do all this paperwork
right it's hire like loers who were
charging like $2,000 $5,000 per hour
there's just no way and so it sounds
like you're having it a lot of ways
you're saying that the big competitors
although the big guys would leave
because it's so onerous but also they
want to make a mode around themselves
and so they want the bill to pass for
that reason it I don't know it's just
not consistent no those two things could
be consistent both be true right they
want so so so it's natural like you look
at Tobacco industry you look at um you
know just about any like business like a
lot of insurance they push for like
regulations because basically it keeps
out startups right it touchs them down
because the startups can't afford the
regulation so yes big companies want
that and with big companies would AI
companies move out yes Elon bu shortly
after 1047 start he decided to out X AI
right one of preent AI companies in
California so yeah absolutely both
comp why do you think that's
it's already happening right so many
other Industries where there's been
regulatory capture right where
these it seems like the story changes on
the flag actually andic supports it you
say oh then it's regulatory capture know
so the big guys they do they don't
support it because it's going to be
economically own to them both are
happening companies are moving out and
it's regory capture yes both are and
both are happening right now and both is
and regory capture happen for so many
different energy Telecom you look at you
look at tobacco right definitely all
happen so Greg how does regulatory
capture explain why open AI opposes it
while anthropic supports it so I think I
think I think it depends on truly they
think the leader is so if you look at
the leaderboard right now Claud son 3 uh
3.5 is actually top of the leaderboard
right and it's been a top depend which
leaderboard you look at it's near the
top I think open the ey they've been
talking about gp5 forever it hasn't come
out yet and anthrop has gained a lot of
open employees and so anthrop feels like
they're King of the Hill right now they
feel like it's okay for them I don't
know I don't know I that open AI doesn't
have the money to also benefit from a
regular they're number two if by any
means maybe like and brought they're
pretty much neck and what you guys
should do is invite Sam wman and talk to
him I don't know I haven't talked to him
but I I can only speculate but I know
he's been pushing for regulation some
all has been pushing pushing for
regulation but the this regulation is
the the fundamental promise of this
regulation is it's the minority it's a
precog of laws right it's it's the only
law we have to forecast to forecast
that's not true all negligence laws are
about forecasting harm all laws to NE
about the expectation of harm we take
terms talking do you want to talk and
then I'll be Qui to talk or yeah I
should get your talk we both talk I
can't respond to you when you talk so we
gotta take
turns okay so I think Greg pushed back
back and he said not all laws are
predicting the future of negligence
sorry Greg did you finish your
question no I I was interrupted but I
could keep going or if you Holly wants
to talk you could talk now yeah I was
saying all liability laws are about all
laws concerning negligence are about the
expectation of
harm okay do you want me to respond this
is not the only law that's asking people
to predict whether something they do
will cause harm go ahead Greg okay this
is the only law that you have to if
you're liable for unforeseen unforeseen
damages the problem with most of laws
there's evidence the reason why the law
was there was because someone defrauded
someone Someone raped someone someone
murdered someone so people know it's a
bad thing and so there's a law for
because there's a bunch of ground right
there's bunch of there bunch of
instances have happened hijacking right
there's bunch of things have happened
where there's known reasons why you want
to law against it this one here is
totally different this one here is
open-ended it's basically it's B
basically I it's like The Minority
Report it's the precog of laws right and
so that's why it's crazy ridiculous
because essentially basically you're
asking someone to forecast fut to harm
and it is Bonkers right with which with
it's okay that's what liability
is whatever happens you're responsible
there's different standards of liability
but this is what liability is you can't
just say like I kind go wrong want to do
it Holly is there another comparable
example to what Greg is saying where the
exact type of harm caused had never
happened at the time that the regulation
was made sorry the entire standard of
strict liability is about that it's
about unforeseen so no matter what
happens unforeseen harms that's on the
manufacturer like they have to foresee
it or they take responsibility for what
happened there's an entire standard in
tort law of liability that means that so
it's not true that there aren't other
laws like
this yeah okay so I guess you're saying
like if you make a product and we don't
know all the ways the product can harm
the consumer but we ask the manufacturer
to take responsibility for any type of
harm is that what you're saying yeah
that's what in this case if you have
strict liability that's what it means
something goes wrong it's on
you okay respond to that so Holly can
you tell me of another
technology where where basically
uh you have to forecast and you have to
be liable for all future harm of that
technology can you tell me an example of
another one where there's a law like
that where it's a pre-call type law you
probably don't think of it as like
technology but like what's there's a lot
of things that have to be precisely
manufactured to be safe no I'm talking
about technology this is not a product
this is not a product law this is a
technology law how do you define
technology then look how they defined it
here right so it's basically any model
any model right that's a very broad
thing that's not a prodct
So when you say Tech like condoms have
to be manufactured to a very high
specification and they have to there's
only there's a defined amount of failure
so H so the analogy here would be that
the the designer of the original condom
and five designers down who took
derivations of each other like all all
that whole chain of people all liable
that's what the not not the manufactur
the designer because that's what this is
saying right because this is these are
just they're not physical products a
good example would be something like
weapons like they if you're making
weapons and they supposed to have
certain safy so they don't go off that's
not a technology that's not a technology
how in what world is a weapon not a
technology what is it like that's not
that's not what I think of technology I
think of I can't think of another
technology out there where you have to F
the future on it sounds like you don't
think a lot of things are techn Oles
what do you mean
software Yeah by terms of
service but for instance is there
another law in place that that has a
precog like thing like this 1047 does
for you reject any Act I give you
examples of how this is what liability
law is and Tech has been an exception
for too long and you say it's not
legitimate or it's not a real example of
tech or the fact that there aren't laws
proves that there shouldn't be laws or
something like that when the real reason
there aren't laws is because there just
hasn't enough
time oh I H it like you can't CLE all
that that's know to Temp can you tell me
what technology is then I'll tell you a
lot technology is innovation technology
is how to make someone more productive
that's what it is how we it's an idea
it's not a tangible
product technology is ideas okay sounds
like H saying that we we want to look to
examples of how we regulate bombs or or
nukes we want to look to that industry
for examples and then Greg is saying we
should look to the consumer technology
industry where we don't have that same
level of stri Regulation and it's in the
middle of these two it's like bombs like
a nuclear bomb right kill millions of
people there's a real clear evidence
that there's harm there right how many
people have ai K how many people have
had AI
killed I mean arguable how many lies
have it saved right right now I think I
saw Google came out with Alpha right
think about all drugs that we discovered
think about all who have cancer that be
about the bioweapons that can be
developed now in the wrong hands these
are dual use Technologies bons can be
developed even without
AI so can drugs just not as easily yeah
that's the whole point you took credit
for it when it was like a a drug it's
also you have to take credit for
the with everything there's always a
potential harm and a potential benefit
an AI it's 99% benefit and 1% harm
potential harm that's you're just
asserting that we don't know
Dev let's try to be a little bit more
concrete about hypothetical scenarios I
want to do it both ways so with Holly I
want to ask can you be more specific
about a future world which is your
nightmare world if we don't have a
regulation like s SP 1047 and then
afterwards I'll ask Greg what's a
specific nightmare scenario where you
have to leave California because it's
just so bad with all this regulation
like specifically you're trying to do
something and it doesn't work so I want
to ask both of you about your nightmares
starting with
I mean my nightmare world is something
where we're not properly there isn't the
incentive to do the proper safety
testing to have the proper safety
planning there's not the accountability
to proceed safely and we take look at
how much destruction computer viruses
have already caused that dumb they just
like some lines of code like they can
cause millions in dollars in damage
maybe some have caused billions of
dollars in damage now imagine that virus
is intelligent and it has a goal that
it's trying to reach with that just the
destruction could be huge and so we
really that the nightmare scenario for
me is that we don't start now with
making sure that these dangerous
possibilities aren't nipped in the bud
so that the better the positive we can
get to that world where it's 99 to1
benefit but that doesn't just happen by
accident these are all used Technologies
this is about we're talking about
developing general intelligence like the
ability to achieve goal states flexibly
open-endedly that could mean Lots that's
lots of different world States we have
to think about so we want to make sure
that we're only moving toward the things
that are good for us I want Innovation
but I want us to not
accidentally go fast and go in the wrong
direction that would be my nightmare
Greg what do you think of hul's
nightmare scenario and then tell us your
nightmare scenario yeah well I I think
the fundament the fundamental thing
she's missing here is that there's only
laws about things you talked about like
so if someone uses a virus or virus is
used
to Destro a bunch of computers laws
about that we already have laws to cover
for the scenario she just actually
talked about my n scenario is that
California today is the epicenter of AI
if you look at all the amazing models
that come out this all come from
California and Silicon Valley if you
look at the the S&amp;P 500 a big chunk of
the S&amp;P 500 comes from countes that
didn't exist 50 years ago right these
are new companies these are new
innovation so many people are employed
so much wealth so much Prosperity has
happened in California and as a result
of technology to me the N scenario is
that California becomes becomes like the
Ottoman Empire right that basically
becomes a all the cutting Cutting Edge
Innovation is done outside somewhere
else and California basically we're Left
To Be A Wasteland right after that maybe
we have be a little bit more specific
about the scenario
though to basically I think what
California is what fifth or sixth bar
economy something that if it was its own
country we we would collapse right we
would become and maybe some people want
that right but I don't right I like the
we I get that's I think that's
representative of the anti-sp 1047
position like I get preserving
Innovation and being competitive with
other states I get that but can you just
make it more specific right so let's say
you work for meta or you work for a
start what day-to-day life are you
imagining here so so meure like if you
work for xai basically you're calling
real estate agents in Texas right
because it's moving out California yeah
so it's just but what is it that's
making them move before they decide to
move what are they trying to do that
they can't if you have laws like this
which are ambiguous incredibly punitive
incredibly misguided basically it forces
people to move right you have to move
and one thing about the world today is
that people are a lot more mobile than
before before what forces them to move
though why do they want to
move can I finish can I address you are
evading the question again can you I did
want to ask the same thing of just can
you just make it as specific as possible
because when I asked Holly the question
she did specifically mention the example
of an overpowered virus and I think that
helped make the conversation more
specific so I want to make it specific
on the other side of okay these
companies want to move because what's an
example of something that they tried to
do why would they
move they had a specific plan that they
wanted to do that they felt like they
couldn't so what's an example of such a
plan is there an expiration date on this
law because as far as I could I I can
tell this thing goes on forever so it's
beut to harm it could it's not just one
year from now there's not like a St
limitation I
know it's like a I don't yeah I don't
think it expires yeah so that's the
problem right so you're on the hook
forever and that's the problem can we
make it specific right can we can we
just go through one scenario that's my
exercise here is asking you for
scenarios told you one so the scenarios
that they want to move because they're
confused but no not confused because of
unlimited liability forever for a future
thing that is il
defined when they're reasoning through
this decision I get what you're saying I
think it's a valid point I just would
like to play it out in more specific
details so what would be an example
maybe I can help you out so maybe they
want to release their next open source
model and they don't feel like they have
enough guarantees of safety and they
want to do it anyway and now they feel
like they can't is that a good scenario
sure that's the one but there's many
others okay so let's discuss that
scenario though because I guess I'll put
the question to you if meta wants to
release llama 4 or llama 5 and it's
open- sourced and they don't feel like
they can write a convincing reason why
it's definitely going to be safe then is
it a good thing to just let them release
it anyway I I think what happen I I
think zuberg has done a really great V
of to by releasing llama 3 and all the
other llama models I think what will
happen if the law passes is that they
will probably stop doing that or they'll
try with the development out in
California where are those two things
will
happen and I think humanity is going to
lose as a
result I think we'll be less smart I
think there'll be less Innovation I
think there'll be less cured for cancer
I think there'll be more misery in the
world and so that's not what I want what
do you think about that scenario
Holly which part I guess that yeah
future version of llama where meta
doesn't feel like it can write a a what
was it called a an SSP oh yeah when I
hear that I think I if they can't write
a sufficient SSP isn't that an
indication that we should take a step
back I agree that's a cost to them but I
think the cost the risk to humanity is
much greater if they're putting out
technology that they're not sure what's
going to happen with
it so I I think that's the bill working
as intended it's unfortunate that there
is this externality at all I would like
the externality to be internalized more
by the developer so they can do
something about
it yeah Greg if there's no no regulation
there's no requirement for an SSP what
do you think is just good practice for
these companies before they put out
these
models what you guys are forgetting is
that there's already law laws about def
farting someone about crashing computers
about killing anyone if the models are
useful hars they already laws to address
it so it's not it's not like this this
is what I'm saying and this is why I
asked po BL point out a law that that
that b is similar Tech
and there isn't one this is the first
time that I know unless H someone else
put it out to me where there's a law
that basically is regulating a
technology basally technology and not
the actual harm this is just it's it's a
open-ended ambiguous thing that's the
kabash on an industry that's just
emerging that unlike say a nuclear bomb
which has C A lot of people there's not
a lot of evidence for
har and if there is going be har there's
law for it
go ahead I'm sorry no sorry
but I'll stop talking I just I get the
the the vibe from you that you don't
really believe that much in the promise
of AI because you don't seem to think it
could be
dangerous you just said did you want to
keep talking yeah you what do you
because I feel that if we really believe
that this technology is powerful of
course it's dangerous it's dual use it's
multi-use so as we develop new techch
and new potential there's going to be
potential
for good and harm and I'm concerned
about the potential harm I use and I
develop AI technology every day and I've
looked at the progress and I I I would
love for all the Sci-Fi super
intelligence to come out I I would love
it I think' be great I think Humanity
would vend from it but um when the
airplane was first ended people thought
that we' be going to people would be
living on Mars right it's been would be
living on Mars 40 years ago and we
haven't got there and I think what
happens is that technology comes up on
the scurve there's a point of Rapid
adoption then it slows down and if you
look at the amount of time between say
DPT 3.5 to 4 to where we are now it's on
the esur it's not progressing faster
right it is slowing down and why I would
love for this to be just the answer to
every question in the universe I think
it's actually a really hard problem it's
it's going to take a while I I I would
love it I I love it for both faster but
everything I see feel that is slowing
down it's not getting faster it's
getting slower the rate of progress like
some moment pumping up GB gb5 like crazy
right strawberries all that kind of
crazy stuff but the reality is there's
not much to show for it if you look at
the leaderboard everyone's kind of
crowed crowed around the same kind of
performance level there hasn't been the
step Punk now maybe we'll get it but
it's not so easy that's the other big
issue it's not so
easy okay I think we're getting uh close
to closing statements here Greg is there
a point that you can foresee maybe in
the future if things seem to be getting
more dangerous can you describe a
potential point where you see yourself
starting to endorse regulations what
would that look like I think I'm all for
regulating the harps like direct Harps
if it's used to impersonate someone used
to Def fraud someone absolutely game for
that but to go after technology with say
all this kind of Il defined terms from
fine tuning to limits to flops on an
industry that's just emerging is
basically throwing
water like on the Ember right it's is
just starting right now and it's
ridiculous and it's done by people that
really don't know what they're talking
about it's done by people who really
watch too much TV right and haven't
really used the technology themselves
that's the fun the problem there and
yeah we have regulations but let's look
at when it's actually doing when it's
actually someone versus trying to
forecast some crazy termin scenario in
the
future great Holly do you want to
respond to that uh yeah so I think we've
identified sort of the cruxs between us
which is it sounds like you don't think
that AI at least in the near term is
going to be very dangerous and so your
focus more on the benefits we're losing
out on by not allowing by your focus is
therefore not getting in the way of
innovation with AI because you're not
worried as I am that there could be very
severe Harms uh which is why we I want a
kind of Regulation that does require
people to think ahead this is very
unprecedented very uniquely powerful
technology and yeah whereas in many
cases it's a lighter touge as possible
where you only uh prosecute harms you
only prosecute applications that are
harmful in this case because of the
magnitude of harm that I fear and that I
think is is credible with developing AI
uh I don't see any signs of hitting a
wall everybody said that for a very long
time with machine learning that it's
hitting a wall and then it speeds up
scaling just keeps working so I believe
that this the potential for danger is so
great that it is necessary to look ahead
and look at the technology which I think
is the source of the new risk it's just
the incredible amount of power that
intelligence exceeding human
intelligence is going to bring the
ability to change the world in any
direction is going to be immense and
that's scary and that's something that I
think we have no choice but to try to
predict and prevent and develop slowly
and carefully because I not I don't
think we'll necessarily have another
shot if there is a failure at that level
of power it might be that Society is
irreparably damaged it could mean human
extinction uh and so we really have no
choice but to regulate this
way okay Greg one last question I'm
curious about so you've made it very
clear that you're opposed to this
regulation and any regulation for now
but let me ask you if we had to have
regulation because there are a lot of
candidate Bills going through the system
is my understanding so it's not just SP
1047 there's a lot of people who want to
have some kind of Regulation so my
question for you is how bad is sb147
compared to other regulation you can
imagine doing how bad did we screw it up
I haven't looked at the other ones as
closely so I don't really know I I hear
about them but I don't know about them
that much fb10 for I looked at a lot uh
and fb10 pres is a total train train R
it's so many it's F in so many different
ways yeah I basally this the only thing
about maybe is okay it's like the Cal
compute thing right that's maybe the
only thing about it but everything else
is pretty much be totally taking it a
dumpster it's just makes no sense and
and can you elaborate what's the Cal
compute thing oh it's basically try to
help provide Power for people to build
models I think that I think I've done a
lot of startups and I don't know if you
done startups but it's wasn't that
through the fmd was that did that
survive the last round of amendments I
don't know did it I didn't T out did it
I don't know I think that was part of
forming the new the the new Department
which got asked okay I'm not sure
actually I thought about
that I trying to think something there
good about it yeah it's a it's it's a i
there's never been a time in humanity
where more knowledge was bad more
intelligence was bad we want the
smartest most intelligent people most
knowledgeable people in the world and AI
allows us to do that I think the one
thing that people get really confused by
is that there's intelligence and then
there's agency so for us we all have
agency and we have intelligence and
there separate things people get them
confused a lot right so the Terminator
the Terminator robot that Terminator
robot had a lot of agency it's also PR
fairly smart as well but have a lot of
agency today if you look at the systems
they have zero agen they're very
intelligent but they have zero agency
and I think that's what needs to be
separated people get confused with the
two and and even on intelligence side
I think the head of meta's AI has said
that tp4 is about smartest as dog right
and maybe not even right it is it is
just getting started and I'm hoping it
advance I want it to advance more but it
was not going to advance more you put
Theos on it Theos on all the R&amp;D all the
research that has to happen to advance
it I I think that the the core Crux
between us is that I'm worried about
potentially very powerful technology and
potentially very soon and that makes the
kind of Regulation that SB 1047 is that
considers harms before they happen it
requires developers to have a plan for
dealing ensuring safety dealing with
harms before they happen I think that's
our only option and I think sb147 is a
very light touch High sealing first coat
of Regulation I think this is my opinion
as the P us executive director is that
this doesn't go nearly far enough but
it's a really good start I'm really I
strongly support it and I urge anyone
listening if there's still time to call
your assembly member call your
California Senate member when it goes to
that vote and to call Governor nome's
office and let him know that you want
this bill to
pass great all right thanks Holly uh
let's give the last word to Greg well
first of all thank you guys both for uh
giving me the form to chat I realize you
guys are don't have don't sh my
enthusiasm for AI or my viewpoints but I
I appreciate it nevertheless I think if
FB 1047 gets enacted we're GNA have to
rename selection Valley be called not
Silicon Valley but compliance Valley
what what going to Happ we would have
armies of lawyers up and down trying to
figure out how to comply with this TR
law that's totally ambiguous that
totally impossible to comply with and
we're going to have to basically there
going to be a business for moving
companies so you guys are looking for a
new startup idea maybe a moving company
for helping people move out California
and all the families going to have to
get uprooted and all the Innovation and
jobs and prosperity that would have been
here in California that's be moving out
to other parts of the country or perhaps
if country adopts s sp14 Zone that
different countries and I think it's
really sad because while we'll have the
safest AI in California we'll become
like the bamax of of AI right will'll be
safer but totally irrelevant would be
left in the dust who would be like the
Ottoman Empire who would be put not in
the history book and I think it's sad
because this is where really AI has
started right it's the birthplace of AI
birth of innovation is where all the
Aman technologies have come out of and
it's so sad that with this Hollywood
inspired Terminator pre-call Minority
Report narrative that we're going to be
king this Innovation we're going to be
make subjecting hundreds millions of
people to live a worse life a less
productive life a life that's less
healthy less prosperous like a short
lifespan and it's I kind think of
something more dangerous to humanity
than making Humanity less intelligent
less smart so I'm hoping that anyone
that's listening will do what Hol says
call up your legislator and tell them
what the Lan brain idealist is and S
this train W and votes are coming up in
November so just remember how people
voted and the people who were voted
against people who voted against
California people who voted against you
know having having inovations and long
Lifest stands and better drugs and
better better Healthcare remember who
who voted which way and vote
appropriately when you vote them in or
out but thank you guys both for giving
me the time to chat and if this passes I
hope you guys are right but
unfortunately everything I see from this
means it's totally the wrong direction
for our state you can make a $1,000 for
free if it
passes I'll bet you $1,000 doll you
don't
move can all right yeah Greg do you want
to bet Holly that if it passes you'll
still stay in California I don't know I
I haven't I've been thinking about it I
like the weather here but we we do
something else maybe right I don't know
yeah it's hard to say but I know that
companies have already started moving
actually is probably World high profile
companies so I think that yeah I just I
don't understand why we want to mess
with people's lives why we want to based
decisions on Hollywood things are just
totally make believe I think we want
laws that are based evidence based on
real evidence of of harm not on some
fantasy Meetup believe that someone who
has to a hands meet up right it just
doesn't make sense if you want to do
that we
should five or whatever and that's
probably their start of time than trying
to uh fictional writing in a Senate
bill I want to thank you both thanks to
Holly uh Holly is director of the P
movement in the US which I myself am
part of so I'm happy to endorse visiting
pa.info and joining this amazing Discord
community and also huge respect and huge
thanks to Greg he came up and stood up
for his position it's one that Holly
disagrees with I personally full
transparency disagree with Greg's
position but he stood up for his beliefs
defended it like a champ and frankly
that's not something you see often
people in his position it's all too rare
for them to come into the lion's den in
a place like Doom debates or other
debate forum and hash this out it seems
like debating is a lost art in our
society that I'm personally trying to
bring back so when you're supporting the
Doom debates Mission you're not just
supporting talking about Doom you're
also supporting the social
infrastructure of having a high quality
debate so thanks again everybody and
I'll see you guys next time on Doom
debates