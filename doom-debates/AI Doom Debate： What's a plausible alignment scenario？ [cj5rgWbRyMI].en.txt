welcome to Doom debates so I got a
Twitter DM from a pseudonymous account
and this is what he wrote hey uh I'd be
willing to go on Doom debates with you I
briefly embraced doomeris last year to
the point I had to start briefly seeing
a therapist I'm probably getting off the
train the Doom train at a pretty late
stop thinking paii is a feudal
resistance that will mostly in effect
just ban open w models for dignity
points my P Doom is around 10% but I
don't live as if I have a death sentence
if these laws and treaties don't pan out
which I think is a likely outcome it's
useless to despair about things outside
my control and I see too many pause
folks doing exactly that it's neurotic
cult behavior for my part I'm a technos
solutionist trying to go back to college
for math Cs and engineering and I want
to see from myself how hard this
alignment problem really is at a
granular first principles level because
I really only understand it secondhand
like most people I don't know who to
believe AI Doom presupposes AGI is
possible at all and isn't just a big
Tech scam bubble like crypto and then
there's tech people I'd Place most EAC
people in this category who don't
believe it's that much of an existential
threat but don't really understand the
hard technical issues and then there's
the doomers like you and optimists like
Quinton Pope who do I don't know who to
believe so one of the origins of this
podcast was me brawling on Twitter and
having arguments on Twitter and wanting
to elevate the format to something where
where we can hash it out and other
people can weigh in and we can document
a record of all the different positions
that people come in with what I call
stops on the Doom train and then I can
help them get back on the Doom train and
eventually we'll all go to doomtown and
realize that our last hope of Doom is
pausing AI right that's my agenda for
this podcast so without further Ado here
is my conversation with
RJ hey everybody Welcome to Doom debate
today I've got somebody pseudonymous
from the internet who I thought had an
interesting position we're going to call
him RJ and he he studies computer
science and we're not going to go too
much into his personal background we're
going to focus on his position which is
that we're all going to make it yeah he
thinks we're all going to make it the
proposition he's going to argue for is
that we're probably on track for aligned
AGI which of course I don't agree with I
think we're doomed but uh RJ kick us off
give us kind of your opening
statement I think that we're on track
for a decent shot at aligning AGI I
don't think we're 90% Doom or even
really 50% Doom I acknowledge the risk
is there but on the whole alignment and
AGI is playing out far better than elzer
or anybody in that whole Community could
have uh predicted you've got this
architecture llms that seem to be
aligned by default the big concern I
guess is over reinforcement Lear
learning models but you know let's see
if we can kind of uh chain those
together see if one can inform the
other we got a foothold to this whole
world of human understanding and values
that's a whole lot more than what elezar
was fearing when he was talking about
outer alignment and fragility of value
the hidden complexity of
wishes
mhm okay so basically you've been
following elowsky and the Doom Arguments
for a while like over a decade uh good
part of a
decade okay and at first you found them
convincing but slowly you started
changing your mind over time right yeah
when he was writing the sequences uh AI
was this distant um pipe dream almost it
was a theoretical thing MH how would you
say your P Doom has evolved did it start
really high and now it's low
uh it started rather
low um it shot way up last year after
well really shot up in 2022 when elezar
started ringing the Doom Bells with his
so-called dying with Dignity post uh
then AGI ruined a list of
lethalities and uh I kind of worried a
bit but I didn't really start worrying
in full until about 2023 last year when
he went on bankless podcast just uh just
utterly defeated this is not the this is
not Harry Potter Evans Varys that I
knew uh and uh at the time Bing Sydney
was playing out and I I really didn't
know who to trust on this seemed like
the entire Community was just um
coalescing around
Doom Eliezer thought we'd end up with a
much uh with a much more controllable
kind of model but turns out the
connection one out and we're all
[&nbsp;__&nbsp;] and I I I I was uh kind of
traumatized by that I actually literally
started having to see a therapist for a
little while cuz I I really I really
felt betrayed this was my faith this was
filling the religious shape hole in my
heart and what I thought would be our
Salvation was our
doom and I was going to die soon you
felt like your P Doom was super high
like over 50%
yep I started seeing a therapist MH but
since then you found some other
arguments and it's come down how low do
you think it's come down now uh just for
the sake of argument maybe it's a little
higher but I I'll go with
10% okay ballpark of 10% and I'm in the
ballpark of 50% so that's kind of a
significant difference 10 versus 50
you're you're I mean 9 to1 odds is still
you can be pretty optimistic like sure
it's there's a risk but most likely
we're just going to make it through and
have immortality and Utopia and sugar
candy mountain and it's all going to be
great yeah and I mean 9 to1 happily ever
after those are decent odds for sure
yeah and um also for the record if I had
9 to1 odds that we're going to survive
super intelligent AI even I would be
very open to the idea of like okay screw
it it's so hard to pause and there's
such high upside by getting a good super
intelligent AI that let's just plow
through I I become very open to the
argument when the P Doom is only 10%
maybe 15 or 20% maybe but by the time
50% pdom it's like oh God or even you
know my pdom is actually higher than 50%
if if it's 50% P doom and it appears
it's a 50% chance of life raft or death
i' I'd still take that chance you know
yeah yeah for sure okay so let's let's
get into it um let's basically compare
our mental models and see how despite
reading a lot of the same content you're
now all the way down to 10 % while I'm
still more like 50% um what do you think
is one of the biggest things you read
that Chang your mind uh I'd have to say
the writings of Quinton Pope and Nora
belose uh the so-called AI Optimist and
John David prman as well great okay yeah
hit me with one of the
arguments um they they believe that uh
the so-called counting argument that uh
in the space of all possible Minds
reinforcement learning
uh is likely to uh Hit Upon A util an
emergent utility function that uh that's
something random and probably not
aligned with human
values that's that's one of the
arguments like we just roll
random yeah I can summarize that so
they've got a push back so let me step
back and I'll try to summarize the
argument that they see themselves as
pushing back on so the argument that
they think is a prodom argument is as
you say the counting argument of like if
we count up how many how many different
AIS can be bad and then count up how
many different AIS can be good most AIS
are bad because the things that we
consider good are pretty narrow like
keeping a bunch of humans alive there's
fewer ways to do that there's lots of
ways to have a bunch of dead humans or
just have a Barren Wasteland there's
lots of different kinds of barren
wastelands value was fragile yeah yeah
exactly so that's kind of the counting
argument that they're pushing back
against and they're saying well it's
hard to even Define a metric right by
the time got in the weeds defining a
metric maybe suddenly there is no
counting argument right is that high
level what they're saying uh yeah kind
of um like it's it's hard to define a
metric of like how are we even going to
count there's like infinitely many of
both basically so how do we even compare
I would think I would think like um most
utility functions wouldn't even be uh
coherent it would be Shakespeare on a
typewriter you get random noise you know
monkeys typing shakes on typewriter if
we consider the the universe of possible
AI utility functions that we can land on
yeah as you say a lot of them are like
gibberish utility functions and then
like occasionally you have one that's
good occasionally you have one that's
bad so basically it's hard to count so
from my perspective I don't think I'm
going to make that counting argument
right that's not why I think we're
doomed so I might just sidestep that
like okay I get that they have a counter
argument on that I guess it was a big
deal for you let me hit you with one of
my arguments and I'll see if you have a
counter argument to something that makes
me convinced Fair go
ahead okay um I think for a big thing is
uncontrollability so forget about
counting arguments do you think that
there's some threshold Beyond which the
AI there's a lot of AIS that just no
longer take orders from humans right
they're Off to the Races and they might
spawn other AI they might listen to
other AI but they're kind of one Doom
State yes uh that's
multipolar um yeah so a lot of my hopes
I guess rest on the more a little more
controlled deploy M kind of outcomes if
if these companies really think they're
making super
intelligence I'm not saying um I'm not
saying it's going to be easy
but there should um there's some there's
some chance we can make one AI that does
what we want it to it's not going to
have this evil inner gulus that does all
these gobble bad things that'll it it'll
have a utility function of gobble deg
that it'll just it's it's like 42 is the
answer of life the universe and
everything it'll just try to
argmax that and end up killing us all in
the
process okay so the proposition you're
arguing for is we're probably on track
for aligned AGI so help me understand
your main line scenario right you just
have to pick a scenario that's more
likely than any other scenario from your
point of view is it a scenario where we
get AI That's pretty uncontrollable but
at least it's aligned or it's like not
super aligned but it's controlable like
what are you imagining um I I think the
former I think it's a line but it's not
really that controllable maybe it's a
little more interpretable than um than
we had uh we had feared like elezar has
this phrase vast inscrutable matrices of
floating Point
numbers um we're starting to crack
interpretability a little bit but I I I
don't want to imply we've fully cracked
it sure okay so you are willing to
concede that it'll be hard to Tinker
within control but in your mind that'll
be fine because it'll already be aligned
enough that it's okay if it kind of
controls us because it's
aligned
yeah
interesting okay so I mean right off the
bat it sounds like if that's the future
you're imagining that presents a pretty
large risk that we'll get to a very
similar future where we have an AI
That's also uncontrollable like the AI
you imagin but we screwed up alignment
right because you're kind of imagining
if future where it's hard to undo or
like it's impossible to undo right it's
like you're stuck with whatever initial
conditions kicked off this future state
of uncontrollable AI
right I suppose
so
um and yes I I I think more coordination
and pausing would be good but I
realistically I I don't think we're
going to get that kind of um that kind
of pause and unilateral treaty that
every Factor will uh not defect on yeah
yeah for sure I mean I'm just trying to
operate your Mainline scenario right I'm
trying to enter your world so it sounds
like your have you heard of the have you
this is going to sound silly but have
you heard of the um uh the rational
fanfic friendship is
optimal I've heard of it yeah uh it's a
My Little Pony friendship is Magic fanic
it's really more of a framing device
where um where they launched where has
alternate universe Hasbro launches a My
Little Pony MMO with a super intelligent
AI running it it just happened to be the
first super intelligent Ai and uh it
just happens to be aligned enough like
it's headed towards takeover and
converting the Earth to computronium but
it'll it'll upload all the humans MH and
turn them into ponies if they want
to and is that unable your Mainline
scenario uh vagu
I I hope we don't get like ponies as the
rest of uh our us what we are for the
rest of time but um that sounds like an
optimistic scenario for me even though
it's uncontrollable it's weakly aligned
yeah yeah and I get that you may be
thinking of multiple scenarios and how
they're related but I think it's a
productive exercise to just let you pick
one that you think is Mainline right
that you think if you were to do a Monte
Carlo simulation of the future you know
flip a coin and land in somewhere that
you think is more probable than any
other point it lands there and I really
just want to drill down on that one so
you know instead of talking about My
Little Pony you can just be a little bit
more serious and you can talk about like
okay let me try let me let me try to
divorce the uh optimal verse from that
uh let's say let's say Ila's Ila's new
company makes the AGI or it uh invents
methods to align which he gives to the
company that wins the AI race
MH say open AI he just takes their they
take their orders from il's
company and um this is this is uh what
eliezar seemed to be imagining with this
world CIS admin that um we'd have
something aligned we know it would fo to
Super intelligence we'd put our trust in
it sometimes you just have to have faith
and uh it
would it would hopefully
uh be cor it would hopefully um be
aligned but if not uh at least corable
for so we can hash this out at a later
date to once we're out of the acute risk
period okay so ilas sgiv company
discovers some instructions or some
architecture where you can have aligned
AI even past the threshold of permanent
uncontrollability and there's a timing
contingency in your Mainline scenario
right so things go right in terms of
like
didn't build an unaligned first they
went slowly enough that Ilia was able to
hand off this alignment architecture and
they right this and this is this is what
elezar seems to be imagining with the
pivotal act a lot of disingenuous
framing around that phrase but um it
this is really all it all it is uh make
sure you cement and lock in your
decisive strategic Advantage so we don't
have any kind of multi-polar Chaos as
long as we have aligned AGI and
hopefully corable
AGI uh I think that should be that
should be
sufficient okay so again just going into
your scenario right this isn't even the
part where I'm necessarily arguing like
I'm just wrapping my head around your
own Mainline scenario so Ilia gives open
AI the secret sauce because maybe open
AI has more scale has more resources on
the capabilities building side or on the
capability deployment side or whatever
whatever it is and Ilia figures out the
secret sauce with his new company so
yeah my first observation or objection
is like it sounds like what I was saying
before where I think you would concede
that the scenario you're describing is
adjacent to a very similar scenario
where it doesn't work out right for
instance Y what if Ila's alignment
Secret Sauce turned out to be wrong
right and like his test showed that it
was right and then it turned out to be
wrong and you're describing a scenario
where we've now passed the point of
uncontrollability so maybe you can fles
out in more detail how the scenario
actually more robust than it sounds on
its
face
um let me think about that for a
second I actually agree with you that
there's a ton of
risk and it would be best if we could
get the pause but I'm just trying to I
wouldn't hedge all of your um all of
your hopes for Humanity's future on this
act of coordination coordination is
hard
um so where do you so where you go NE
your next kind of hope I guess is this
kind of
scenario of of we just happen to get
aligned AGI and we kind of walk walk in
past the acute risk period and we hope
we don't end up with a destroyed
world
and I don't know it's sorry
okay all right um so I yeah what what do
you want to talk about what do you think
is like the strong point of your
position uh I just I just wouldn't um
I'm trying to think of I'm I'm kind of
doing this Terror Management in my mind
of okay well are we likely to get a
pause no well we could
still we could still invent some kind of
new um alignment technique in the
meantime but and a lot of people say
that uh alignment research
is risks C cap
abilities
um so I guess alignment necessarily has
to go hand inand with
capabilities right it sounds like you
don't have like a really wellth thought
out mainly Mainline scenario of how
things go right but you've mentioned to
me that you've just spent a lot of time
on what you said you know the psychology
management of it right almost like a
poker player being like you know playing
to your outs right be like look this is
how I want to think I don't like the
phrase dying with the I like playing
with your routes okay sure sure sure
sure yeah um which is fine I mean so
maybe maybe just talk a little bit about
that because it sounds like you're
almost um you're kind of being fallible
you're saying look I'm not going to be a
perfect rationalist about this I'm going
to be a human and I'm not going to just
accept whatever the P Doom calculations
tell me committed the cardinal
rationalist sin of privileging the
hypothesis where we don't all die
okay um yeah so I mean you know normally
my debates are just object level so it
sounds like you're kind of conceding
that you're not going to make a purely
rational argument for a high P Doom uh
you mean a low P Doom that's
right I
guess this is kind of what was going
over through my head over the past year
when everybody was sounding the Doom
alarm
Bells uh I just don't didn't know who to
trust I uh I I've joined whole servers
of people with like most of them had a
9p doom there was this Discord called
the Doomsday cult it's really more of a
[&nbsp;__&nbsp;] posting server but um you can flare
your username there with what your P
Doom is and most of them were red
colored 80 plus per Doom I felt like I
was in a cult you know yeah yeah yeah no
I I hear you I mean look I I saw Dr
Roman yampolsky on Lex fre the the other
day and I'm a fan of Dr yampolsky I like
a lot of his podcasts but I'm not a fan
of saying that P Doom is
99.9999% as he says and uh elas owski
even replied to his tweet being like
that's too high like you can't there's
it's crazy to put that many nines on
Doom and he makes doomers look
irrational to put that many Quantum
fluctuations would destroy the
universe uh that has a higher chance of
happening I mean he did I think he
didn't clarify how many nines he wanted
to put on it he made it sound as I
recall like he was going to keep going
putting nines on it which as you say
yeah Quantum fluctuations can randomly
cause Doom or can randomly cause Heaven
I mean you just can't dump a bunch of
nines into a probability like that so
that's a huge strike how do we even know
that our universe is the real one you
know yeah yeah yeah so you know I was
pretty appalled to see that many nines
because there's not that many leaks in
Dr yol's game like he usually just says
a bunch of stuff that makes sense and
then when it's time to show his hand P
Doom it's like this crazy P Doom that's
way too high right even I as a Doomer
it's like okay you can't get that many n
so so we both agree and then it has like
not only is it irrational from my
perspective but from your perspective um
it you know was psychologically damaging
cuz you're like what the hell now I'm in
this cult where people are putting way
too many nines so it's it's tactically
not a a good ide I thought this would be
our Salvation it's going to be our Doom
right you know what's funny is I should
get Dr yampolsky on Doom debates because
it'll be the only Doom debate where I
have a much lower b doom than the
guest maybe with
elzer yeah I think probably won't go
beyond
99.9% yeah he probably appealed to some
Multiverse uh
aliens yeah um okay great so I think we
we might be wrapping up soon I kind of
like this because I think I mean look I
think I'm nakedly honest you're nakedly
honest and you did start off talking
about a scenario that some people might
be nodding along with so I'm happy that
I'm like hey look this has some problems
and you basically agreed what you're
saying about like I really want to
believe pom is not super high I don't
want to be in a cult I think people are
going to resonate with that so I guess
my question is just like are you
comfortable with me essentially winning
that little debate we had yeah yeah it's
fine I don't take it seriously I'm
trying to learn more you know I've seen
much uh things that are over my head uh
from optimists who have who have about
the same P Doom as me but are much
better informed on the topic and have
much more of an actionable
plan right that makes sense so I think
we can probably agree that on the object
level you are not as good as defending
the position of some of these other guys
like Quinton Pope and Nora belose so
what I'd like to do is get one of them
on Doom debates and I actually have had
a debate with Quinton in the past I
think he held up his position well I
can't say that I was convinced but I do
think that some of the people in the
audience would say that he won and I get
why they would say that like I think he
argued well and I think that I can
operate his position like I think I can
turn the tables and pretend like I have
his position and I could theual Turing
test exactly right yeah I think it's
technically the ideological Turing test
but I always say intellectual drink test
right yep um yeah and and that would be
a fun exercise right for me to come in
as the Quinton position um but despite
being able to operate I think I still
find it unconvincing but anyway I think
we should do a rematch uh with Quinton
and or nor maybe a tag team well maybe
when I have more education into the
subject I'm I'm trying to go back to
college right now nice but I I
admittedly my understanding of a lot of
things is very
superficial like I can't really delve
into the math of good heart's law and
stuff like that
right yeah a lot of people accuse
doomers of just being psychological
doomers like I just love to say that the
world is burning and that keeps me
locked into this Doom position because
my personality is a Doomer personality
mine is very not much not a Doomer
personality I'm an
optimist you're an optimist but also
there's like some flinching too right so
you're not only do you like to be an
optimist but you also hate to be a
pessimist so you're kind of locked into
this optimistic position I when I when I
really engage with the Doom positions
the other day like I my head was feeling
physically uncomfortable like holy [&nbsp;__&nbsp;]
I need to go touch some grass mhm yeah
like that's how hard it can be yeah well
I really appreciate you coming on Doom
debates because you've been very candid
with your own feelings right you've been
very introspective you've been very
self-aware so I thank you for this this
is what high quality discourse looks
like right we're not trying to do
rhetoric for the audience we're just
telling it like it is
yep cool man all right RJ uh I'll give
you the last word any other points you
want to make
I think we're all going to make it
somehow thank you very much and thank
you audience for watching this episode
of Doom debate
[Music]