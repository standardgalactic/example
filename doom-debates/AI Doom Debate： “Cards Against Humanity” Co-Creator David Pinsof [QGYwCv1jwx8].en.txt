I want to talk about what I see as a
different version of the fmy Paradox
intelligence if you're right about it is
a very simple easily scalable copy and
pastable thing and it literally gets you
everything you want it achieves all of
your biological goals and only one
species with a nervous system has fully
invested in this where are all the
intelligent
[Music]
animals hey everybody Welcome to Doom
debates today I'm speaking with David
pinsof he is one of the co-creators of
the wildly popular Game Cards Against
Humanity he's currently a social science
researcher at UCLA social Minds lab
researching political beliefs and Status
hierarchies with the lens of
evolutionary psychology and he writes a
Blog called Everything is [&nbsp;__&nbsp;] the
reason we're talking today is because
he's working on a post asking whether AI
Doom is [&nbsp;__&nbsp;] welcome David glad to
be here thanks for having me
sure thing so I had the opportunity to
skim through a draft of your post and
the first thing I noticed is that it's
it starts off pretty charitable to the
AI doomers like you point out a lot of
things that I think are positive so
maybe you can start off uh with the good
stuff and then we'll go from there yeah
sure I mean part of the impetus for the
post is that um I have a lot of respect
for AI doomers uh they're some of my
favorite intellectuals I'm actually a
fan of less wrong um I was into uh
eleazer yudkowsky um very much into
Robin Hansen Scott
Alexander um I uh you know I I listen to
80,000 hours I'm a fan of EA as well um
and uh really it the impetus to write
the post came from kind of trying to
resolve my own cognitive dissonance
about here's you know a group of people
that I really uh intellectually respect
who are right and I agree with on pretty
much everything uh and here's this one
case where I I see seem to really
disagree with them and I'm really
skeptical of of of the claims being put
forward um and uh that's that's part of
why I was interested in this topic I
just wanted to get to the bottom of of
how a group of people who were so smart
and thoughtful could could be so wrong
in my view and maybe I'm the one who's
wrong I'm open to that possibility too
that's part of another part of why I
wanted to write this piece just to see
if anyone could tell me uh where I went
wrong um so yeah it's it's kind of an
odd topic for me and then I'm I'm
psychologist by training evolutionary
psychologists um I write about [&nbsp;__&nbsp;]
and and the subtle social games we play
um but uh I recently came up on my
one-year anniversary of the blog and I
asked my readers if there was any sort
of weird you know outside my comfort
zone topic that they' be interested in
in reading about and AI
dorismar of my readers wanted to get my
take on it so that's that was another
one of the impetuses for me writing this
um and I was happy to get a lot of
really awesome feedback from um highle
uh AI doomers such as yourself and I'm
currently in the process of editing the
piece and and response to that feedback
and I'm looking forward to to seeing if
we can get to the bottom of our
disagreement likewise so one thing we do
here at Dune debates and as a
rationalist is I try to separate my
opinion of somebody's object level views
on a topic separate that from are they
being like a good faith participant in
highquality discourse are they meeting
the standards of discourse and so far I
think you're doing a great job because I
think your article shows that you've
talked to doomers and tried to
understand their position and when you
post it on Twitter you solicited
feedback and you are quick to respond to
me and you're engaging with me so that
definitely puts you in the upper echelon
of of people coming out because a lot of
the non- doomers come out and they're
like we shouldn't even take these
arguments seriously we should dismiss
them so thanks for that thank you yeah I
I would say another reason why I wanted
to write the pieces because I I felt
that the quality of AI dmer criticism is
so low there are just so many dumb laser
takes on the topic and I and I wanted to
sort of fill the void and having sort of
a more smart educated critique of it and
hopefully I you know I'll be able to
pull it off we'll
see oh yeah all right so let's start
with the bottom line where did you net
out after doing all this research what's
your P Doom oh oh something pretty close
to zero I'd
say so I'm just curious when you say
pretty close to zero are we talking sub
1% or maybe just like sub 5% uh sub 1%
wow okay you know I I always feel like
sub 1% is pretty hard to defend pretty
obviously just because it's like look
even nuclear war right even extinction
by nuclear war is is roughly 1% a year
type of risk so you're really not
crediting AI RIS with even
1% uh I'm I'm happy to be talked up to
it um I do think that um it's easy to
give oneself a false sense of precision
with these kinds of numbers and that we
just kind of pull these numbers out of
our ass
um so I I'm reluctant to try to um get
too precise about it basically I'm
skeptical I don't I don't think we're
going to get killed by AI I don't know
how to represent that mathematically
it's it's um I I think we can too easily
um you know point to a number and and
create the illusion of of
confidence um yeah yeah yeah yeah okay
but it's it's low enough that it's like
when we're making policy decisions we
shouldn't even have like a plan B for
the like AI goes Rogue and everybody
scenario it just shouldn't even be on
the game board basically yeah uh I think
that's accurate yeah okay fair enough uh
yeah so so hit me with your first point
point of potential disagreement sure so
I think the at the heart of every AI
doer piece I've read and you know maybe
I'm maybe I'm missing something I'm open
to that but at the heart of it is this
core assumption um that intelligence is
one thing that it's this uh one um
optimizing power as you call it or a
block BL of compute as Scott Alexander
calls it or a blob of learning it's this
general purpose thing that um helps you
achieve literally any goal um it uh
improves all your skills um essentially
the more of it you have the more you can
just get whatever you want um that's
basically the intuition here that I'm
seeing in a lot of AI Doomer uh articles
um sometimes it's implicit sometimes
it's explicit uh but I really wanted to
you know call out that assumption and
say hey that's an assumption you're
making and that needs to be defended um
and so that's sort of the first point
that I tackle in the piece and um yeah
yeah yeah yeah let's let's talk about it
sure um so I think that that's a pretty
good characterization of what I would
claim like I do claim that intelligence
is one thing in a lot of important
senses um so let let me make it more
precise what I would claim optimization
is one thing it's a lens that explains a
lot of different scenario
in terms of what algorithms are doing so
for example if you have an AI that beats
you at chess I would call that an
instance of optimization because the
winning states of Chess are such a small
fraction of the possible chess boards
and the winning moves are the sequence
of winning moves is an exponentially
small fraction of all the different
sequences of moves you can get like
there's no way you would ever win at
chess if you're just trying to guess
moves or do a Brute Force search so
somehow we've targeted the winning
configuration in chess which was a tiny
Target in an exponential search Bas and
whenever you see somebody doing that
somehow reaching a tiny Target that a
naive algorithm couldn't reach in a huge
space that's what I call optimization
and I do think that talking about cross
domain optimization where you talk about
all kinds of different exponential
search spaces and you draw connection
like it's not just chess look when a car
is able to self-drive the sequence of
actions it's taking on the steering
wheel to safely have a whmo car drive
you around San Francisco I would say
that that's an instance of optimization
and if you look at the accident rate
compared to humans you could even argue
that it's superhuman optimization
because it's it's like something like
two to three times already fewer
accidents per mile than humans so I
think it's it's hard to even argue
against the the fact that this analysis
is logically valid right to just point
to a small targeted and exponential Sur
Bas so would you at least follow that so
far yeah so I think there's a difference
between optimality and intelligence um
so yeah I agree for any goal there are
more or less optimal ways of achieving
it um and you can do the you know
mathematical analysis to see you know
what what's going to optimally get you
your goal you can do rational Choice
analysis decision Theory um if it's a
social problem you can do Game Theory
and so I agree that there are different
optimal strategies for different games
either games against uh other players or
games against nature or or the universe
itself um so yeah optimality is a thing
um but I think it takes very different
shapes depending on on what is being
optimized and the nature of the game
being played I think that that might be
where the where the disagreement lies
yeah so I see what you're saying so you
could be like look there's a bunch of
different problems and all of the
problems do have this unifying High LEL
structure you know at least through this
one lens where they are all search
problems right so like searching through
an exponential space in a way that's
non-trivial not a Brute Force search but
like some kind of intelligent search
right there these are all intelligent
searches but your claim is that you just
need different intelligence depending on
the search right yeah I think that's
pretty close to what I would say yeah
okay great so I I do think that this
actually is a good Crux of your
disagreement with the doomers is
probably one of the biggest most
important cruxes and you do have some
smart allies on your side I don't think
every non- Doomer is on your side but
certainly Robin Hansen doesn't see
intelligence as quote unquote one thing
I think he'd be cheering on what you're
saying right now um now let me throw out
an analogy that I find quite powerful to
my own thinking which is the analogy of
computation so maybe you can at least
admit that I can win in the analogous
argument before we get back to the real
argument so the analogous argument is to
draw an analogy between intelligence and
computation it seems to me like you're
looking at it's 1970 and you're looking
at different video game circuit boards
you're like oh pong works like this
breakout works like this they're all
just different circuits and when you
want to play a different game you need a
different circuit and then you know
Flash Forward half half a century and
today we're like yeah these are just all
computations you just load the software
onto the computer chip and you run it
and it would be insane to go build a
dedicated circuit board to a video game
because it's just clear that when you
want to run a video game you just do it
on a computer trip because it's
computation so are you buying the
analogy at least um I yeah I think
computation is a very broad abstract
thing uh you can do all kinds of
computations on a on a circuit board um
but uh to me at least what's what's
ringing my alarm Bells is is the
assumption that therefore all video
games are the same right so the
structure and and the code of each video
game is going going to be very very
different and and the goals you might
have for Designing one type of game or
one type of experience and the criteria
for success or failure are going to be
very different from from game to game so
just because there's this um abstract
notion of computation um that you can
Implement on a circuit board doesn't to
me suggest that all video games are the
same or or that you could have a general
video game which is which is fair right
I mean it's not proving right the anal
the analogy isn't a proof absolutely
right but it sure seems like a strong
intuition pump because doesn't it seem
like you could be standing here in 1970
being like look we're going to have all
these different circuit boards in our
home and when you want to do a new
application you're going to want to
design a different circuit board for it
you see what I'm saying I just want to
make sure that you're in a mind space
where you can anticipate that no the
circuit boards are going to
unify uh yeah I'm not sure I I I follow
the analogy um yeah I I mean like so
math would be another example so like
all mathematical operations can be
implemented in in our number system
right uh right um so that's but
but but math is there are different
types of math that are very different
that doesn't mean that all math is the
same right no for for for sure right I
mean and as you say like software
programs are different too right and
they they connect to reality differently
um but like so basically I would claim
that the way we saw convergence in video
games where it's like at the end of the
day because they're all computation the
architecture that you end up using to
implement them is a general Universal
architecture and I would claim pretty
confidently that what you're going to
see with software is going to show the
same convergence that we saw with
computation and also the same
convergence that we saw with the human
brain because the human brain suddenly
is this general intelligence evolution
by natural selection decided you know
what let's just stop having U sub
routines let's just stop having
hardcoded reflexes let's just build a
general intelligence if we want to you
know solve all these problems in in the
animal kingdom and yeah the species can
randomly go to the Moon we're going to
give it the kind of intelligence that
lets it go to the Moon we're just going
to throw that in for free you see what
I'm saying so we're observing
convergence both in computer chips and
in evolution the fact that it popped out
the human brain and one more place we're
observing the convergence is if you look
at AIS the AIS are actually getting
broader so a chatbot AI the the amount
of topics that you can ask about that is
the broadest AI we've ever had and also
board games you're seeing the same AI
train on multiple different board games
and beat previous AIS at all those
different board games so like I hope
you're seeing some amount of convergence
here yeah I mean yeah if if you have um
you know a a problem that is best solved
through trial and error and you have a
better trial and error Learning System
then yeah you're going to do better on a
lot of different trial and error
problems and if you have all enough data
and you have the right kinds of Trials
and the right kinds of feedback and the
right kinds of errors and and you know
and you have the right system in place
yeah you can get good at a lot of
different problems but each problem is
going to have different types of Trials
different types of things you need to do
different types of feedback different
criteria for success or failure um
different um optimal strategies and
different types of available data like
some so like the internet for example
has a lot of easily accessible data in
the form of you know words and images
but not every problem is like that not
every problem has a lot of easily
accessible data like if I want to solve
the problem of becoming the president I
can't simulate the entire United States
of America and all the millions of
people in it and run that simulation
millions of times to see which
presidential strategy is going to be the
best that data is just not available to
me so while that that type of Brute
Force like uh pumping a system full of
tons of data and learning by trial and
error what works and what doesn't yeah
that's a pretty effective solution to a
lot of different problems but I just
don't see that that's going to be the
solution to all problems uh everywhere
yeah yeah yeah so I think that we're
we're starting to shift to a different
argument which I think is fascinating
but let's try to package up uh let's try
to isolate the argument of is
intelligence really one thing because it
sounds like you're basically saying okay
I agree that there's a trend that maybe
at some point you'll have the same AI
the same AI Core that out of the box you
it can self-drive and it can play board
games and it can do natural language
really well but that still doesn't mean
that it's going to take over the world
and and we can have that conversation
but it sounds like you are potentially
conceding the point of like okay
intelligence is one thing in the sense
of like you can just unpack your new Ai
and set it to a problem as long as the
problem has a feedback loop is that
basically what you're saying as long as
you have uh sufficient data to train it
and you have the right behavioral
repertoire uh with which to respond to
the data and the right kind of feedback
system uh and you have and you've
correctly identified the you know the
the criteria for success and failure uh
yeah if you have all those things in
place yeah you can get pretty good at
that thing okay so maybe what you want
to say is that the training processes
required to succeed in different domains
they're so varied that like sure you
could take the same brain you could take
the same like raw neuron core of
intelligence and you can apply it to
different domains but you're going to
have so many different training
processes and the more power you want
over the universe the longer the
training process is going to be so
you're never just going to have like an
AI take over the
world uh I I would sort of um I I guess
the the core claim would just be that
you know different problems require
different
solutions um and this the the strategy
for solving one problem is going to be
different from the strategy for solving
a different problem um and the types of
training data you'll you you need to
solve one problem are going to be
different from the types of training
data you need to solve another problem
and the accessibility of that kind of
data is going to be different and the
unpredictability of that data is going
to be different you know so if you think
about Chess it's a very rigid system
where you where there's a finite number
of moves finite number of spaces on the
board but not all problems in the real
world are like that some problems are
unpredictable chaotic uh and the type of
you can't sort of uh uh use the same
strategies that you'd use chess and as
you use in say the the social world uh
or or the political world where where
there's lots of unpredictable Dynamic
strange things that happen where people
where agents are actually actively
trying to be unpredictable uh and
outmaneuver
you you know a lot of people bring up
this idea of like finite versus infinite
and that feels to me like not quite what
they really mean it just seems to me
like it's a matter of degree because
like sure chess is finite but it's much
larger than the universe right so like
does it really matter that it caps out
at this you know 10 to the 80th or
whatever right as opposed to capping out
at Infinity does that make the
difference or is the difference just
that the the branch factor is just even
larger than chess in many applications
isn't isn't it just a matter of
degree um maybe uh yeah I guess I would
return to the point of like you know you
can train a system to predict words and
give sensible responses to prompts
really well we've seen that with with
chat
GPT um but right now at least you can't
train a system to become the president
right um and so what's the difference
between those two problems between
predicting words and becoming the
President right obviously there is a
difference there you know as you become
better at predicting words you don't
also magically become better at becoming
the President right and I would
argue yeah go ahead yeah and as you
become so if if as you become better at
predicting words you don't become better
at becoming the president then uh why
should we assume that as you become
better at predicting words you'll become
better at you know hacking into the CIA
or uh Manufacturing
Mur scale I I think gp4 is better at
becoming the president than anything
that's come before it it feels like
progress toward becoming the
president uh I I would disagree there I
I one of the big stumbling bucks to
becoming the president is you need to be
a person with a legal identity and and a
citizen of the country in which you're
running for political office so right
off the bat there's this very hard
barrier that g GPT has not even
surmounted sure but isn't but I mean if
that's really the only problem isn't it
at least making progress toward if you
ask it hey how would one how would an AI
go about becoming president wouldn't
it's I actually wouldn't be surprised if
gbd4 or Claude or whatever would tell
you like well I might use a person to
help me out and then I would tell the
person what to do I mean I feel like
even that kind of reasoning is very
close if not if it's not already doing
it today so doesn't it seem like it's
like it's getting I'm not saying it's
going to be president tomorrow but I'm
just saying compared to 5 years ago
doesn't it at least seem like if it were
on the path to becoming president isn't
this kind of what it would look like to
be on the path no because I think
there's a very big difference difference
between giving a sensible response to
the question how do I become president
and actually becoming the president
actually behaving in the world and doing
things and responding to feedback and
perceiving things uh and and uh
responding to setbacks and overcoming
obstacles and committing to the the plan
over a long stretch of time like that is
very different from just uh you know
giving a plausible text response to the
question how do I become the president
okay fair enough um let's bring it back
to one your more key points for the
essay so you can pick what to hit on
next sure um yeah so I guess the so the
next thing it's it's somewhat related
but I think it actually can be decoupled
um it's the idea that um intelligence uh
is on sort of one single Continuum that
encompasses like all animals and and and
humans are just really high up on this
intelligence Continuum and and ants are
just super dumb and and you know maybe
rats are a bit smarter and dolphins are
a bit smarter and we can just sort of
rank all an based on their their general
purpose intelligence I think that's
another assumption I see um in a lot of
doer literature and I think they they
kind of need that assumption because if
the if yeah um general intelligence or
super intelligence is in the future if
it's coming for us then then we need to
be moving toward it along some kind of
continua if we were moving toward it
along you know thousands of different
continua the odds of us converging on
all those contina would be very low so
so AI doomers really need something like
one single Continuum that we're
progressing along in order to to
postulate this this future general
intelligence yeah totally I agree and I
think that's an interesting discussion
to have so I see it as an important
observation that we as humans can go
head-to-head against any other species
in any Niche right that's just it's like
what do you make of that observation
because you there's that's unprecedented
for for the Animal Kingdom right or any
Kingdom of life you can't just take one
species and have them dominate every
other Niche so what's going on there can
we go head-to-head against I don't know
I mean like mosquitoes are really
annoying they spread disease and we
haven't wiped them out yet nematodes
outnumber us 57 billion to one um many
species have done better under humans
than than I so it is it is a little bit
subtle right because what I would want
to argue is that like okay to the extent
that you can be like oh my God ants have
more total biomass if that's the case
I'm not sure who who wins there but like
whenever you have a criteria like that
usually what I would what I would say
which I agree I have to explain it and I
would just say It's a combination of
like we didn't care enough and we're not
smart enough but we're getting there
right or like you know like give give us
another 100 Years of technological
progress like I don't see ants randomly
crawling around the earth at that point
I I don't know that that's necessarily
true I I don't I think you know if we if
we really put our minds to it we could
probably wipe out ants or we could
figure we could figure something out I
think the reason why we're not doing it
we could take we give him a blow we
could at least beat him in the biomass
game right even if we're not 100% I I
think it's more of a of a cost benefit
analysis that just like the costs of
wiping out all an would just be enormous
and just not worth the benefits to us
right okay okay but I guess so what I'm
getting at is like okay yeah we can have
this discussion how effectively are we
going to wipe out ants but like have
that discussion with a beaver the beaver
will be like um if it doesn't involve
building dams I'm I'm out right the
beaver is not even going to be having a
strategic discussion with you about how
to wipe out the ants it's it's not right
so like what do you make of this
difference the the difference between
beavers and and humans the difference
between the fact is you can sit down in
the war room with fellow humans and be
like how do we wipe out the ants you can
have that discussion about every species
and we're going to have a Fighting
Chance right so let let me not say okay
we beat every species decisively but
like clearly it's something that we can
take a stab at and we might succeed
right give us a 100 years give us
something like we're getting there okay
but put any other species in the War
Room they're just going to be like
[&nbsp;__&nbsp;] around they're going to be
useless right so like what what is your
takeaway from that I don't know I mean
viruses have given us a run for our
money they're they're doing pretty well
we haven't gotten rid of them yet um and
they're extremely dumb they're millions
of times they don't even have a brain um
right so yeah you expect us to be
totally demolishing viruses because they
have no brains at all and we have the
biggest brains in the animal kingdom
which suggest to me that you need
something more than just there are some
problems that you don't really need a
brain for that that are just for sure
for sure but the but the question I'm
asking is so in the case of viruses like
once again maybe we won't fully
eliminate the virus but at least we can
get in a war room and we can take
effective action right we can neutralize
it we can do something whereas you bring
in any other animal into that war or
bring another virus into the war room
right there's why how come every other
species is useless to come into the war
room and strategize how to take out
viruses how come it's only humans no
that's a that's a fair fair point I I I
will accept that point that if if we
wanted to you know uh wipe out another
species if if we were in conflict with
another species an existential conflict
or whatever um then humans would
probably stand a better chance of of
winning than any other species uh locked
in an even that framing of our species
is in an existential conflict right tell
that to a dog they're just going to bark
right they're not even they're not even
going to understand yeah so I I accept
that point yeah okay so that's and and I
mean it did seem like you came in saying
like why why are we so like human
chauvinistic thinking we're the best but
I feel so I feel like that's why we're
the best right because we have we we we
just have something that we overturns
the game board like the the way you
normally think of natural selection the
whole concept of a niche the concept of
ecological competition it's different
when you have a general intelligence on
the scene right it's a different ball
game now uh you're implicitly assuming
that intelligence is is one thing you
said we have some thing that that other
animals don't I I I would challenge that
and say we have many things that other
animals okay okay okay okay yeah I see
what you're saying but then isn't it
weird that we have all of these things
at once and they're all localized in a
single species and there's no runnerup
species right like people say dolphins
are smart okay I I've never seen a
single research paper from a dolphin
like you know you think there would be
at least a few yeah and I think that's
more understandable under my view than
your view because if intelligence was
this complicated multifaceted thing
consisting of many parts it makes sense
that only one species in the animal
kingdom over four billion years manage
to uh uh hit on this lucky combination
of traits right it would make sense that
we're the only ones and we don't see
intelligent animals all
around let me just finish if
intelligence was one simple learning
algorithm you could just copy and paste
endlessly um and uh and and the more of
it you have just the more you can just
get whatever you want then it's really
surprising that humans are the only
Intelligence on the planet that is a
really profoundly surprising uh
empirical finding yeah so regardless of
how you're defining that term I'm trying
to argue that you just should be
surprised by the low-level observations
that you're saying like you see what I'm
saying put the term aside I just think
you should be surprised by your
observations like the war observation
for example the observation that if you
want to fight a war against a species
you don't need to recruit any other
species you just get humans in the room
right like I think that's a surprising
observation
uh I suppose
sure yeah and I think there's a lot of
surprising observations like that so you
were saying like Okay well we we just
you know natural selection just stumbled
on one brain that had all the different
pieces in a single brain but now I'm
saying well that observation is
surprising too because compared to other
Apes the genetic information that
differentiates our brain if you want to
build a human brain instead of an a ape
frin you need maybe a megabyte of
genetic difference probably less right
probably much less because we're just
talking about like you know
a million years of differentiation
that's I don't even think that's a
megabyte right so how did that small
differentiation make such a big impact
of like a massing all these different
types of intelligence right I would just
dispute that it actually is a small
differentiation because a lot of the
information in our genome is epigenetic
it's regulating pre-existing uh
instructions and and a lot of a lot of
uh complexity you can get just by just
duplicating certain sections of the
genome talk about epigenetics actually
doesn't affect my point because my
there's a speed limit to how much
information complexity natur selection
can add to when I say genes I'm talking
about whatever you want to call the unit
of reproduction genes plus epigenetic
there you can't just have natural
selection put tons of megabytes into it
in a small amount of time it has to go
very slow and it also has to be
maintained so you can't just it doesn't
just build up it a lot of it leaks out
so you have to use your optimization
power to keep the information there if
you like the information like there
there's just a speed limit and there's a
total information limit so it's pretty
clear that no matter how you analyze it
there's a small information Delta
between apes and humans uh I would
disagree with that I think there there
is this um line that I see a lot uh in
the AI Doomer subculture that that
humans are just chimps with like bigger
learning blobs um I think if you were to
put that hypothesis forward in a room of
evolutionary anthropologists or
evolutionary biologists or evolutionary
psychologists they would laugh at you I
think I mean that's a qualitative
statement right so I don't want I don't
want to make that statement because it's
qualitative it's vague the statement
that I do want to make is if you just
measure in terms of bits of information
if you compare one spe one animal
specification to the human specification
you're going to see a very small Delta
in the specification because there
weren't that many years of Divergence
right so that's just like logically true
that you just can't have a wildly
different specification when there's
only been a million years of Divergence
oh it's more than more than a million
our last common ancestor with chimps I
believe is a little over six million
years ago um but even so I think we got
a hug it's not a lot right um it depends
on what you mean by a lot it's kind of
subjective um and also we as a ballpark
estimate right megab I think is on the
high end of a ballpark estimate of how
much information it takes to specify a
human and not an
ape uh yeah I think we'd have to get
into you know the specific uh papers or
data points you're drawing from and and
and get into the nitty-gritties on that
I the reason I'm bring the reason I'm
bringing it up right is because it's I'm
just seeing a contradiction between like
your point is that like humans just have
this amazing grabag in our heads right
we just we nailed all these different
intelligences and I'm just saying okay
but the Apes don't even seem close right
they if like an ape isn't doing 90% of
the productive output of a human an ape
is really just digging around useless
right like and and so how is a small
difference suddenly giving us this giant
bag of tricks that no other animal has
right well I don't think the grabag is
solely in our heads I think it's also
out of our heads in in the realm of
cultural Evolution and economic and
technological Evolution I think one of
the big superpowers that humans have is
enabling a whole new kind of evolution
which is cultural Evolution and and that
we can we can get forms of uh of
evolution that happen across Generations
we can pool knowledge we can divide
labor and so I think a lot of our
intelligence if not most of it comes
from this second form of evolution uh
called cultural Evolution but I do think
we need a specific grab bag of cognitive
abilities to even make cultural
evolution possible so we need not only a
cognitive grab bag of of social
cognitive adaptations to enable cultural
Evolution but we also need a variety of
different cultural adaptations for
solving specific cultural problems and
technological problems and economic
problems and so on do you read a lot of
Robin Hansen on this topic because
you're I think that he would agree with
what you're
saying uh yeah I'm a fan of Robin Hansen
I generally am aligned with him on a lot
of things so yeah it's possible I got
some of this from but I think Robin
Hansen probably just got it from just
reading the literature and evolutionary
social science which is basically the
standard view yeah let me try to make
your your point that you just made
because it's like so I asked I basically
said How can you explain humans having
all these diverse abilities all of a
sudden all these diverse intelligences
right domain specific intelligence as we
got them all uh when there's only been
maybe a megabyte of additional
information in our genome and our
epigenetics well you could basically
answer that I'm not counting this
additional information accumulation
that's happening with culture right it's
written down and it's passed orally and
it's it's in the ram it's not in the ROM
it's in the ram so you could make that
argument okay so then here's my response
to that if let's say there's a newborn
okay and he's raised in society so the
newborn has the information in his brain
let's say one megabyte difference from
the Apes and yeah the newborn is going
to go to school and and learn like
another few megabytes of information and
then that newborn is going to slot into
some cultural role like the newborn is
going to have a job and the job is
complex so the weird thing for me is
doesn't it seem like every other species
is not even close to being able to like
be a newborn uh take their current
genome you know learn a few megabytes of
information and then be useful like how
come no other species seems to be
anywhere near the ability to do that and
humans are it seems like I have a good
answer to that and you don't right
my answer is that it's a really rare
combination of abilities that only
happen once in our 4 billion year
history what's your
answer okay so I think just to repeat
what I'm hearing which I think is also
Robin Hansen's claim which is basically
saying like look humans they do have one
trick but the one trick isn't
intelligence the one trick is cultural
absorption power would you agree with
that I wouldn't call it cultural
absorption power I would call it you
know a bunch of different things um I
would call it you know social learning
cooperation of Labor pedagogy
communication uh Innovation uh feedback
memory uh things like that I mean but a
lot of the things you listed are
actually common other right I mean
division of labor and and memory for
example right I mean a lot of species
have that sure but they're far less
advanced than than in
humans um I don't know if humans
necessarily have the best
memory um it depends on what kind of
memory you're talking memory memory is
probably not the best example but in
terms of cooperation division of labor
humans are far and above all other
species
are you sure because there's some
species I mean if you look at like the
queen ant compared to worker ants that
seems like a Starker division of labor
than than two humans doing different
jobs that's an interesting example so
that that would be a case of J ants
having a different genetic system than
humans so they're Diplo Diplo deployed
instead of Appo deployed which means
that ants are actually more closely
related to their siblings than they are
uh to their own they would be to their
own offspring and so it's actually in
their genetic interests to act in the
interests of the queen and of the of the
The Hive um and so you can get much
higher levels of cooperation among uh
ants and other kinds of insects than you
can among humans where there's much more
genetic conflict uh happening um so yeah
ants would be an exception to the rule
of of cooperation and division of labor
short um I mean I think that there
probably other species that have a lot
of division of labor I mean what about
species where you just have a male and a
female that are more differentiated than
human males and females are right
there's like wildly different like I
don't know the male might be like much
bigger and have a very different role
uh yeah I mean there there's there's
certainly some of that but I think it's
pretty mainstream in uh evolutionary
social science right now that Human
Social intelligence social learning
theory of Mind cooperation coordination
that our abilities in these domains are
far more advanced and elaborated than
they are in other animals and that's
sort of our superpower okay so just to
to put this in context again like why
why are we going down this rabbit hole
it's because we have some observations
that to me seem very surprising like the
War Room idea that like that we don't
want any other species in the War Room
humans only please you guys are useless
right when we want to like we want to go
to the moon like it's okay Apes just
stay outside the humans are talking
right nobody else is helping us go to
the Moon um except like the dog that we
like brought into space because we
didn't value its life right like the
Russians just shot the dog into space
with test okay but besides that dog
they're just useless um and so we had
this curious observation and we're
trying to understand like what's going
on what is special about humans that
explains that lowlevel observation and
it sounds like to summarize what I'm
hearing from you so far you're basically
saying like well we there we made some
slight variation compared to the Apes
like we just push the dimension of
specialization labor maybe push the
dimension of memory and and then it
seems like I still think that the key
thing you're saying is like we push the
ability you know what Robin Hansen says
like we we push the ability to basically
copy other people kind of like Monkey C
Monkey do but it's like more than Monkey
C monkeyd it's like a deep version of
Monkey See monkeyd would you agree with
that um more or less yeah okay
so that claim seems to be sneaking in
intelligence because like how do you
really talk about a deep version of
monkey see monkey do like oh yeah I
learned what the other monkey does like
the other monkey I see him like do a
physics equation so then I can do a
physics equation it's like wait a minute
wait a minute wait a minute right like I
feel like intelligence sneaks in here
general intelligence how so I mean if
you're literally talking about the
example of of solving a physics equation
right then I mean you can't a monkey
isn't even close to doing that and it's
not like oh we'll make the monkey better
at coping no I think it's you have to
have the internal structure where you're
like okay the relationships between
these mathematical Concepts I'm now
modeling that in my brain and if if
that's what's happening I wouldn't
describe that as like wow you sure got
better at cultural coping I'd be like oh
you now have the platform to be general
intelligence just like oh you have
Turing completeness right by analogy to
computation I'm like you have now
crossed the threshold where you just
understand stuff in general that's how I
would describe it yeah I wouldn't
describe it that way and I think it's
more than just cultural copying I think
uh it's not that you know humans are
just Apes plus cultural copying I think
humans are Apes plus a whole variety of
of complicated uh social and cognitive
abilities including numerical ability um
that that's probably one of the building
blocks uh of of you know solving
equations for example Stanis L to H has
so you think you think monkeys and trees
just evolved numerical
ability mon monkeys and trees evolved I
mean like when humans were splitting off
for the like you think primitive humans
they just had a numerical ability Module
come
online uh uh a pretty basic one of of
understanding you know small quantities
you know like 1 two 3 that's that's
pretty Universal among human societies
and the the to to map uh number onto
geometrical quantities um so so yeah I
think those kinds of building blocks
were present in humans and and and uh uh
uh without them uh modern mathematics
probably could not be possible um and
the person who's who's studied this a
lot is uh the neuroscientist Stanis L
Deane uh who who's written about our
number sense okay let let me let me try
this angle of attack okay and I think
this this is interesting we're digging
into a pretty good Crux so I'm I'm
pretty happy with the the back and forth
ring um okay imagine that uh I teleport
you you know you're living in 1970 and I
teleport you to today and I show you a
monitor and the monitor is uh rendering
a simulation and you can see it's
interactive like let's say it's a
simulation of a fish tank and you could
be like I want to feed the fish or like
I want to poke the fish right and like
and it simulates it and stuff is
happening and it's like a nice high
resolution like there's light bouncing
off and I'm telling you this is all
artificial this is all generated you can
change the colors okay so you see that
you're from 1970 um what would you
conclude about the circuit board that's
running that
thing uh I um I I I would probably not
conclude much of anything I don't know
what I'm I'm not quite following yeah
yeah no for sure I I that's that's
that's a tricky one it's unfair to
expect your answer but what I'm getting
at here is I would with my perspective
uh with my study of theory of
computation I'd be like look there's
enough stuff going on here there's
enough complexity there's enough
equations being solved in order to get
the light and shadows in order to change
the colors in order to let's say hear my
voice if it's like you know uh if it's
speech recognition when I'm like
instructing AT commands there's enough
stuff going on here that you've probably
got a chip in there which is a computer
chip which is Turing complete right
you've probably got a software program
that was written in a high level
language in order to be able to to
specify all this of because there's
enough going on that the convergence has
gone past the threshold where it's a
full-on computer and you know even if
technically the fish tank isn't turn
complete I'd still be like okay yeah
technically it's not turn complete that
means technically it doesn't doesn't
have to be a computer implementing it
but let's be real it is a computer it's
not going to be an Asic running that
it's not going to be an application
specific integrated circuit running that
fish and so what I'm trying to get you
to see is in the case of like okay you
teleport from a million years ago to
today you walk into a physics class you
see a student learning and and solving
equations and what I want you to
conclude is like okay this brain has
reached the threshold of general
intelligence where the teacher is giving
them like an arbitrary problem and then
they're using general intelligence to
solve the problem are are you open to
that
perspective uh no okay uh yeah no I I
see the intuition you're trying to pump
but I think it's a misleading intuition
and we shouldn't let it be pumped I okay
so so what's your analysis so in the
case of in the case of fish ink you can
imagine somebody being like I don't
think it's certain complete I think you
got a really good fish tank rendering
circuit board in there so in the case of
the student learning physics right right
you you teleport from a million years
ago so you don't have any context you've
just observed animals and now you're
obser observing a human being getting a
physics problem and and like unpack what
the variabl should be and solving the
equation so what what's your conclusion
as to what's happening I I I don't get
what what
that regardless of what I say in that
scenario what what relevance it has like
what's the therefore let's say I I say
that the the person from a million years
ago uh is totally bewildered let's say
say the person from a million years ago
says wow they must be they must have
some general intelligence what's the
implication the kind of point I'm making
is there are certain observations that
suggest certain deep forces at work so
another example so I gave you the
example example of like hey if you see a
fish tank simulation that suggests deep
computation at work as opposed to like
an ad hoc circuit board another example
of a piece of evidence that you observe
suggesting a deep process at work is the
famous story of you the blind watch
maker so you're walking along on some
deserted Beach you stumble on a pocket
watch something that looks like a pocket
watch you pick it up like you've never
seen you've never seen Humanity before
right you're just a time traveler from a
million years ago you open the pocket
watch you see all the all the gears and
moving Parts you have to conclude that
there's a watchmaker like even though
creationists use that argument they use
that to prove God they're obviously
wrong about that part but the part where
they say this is an optimized device
there must have been an Engineering
Process to build it an optimization
process they're spot on right so that's
another example of observing something
and working backwards to like okay
there's a deep optimizing process here
so that's that analogy is quite abstract
but like I do think there's a similar
flash of insight when you look at a
human solving a physics problem and you
your flash of insight tells you like
okay the thing going on in their brain
is a general
intelligence uh yeah so I was with you
until you got to the end so I I I love
the blind watch maker by Richard Dawkins
is one of my favorite books uh and I
think I I agree that when you see the
improbable arrangement of functional
organization in nature it cries out for
an explanation William py who made the
observation originally uh used it as
evidence for a a
Creator um and uh Darwin came and and
turned that idea on its head and you can
have this appearance of design uh this
elaborate functional organization
independent of a design designer um and
uh it's kind of kind of weird to me that
you're moving from that argument to
therefore intelligence as one thing
because even if you look at a watch a
watch is not one thing it's an intricate
Arrangements of different parts with
different functions that are elaborately
arranged and organized to fulfill the
function of telling time um and watch
I'm being a little bit confusing because
I was just trying to make a very high
level type of analogy like because you
were kind of asking me like where am I
going with this where am I going with
this is I'm I'm presenting you with some
data that it should spk you to realize
that this particular Deep thing is
happening and it's not the same Deep
thing it's not oh I'm seeing a pocket
watch therefore there's an optimization
process that's a different Deep thing
but the point is that there's a deep
thing um I I would agree you know if if
you look at a pocket watch I would agree
that there needs to be some kind of
optimization process to explain it if
you look at the human brain or if you
look at a human solving an algebraic
equation I would agree that you would
need some other kind of optimization
process to explain that but just because
that's true it doesn't mean that the
watch is just one generic thing rather
than a system of intricate parts that
work together and it doesn't mean that
the human solving algebraic equations is
just one thing rather than an intricate
system of Parts working together okay so
now to get to that point if you look at
a pocket watch or if you look at a uh a
moon program right like with a lunar
lander and the the spaceship that takes
off the rocket that that boosts it in
orbit if you look at the whole moon
mission right if you again transport
from a million years ago to today oh
this species is going to the Moon with
this kind of
mission what would you think about the
feedback loop that birth the moon
mission do you think that it was uh the
the typical feedback loop of natural
selection where Generations that didn't
go to the Moon died more and the ones
that did go to the Moon their genes got
selected like what what would you think
about the designer of the moon mission
species uh I mean it's incredibly
difficult economic cultural and
political question that is uh Way Beyond
my expertise but the answer is
complicated it's not just we put our
general intelligence powers on this
thing because we wanted this thing it's
it's a much more complicated story
involving geopolitical between America
and Russia and the look at it purely as
an engineering problem right just like
some species wanted to build this thing
and they did and they got to the moon
when if all you see is the observation
right you don't know anything else about
the human race you don't know that we're
descended from apes you just know that
we're a species and we all got into
these things that we built and they took
us to the moon and then we landed and we
came back and no other species have ever
done that like what would be what would
you conclude as to like some deep thing
that's going on I would conclude that
there must be a very elaborate network
of specialization division of labor and
infrastructure uh that allowed something
like that to
happen okay but don't you think that
there are individuals who could in in
their own head give you a lot of the
different pieces that are necessary to
make a mission and if you think a moon
mission is too complicated just
something else ridiculously impressive
right like whatever I'm sure Elon Musk
could get or or like you know that guy
from rocket lab I forgot his name but
that that guy's kind of like another
Elon Musk figure and he kind of is
personally built pretty impressive
Rockets So when you look at a
Contraption made just by a single mind
like that like what you think about that
mind I could I could not more strongly
disagree it's not by a single mind I
mean if you think just just about the
mathematics that allows you to to build
something like that it's it's built off
of generations of
intergenerational cooperation and uh
pooling of insights over you know
centuries of scientific uh progress and
debate between scientists so they're
literally you know there they're not
literally but they're standing on the
soers the shoulders of giants um and so
I would not say it's one person that
that yeah yeah yeah so so let's say
let's say the Guy Peter Beck is the the
rocket lab guy let's let's say Peter
Beck has had his whole life to read a
lot of books and study and then you give
him all the parts to make a rocket you
lock him in a a rocket laboratory for a
couple months and he comes out with a
rocket and he shoots it into space right
this maybe a little you're hand waving
away all the most interesting and
complicated Parts like how does he get
all those parts that's a huge problem
how do you mind the different metal
question right so so you know that the
starting configuration is that he has
all these parts like you just know that
there exists an organism that can start
from that configuration if he had to
right if that's the test the organism
can start from that configuration and
end with a configuration of a successful
let's say orbital rocket launcher just
the rocket goes really high you got the
idea so yeah I I I guess I'm I'm kind of
beating the dead horse but I'm just
saying if all you know is that piece of
data then it's it's a little bit like
the blind watchmaker situation where
it's like okay whatever is contained
inside that organism has to be like
deeply thinking has to be like a general
engineer like there's no domain specific
engineering can tell you to do all those
steps I just don't think that
follows so you you give me the
observation of a you know human who can
build a rocket that goes to the moon
whatever I I don't get how you can
connect from that premise to therefore
there's just one homogeneous thing that
caused that to happen I don't get the
connection there if anything I think
that there's a stronger connection
between wow this human did this
impressive thing there must have been
many different working parts uh that
that are behind that and and and a whole
history of cooperation division of labor
and specialization that led to that that
I think is actually the stronger
inference um the inference to therefore
this must be one general purpose thing I
don't get that that connection at all
okay yeah and then I guess I mean I see
what you're saying and we should
probably move on I would just if I were
to respond again I guess I would just
drill down into like okay let's see how
the bits of information got into his
brain how many of the bits of
information are from Evolution how many
are from like the books he read he's
read which were prepared by his culture
and to me it just seems obvious that
there's no way to pack enough bits of
information into a mind where it's like
oh yeah just here throwing in a few
books and and now you know that that
explains why he can do all these things
like to me it seems clear that it's the
inbuilt actual intelligence that's doing
the heavy living but I understand what
you're going to say back so I I think we
um you think we can move on yeah sure
okay let's talk about instrumental
convergence you kind of have a beef with
that claim right so say your side um
actually no I I I don't uh I think you
know if you're assuming that an agent
has long-term goals and the capacity to
sense its environment and perceive to
what extent it is successfully achieving
those goals and to what extent it's not
and can respond to obstacles and
setbacks and is context sensitive and
can pursue that goal over long stretches
of time in the face of trade-offs and
handle conflicts with other goals uh if
an a if we're assuming that an agent can
do all those things in the world and has
the behavioral repertoire to do all
those things then yeah probably it'll
have a lot of the same instrumental
goals as us you know getting resources
and power and and whatever uh what I
have a problem with is that pre-existing
set of assumptions so I think okay
doomers are sweeping a lot under the rug
with this word goals um I think goals
are way more complicated uh than doomers
think I think goals is actually kind of
in a similar boat as intelligence and I
think it's another really misleading
word that masks a lot of complexity uh
with with the shortness of of the word
itself um and I think when you combine
these two concepts of intelligence and
goals you sort of get sucked into an
intellectual black hole that's that's
hard to escape from and so I think it's
really important to unpack just how
complicated this concept of goal is and
how little progress we've seen in AI uh
on this dimension of of intelligent goal
seeking in the world and uh and um yeah
one one I threw out a lot out there
let's see what about that so let me also
put this in context when when you look
at the claim of a Doomer like me I
usually Factor my claim into two parts
where part one is that it looks like
we're on track to have a super
intelligence soon so it's coming and
then part two is if we were to have a
super intelligence and it wasn't aligned
and it wasn't controllable then it would
be incredibly deadly and probably kill
us all so so it sounds like when we talk
about instrumental convergence we're
talking about the second part of the
claim where you already grant part one
if you want to talk about it you already
granted as a premise of like okay we
have a super intelligence it's here if
it wanted to achieve a goal it could and
now we're just talking about like okay
well why would it want to achieve goals
like seek power is that a fair framing
that's a fair framing yeah so yeah so I
would I would combat the first
assumption which is that there is going
to be such a thing as a super
intelligence that can get whatever it
wants but yeah if we assume that then
yeah sure it'll probably want resources
okay and you're willing to assume that
you're willing to assume part one for
for the sake of this argument you're
will assume part one and see where it
takes us okay so um so don't you think
that if if I just had an engine not even
an AI just like a magical engine that
all you do is you input a goal and then
it flashes forward in time to some state
where the goal is achieved if we were to
do that don't you think that we we might
observe power seeking in that end State
like for example if you just said like
hey I just want my business to make the
most money possible don't you think that
like you flash forward in time and
there's like a bunch of guns pointed at
everybody in the world who has power to
like keep them in check to make sure
that they keep giving me money like
isn't would if it was truly an Optimizer
wouldn't it be likely to pull those kind
of moves if it
could um I mean you seem to be positing
something like a wish granting Genie
that can grant you any wish you want but
if you actually walk through so like a
lot of goals have constraints and
limitations and and obstacles that are
insurmountable uh so yeah I think it
depends on the specific goal we're
talking about and what the constraints
are and what and let's call this let's
call this the strong form of
instrumental convergence I'm just making
up this term but the strong form is a
statement not even about AIS but a
statement about goals it's just saying
if there's a perfect goal Optimizer
right what does it mean to perfectly
optimize this goal it logically implies
having these sub goals that are power
seeking and again we're not talking
about let's say the weaker form where
what what will actually AIS do we're
factoring that out and we're just saying
if you just snap your fingers and search
goal space just do a you know in in
passive search over end states which end
States count as optimizing toward the
goal you're going to find a lot of end
states where the universe has just been
like remapped to concentrate a lot of
power that would be the strong form
instrumental
convergence sure yeah so um I I actually
think You instrumental convergence is
was you know fairly plausible um but I I
guess the part that I might question is
is whether annihilating humans is going
to be an instrumental goal for for most
goals I actually don't think that's all
that plausible um so I I would question
the the preceding Assumption of that you
know we've already U moved beyond that
so we're we're at the point where we
have this super intelligence that's
that's crazy good at getting whatever it
wants even there like I I still don't
have this intuition that like
annihilating all of humanity is going to
be uh the main thing that it wants to do
or the best thing for it to do to
achieve its goals um and I'm just like
just thinking by analogy like you look
at humans compared to say mosquitoes
that's a that's a that's a case where
you know mosquitoes are are really
annoying they give us annoying itchy
bites they spread diseases we don't like
them we can't trade with them we can't
talk to them so there we have no benefit
for them it's all benefit no cost and
yet they're still here we haven't wiped
them out and you could make the same
point about any other pest um you can
make the same point about bacteria or
viruses um and but there are some pests
that we've done some damage to right I
mean at least viruses right like we've
at least eradicated some viruses that's
an example that's true that's that's
that's a totally fair point um but
returning to mosquitoes like why haven't
we wiped them out um well my guess is
that it would just be incredibly costly
to do that and the and the benefits uh
don't outweigh the costs and I bet that
that is true for wiping out pretty much
any speed especially humans if you think
about how hard it would be to wipe out
humans um uh I think it would be
extremely hard even for a quote unquote
super intelligence um and uh if you if
you think about how costly that would be
and how uncertain of a goal that would
be um it's not at all obvious to me that
an ai's best move is going to be to to
go down that path I think it's if you
look at the history of economics and the
law of comparative advantage it seems at
least just as plausible that the best
move for an AI would just be to trade
with us and just you know max out
division labor and yeah so if you look
at the mosquito example it seems pretty
clear that it's just bottlenecked by
intelligence because we have a proposal
how to wipe out disease causing
mosquitoes basically like a gam drive
right I'm not an expert on the details
but the idea is like you genetically
engineer sub mosquitoes and then they
won't be able to carry the virus and
then they'll reproduce and their genes
will out compete the other genes like
basically you do have a plan that's
going to wipe out the population and
potentially one bottleneck of the plan
is you're just not sure it'll work and
you don't want to accidentally cause
side effects right maybe another thing
is just like okay you just need money
and you need execution that could be
another thing but like point is if you
had a sufficiently good genan drive like
if you had a mosquito that was like so
good at mating with other mosquitoes and
also had this genan drive you can
imagine that a pure Gan Drive solution
like you could imagine God as a genetic
engineer would engineer such great genes
that you really just need to release a
few of these mosquitoes and you're good
to go and so the point I'm making is
that it does seem like the bottleneck to
eradicating disease caused mosquitoes is
just intelligence is just the ability to
map that desired configuration to
knowing how to engineer the right genome
and you're done I would uh but you waved
away all the constraints and limitations
right and and the costs and like that
that's stuff I don't want to hand wve
away those those concrete details I mean
I but if I gave you if I gave you the
ultimate genome that you could put in a
mosquito to to be on your way to
eliminating mosquitoes we could just
synthesize it and and be on our way
right it's just like we're just not
confident and the genomes that we can
synthesize they're they're just less
than perfect right like they'll do part
of the way yeah but you're you're you're
assuming something that that is not a
part of reality you're assuming a
fantasy world where there are no
constraints and problems and limitations
and obstacles and setbacks and I don't
see any reason why we need to entertain
that fantasy world when we have the
fantasy world I think it's if if the
only premise I'm asking you to accept
for the fantasy world is the premise
that God just told us which genome to
print right if once we have that we can
handle the rest ourselves easily at low
cost sure but God is not real God is not
real but what I'm what I'm saying is
that the bottle to eliminating
mosquitoes is understanding genetic
engineering right so to the extent we as
humans are only decent genetic engineers
and not god-level genetic Engineers we
can't wipe out mosquitoes but the point
I'm making to you now is if you're
willing to run with the premise that AI
is a super intelligent engineer close to
a Godlike engineer then AI can just snap
its fingers and have all these problems
get
solved uh I suppose by definition if
you're assuming a god-like being then
yeah it can snap its fingers but I just
I see no reason to assume that I don't
find that plausible at all I think you
you mentioned the point about you know
maybe there's some uncertainty about
embarking on this goal whether whether
we might succeed or not I think
uncertainty plagues all problems and all
goals including a goal that a super
intelligent is facing and I think it
would likely have a lot of uncertainty
of whether it would succeed in in wiping
out any species particularly humans and
I think if if it were actually a super
intelligence it would weigh those costs
and benefits it would weigh that
uncertainty and say hey this is an
extremely costly risky uncertain thing
that I'm not sure if it will work I
don't have a lot of experience pursuing
this goal I don't have you know a set of
millions of training runs where I've
simulated a universe and wiped out
humans and and I've picked the best
strategies for wiping out humans I don't
have that data available to me so this
is an extremely uh risky uncertain
Venture that's going to be extremely
costly is it worth it for the expected
benefit I see at the other side and to
me it's not at all obvious that that
that's true um and if you look at just
the logic of trade and that trade is
inherently mutually beneficial even with
one party who is
better at everything than the other
party like that's a counterintuitive
truth about economics that one society
that is awesome at everything still
benefits from Trading with a society
that's Dumber at everything presumably
super I want to get to the comparative
advantage argument in a second but just
going back to what you're saying here
the form of your argument seems to be
that engineering is just fundamentally
very hard so even if you had a much
bigger brain than a human brain right
even she had something smarter than
human it doesn't matter because humans
sure humans aren't that smart but we
just happen to be smart enough where
we're pretty much getting smart enough
that we can kind of engineer what one
can engineer and there's just not that
many higher tiers of engineering beyond
what human can do like when you imagine
like a perfect genome sequence for a
mosquito you're making the argument of
like H perfect shurf it's still going to
not know what that mosquito is going to
really do it's still going to be
confused right like you're kind of
arguing that engineering is
fundamentally just not that easy to do
past the human level because of our
universe is obviously your argument I
think that's fair yeah okay so I do
think that it's an important Crux when I
say actually I think that the universe
is hackable significantly beyond the
human level I think we're in like a
Minecraft situation like I think like
sure there's Chaos Theory but I think
the universe is definitely I mean
there's a couple pieces of evidence um
that I could tell you why I mean I would
generalize your claim as being like it's
almost like you think that um humans are
like almost the smartest possible thing
because if something really were much
smarter than human at least by my
definition I would expect it to like
easily engineer a bunch of stuff that
humans can't engineer and you're
basically saying like no that that can't
even possibly exist even if you knew
exactly how to build it there's just to
build it's just aent Concept in our
universe right am I speaking for you
accurately here um uh in some ways yeah
I think it depends on the domain we're
talking about I think in in some domains
yeah we're pretty close to hitting the
max there are some goals where there are
just fundamental constraints and limits
and and humans are pretty close to them
there are other problems where yeah I I
would I would agree that maybe an
artificial system could do a better job
than us certainly they've already
displayed that with you know things like
chess and and Jeopardy and stuff like
that um so yeah I think it it depend on
the domain you know how much how much
room is there ahead of us how what what
are the constraints what are the
limitations I think it it depends I'll
give you two pieces of evidence why I
think that human level intelligence
compared to the ideal engineer that can
be effective in our universe I think
there's a large gap just a couple quick
pieces of evidence evidence number one
the human brain Evolution was trying its
hardest to make human heads and brains
bigger right the birth canal limit the
fact that like 20% of child births in
history were like extremely difficult
the mother I got to get that stat and I
keep forgetting but like some crazy high
percentage of stats like child birth
became a tragedy among humans right
because evolution is like I got to get
this head bigger there's so much Fitness
from taking this metabolic Beast of a
brain and making it even bigger right so
that seems to indicate that we're we're
getting high marginal Returns on making
our brains bigger and that seems to
imply that we live in the kind of
universe that would do better with with
a bigger brain so that's just one piece
of evidence other piece of evidence is
just throw in the cave that doesn't mean
that the that the brain is being grown
in a homogeneous way as opposed to being
packed with useful gadgets for solving
adaptive problems but yeah continue okay
um yeah the other piece of evidence is
just that if you look at the actual
breakthroughs that humans have made they
all seem like they could have been made
faster with more intelligent not all but
like quite a lot so for example like
okay Einstein's theory of relativity if
you took a super in intelligence back to
when Newton made his theory the super
intelligence be like oh nice one newton
do you mean the special case of this
special relativity equation when you
talk about this Newtonian equation and
you would be like huh I guess you're
right but why would I care about that
special case well it should be one of
top 10 special cases because these are
the top 10 formulas that describe this
un you see what I'm saying like it
should have just popped out as a
candidate back in the Newtonian days
right and so it seems pretty obvious to
me that Einstein was bottleneck by his
intelligence and it's not just
physicists if you look at like inventors
it's like wow the phonograph has such a
Brilliant Invention you can like scratch
on a record and it'll make it vibrate
okay yeah that it was brilliant at the
time but it's not complicated in
retrospect right like it works in a very
simple way so why didn't somebody just
sketch it out earlier well that's I
think that's hindsight bias I think you
don't know how complic at something is
until you try to actually right but
you're you're arguing that the Universe
has an engineer ability bottleneck and
so so my point that the phonograph could
have been diagrammed earlier is a point
that unct enough I'm just I'm saying
that some problems have an engineering
bottleneck and some bottlenecks are
small think we should assume like a a a
generality of you know there are no
bottlenecks anywhere or there are Bott
leg everywhere I think it's going to
depend on the specific problem okay so
so you could be like okay Lon you gave
me two pretty good examples but I bet if
you try to like analyze everything most
of the examples aren't going to be good
like that that's kind of what you're
saying yeah basically yeah okay um
another point I'd throw out is yeah
there are some constraints with you know
um brain size and that you know it
increases um death and child birth um
but at the same time like that's
assuming that all of our intelligence
comes from a single brain or brain size
where as as opposed to distributed
intelligence that's you know across
Generations or across entire Societies
or entire economies um and I think if
you look at say capitalism the modern
global economy you could think of it as
a super intelligence designed to satisfy
consumer demand it is it is incredibly
intelligent at satisfying consumer
demand I have Amazon that magically
gives me whatever I want at the click of
a mouse um but that super intelligence
is not one homogeneous blob it is an
incredibly elaborate system of
cooperation and division of labor spread
out across the planet it's very
complicated no one single person
understands at all um and uh I think
that is a very good analogy to what a a
real super intelligent would actually be
like in practice now could we have a
global economy that's even better at
satisfying consumer demand sure there
are lots of fixes we could make um but I
think it's plausible that we're pretty
we're running pretty close to the limits
of of of satisfying consumer demand
there capitalism capitalism is a really
good way to just coordinate
intelligences that are like us where
each each of us has like a lot to
contribute but we're also like very
limited in our powers like you can't
just make a human dictator and have a
micromanage everything that's actually
the funny thing is like I guess I would
agree with you there because I think
that if you had a super intelligence the
economic model that the super
intelligence would choose is going to
look a lot more like a centrally planned
economy but the catch is it's not going
to be like okay a bunch of humans it's
not going to be like Russia where they
like just set these arbitrary thresholds
and they're actually clueless it's going
to be like oh I actually have sensors
everywhere and I actually know as much
as you guys know because you guys are
just parts of me so I'm just going to
micromanage everything and you might be
like oh my God Central planned economies
don't work but if you look at like Elon
musk's companies right within the
company you've got something that looks
more like a centrally planned economy
and Elon Musk is known for being a nano
manager right not just a micromanager
but a nanom manager and he's known for
getting better results that way like if
Elon Musk were LZ Fair on the design of
the SpaceX rocket then you probably
wouldn't see as many Innovations on
those Rockets so I'm going to go ahead
and be an AI communist and tell you that
you're going to get a centrally planned
economy among the super intelligent AIS
that's an interesting argument um I
think what you're assuming there is
basically distributed intelligent you're
saying it has sensors in all the
different parts of the economy and it's
like getting feedback about whether
whether this price is too high or this
price whatever basically what you're
positing is is the economy as it exists
right now instead of a sensor you have a
group of people who are trying to to
make money right uh so I think you're
just replacing a distributed economy
with with a super intelligence
semantically U but I still think what
you're talking about a huge amount of
specialization and and and distributed
intelligence and not centralized
intelligence yes one one commonality I'm
expecting is it's probably going to have
some current I mean the way we use price
signals like price signals are great you
just like put a price on everything and
and that tells you how like trade-off
resources I think internal to the
central planning of the centrally
planned economy it's going to have some
measure like that of how it runs its own
trade-offs but it doesn't need other
parties right it's just all going to
take place in a single brain in a single
giant data center just going to be like
yeah let me just shunt these quantities
around so I know how many resources to
allocate and then go everybody execute
uh which is again a good model is just
SpaceX right an Elon Musk micromanaged
company I think is a good model of how
AI would manage the world uh yeah but
Elon Musk doesn't know how to do
everything that the people he overseas
does right he has some pretty you know
basic high level information over and
the AI would right making it even more
that's right Elon Musk is not even the
best at Central planning that one could
be so it's going to be even better right
it seems like what you're talking about
is just taking our distributed
intelligence and just like uh putting
you know just putting it all in one
thing like sure you could because the
fact that humans have a coordination me
like it's amazing that capitalism exists
it's amazing that you can just take a
bunch of humans who all get to be
locally focused and somehow coordinate
it so you get Global not Optimum but you
get a ton of really good local optimums
and you know you get a tide that lifts
all boats and the incentives usually
have an equilibrium that's good now of
course occasionally you can get like a
ozone or global warming scenario where
potentially it can derail but until then
like it works great most of the time and
and that's amazing it's just that an AI
wouldn't need to lean on that as a
crutch because the AI would just be a
big brain like the AI doesn't have
coordination problems with itself
necessarily you know it solves much more
e yeah we're getting into an interesting
sort of philosophical semantic issue
like if you want you could just describe
the modern economy as a big brain like
you could think of it as a big brain
like information has been routed to
different places like um like I I see no
reason like if if you if you take the
global economy and put it inside of a
bubble like that seems to be what you're
talking about with super intelligence
it's just the global economy but it's
inside of a bubble now so therefore it's
one thing well no that doesn't discuss
Point yeah and you know and like I said
you duvail a lot with Robin Hansen you'd
be like look corporations are super
intelligence so you're doing it at the
level of the entire world economy you
can do it at the level of Walmart you
can be like look Walmart can optimize a
store shelf better than any one single
human and like I agree and you can say
that Walmart has a higher IQ than any
human the problem is that Walmart's IQ
still tops out not much higher because
if you tell Walmart okay great give me
the physical Theory of Everything right
or like great give me like a a full Mars
program it's like oof that's a little
bit beyond our capabilities right
because we just whereas if you just had
a single or a much smaller race of
humans that was significantly smarter
that could still beat Walmart like
corporations you can't just parallelize
everything you can't just paralize and
coordinate there's more to intelligence
than that at least that would be my is
there I mean that I think that's that's
a good framing of the Crux I think you
can paralyze and integrate I think
that's the key to understanding the
human species animal species the economy
I think that is the key to understanding
reality that's that's my sort of I think
at the very least you would at least
have to grant me that there has to at
least be a minimum threshold before you
can parallelize because you sure can't
parallelize
chimps what what do you mean like you
can't what I'm saying is like how many
chip how many chimps do you want me to
hire for you in order to better than
Walmart right like there's no number of
chimps I can give you that's true yeah
you need I mean you could think of you
know a chimp itself as a parallelized
and integrated thing with different
organs and tissues and you know cells
and organel and specialized macro
molecules can't but you can't just you
parallelism doesn't get you serial
intelligence right like there's at least
some threshold before your parallelism
trick starts working to get you serial
intelligence uh I feel like you're
you're you're assuming you're trying to
argue which is that intelligence is is
is one thing um I I think intelligence
just is the massive integration and
organ and organization of of parallel
specializations uh that's what I so I I
don't think there's any other way to to
get to intelligence other than doing all
that BR Brute Force specialization and
modularization and division of labor and
organization and and
integration okay fair enough um okay so
yeah just uh last one or two points um
let's come back to what you were saying
about comparative Vantage okay so make
that argument because I feel like I'm
not buying that one compared oh so um
there let's let's do the one of of just
the the um the logic of economic history
and economic development so as I see it
we don't see any super intelligences in
the economy right now we don't see super
employees who are capable of performing
any task we don't see super professions
whose members can do everything um from
brain surgery to um being the president
to uh playing basketball at in the NBA
um we don't see super organizations that
can uh uh provide all goods and services
to everyone all the time we don't see
super tools that can fix anything we
don't see super factories that can
manufacture anything so no matter what
you what part of the economy you look at
you're going to see massive levels of
specialization and hypers specialization
and Hyper hypers specialization that is
how the economy works and if you look at
the trends over time of Economic
Development what is economic development
well economists will tell you economy
developed when they specialize and
specialize and hypers specialize and
divide labor and experience massive
gains in trade from that specialization
and division of labor that is the master
principle of Economics it's the master
principle of how economies develop and
if you just fit AI into that Master
trend for which there's no exception to
it as far as I'm aware well AI is going
to fit into that Trend it's going to be
specialized you're going to have AIS
that are specialized to perform specific
functions that are economically useful
and you're going to get gains from trade
with AI specializing in one thing or
another thing and you're not going to
get an AI that just gets better and
better at doing everything because
that's not how anything in the economy
Works nothing in the economy has has
gone in the direction of getting better
and better at everything it has gone in
the direction of getting more and more
hyper hyper hypers specialized and so if
you just it's just an induction argument
this is the trend that we've seen
throughout all of economic history AI is
going to fit within that Trend what do
you think of that
argument um I yeah I really don't see
specialization as being very interesting
I mean if if you look at a modern
computer if you look at that fish tank
simulation maybe some of the work is
being done by the CPU some of the work
is being done by the GPU the GPU
specializes in more parallel processing
who cares doesn't matter what's
interesting is that it's tur complete
yeah there's some degree of
specialization there's always
specialization um the human brain yeah
it has a visual cortex and an audio
cortex okay they're a little bit
different right we have like language
processing areas fine it doesn't matter
those are relatively small distinctions
there's still a big underlying Force
which is general intelligence in the
computer case there's a big underlying
Force which is computation of course you
always have specializations you can take
a if if you if you were to my premise
that intelligence is a thing and super
intelligence will be a thing and there
will be a Jupiter brain that can
micromanage everything even if you grant
my premise that Jupiter brain is going
to be like okay uh to if I want a
janitor I'm just going to make a Roomba
right I'm just going to make like a
robot that just it's dumb and it just
knows to clean toilets but it does it
really well right so there's always
going to be specialization but I just
see that as a minor detail to the C of
our
disagreement so why do you think that AI
for the first time in human history um
violating the trend of every other IND
idual employee organization tool Factory
anything will for the first time become
better and better at doing everything as
opposed to better and better at doing
one thing very well so your observation
that we have you know we don't have a
basketball player who's also the
president so first of all I would I
would push back very significantly and
I'd be like look imagine you're a career
counselor somebody with an IQ of 80
comes to you you're like hm uh okay you
seem tall maybe you want to like coach
basketball somebody with an IQ of 120
comes to you you're like okay you open
up this big brochure it's like okay you
want to get into like Marketing sales
politic you know what I'm saying like
this person is a lot more versatile
because their brain is better right like
what can I say yeah I I yeah sure I
agree they're they're more versatile but
that doesn't mean that they are going to
actually become amazing at everything
they're going to pick one thing and get
very good at that thing right okay and
then my other uh observation that
contradicts what you're saying is like
okay Elon Musk Tesla SpaceX neuralink
it's the same guy running them all at
once yeah I mean they're running a
company you could think of as one thing
one type of that El musk has in space
okay okay okay so so that would be my
one point about like well you do see
this class of humans who see vers like
if you ask literally why isn't Elon Musk
literally um cleaning my house at the
same time that he's running SpaceX it's
very obvious why which is just he's one
person and he can't clone himself a
billion times which is not a constraint
you're going to have with the AI the AI
can clone itself a billion times you can
have one AI running everything I I would
disagree with you even if he could clone
himself a million times the reason he's
not cleaning your house is because his
time is better spent doing other things
things for which he's more productive
and that's the Crux of of comparative
but that that constraint is only because
copying is expensive if it cost Elon
Musk 10 cents to make a copy that could
clean my house and I'm going to pay him
$100 for cleaning my house he'd do it
why shouldn't he well because there are
opportunity cost to cleaning your the
copy of him could be doing uh other
could be running other companies why
would he be clean but when I say that it
costs 10 cents I'm already implying that
the opportunity cost is low because I'm
saying he just needs to go purchase 10
cents of materials right of data center
materials right but if if you're
assuming he's cleaning your house
instead of running other companies he he
would be much better off running
companies but it's not instead that's
that that's what I'm saying is you just
put the intelligence everywhere like
you're you're saying look at the human
economy everything specialized right
because nobody can just take the best
intelligence and clone it that's a major
bottleneck for Humanity well you're
assuming that there what there infinite
Elon musks now that that are totally
without cost like I'm assuming the AI
future where you just have Elon an Elon
Musk level code base right and then you
can just run it on a Data Center and you
can just have another one if you're
willing to to buy you know to to pay the
compute costs which are
cheap right but why wouldn't he do
things that Elon Musk is is already good
at at you know at the highest level and
do the most productive thing possible as
opposed to the least productive thing
possible so this gets back to what you
were saying before about like
comparative advantage where it's like
hey imagine that we discovered an island
and everybody there had an IQ of 80 and
they really couldn't do much science and
like all their products were inferior
but they were like so cheap that we
might as well like if if they're just
like they Farm apples and the apples are
like not that great but they're like so
cheap we might as well just buy their
apples and they want to sell it to us
right that's kind of what you're talking
about where like even a much Dumber
tribe can still trade with us right you
want to flush that out yeah more or less
yeah okay so my counter argument to that
the comparative advantage case of like
um you know why don't we just why why
doesn't Elon Musk leave cleaning to the
cleaners right kind of a similar case
along the same lines so my argument to
that is when you have the economic
framework where comparative advantage
becomes a theem that builds in
assumptions it builds in the assumption
that you can't just go in and replace
the island right like the is just like
take it or leave it right you can either
trade or not trade but there's a third
option that isn't in the formalism of
comparative advantage the third option
is like how about I just take their
atoms and repurpose them and then I'll
get even more utility than trading with
them sure but you're assuming that
taking their atoms and repurposing them
is zero cost and I think that is a
completely implausible assumption I
think that's hugely costly and uncertain
I mean imagine you just had an island
with cows on it right like maybe you
could try to trade with the cows and
like give them incentives and get them
to do what you want but like why not
just come in you know kill them all or
like eat them or just do something else
with the cows like is you might be like
oh it's too costly to do something with
the cows but I mean there's plenty of
examples in human history where we just
come in burn down the wildlife and like
do what we want to do uh well cows can't
talk but if you're assuming the if
you're positing a relationship between
an AI and humans humans would be able to
talk to the AI so that already puts us
in a totally different ballpark than we
are with respect to every other animal
on the planet um kacha Grace has an
interesting post about what the world
would be like if ants could talk and she
makes an interesting argument that yeah
we would almost certainly want to trade
with them I could pay them very small
amounts of sugar to uh you know stay
away to to you know clean little
crevices that I couldn't reach or to you
know not bother me or or whatever um and
so I think a pretty huge bottleneck on
us being willing to trade and cooperate
with other species is their inability to
talk and so as soon as you're positing
that an AI is going to be able to talk
with us it's not at all obvious to me
that its best move is going to be to try
to wipe us out as opposed to talking to
us and and uh reaping benefits from from
trade let's play out the ant
hypothetical so imagine I'm like here
ant here's a tiny bit of sugar that
doesn't cost me much and then you clean
my house a human I'd be like wow this is
great because I don't have an
alternative that I've built I don't have
a little ant robot but at some point I'm
going to get so good at just engineering
right nanoengineering I'm going to have
nanofactories it's not going to be a
problem if I want an ant robot at some
point I'm going to look at the ant and
I'm like oh you're taking my piece of
sugar and you're using it to metabolize
and like go feed your queen like I don't
I don't approve of these other things
you're doing I don't approve of the fact
that you need ant sleep or like these
other things that the ant is doing as
part of its day I would rather just hire
the robot ant who would be even more
optimized than the ant so the ant is
going byby at that point yeah I mean
that's it's a very wild hypothetical and
I and I I don't find it very plausible I
think at the end of the day so we're
we're getting stuck in this assumption
you what will AI if if super
intelligence exists will it kill us um I
think that's one of many assumptions
that doomers make I think it's an
independent assumption from their finish
playing out the an example I mean it
seems like the part that you're not
following is you're like look Evolution
built us this beautiful creature The Ant
and we're just never going to have human
engineering get to that point so we
better just take advantage of of the
creatures that we have have to work with
right is that kind of where you're
coming
from no what I'm coming so I think look
I I think there's a case to be made that
a super intelligence if we're assuming
that it exists which I I don't think we
should assume but if we assume that it
exists yeah there's a decent case to be
made that it would want to wipe us out
but I also think there's a decent case
to be made that it won't I think it's
actually very uncertain I'd put the odds
at like 50% or something like that um
just close out let's close out the
comparative advantage topic though right
I mean did you agree with what I said
that like it does build in that
assumption that you that you know you
don't have an easy but you could push to
repurpose the atoms right isn't that an
assumption um sorry that
uh you're you're you're assuming that
repurposing the atoms is cost free if
you assume that and not cost free just
just like affordable right and and an
example of it being affordable is like
whenever you see humans come in and
clear out an ecosystem right like burn
burn down a forest in order to plant
crops right like we didn't try to trade
with the forest yeah sure if you're
assuming a hypothetical Universe where
it is cost effective and the Ben it's
outweigh the cost of wiping out ants and
that is a better option than than
trading with ants then yes by definition
you are correct that we will wipe out
the ants I just don't know that that
hypothetical scenario is like I'm I'm
all that certain about that scenario
that's follow would you follow then if
if we're asking okay is it going to be
economical to just to repurpose their
atoms or not it seems like the Crux of
that question is just will human
engineering ever make evolutionary
engineering useless right because for
you and intuitively it's like wow
there's all these great things that
Evolution gave us like other other
humans and other animals we should use
those and I'm saying no no no those are
just lowquality uh robots right animals
are like lowquality robots from the
perspective of a better
engineer um yeah I think if we could
talk to animals would the world would be
a very different place we would see much
more cooperation and symbiosis between
humans and other animals okay okay nice
well I know we're we're coming up on
time you've been very generous with your
time so um yeah what other point do you
want to hit on if any and then we'll
close it out yeah I want to throw a
question at you so um want to uh talk
about what I see as a different version
of the fmy Paradox sure the fmy Paradox
uh is is where are all the intelligent
aliens you know the universe is very old
it's something like 14 billion years old
you you'd expect you know uh you know
intelligent civilizations to have spread
across the Galaxy by now why don't we
see them um I think you can uh I think
you could ask a similar question about
where are all the intelligent animals
you know the Earth is 4 billion years
old intelligence if you're right about
it is a very simple easily scalable copy
and pastable thing and it literally gets
you everything you want it achieves all
of your biological goals this is a buy
one get 100 free deal that is just lying
on the table and evolution has not
picked it up for four billion years and
only one species with a nervous system
has fully invested in this buy 1 get 100
or or buy 1 get 1,000 deal where are all
the intelligent
animals love it okay so first if you're
talking about the regular firming
Paradox which I know you're not I know
that was just part of your setup but I
still got to give a shout out to my
friend Robin Hansen friend of the show
uh because you know grabby aliens.com in
my from my perspective and a lot of
people I know and respect Robin Hansen
solved the fmy Paradox he should get a
Nobel Prize okay if you're listening to
this in Switzerland uh go go look up
Robin Hansen solving the fmy Paradox
with grabby aliens because he's that's
still underrated even though it has like
millions of views on YouTube just search
for grabby aliens but anyway we're not
talking about that FM me Paradox right
we're talking about the FM Paradox of
the animals and just to repeat it's like
uh you know why don't we have other
generally intelligent animals if I think
general intelligence is one quantity
right that's your question yes okay yeah
so to me I just think we're first so
it's kind of like imagine you're on I
love going back to a few billion years
ago or a million years ago on Earth
that's a common intuition pump that I
use so go back four billion years before
there was life on Earth and suddenly you
see the first replicator like the first
strand of RNA like some a bunch of
chemical reactions happen and and
suddenly it's like oh there's two of
these strands of RNA oh wow and like oh
this of RNA that's better at replicating
suddenly there's four strands of that
RNA Imagine You observe that and then
you turn to me and you're like Leon
what's with what's with the RNA fmy
Paradox how come there's no other
replicating rnas I'm like what this it
just started now and this is the first
so if you look at humans we just
randomly became the first generally
intelligent species on earth and the
moment we did that the moment we could
like talk to each other and and build
things like beavers can build Dam
spiders can build we webs we can build
anything the moment we hit that
threshold barely you know and and not
even all of us right there's plenty of
us sorry iq8 people right I don't think
you guys are going to be being
astronauts anytime soon right you're not
quite qualified to operate that rocket
but some of us right are able to like
write down some equations do some the
moment that we stuck our head above the
water that half of our species stuck our
head above the water of being able to
engineer things in a general way
suddenly we built a civilization
suddenly we we thought to ask about the
firming Paradox suddenly we thought to
make AI so this is very early days right
we're just first simple as that I would
buy that argument if intelligence were a
really complicated thing with lots of
moving parts that all need to be aligned
in the right place so you can make that
argument for RNA just the odds that any
molecule will necessarily meet all these
requirements are very low and so that
explains why it took a long time for RNA
to be hit upon but you're not arguing
that with intelligence you're arguing
that intelligence is actually very
simple it's just a simple kind of
reinforcement learning or simple basian
learning or whatever and as a matter of
fact we see a lot of animals that
already have it they already have
nervous systems nervous systems have
been around for a very long time uh and
learning systems have been around for a
very long time and if what you're saying
is learn humans just got more general
purpose learning stuff they just
expanded their Blobs of compute well
lots of other animals have Blobs of
compute why haven't their Blobs of
compute expanded and mushroomed the way
uh humans have I love the question and I
think you could rephrase it as this you
could be like Lon you're arguing that
general intelligence is an attractor in
the space of mind architectures right
it's a it's an attractor and and once
you get generally intelligent you're
going to get more generally intelligent
so if you've got this big attractor in
design space and you you know the same
way computation is a big attractor in in
circuit design space just everything
becomes a computer everything becomes
General Talent so if it's a big
attractor why isn't anything else get
attracted and my answer to that is
simply like yes it's a big attracted
attractor but you still have to start
falling in and in the case of human
evolution we occupied what Steven Pinker
calls the cognitive Niche where okay we
became these naked Apes we lost some of
our muscles we we started going down
this gradient in in Niche space right in
in in our relationship to our Niche
where it's like we started surviving
more and more on our on our wits right
like on our team work like but these
things took time and like for example
the opposable thumb before we were very
smart any gain that we had to
intelligence the fact that we have hands
with opposable thumbs that gives us a
wider range of actuator abilities so
when our brain gets a little smarter it
gets more ideas while our hands are
better able to execute on those ideas
than like a dog's paw right so we had to
start sliding down the gradient before
like the attractor is just not close
enough to other places in in Fitness
space to attract them but like now we're
in right and now we're going to be
accelerating we're going to be skating
we're going to be fing we're just the
first well other organisms have
opposable thumbs you know octopi have
eight tentacles that are fabulously
versal um plenty of animals can pick
things up um totally so I I I I don't
know that yeah I mean so and the
argument that Pinker makes is not that
we had a general purpose learning blob
he's in fact quite the opposite he he
argues very strongly that the human
brain is filled with specialized gadgets
and even the cognitive n argument is an
argument that that intelligence depends
on Specialized gadgets for social
intelligence and communication and
cooperation and Innovation and and
modeling certain domains of of the world
and having specific specializations for
for learning about tools or for learning
about plants and animals or for learning
about other people or for learning about
uh physics and how objects interact and
we have different specialized abilities
for learning about these different
domains of the world so Pinker is very
much on my side here so um I I don't
think that the cognitive Niche argument
really helps you out here I think it it
hurts you um so I would return to the
question um if intelligence is a buy 1
get 1,000 free kind of deal and it's
simple and other animals already have
nervous systems that are easily capable
of doing it and they have nervous
systems that can do it to a certain
degree it is it seems extremely
mysterious that this massive selection
pressure where you're literally solving
all your problems with one thing has not
been selected for in every other species
on the planet with a nervous system this
is an excellent question I think and and
just to make sure I'm summarizing
correctly so for example like why didn't
octopi octopuses why didn't they slide
down the fitness landscape if they're
already as intelligent as they are why
did they stay there right shouldn't
doesn't Leon think that there's a steep
Fitness gradient for them to get more
intelligent the same way there seems to
be a steep Fitness gradient for the
human brain to get bigger as as we know
from the difficulty of child birth and
the pelvis like where's the Steep
gradient for the octopus I think it's an
excellent question I think that I we
have enough evidence to know that when
we research the question we're going to
just see an answer of like okay well
they're just in the water so there's
just like if if you just made them like
10% smarter like there's just not that
many things for them to do in the water
if they get so much smarter that they
can like do physics right that they can
write down equations I think at that
point a discontinuous bump in their
intelligence probably would make them
start to take over the world but the
problem is until you reach some
threshold like they're still much dumber
than humans like if you put them in a
tank and you try to like teach them
their whole lives they're still not
going to be doing physics right so
they're they're not that close to
general intelligence and unfortunately I
think you're just seeing a hump right
you're saying activation energy and
evolution is not able to to get over the
pump like it's the gradient is not
always Steep and I think being in the
water is one part of that
explanation um I don't know if if it if
intelligence literally helps you achieve
all your goals it could help the octopus
attract mates Escape Predators find food
you know move to more uh preferable uh
habitats in the sea it could literally
do everything that increases its Fitness
if that's what you're positing that
there is a single wonder algorithm that
can solve all the octopus's problems
with one bit of nervous tissue it is
extremely surprising that the octopus
hasn't maxed out that tissue yeah so my
the nature of my response is just very
simply yes that there is it's just far
away in the search space and the only
way to search over to that space right
there the octopus has a nervous system
and it already has a nervous system but
it's it's a good question and so so the
only way that I'm correct which I'm so
I'm willing to stake my correctness on
on this claim is just that the amount of
genetic search that you have to do to
bridge the gap from an octopus's current
intelligence to to a general
intelligence you know like 100 IQ that
Gap is just so big that you're not going
to get local tweaks to get there because
when you start making local tweaks you
do a local tweak suddenly you've got a
higher metabolic cost and what skill can
the octopus actually do that can help it
one step at a time is it really going to
run away Predators better if it gets 10%
smarter it's probably already kind of
maxed out the running away predator's
ability so the 10% Improvement is not
going to pay for itself metabolically so
it's stuck in this local Optimum that's
not the global Optimum ah so you're
saying that some goals have limitations
and
constraints and some goals are easier
I'm saying it's not the global Optimum
so I'm I'm granting you that it would be
better for the octopuses to be generally
intelligent I'm just saying Evolution
can't get them there using local because
the problem is they're not generally
intelligent yet so I guess I guess I get
what you're saying you're saying Lon you
think intelligence sprinkling
Intelligence on things always makes it
better yes but when you're but when
you're currently very dumb when you're
currently very far from general
intelligence it's not necessarily true
that it's going to change your life to
become a little less dumb okay so that's
an interesting additional assumption
you're adding that intelligence is not a
smooth Continuum where the more of it
you get the better it's uh you know at
some point you know the more of it you
get it doesn't actually help you much at
all and so it's only when you accumulate
lots of it then it really starts to pay
off you're pausing kind of like a
discontinuity I mean if you're a clam
right what does it help a clam to be
more int if a clam is so much more
intelligent that it can like crawl its
way to like a physics lab and like
bootstrap a whole technological Society
okay great that's going to help the clam
but if the clam is 10% more intelligent
what is that going to do right like it's
the the shape of its life of the
optimization that it needs to navigate
during its life is just not going to be
helped by 10% more clam intelligence
well it will by definition if you're
assuming that intelligence gets it
everything it
wants well but there there's such a
thing as Fitness Landscapes right where
you okay you make a 10% but I I see what
you're saying and I think past some
threshold right if a chess player gets
10% better if the ELO scorer gets 10%
better once you get past the threshold
it does get kind of more simple to
analyze but when you're talking about
things that aren't even generally
intelligent then there's just
you you know giving it uh putting more
energy toward growing this other organ
is very plausibly going to pay off more
than sprinkling a little bit of
intelligence when it's still dumb well
improving one organ is improving its
ability to solve one goal improving its
intelligence is improving its ability to
solve literally all of its goals at
least that's what you're positing that's
what you're assuming okay so it it
sounds like now you're you're kind of
pushing my definition to a place where
it's it's just it's the kind of
definition that works better at the
higher level if it's just like hey Leon
what's the intelligence of a Rock of a
sock it's just like okay well hold on a
second like normally you start with an
algorithm that you can input an
arbitrary domain to and it can at least
like put in a good try even if it fails
so like you know an ADI human can still
IQ human you can still talk about its
chess skills like it'll probably not be
great at chess but at least it'll like
play and you can talk about okay I'm
going to make it 90 IQ so it'll play
chess better a clam it's not even really
playing chess to begin
with uh sure but like let's whatever the
the clam needs to do to survive and
reproduce it will get better at that if
if you give it more intelligence or the
octopus for example like why wouldn't
giving it more intelligence increase its
ability to find food court mates Escape
Predators you're asking the question as
if intelligence is like a substance
right you just like spray on more
intelligence right but in fact what
intelligence that's that's what that's
what doomers think that's what I'm
doomers don't think that you can uh that
there's like a physically simple
modification you can do to a clam to
make the clam survive better in every
way that's not what give a bigger blob
isn't that what they're saying okay well
this this I'm I guess I'm glad we're
touching on this because I do think that
when I was reading your essay I do think
there's a lot of times when that's how
you think doomers are thinking you think
that doomers are thinking that
intelligence is is something physical or
something algorithmic and like well okay
I I do kind of think it's something
algorithmic but like something physical
or something simple right like a black
box or something that you can spread
onto anything um there there's two ways
that I use intelligence number one is
just the the observational way or the
the behavioral way as like you you look
at a function the way it Maps inputs to
outputs it finds outputs that are small
Targets in a large exponential space and
that is like your observation that
intelligence has happened and then
there's a question of like okay but what
is the algorithm what is the physical
system that's giving you the
intelligence and on that Dimension I
don't claim that you can just go do
something simple to a clam and give it
more intelligence I I I claim that you
can observe a clam surviving better and
argue that the clam is more optimized
but I don't claim that there is
necessarily a simple physical tweak that
you can give it to make it more
intelligent so by your definition a clam
would already be optimized because it's
been selected to do the particular thing
that that the clams do uh and uh it's
already surviving and reproducing really
well because it's been you know selected
for over you know millions of years to
to do that thing well so by your
definition would a clam be
intelligent I think it's it's optimized
for its Niche and you know when I talk
about intelligence I'm just talking
about optimization so there is such a
thing as being optimized for for to live
in a certain part of a fitness landscape
from now I would actually claim I this
is where I kind of go out on a limb a
little bit like my my b disclaim is like
yeah if you put an entire human brain
into a clam and give it like some very
basic actuators right like let it crawl
around a little bit and use more code I
do think eventually it would create a
technological clam society and I do
think that it would it could then use
that to come back in and dominate The
Clam Niche and dominate other niches I
do think that general intelligence
trumps everything the problem is that
that Global Optimum that attractor that
you know Cas that Cascade where not only
does it get intelligent it'll get super
intelligent it's just not genetically
near where a clam is right so it's not
going to get to that Cascade it's just
going to stay a clam and any
modification is going to be negative I
would buy that argument with a clam
because you know it's it doesn't really
have much of a nervous system at all um
but I would not buy that argument for
any animal with a nervous system because
animals already have nervous systems
that already do learning that already do
the things that you think intelligence
can do so why not just add more of that
it's just it's just a matter it's not
like far away on the fitness it's right
there that's what happened to the Apes
right I I think a certain branch of the
Apes okay but why didn't happen to every
other ape and every other monkey and
every other species entirely a question
of the fitness landscape so my claim to
you is that you're going to see if you
diagram the fitness landscape you're
just going to see the shape of it
there's just not it's just not near you
can't apply a little bit of activization
energy and get a lot of Fitness out
anywhere except for Apes I just don't
buy that it's right there on the fitness
landscape you already have the stuff you
already have the blob of compute in your
nervous system you already have a thing
that can learn why not just copy and
paste just get more of that
stuff it's right there on the fitness
line it's very reachable fair enough I
mean I'm happy to leave it at that just
because I think we've we said back and
forth that I think we got the argument
out um but yeah I'll give you the last
word so how about give me a closing
statement wrap up all your reviews and
where you
disagree yeah so uh my problem with AI
dorismar in the piece I think it's more
of death by Thousand Cuts um and you
know in this conversation we've really
only gotten to a handful of the
assumptions the dubious assumptions that
AI doomers are making to bolster their
arguments and there actually many more
assumptions that dors are making I think
it is a delicate and fragile castle of
assumptions and and most of these
assumptions are largely independent um
and so I I think you know in my training
as an evolutionary psychologist and as a
scientist and and thinker um I am really
attracted to parsimony I love parsimony
that's part of why I love Evolution so
much evolution is the most parsimonious
Theory you could possibly imagine it
just makes a handful of assumptions
about you know members of a species are
different um the differences are
heritable and the differences matter for
survival and reproduction with those
three simple assumptions you explain
literally every functionally organized
chunk of matter in the biological world
it's the most parsimonious elegant
powerful Theory ever devised by a human
being um and I look for parsimony in all
my work as a scientist I try I I have a
paper that tries to explain political
belief systems by making as few
assumptions as possible and and
providing robust empirical support for
each of the assumptions I make aism does
not follow this trend it sets off my UNP
parsimony alarm Bells it makes tons of
assumptions often implicitly often uh
and uh often hastily uh often one after
another whenever I listen to doers I
feel like you know uh right when I
identify one assumption that's
questionable they've already made five
more assumptions they just kind of go
off on this on this string of
assumptions and I and I I just kind of
want to say slow down slow down let's
let's start with this simple assumption
that you're making right at the bat that
intelligence is one thing that already
is a huge assumption with lots of
uncertainty and lots of you know
empirical data that bear on that claim
let's start with that one and see if we
can get that one down then we'll move on
to the next one then we'll move on to
the next one then we'll move on to
whether you know if we have the super
intelligence thing whether it will kill
us or not which is another dubious
assumption added on top of the other
dubious assumptions so my My ultimate
plea to doomers is number one be clearer
about your assumptions list them out and
and give the best defense of them you
can with empirical evidence um and
number two try to minimize those
assumptions try to be parsimonious in my
view I'm open to changing my mind in my
view AI
hideously unparsimonious and that's why
I uh uh am uh repelled by it um and uh
you I'm open to changing my mind on this
but but the the piece I'm writing is
basically just walking through all of
the assumptions I see a a doomers making
and just pointing out how they're how
they're all questionable and how it's
really just a very delicate house of
cards that a gust of wind could collapse
um and uh you know I have a lot of
respect for doomers but I think the Trap
they're falling into is being overly
propagate with their
assumptions cool yeah thanks for that I
think that's a good closing statement
like that does kind of reflect the
impression I'm getting reading all your
stuff so way way to wrap it up um okay I
will go ahead and give you a response
from my perspective so the accusation
that doomers aren't parsimonious I mean
from my perspective the theory that I'm
positing of like hey there's this thing
called intelligence we're observing it
humans have it you're going to see more
Intelligence coming soon intelligence is
powerful um I see it that as being as
parsimonious as trying to tell somebody
in 1970 what's going to happen with
their video games and why they should
expect much better video games and a
different video game platform I see the
level of parsimonia as being comparable
and in our conversation I guess from
your perspective you saw it as like me
layering on assumption after assumption
whereas I saw it as like look I'm just
telling you logical implications of this
thing I believe like you're you're
asking me really good questions you know
you're saying how does this interact
with comparative advantage how does this
interact with Evolution from my
perspective I'm just following simple
logical implications of my parsimonious
idea and I'm just explaining them to you
and they're all satisfying but you find
them unsatisfying right so that's like
where where we reach an impass um yeah
so that that's my perspective um but
yeah I promise to give you the last word
so so I'm done so go for it cool yeah no
that this was a really fun conversation
I had a great time um thank you for
likewise I thought this was excellent
and and I I still think you're you're
going about this really good in good
faith like I believe that you you're
totally willing to change your mind like
if you saw maybe you're going to see a
warning shot like I feel like you're
somebody who if you see the warning shot
you might actually get scared and be
like a [&nbsp;__&nbsp;] I got to revisit these
doomers right like open-minded yeah
totally yeah and likewise I'm also
open-minded to like become more
confident that we're not doomed so just
two open-minded people modeling a high
quality conversation um all right David
so where can people go find you on the
internets uh you could find me on
Twitter at David pinof you can check out
my blog everything is [&nbsp;__&nbsp;]
everythingis bullshit. blog um you can
feel free to DM me on either platform
I'm uh quite friendly uh yeah yeah
everybody follows David's blog cuz he's
going to be dropping this essay you can
go Point by Point there's also points he
wants to make that we didn't have time
to get to here so yeah highly recommend
it also check out some of David's other
podcasts he talks about you know what it
means that everything is [&nbsp;__&nbsp;] and he
talks he says a lot of interesting
points about how like we're seeking
status very much like Robin Hansen in
the elephant in the brain but he's got
his own unique insights like why we're
not actually truly seeking happiness so
I highly recommend checking those out as
well um and yeah with with that uh let's
let's wrap it up thank you very much
David and I'll see you right here on the
next episode of Doom debates