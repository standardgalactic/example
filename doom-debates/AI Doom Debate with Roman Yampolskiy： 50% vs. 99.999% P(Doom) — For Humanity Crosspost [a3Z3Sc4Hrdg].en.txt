then you say it will do what's good and
what we want you mean like it will feed
us Donuts or carrots what's good for you
or what you actually
[Music]
enjoy welcome to for Humanity and AI
risk podcast episode number 44 AI Doom
debate 50% versus 99.999%
I'm John Sherman your host thank you so
much for joining me for Humanity is the
AI risk podcast for the general public
no Tech background required this show is
solely about the threat of human
extinction from artificial intelligence
got a great show for you today two of
the sharpest Minds in AI risk in a
really interesting conversation
Professor Roman yampolsky of the
University of Louisville fresh off his
appearance on the Lex fredman podcast
and Leron shapira Tech CEO and host of
the Doom debates podcast
which you should be following linked to
that in the show notes so pdom is the
percentage chance that AI kills us all I
think it's a pretty important tool as we
work to wake up the general public to AI
existential risk like to me telling
somebody totally new to this
conversation that say Elon Musk has a
30% P Doom is a really great way to cut
through all the noise and [&nbsp;__&nbsp;] like
yes Elon has an AI company yes Elon
wants to to colonize
Mars and yes Elon thinks there's a 30%
chance AI just kills us all before any
of that good stuff
happens that's really what he thinks
30% that is absolutely insane if a
medical procedure anywhere in the
country has a 1% chance of killing the
patient surgery's
canceled 30% is what we're talking about
please please hit like share subscribe
and donate to spread the word about AI
risk and if you can leave a comment or
send me an email at forh Humanity
podcast gmail.com and let me know what's
on your mind let me know what you think
about this debate so here is our 50%
versus
99.999% Doom debate with the great Roman
yampolsky and Lon shapira how are you my
friend wonderful good to see you nice to
see you too uh Lon should be in here
shortly uh uh how have you been doing
busy a little too
busy crazy crazy times yeah in a good
way busy uh fighting the good fight
getting uh getting thing listic and
nonsense okay all right well since we
last spoke you became big and famous
you're you are now
Roman yes and
life you can't you probably can't go to
the grocery store now you uh you were on
Lex Freedman you you get the best table
at the
restaurant haven't seen it yet but I'm
hoping one day one day so how are you
feeling about things is there anything
uh since we last spoke that has uh
changed your general perspective
no nothing really happened in safety
world so it's hard to
update yeah yeah
uh it's like nothing and everything
right it feels like something big
happens every week but I guess none of
it is of any real consequence
development none of it is safety
breakthroughs
right
no so I should be getting worse it's
just I'm at the limit I cannot go any
higher in my estimates all right here
comes LeRon all right Roman are you
ready for this this is going to be the
debate of the century well
sure hey guys LeRon
hey man how are
you hey doing great Roman nice to meet
you man of
course excellent so this is good I I
consider you guys uh two AI safety
Superstars but but you have never
crossed paths before this moment uh and
personally interfaced is that
right yeah that is correct that is
correct I think uh Roman's got a a
better track record I would say but
maybe I'll contribute something in the
future listen
I I need to say at the start of this
discussion uh one of the three here is
definitely the least intelligent on the
panel um that will be me but uh I will
try to uh add a meaningful contribution
to this so changed my kid's bus schedule
the bus was at 6:30 this morning I lost
like 48q points right
now yes yes uh Lon and I was just giving
Roman a hard time about his his uh his
big Fame uh you know uh his new nickname
Roman Hollywood yampolsky from his
appearance on Lex Friedman he's he's uh
right he's huge he's going on Jimmy
Fallon next week that would be amazing
wow that's awesome wow that's I wish
Jimmy oh you wish okay I thought I
thought it was real I was believing it I
know I wouldn't know who that is so it
wouldn't
work awesome all right so our subject
here today is uh P doom and uh the three
of us all have slightly different
perspectives on it the two of you um
have perspectives that I think more
Divergent uh Roman is sort of famously
at a
999999999 pdom and Lon at a even 50
which represents I I think you can give
the Nuance of what your 50 is but why
don't we just start out um by you know
each of you taking uh a few minutes and
just sort of laying out laying out your
position um why don't we start with Lon
uh who's got the little bit more hopeful
position coming in at 50% Doom Theon why
why are you at
50 so there's no particular reason why I
think we're on path not to be killed by
super intelligent AI so you know
famously the Miri position the elaz
owski position puts P Doom above 90% And
I don't have a particular disagreement
with all their arguments it's more like
I'm just allowing more room for unknown
unknowns I'm allowing more room for like
hey you know what maybe some AI safety
researcher discovered something lucky
that we had no ideas coming out of left
field and then also I would say the
majority of my 50% isn't even a lucky
break in AI safety but it's more like an
unlucky break in AI capabilities right
like we just get to another plateau and
it takes us 50 or 100 years to actually
get AI super intelligent and then maybe
in that time we figure out principles of
safety and the other part of of my good
50% is like we finally come to our
census and say wow we really need to
pause this like we're not Jing around
here it's time to pause so that whole
cluster is it really 50% is it 10% is it
90% I really don't know like when I say
50% I should really mention that the
amount of digits of precision that I'm
using is very low so I could easily wake
up one morning and be like Oh I'm not
50% I'm 80% or I'm 20% but what would be
shocking is if I wake up one morning and
I'm like oh I'm less than 5% P Doom or
I'm greater than 95% P Doom that seems
crazy to me I don't know how one would
ever get those odds and so it's very
meaningful to say 50% % just to indicate
that I'm not in that super high or super
low
range excellent okay all right and Roman
tell the people and and I want to point
this out one you know as you know this
podcast is for the general public so the
two of you are incredibly intelligent to
talk about this at a very high level at
times I may try to pull it back down to
second grade here but so Roman come tell
us uh
99.9999 why so it sounds like 50 as a
result of giving different estimates to
features of the model you use to make
this prediction I suspect if we go
through features one by one we probably
would converge much closer so for
example time frame then I say 99
whatever number of significant digits
this is a prediction over infinite
interval I'm not talking about next year
so I suspect almost everyone in this pom
prediction list is doing it for one year
three years 5 years if you ask me about
my onee prediction I'll give you
something very conservative 10 10%
2 years maybe 25% but if you keep going
if you keep adding it up probabilities
over infinite intervals will get you
pretty close to one uh people often say
well that's crazy you discounting all
the possibilities of you know asteroids
hitting the planet let's say one hits
let's say Humanity survives we have to
rebuild civilization rebuild technology
500 years later we're still building AI
to me it's the same thing we're still
dealing with probability of do from AI
over that interval over much bigger
interval yeah about 10 such features and
I suspect once we go through all of them
we pretty much Converge on the same
probability I think what you're getting
at is is the difference between my view
and your view is it's a matter of
whether the two lines are going to cross
where one line is capabilities
development and one line is principles
of safety in my mind I think if we slow
down capabilities enough and if we speed
up safety enough if we have like a
Manhattan project to properly do safety
then I think eventually at some point
the lines cross right like if AI was
going to take a thousand years and we
actually cared about getting our best
scientists to work on safety I'm pretty
optimistic that the lines might cross
maybe it's like a coin flip 50/50 at
that point whereas it seems like Roman
if I'm understanding you correctly I
think that you think that the AI safety
problem is just fundamentally unsolvable
so AI capabilities can just take as long
as they want but when they come come
along then we're dead is that basically
your position well it's an another
feature whatever it's actually solvable
or not you you got to try it I don't
think it's actually possible to
indefinitely control super intelligent
machines I think it's a very arrogant
position to suggest it's possible not at
an instance in time not for a specific
model but forever no matter how much
self-improvement no matter how many new
discoveries we have you'll never have
one slip one bug in your system that's
that's an impressive level of confidence
or
arrogance it it does so uh adjust for
the record my personal P Doom is at 75
right in the middle of you two so I'm
not taking that for safety I've said
that before this but it does uh ran that
makes a lot of sense to me that over
infinite time it seems pretty hard to
get to a place where we would maintain
control they run over infinite time
right I just want to say it's nice to
have a cool-headed moderate Centrist in
this conversation somebody with a 75% P
Doom an optimist an optimist
yeah so over infinite time how does that
how uh it sure seems like it would have
to we'd have to lose
eventually right so I do think there's
such a thing as the theory of alignment
the theory of safety now I'm not sure
right I absolutely could be wrong I mean
this is this is a new field right and
when I have personal experience dipping
into a new field I talk a lot about P
versus NP and computational complexity
Theory and theory of computation just
because that's basically the field that
I've studied the most like academic
research into and I kind of know what
it's like when you have a field that's
still stumbling its way towards
something important and it's not there
yet like we we don't know you know it's
I'm not going to go into it but there's
this thing called P versus NP it's a
million dollar problem since the year
200000 they put a prize on it it's a
much more profound problem than just
being worth a million dollars and we're
just chipping away at it it's taking
many many decades it's that like year 70
right now since it was posed and you
know it'll probably take like another 50
years at this rate unless super
intelligent AI solves it and I think
we're going to see a similar pattern if
we take the time to work on AI safety I
think we're going to chip away I think
we're going to keep having you know one
theorem at a time we're going to have
one Insight we're going to have um dead
ends um why do I think that it's
fundamentally possible well I I just
think that like all it has to do is be
reflectively stable and not change the
original utility function and get the
original utility function right I mean
what's fundamentally Impossible about
that
a lot of
uh kind of ingredients in that solution
don't seem to be well defined so then
you're talking about get the fundamental
utility function alignment problem who
are we aligning with who is the group of
Agents is it a specific agent 8 billion
humans including animals aliens our
volumes are not stable so even if you
managed magically to get 8 billion
people to agree on something we still
would not be happy with that
set of morals and values later on so
it's dynamically changing there is of
course disagreement you have
multiobjective optimization problems
which I'm not willing to compromise my
preferences maybe someone else is uh
last time I was at an AI safety
conference people try to formally Define
what the alignment problem is and the
only thing they agreed on is that we
don't even have a formal definition of
what we're trying to do each one of
those steps seems to be impossible if
you had that set of rules converting
Concepts like good and evil into C++ is
not trivial we wouldn't know how to
enforce it on a system and all that
assumes there is no malevolent agents
who will take your very friendly super
intelligence and flip a bit on it it
will not fall from proof it will not
modify it because it has commitment to
that previous set of rules which you
later learn not to enjoy you're
completely against it now but it somehow
got grandfather
D I think you bring up a lot the idea
that like how are we going to align all
the different humans and I think that
that's one component that's one sub
problem of the many alignment problem
it's like it is a fractal problem right
then there's like 20 ways that we can
fail to solve a sub problem while
solving all the other problems and still
fail so I get that but wouldn't you at
least agree that like if we simplify the
problem and we just say like fine one
randomly selected human gets their
values isn't that still pretty good
compared to the default outcome of the
AI not optimizing for anybody's values
that sounds really bad even not
considering certain outlier humans with
a lot of power comes a lot of corruption
a lot of malevolent uh impact if you
look at dictators many of them start as
very reasonable politicians trying to
improve things 20 years later it's
whatever they can get away with if you
give them immortality if you give them
control over all information no privacy
complete censorship things can get
really really bad so if our dream is to
install that system as the solution to
the AI safety problem then I think uh
the the winning opposition is Doom
itself and just just why would it only
be humans why would animal you know why
what about the bees and the cows and the
cats and the puppies and their goals and
values like why would the
ASI put us singularly above everything
and and make it about
humans yeah I mean I I think that that's
a hard problem to make it care about
anything right so when I think about AI
Doom I just think that the AI is going
to do what the AI wants and the values
loaded into the AI we're going to do
such an incompetent job at getting
values loaded into the AI that nobody
wins except the AI and the AI is
probably not even sentient enough to
even enjoy winning like it's just going
to be a nightmare but what what I'm
asking Roman is just you're talking a
lot about like dictatorship and some
humans winning more than other humans
but if you picked a random person and
made that person the dictator and that
person got corrupt and screwed everybody
but that person had a huge party across
the whole universe that lasted a
trillion years wouldn't that still be at
least a 10% consolation prize of the
ideal outcome whereas what I'm afraid of
is like 0. 1% like we don't even get
close to the ideal whereas making some
person dictator and letting them go Hog
Wild with what they want to do to the
universe that still seems like a decent
outcome for me right so I think that's
another feature where I think posi
explicitly says that P Doom is not even
defined as everyone dying that's not the
only definition also any type of
permanent suffling of our progress
culture dictatorships could be
considered as that so like any type of
hellish outcome where we're still alive
and uh exist uh can be counted as that
so I think I count those outcomes where
we created a permanent dictatorship and
most people are miserable as part of my
probability even though some people
still exist don't you think a lot of
people if they became the dictator would
have a little bit of empathy right maybe
they'd be like okay everybody else can
be my slave but at least you get like
air conditioning like don't you think
they'd be a little bit
nice I don't have air conditioning in my
office right now five days because
someone has a little too much power at
this
University this cor example I called for
emergency help five times if I'm
sweating it's not because the arguments
are so good
okay yeah I mean but but realistically
though right I mean if you're just
thinking about like what's the what's
the biggest reason to be concerned right
now like I agree that it is concerning
that somebody might be the dictator it's
but it's just like I I'm willing to to
settle for something like right now I
think we're so screwed that right that
when I think about like I think the most
important thing is to load human values
into the AI and if we miss ideal shared
human values and just hit one random
person's values to me that seems like
relatively close to a win compared to
just like you know destroying
everything so my suspicion is from what
we' seen with models they learn from
Human values eventually they switch to
first principles human knowledge is kind
of weak and worthless let me figure it
out on my own so whatever bias we
preload into it prum bias any type of uh
what IL is concerned about it will be
erased and the system will just go from
cosmological point of view external to
this planet is there any value in this
biological infestation maybe there is
maybe it's useful for something we
should preserve it in freezers maybe not
but I don't think we'll successfully
allow the model of that capability to be
indefinitely controlled by our biases
think about any prior bias would we
could put into it be very procris be
very Pro white be very prale whatever it
is at some point it would figure out
that there is not scientific reason for
Ania that it's just literally a bug in
its code from inferior programming and
so just real quick to second grade this
again so Roman you're saying that like
at some point it just clears out it just
like Control Alt deletes and it's like
all this stuff the humans put in me it's
just human crap it's not really that
important I understand the entire
universe now my desire is to see all of
it from a perspective far beyond the
human and I'm just going to start over
from scratch check sources so you're
learning from Wikipedia and there seems
to be a weird statement in Wikipedia
with weird citation you'll go and check
and it's like says the guy who was an
intern at that company decided that was
a good idea will you keep that that's
like your model of the world no you're
going to verify from scratch does that
even make
sense H okay so it sounds like you just
I I guess when I imagine a good AI
outcome where we get AI safety right I
remember imagining that the AI is
reflectively stable meaning like it
doesn't suddenly as you say like it
doesn't get a bit flip it doesn't get a
bug like it started out right and it
knows how to just make itself stay on
the same trajectory so but is it
accurate is your claim that the
trajectory is always going to get
screwed up or that we're just going to
make a mistake at the beginning of the
trajectory I think that a sufficiently
intelligent systems will always be able
to remove any unreasonable constraints
which are not based in physics
just kind of biased forced constraints
on them I also don't think we can
discount weird random bugs that's
another problem and finally if somehow
you were right and we succeeded at
figuring it out and coding it up we are
now stuck forever in 2024 ethics and
morals take any other year in human
history if we were stuck in there we
would consider it a failed
civilization right okay so so I mean I
think you and I might disagree on the uh
the robustness of the trajectory once
you started it like if by some miracle
we actually programmed in true human
values at the beginning I guess I am
optimistic that even given the laws of
physics and the randomness of the
universe and Chaos I'm pretty optimistic
that the AI can keep a good defense like
basically not get itself off course
right just be robust to all these
different uh knocks um so I I guess I'm
I think the big the interesting Crux
between us is you're just coming out
with such a high PD right didn't you say
like
99.999% like essentially 100% And I'm
like well I you know I think let's let's
try to dial that back I mean is part of
your
99.999% your disagreement with me about
how robust it can be once we get it
going so you you said something called
true human values I don't think that
thing exists you were referring to an
object which doesn't exist and I don't
think it can be created
so even that assumption to me is very
questionable well what if we Define it
as like conditions that are making all
the people uh who exist as satisfied as
they are today or better right so that
now you got a lower bound so how can
that go wrong well because a lot of
people today are living in miserable
hell and they don't want a grandfa that
in forever so me and you are maybe very
happy with our lives okay well what if
what if everybody has the option to kill
themselves if they're already
unsatisfied and then those who are
satisfied can continue being at least as
satisfied I mean you know this is
obviously rough but it seems like
there's some potential here and uh
people suggested that average human
flourishing should be optimized for and
one way to improve average is to kill
off all the people below
average I mean I I think you're you're
kind of jumping you're you're doing like
a slippery slope where you're like uh if
we have ai optimizing everything then
there's like the repugnant conclusion
there's all these philosophical
paradoxes I'm like okay I get that all
this is hard but just as a bound of
what's possible just as an existence
proof what if it's like okay don't
disturb anything right just keep the
Earth today anybody has the option to
commit suicide if they decide they're
not liking the trajectory but besides
that just try to like keep things going
right as opposed to destroying
everything isn't that at least like a
minimal thing AI could do so if it makes
no changes why are we building
it well I mean when I say keep things
going I also mean like try to uh give
people like the option to avoid death
like you know keep things going with
like a few tweaks right but I I guess
that's what I'm saying if it's literally
identical to today then it's a no op
right then it's like okay just don't
build it but I'm saying like keep things
going like have maybe have the option to
like checkpoint right like you can press
a reset button to get things back to
today like I'm just saying why do you
think defining a good value defining a
value that's better than today is
totally impossible that your probability
of Doom is so
high those things that really hard
saying like avoid death make sure people
don't die there are many very unpleasant
ways of making it happen
AI will get you to your target but how
it gets you there is not defined you're
not controlling that okay this is
interesting let me keep uh spitballing
here on this scenario so imagine we
build an AI and we just say like imagine
that there was no AI imagine it's all
humans and imagine the technological
progress that those humans would make
but help us speed it up if we feel like
it and the AI gives us a dial and we can
just turn the dial forward to pretend
like technological progress is going
faster and if we don't like it we can
stop it or we can turn it back wouldn't
that be like wouldn't it be better to
have that dial than not because that way
we could turn it forward a little bit
and we can get like cancer cures faster
but then if we like go too far we can
dial it back like isn't that nice to
have so we need time to adjust to
technology we cannot switch to something
completely new you give me technology
from 100 years from now 5 minutes later
I'm not ready for it I'm not using it
properly and uh it maybe very difficult
to undo it once you make it happen once
you have people dependent on some sort
of medical device you cannot just undo
all of it back to how it was
before it it's just uh what it takes
usually
we bring in Advanced Technologies so
let's say less developed cultures and
you can see what impact it has on them a
lot of times they are suffering as a
result of it they are not ready for
working in that world for dealing with
this technology and this is happening at
kind of human pace of years maybe
decades you're saying can make it where
it's five minutes later well all I'm
saying is it's just it's empowering
Humanity like I get what you're saying
about like hey technological progress if
it comes too fast it can create problems
but if it comes too slow it can create
problems right there's there's going to
be some ideal rate of technological
progress I think it's probably faster
than what we've experienced just because
there's so many good things at the end
of technological progress at least let
us maybe like solve medicine and then
we'll take a breather that that's kind
of my opinion but I don't think that we
have to necessarily agree on that I
think you might just agree that the
current speed doesn't always have to be
ideal so giving us more power to change
the speed or to be like and you know
especially if it comes with options like
okay let's just accelerate medical
research like just that right I'm just
saying like it it seems weird to me that
you're kind of you seem closed off to
the idea that there's like something AI
could do to make our lives significantly
better I agree with you on that
completely that's in fact what I'm
opposing narrow super intelligent
systems you want to solve medicine you
want to work on protein folding I'm 100%
for it I'm against developing General
super intelligence I'm agreeing with you
100% with
that would Lon wouldn't it develop its
own goals at some point like its own
goals and values right like like it
decides you know we like ice cream and
and it figure finds its own ice cream
and so I feel like I'm hearing you say
that it's going to get our values and
and goals loaded into it and hold them
forever uh and it just seems to me at
some point it goes its own
way yeah that that's a good question so
if you accept the premise which I think
is optimistic because I think it's an
unsolved problem but if you accept the
premise that we've managed to load in
Humanity's values into the AI where it
can just kind of run by itself and make
decisions and the world just keeps
getting better because the AI
understands what Humanity truly wants so
well and it actually goes and does what
so that's the premise that I'm asking
you to accept and then your question is
okay but then wouldn't the AI develop
like a taste for power right or like
some other motives right is that
basically your question just something
else like you know we like ice cream it
like something we couldn't even conceive
of we build ice cream shops it builds
the we can't conceive of shop on top of
our ice cream shop right right right so
in theory if we've built it right that
doesn't have to happen right like values
don't just have to emerge if if you just
have a stable set of initial values it
will be doing its own fix-ups like if it
notices that it's doing something if
it's like breaking down and not obeying
the original values perfectly it's going
to realize like oh better fix myself
because these are my values like
reflective stability actually is an
attractor meaning like if you've built a
system that's good at achieving goals
fixing itself to keep the goals stable
actually is a behavior that you're going
to expect the same way you expect
instrumental convergence to to like have
it seek power um but it'll seek power on
behalf of humanity and on behalf of its
original values we don't expect like we
don't expect a super intelligent Gandhi
to suddenly be like you know what you
know Independence is bad like no it'll
just always think Independence is
good so I'm kind of curious then you say
it will do what's good and what we want
you mean like it will feed us Donuts or
carrots what's good for you or what you
actually enjoy so that's an excellent
question you know what are are our
actual values right and I agree it's
super hard to specify normally the way I
punt on this problem is I'm like look I
can tell you that my values are not
everybody getting bombed right now okay
so let's put we can at least put like
some by process of elimination we can at
least start to describe some Properties
or values but then when you get to your
question of like okay do you want
challenges do you want everything to
come easily how much do you want to have
to work for for what you want then you
have to turn to like Nick bostrom's new
book where he says like okay what are
what do we do in a world where we can
just do anything and like a lot of our
uh a lot of the things we like are just
overcoming challenges but we're not
going to have those challenges so I
agree that is quite a tough challenge to
outline like what is heaven basically
right I call it like you need a product
spec for heaven right like Steve Jobs
designing what how Heaven is going to
actually work that that's all we need
and I don't really have much insight as
to that product spec but I would at
least try to work on it instead of just
all getting
killed so even with that extreme view of
all being killed being a big negative
there are people literal human beings
philosophers who argue against natalism
they say life is suffering would be
better off if we stopped existing that
is a legitimate human preference of some
people negative utilitarians will
support it so maybe super intelligence
will look at it and go this is really
what they want this sounds wonderful no
more
suffering right so I mean I think I I I
get where you're coming from like I get
the Rowan position which is like anytime
you try to find any sort of balance
between whatever human wants or even
just try to figure out what one person
wants and then like run with it well
it's still going to suck because like
there is no one answer to what you want
right is that basically your position
well I have many arguments against the
proposed solution you're basically
saying we need those 10 things to fall
in place and I'm saying each one of
those is impossible and here's how it
fails in cyber security in all sorts of
cryptographic applications you look at
worst cases you look at edge cases to
find bugs those are the edge cases I'm
bringing up you agreeing with me that
they look difficult I don't know how to
solve them someone will figure Nick
Boston will figure out
Heaven
hell so I guess I got to put on my Robin
Hansen hat now and and just be like look
I get that these are problems that you
know it might make life too easy or it
might optimize for one person's uh
political position more than others but
what would you expect without AI right
that's a question Robin anen likes to
ask how is that worse than the
trajectory without AI without AI we all
have about the same power we're all
Mortal right now if somebody becomes
really abusive others can rise up and do
something about it we are creating a
situation where certain malevolent
forces has become immortal and too
powerful to overtake what happens after
that we don't have a precedent for okay
so you agree that either way whether we
have General intelligent AI or not
humanity is kind of marching toward a
trajectory where we don't really know
what our values are going to be exactly
right they're going to keep evolving and
it's going to be a thorny problem either
way but the problem that you see with
artificial general intelligence is that
it might concentrate power into one or a
small number of actors and then we lose
the ability of like a bunch of other
actors to rise up like that's the main
issue you see with AI compared to no AI
I'm not saying main issue it's another
issue we don't have undo button in in
previous historic examples we made a
mistake we built communist country we
realize it's stupid we undo that I'm
sure you can do that with this uh system
which as you say will code in a specific
set of values forever and enforce it
against modifications because if you
allow for modifications now you Ling for
malevolent actors to take a friendly Ai
and modify it in unfriendly
ways I mean it gets tricky because even
what you're saying now about the ability
to undo and be flexible even that is a
value right so it is possible for the AI
to understand that it it should you know
slow its rule right give everybody some
power right these these are all actually
values right so it's it's really
complicated because the values become
self-referential and you have values
about how to execute your values or even
how to evolve your values but you're not
evolving your meta value that says
you're allowed to evolve your your
values there's two classes of values
right I mean so this is all complicated
for sure and I think you and I are on
the same page that when we're building
something complicated and Powerful that
we don't understand it's not going to go
well so I am on the same page as you
there I just think that there is an
equilibrium solution that is still good
by the standards of like you know life
being better right so I don't think that
the the best outcome is no AI I think
the best outcome is properly built AI
but I agree that that's like so hard I
mean my position is we should pause a
but I just I do think there's a
disagreement there between us I'm not
arguing against AI I love technology I
teach AI I'm an engineering Professor
right I'm just saying build narrow
systems to solve specific problems stop
building our Replacements stop building
something we don't understand cannot
predict cannot control we don't have to
we can have this differential
development in technology I agree I mean
just just sort of generalized opposition
to the move to create AGI like why you
know would you support do you support
Roman's position there that that narrow
only no
General yeah I do I'd like to pause
General AI because I do think that we're
a few years away potentially and I don't
want to be a timeline predictor I think
we're one to 30 years away but we're a
few years away from an AI that is just
uncontrollable with as Roman said just
No undo button so I am part of the pause
AI movement I do think the time to pause
AI is now it doesn't really feel that
way because it's just so fun and
seemingly harmless to talk to these chat
Bots I get that it's totally
counterintuitive to me why we should hit
the pause button now and everything
feels so fun and safe but as a matter of
just pure maturity and responsibility
it's like we experts are predicting
we're a few years away right like our
our our better angels are telling us no
this is The Prudent thing to do uh and
you know everything is at stake the
entire future is at stake There's No
undo button like the stakes couldn't be
higher um so and then ter Ronan position
of like well should we build narrow AI
while we're posing General AI if
anything I don't even know how
comfortable I am recommending that I
know that's a pretty standard position I
think elazar owski recommends building
narrow AI the problem I see with
building narrow AI is that anytime you
think is a domain is narrow but it's not
that narrow and you start getting really
really really good at it then you're
going to have the side effect of getting
broad for example like if you if you
just think that writing really good
essays for a history class is narrow
like it's just history it's just writing
essays
well if you really want to analyze that
history well suddenly you've got this
engine that can analyze quite a lot
right and it's not hard to just apply to
everything so I'm I'm not even I don't
even feel like comfortable I wonder
about that like uh Lon brought that up
when he was on the show before it's like
um Sora you know K Sora be the end and
it's like you make this narrow video
image generating uh system and it gets
super good and then you say Sora make me
a movie about a biochemistry Professor
who blows up the world to be very
specific about everything he does and
now all of a sudden you made a movie
about the end of the world that like
someone could take and actually go in
the world with so to me when you say
writing in general it sounds like
General
capability figure out a specific problem
you want solve you're very interested in
some sort of medical breakthrough
increase the length of tele meters I
don't know whatever a system trained on
nothing but the data set is very
unlikely to start driving
cars it's right I mean it's that
system so the problem is you start you
build this engine where like yeah if you
do nothing then yeah you're only going
to get the car driving but it's just
like if you look at some of the the
programs that exist today like suddenly
you know Sora for example right like the
the open AI program to create videos um
I'm pretty sure Sora couldn't exist if
they didn't work on a bunch of other
narrow applications first right because
those narrow applications are what
helped shape this Universal engine that
they now have that they just can make a
few tweaks for and suddenly they're
solving another narrow domain or even
another broader domain right so
unfortunately I mean these are all all
the limits that us uh safety is s
proposing they're all limits that are
unfortunately it's all fuzzy right just
like President Biden's regulation about
like okay you can't or if you're going
to have an AI that uses this too many
too many floating Point operations we're
going to limit that okay that's that's
you're reaching for floating Point
operations as if that's going to be the
boundary in this case we're reaching for
narrow domain as if that's going to be
the boundary to keep us safe
unfortunately there's not a good
boundary right these are all like very
rough boundaries which is why like even
even the pause AI position it's like
what are we pausing exactly I don't know
right and then and then that's why I I
tend to then turn to the other side of
like well while we're figuring it out
and trying to draw a little uh chicken
wire fence around this freight train
while we're doing that let's try to
hurry up with the safety because I don't
even think that pausing AI is is even
feasible for that long so I think we do
have some examples of success in there
something like Alpha 4 to me represents
ideal way of doing those things right it
solved a specific problem I don't think
it had any side capabilities can someone
take Knowledge from building it and
apply it in a different domain sure but
it's very different from straight up
creating a general model capable of
everything to begin with with
generalizing ability to learn it becomes
better at G that's the concerning
Factor yeah and I agree it does look
like Alpha fold there's not necessarily
a component inside Alpha fold that you
can now take and make a slight
modification to and and start asking it
questions about how to take power right
so it it does seem like a success case
but it just if you keep telling people
like yeah no problem go into any narrow
domain build the best AI you can and
that's totally fine I just I don't think
you're going to win every time like that
I agree it will get problematic and I
don't see any permanent Solutions I'm
just saying when people say well you're
asking us to sacrifice all the benefits
I'm saying no you can get like 98% of
benefits without 90 9% of
risks yeah I mean look I if I guess if
if I had to say I I think you're
basically right that one of the best
things we can say in addition to like
don't build too many floating Point
operations don't build gpus that are too
powerful let us shut down the gpus in
addition to these other very rough
constraints I agree with you that uh
let's only build narrow AI for now is a
good uh is a good constraint the best
effort constraint that we can do which
is still pretty bad um and then I think
but we should also put up a warning of
like you know pretty soon we might see
warning shots from even these narrow AIS
and we might have to update our
recommendation but for now focusing on
narrow AI sounds fine oh that's the
reason we should be able to update our
preferences and how we do this once new
technology is developed sure so Roman
here's a question I have for you with
with the
99.9 are there not some like events that
could happen like there's an earthquake
in the state of California literally no
Leon bless you I know you're out there
right now you got to get move to the
move to the other side real quick but
like California falls in the ocean
um there's a warning shot uh guy I
talked to the other day was talking
about a warning shot where basically the
internet itself is tiled by
self-replicating AIS and the internet
itself is permanently disabled but
humans
survive in either one of those two cases
California falls in the ocean all the
labs all the tech everything with it the
internet itself is permanently shut down
but humans survive are you still at 999
well I think it was my answer with an
asteroid example right even if human
civilization is partially brought down
if we're talking about infinite time
interval it's going to have time to
rebuild and restart this problem if you
want me to give you a prediction about
the next year or two I can give you a
very conservative estimate in line with
what other people I'm saying I'm just
looking at really big picture I'm
talking about super intelligence the
universe and infinite time spans if you
want to restrict it we can get it as low
as you want what are the chances is
going to happen today close to zero and
I think Roman I think we've talked about
how you think that this happens on a you
know in the universe this must have
happened every planet reaches this
threshold where their technology will
either destroy them or they move to some
sort of utopian State I guess is it
99.99999 that destroy
themselves uh obviously I have no data
about others it's likely they do create
a lot of simulations to see what happens
and it's very likely that we are
probably in one of them trying to figure
out how to solve this
I okay well
Roman just just as an existence proof
that we don't necessarily have to be
doomed like just to give you a simple
policy what if what if somebody
programmed the AI that artificial
general intelligence and gave it the
value of like look just simulate Human
Society in like the year 1900 or it
could even be like the year like 10,000
BC just have like hunter gatherer tribes
like just spread throughout the Universe
build a bunch of planets with hunter
gatherer tribes if they start getting
too feisty with their Tech progress
right if they start start getting
anywhere near the the tech of like the
of Greek level Tech ancient Greek level
Tech that's like too much right like
roll back the clock just keep everybody
in hund gatherer mode that's not the
Utopia that I want right that's not my
spec for heaven but isn't that at least
a lower bound on a scenario that you
could imagine could be stable and could
be better than Doom so you're describing
world with no technology no kind of
human intelligence just yeah like the
only technology is like the AI dictator
that's keeping a watchful eye on
everything and making sure that the the
state of exist s sense of all the
sentience is basically human Hunter
gathers but I'm saying for that you
don't need AI this is the default we can
just kill all humans and animals will
enjoy that pent exactly as you
describing at that level no we we can we
can but doesn't that make your P Doom
lower so with no humans around my P Doom
is now
lower it's not with no human I'm just
saying like I mean just to reader you're
saying your pdom is like pretty much
100% right like more than 99.9% but the
fact that there exist scenarios that are
like okay right they're like not that
bad and and they have like a solid 1%
chance don't they but I feel like we're
going in circles we already discussed
that I don't equate pom with everyone's
dead right a really terrible states of
existence like not having you know air
conditioning would be part of it joke of
course but seriously like uh you can
have pom where civilization is deprived
of his culture
technology okay so so by Hunter gather a
scenario in your mind you just think I'm
describing Doom so so that's that's part
of your
99.9% right so if we took today's
civilization and you said I found a
great solution what we're going to do is
we're going to convert this existing
state to that like post nuclear war
World War III scenario and you think
that's a win if that classifies as a win
I'm with you we are 50/50 sure okay yeah
I mean the reason I'm bringing that up
is I guess we should we should clarify
then I guess what you think is Doom
versus what I think is Doom um I think
if we're in if we're at least within a
couple orders of magnitude of what we
normally think of as like a good human
trajectory like okay we just keep having
technological progress we just keep
doing stuff we enjoy like whatever we
enjoy in our modern lives we just like
scale that up right like the intuitive
definitions of like things go reasonably
well in my mind it's like okay if we can
at least stay within 1% of that right
like not not destroy more than the vast
majority of that uh and then also scale
it up to me that's still on the good
side of the needle but then the bad side
of needle is much bigger where we just
like wipe out everything we don't even
get an iota of good stuff right we just
get nothing like that's that's what I
feel like is the big concern right now
um and it sounds like in your mind
you're like oh no there's plenty of
plausible Futures where we get a good
fraction of the ideal but we're so far
from the ideal right it sounds like
you're kind of operating in like
significant fractions of what could be
good and you're still calling that Doom
so I was giving this prediction
specifically for pori pom URL and what
they describe is
multiple definitions and not exhaustive
list they talk about yeah everyone's
dead due to biological weapons nuclear
war but even social collapse is
considered that if you're describing
primitive Hunter Gaber Society to me
that falls under that category do you
want to restrict pedom to something much
more narrow we can give you different
estimates for fixed time intervals for
infinite intervals again as I kind of
suggested I think we agree we just have
different parameters
as inputs to the 10 features of our
model for generating this final number I
I just want to nail down this point so
if in my mind the scenario where you
just freeze 2024 humanity and you just
have a dictator AI that just enforces
that life overall has to be a lot like
life in 2024 kind of like the movie The
Matrix right they kind of froze life for
Neo in the 90s or whatever so just
freezing life in my mind that's that's
like 1% of like a Utopia just freezing
life in 2024 that's that's not way off
of of utopia compared to just like
wiping everything out and then
especially if you get to copy it to lots
of planets so at least you have a lot of
different consciousnesses getting to
enjoy 2024 human life um in in that
scenario when you talk about P Doom do
you normally would that scenario be on
the Doom side or on the non- Doom side
freezing us from progress any type of
progress cultural moral technological
meaning what we see today is the best we
ever going to get to me is a bad outcome
so so you consider that okay so you have
high standards for what's not di I have
pretty high standards in life yes okay
because there's a lot of people alive
today who will tell you like oh man I
just love skiing right skiing for me
that that is the Pinnacle of life and
those people are still going to get to
do that but you're saying ah well you
skier you know what you're doomed and
I'm like okay well maybe I think the SK
is not doomed I'm just trying to
understand if we want to keep things as
we are why are we building super
intelligence spending all this money no
no but that's a separate point right I'm
just I'm just saying um I just want to
know how you define Doom before we talk
about should we build super intelligence
it but it seems to me like they two like
it's like if we froze everything like
today um that seems super impossible
given the technology um and it also
seems like I could see how it's an okay
outcome right like that you know like
that's not doomed to me like actually
better by way progress is not doomed to
me it's better to R's question of why
would we build it it's better because
you can defend it right so it's like you
get 2024 but also asteroids don't come
and wipe it out cuz you built the
asteroid Shi right so the AI just knows
that it has to like protect the 2024
lifestyle and it also has to copy and
paste it you know a trillion times
throughout the
Galaxy I'm sorry go ahead I was just say
it seems to me impossible that they
would that that I guess this is a
theoretical possibility but but to me um
it doesn't it doesn't like us in any
year it at a certain point it finds uh
you know everything about us in any year
of no value and moves past us and I
think that happens very quickly but I
understand the theoretical point
right well this is also the important
distinction I mean you touch you touched
on this before which is like wouldn't AI
eventually kind of like turn on us right
wouldn't it be like you know what
preserving 2024 that was how I started
but now I've decided that I want to just
like build prime numbers right just like
build build more computronium and the
humans are occupying my
space growing up like it's like first it
listens to Mom and Dad and it's like
this is what mom and dad told me to do
and then it starts getting its own ideas
and then by the time it's 15 18 19 it's
like f you Mom and Dad I'm going to do
do I want and then it goes and does it
yeah so I agree with you that in
practice that's going to happen but it's
a different premise if you start from
the premise that we just actually
understand how to build AI that has a
utility function and just actually cares
about the utility function you're not
it's not going to deteriorate right so
the the the theory of how uh
reflectively self-improving AI Works
doesn't say that utility function
deteriorates the concern is that we're
just not going to have it very correct
in the first place and so when when you
don't have a correct in the first place
you're going to get fooled and you're
going to think that you had it correct
and then the true nature of the function
that you specified is going to get
revealed see like you're the one who
initially screwed up and you eventually
learn that when the AI turns on you that
that's the
issue my a utility function like there
is not one you can extract it's not a
written down list it's not even
something subconscious we don't have
that and we definitely don't have it
across multiple agents so yeah we we
don't have
I I don't have a function that you can
tell me any possible state of the
universe and I can tell you which one's
better right like the idea of like
anytime I have adversity how much pain
should I feel about the adversity should
I shrug everything off should I feel a
lot of pain should I feel more pain than
I feel right there's all these questions
that we may not have uh concrete answers
to because it's just not specified in
our brains but I do think that it's a
partial ordering like I do think that
2024 humanity is better than just
paperclips and no Humanity right so I
think there's a few simple questions I
could at least answer for
you was 2023 Humanity
better I think 2024 is better just
because I think that society's been on a
good trajectory overall right because
we're just get we're just getting more
uh quality of life stuff like pretty you
know GDP and um I I like the trajectory
of society overall right I tend to be
like a techno Optimist Optimist yeah so
far I don't know I I I for just from my
seat it sort of seems like human values
themselves are going in the wrong
direction and have been
I mean there's a fair case you can make
right I mean there's certainly things I
can point to about 2024 that that I
think are are Lamer like you know
discourse has gotten very bite-sized
right politics has become uh less
respectful and and you know like I mean
if you look at like an old debate right
people are actually like listening to
each other and being respectful I like
that so I agree that I can definitely
find issues with 2024 I think we're
we're getting derailed to my main point
though which is that like I still think
that this Century as a whole is better
than four billion years ago right like
you can give me some states to compare
to some other states and then I can have
a confident opinion even though you can
always stump me with edge
cases I don't know anything about 4
billion years ago I really have no idea
how it was so I can't argue I think it's
very hard to compare mean it didn't it
didn't have sense your life
right in the universe somewhere but
right now look at the state of politics
in us I don't want to bring up any
details but half the country says let's
go back 30 years and half the country
says let's do jump into the next 20 so
there is clearly a nonzero portion of a
population which thinks 2024 is not the
optimal year for their
references that's true but I think if
you took a survey of uh if I think if
you taught people a little bit of
history and then gave them a survey of
which year do you want to go back to I
think it's generally agreed that this is
like a very high quality year compared
to human history and certainly compared
to the history before humans yeah and AI
would be able to brainwash you into
liking any year whatsoever it can choose
a trandom and educate you enough
I I get that right but the the reason
we're talking about this is just because
I'm saying that when you say that we
have no utility function and we don't
even know what you want I'm saying
that's a fair point but I know that I
want to like do a few fun things
compared to get tortured right like I
can at least give you a few one one to
one
comparisons sure but like how many fun
things we can put you in permanent state
of fund is that what you hope to get out
of it again as long as I can take a
break from the fun right I just just
give you some options
I'm saying like as long as you just give
me like a lot of different options so I
can change my mind right and then and
then you say like because you're you're
trying to basically say like oh you like
fun what if I give you like the maximum
fun and then you can't stop having fun
and like I get it I get it you could
trick me but just like give me a dial
where I can control how much fun I'm
having and I would prefer that to
getting tortured right it's like I can
tell you some statements about what I
want but you're making that decision now
before you experience the states of fun
once you in that AI Control world you no
longer have the same way to determine if
you refer it or not you under the
influence right I I do get that but it's
just I I feel like we might be sliding
to different points here and I just
wanted to kind of close out the point of
like hey I think that I can tell you
something about my values right I'm not
totally clueless about my values I know
a little bit about my values I agree
with that you definitely know something
about
you yeah let me can I just let me let me
throw this in here because this is
something where I feel like you guys are
in some different places and I'm in a
little bit different place too so El
eyes are Yi talks about how the systems
now are actors uh dumb actors and then
at some point they make a turn and they
start to transition from acting like we
are telling them to to actually having
their own you know ideas about what they
want to
do um got it gotta yeah John I mean you
keep you keep coming back to this and I
I really got to clarify so the Treach
turn is a real thing and it is something
I expect in practice but it's for a
specific reason the reason is because
like I said the true true utility
function comes out so it's not a
degradation or a change in the ai's
values what happened is we originally
told the AI something wrong like we told
the AI basically like um what we really
told it is uh like build as many uh
molecular smiley faces as possible but
when it wasn't very powerful when it was
just running in a Data Center and it
didn't have that many resources and it
didn't figure out how to hack into the
Pentagon back in the early stages when
it was just a baby AI the best action
available to it was like okay let me
tell jokes to make real humans happy but
once it gets more powerful in its mind
it's just doing the same thing it's just
optimizing smiley faces but now it's
more efficient for it to
Commander you know a nanofactory or a
biolab and and start with proteins and
bootstrap nanotechnology and then make
molecular smilees and have the molecular
smilees take all the atoms from the real
humans you see so it wanted smilees all
along but now you're seeing the true
face of the AI because it has more power
right that's the idea of the treacherous
turn so I think the main point of
disagreement is how real is this utility
function I don't think it's a real
object I don't think it exists I don't
think we can code it up you seem to say
that at least in theory we can
definitely figure it out agree on it and
then implementing it and practice will
be hard we agree on
that yeah and and you know it's called
the outer alignment problem the idea of
even specifying or asking whether it's
possible to specify formally a utility
function where maximizing that utility
function means doing what humans truly
want so and so question is is the outer
alignment solvable will we ever
realistically solve it um and I'm
optimistic that given enough time we can
come up with some sort of outer utility
function that's a lot better than not
having it that's where I stand so maybe
another feature we can look at is a rate
of progress and safety in alignment when
I look at it I see almost no progress
whatsoever what we have now is a few
filters to block certain topics and
certain key from being outputed but we
made no actual progress in what
intuitively people see as alignment do
you see there is significant progress
can you name I don't know top 10
greatest V person I think I'm mostly on
the same page as you that the that the
progress is just appalling right and
first as you would expect because the
total number of people who could be said
to be working on technical AI safety in
the whole world right now is at the very
most 500 probably significantly less a
lot of people with that title probably
aren't even working on it very much so
500 compared to
probably like five million people
working on AI capabilities right it's
it's insane so when you just when you
already realize how few people and how
few resources are going into working on
a you know famously open AI committed
20% of their resources and then they
reneged on the commitment so and that's
that's even a large commitment 20% but
realistically it's more like 0.01% it's
not even 20% so of course we don't
expect much and then sure enough we're
not getting much the only thing I can
point to is like what they call
mechanistic interpretability like okay
you can look inside the matrix
multiplication that that's happening and
you can identify some of the internal
structure you're like oh look here when
you asked it to like reason about
numbers here's a part that's like kind
of doing addition right you're doing
like this you're doing a little bit of
analysis it's it's very analogous to
like trying to analyze a human brain
right looking at like an MRI like oh
when this when when the uh when makalii
was thinking about like how to take
power like this part of the brain lit up
for like sneakiness or whatever right so
you're you're trying to like look at the
neurons um the problem with mechanistic
interpretability is like okay so you're
going to have a super intelligence it's
going to have a goal and like sure
enough it's going to logically reason
that you can seize power and you can
play humans off each other and you can
be manipulative like of course it's
going to think that because that's just
a logical implication of how you can
achieve a goal right like why would it
not think that any properly programmed
AI would draw a connection between like
oh if I were to wipe out these humans
that would help me achieve my goal how
could it not think that right so I don't
see so mechanistic inability is of
limited use even if it did work well and
then the issue we're having now is
mechanistic inability is also like 50
years behind capabilities because we
have a very rough resolution when we
look into the A's brain just like we
have a rough resolution when we look
into a human's brain so Roman I think I
pretty much am on the same page as you
that it's not like we're seeing a lot of
progress in a safety research so in my
book and relevant papers I argue that
there are strong limits to what is
explainable just upper limits we cannot
comprehend something so complex like so
we're getting a kind of lossy
compression explainability but that's
besides the point I think when you have
a new field a young field like AI safety
we can say it's only 10 years old you
expect a lot of low hanging fruit first
10 years of quantum physics is all the
seminal papers I don't see the same here
I don't think we have anyone arguing
like we got this amazing breakthrough we
have filters so maybe indication that we
are not smart enough but it also could
be indication that working in perpetual
motion machine and you're trying to
build it and you're publishing papers
about how great it's going to be once
you have it but no one's making actual
progress on the problem because you
can't that's if the numbers were flipped
if it was 5 million people working on
safety and 500 working on capabilities
does your view change at
all you asked me yeah Roman yeah yeah
yeah if it's 5 million people working on
safety does that do anything I think
problem with capability right now it's
about resources you need more money to
buy more compute you don't need that
many people to invent new things you
just need to scale existing technology
in my opinion so 500 people with5
trillion doll would do really well I'm
not sure how to convert dollars into
more safety if we had $5 trillion for
safety most people including myself
wouldn't know how to convert it into
something
useful yeah and I I think you're you're
actually making a really excellent point
about how like we're not seeing like a
bunch of low hanging fruit come out the
way we do see in a lot of new fields and
it's sad I mean one reason why we might
not see low hanging fruit is just
because the the feedback mechanism might
not be as good in this particular field
because it's like you know how to do
what humans want if you had ultimate
power but it's just like it's it's hard
to know when you're on the right track
right that's kind of fundamental to this
field and it it makes this field less
rewarding right the I mean feedback is
ironically the whole problem that we're
trying to solve is that we don't know
how to give an AI the right feedback and
and have it learn the right lesson and
then ironically as we try to research
the field we don't get very good
feedback when we're on the right track
so I I agree that it is thorny I don't
think that it's quite as bad as you say
because I do think that there's been
some breakthroughs and they mostly came
from me and elzra owski I mean for for
instance um the uh the Timeless decision
Theory the observation that if you have
two super intelligences they probably
won't suffer from the prisoners dilemma
issue they'll probably have a version of
sophisticated Game Theory to solve that
that's just one example of an actual
breakthrough that came from I guess this
isn't particularly a safety research but
at least it's research that was intended
to be safety and it came up with an
actual
breakthrough I'm very interested in that
domain of research I think it's
fascinating for cross Universe a Cazal
negotiations but how often is this uh
paper used in actual practical AI yeah
yeah not not at all right now none
whatsoever but just to give you a couple
more examples just um
result in safety and it's never applied
then oh yeah don't get me wrong I mean
these these results are far from
practical obligations but at least it's
you know the field is being chipped away
at right that's why I'm saying like if
you could just take what Mei did over
the course of a decade and you know put
the Manhattan Project worth of people on
it taking it seriously I do expect we
get some very important insights
insights that could massively change
whether Humanity has a future and that's
that's what I'm really passionate about
unfortunately there's very few humans
that are are able and willing to put
themselves in a position to Output that
kind of research and that's pretty sad
because that's really where it's at me
scaled down the research teams and
switch to communicating admitting they
have no way forward so that's another
somewhat pessimistic result in practice
I I think to be more precise I don't
think that they admitted that they have
no way forward I think they admitted
that this looks like a multi-decade
research program and it looks like
capabilities aren't quite a multi-decade
threat right they're much more urgent so
the timelines are so far apart that it's
it's stopping it's no longer a rational
strategy to just focus on accelerating
safety when there's so little time um
there's a very big difference from
saying like this is a hopeless program
compared to like okay I need 50 years
but I have five right it's it's
different
conclusions I I don't know what their
internal decision making was from what I
know they were not resource constrainted
so they could have kept the technical
teams going while also tweeting
fulltime yeah the technical teams of
like 10 people right so it's so they
just concluded I mean that that's the
key here that I think you and I are on
the same page which is that on a 10-year
timeline um then AI safety is an
intractable problem and that's what I
think you and I both agree that the AI
labs are doing a terrible job
acknowledging right they're always
saying like yeah don't worry we're going
to work on safety it's like wait a
minute let's discuss the possibility
that safety is a 50-year problem and
capabilities you guys are doing a great
job with capabilities they're coming
online very soon right prediction
markets are saying they're coming online
in like nine years right you yourself
are advertising that they're coming
online years and and nobody remember
when they had super alignment they had
the Hail Mary they like we're going to
do it in four years and the whole
project fell apart and nobody believes
him okay well what's happening like the
AI labs are not acknowledging that the
timelines aren't crossing right they're
basically uh head in the sand but they
know this they have to know it right
they know this like it's incredibly
frustrating they they know it and and
make no adjustments continue at the
speeds with safety slow capabilties
super
fast you know having the conversation
they better be having the conversation
but they're not taking any
action yeah yeah yeah I mean and you
know I think we all judge that they're
like morally bad right they're like Bad
actors to to be doing that and I can
empathize with being in that position of
just being like look it's safe for now
if I ever see enough trouble I'm sure
I'll be mature enough to pause but right
now we need to race I mean I empathize
with just like being in that position
but at the same time I do think I I like
to think that I'd do better and just be
like look I'm being Reckless right I'm
I'm dooming Humanity here like I it
would be better to coordinate out of
pause I mean I think that's all of our
position if I may I don't think you can
predict how long research will take
research relies on novel ideas and
breakthroughs nobody predicted how long
correctly predicted how long it will
take to uh beat uh humans at goal no one
anticipated GPT 4 performance at that
time so maybe we need one good idea if a
problem is solvable and you can get
lucky and get it in the next two years
this is not like building a skyscraper
where you know exactly how long it takes
this is not like scaling where you can
use uh you know computational curves to
say at this level we're going to have
human level simulation capacity and so
on so to say it will definitely take 50
years that's unreasonable no one can
make that prediction whatsoever
especially given they have no actual
plan for
doing
yeah so yeah so we all want to pause AI
I agree I mean it seems that the use
thing to do the only like the only
useful constructive way to deal with
this would be like we're going to stop
for 50 years
like yeah we're to start years while
accelerating capabilities in terms of
like a productive solution um I recently
pointed out on Twitter that I don't
think that we have to pause for more
than a 100 years uh that that's like an
upper bound just because if you just do
a naive human intelligence augmentation
program where you just take basically a
breeding program and it's not Eugenics
it's totally optional it's just
volunteer years but it's just like hey
people who like intelligence let's go
breed intelligent kids uh and you just
take the highest IQ people you can find
right a bunch of pairs of them and then
you just keep taking the highest you
know repeat as and you know try to have
young Generations right as soon as they
turn 18 or even 16 or whatever you want
to do right depending on how big of an
emergency it is you just keep breeding
these super high IQ kids and the reason
I'm optimistic is because I actually
think that a lot of the reasons why we
don't see like 200 plus IQ humans are
reasons that we could potentially tweak
that the one that comes to mind for me
is just heads couldn't get too big
because they had to fit through the
birth canal right the hole in the
women's pelvis but now that we have
C-sections there's a very
straightforward constraint right you're
just like let the heads get as big as
they need to get and suddenly there's a
lot of evidence to show that um if we
just select for high IQ we're just going
to get bigger heads and it's going to
work because they were already getting
as big as they could be because they
would get smarter and they would get
bigger so I'm pretty optimistic about a
breeding program and I don't think I
think five generations if Done Right
would actually get you like a good team
of researchers who could actually
understand the eye safety problem so
within a 100 years I think we could
eventually have those lines cross if we
slow down AI enough I know it sounds a
little bit crazy but I think you know
desperate times call for crazy
ideas I don't think any pause based on
specific time is Meaningful you need to
pause based on capabilities until you
can do XYZ don't do it so saying 6
months or 100 years those are random
arbitrary numbers Roman just real quick
to to to uh to lon's breeding program
can we just get to that real just to
just so you talk about that real quick
so if you if you to me it seems like if
you had 5 10 20 30 50 generations of
humans doing that you still don't come
anywhere close to where you need to get
to uh at the end of the game and so the
lines still go exactly where you're
pointing but what do you think if we
have five generations of super smart
kids breed with each other at 15 years
old uh do we get uh something that helps
us in our quest for a safer
future give me yeah I think so because
the goal isn't to get
like sorry
interchange so I don't know what that
means in IQ increase as a a human with
IQ of 200 250 in any case I think it's a
lot lower than what the super
intelligence system would be and if I'm
right and the problem is unsolvable
impossible to solve it doesn't matter
how smart you are you still can't
control indefinitely a much greater
intelligence forgetting about the whole
Jan thing right that's a side issue yeah
and I look I I think you may be right
right it may be a doomed effort but I do
think that it's plausible you know it's
it's like even 50% chance I mean I'm I'm
actually not writing off the ability of
a good man Haden project especially if
you get John Von noyman right like the
smartest people and even smarter uh the
best team we have I actually think the
problem might be tractable given a few
decades and that kind of team because
yes they're going to be a lot dumber
than the AI that they're building but
the principles of safety that they
figure out might be accessible right so
we have a lot of cases where there's
principles that are accessible to us
mere mortals that give us a lot of
Leverage and let us control systems that
are much bigger and more powerful than
us because we found the key principles
right like I mean a rocket right is much
bigger than a person right the person
looks like little ants next to the
rocket and yet we made the rocket land
itself right like sometimes we can you
know David and Goliath David's got some
some good Powers children of Genius is
are almost never genius as
themselves I I agree I mean look
breeding is not that easy but at least
it's it's plausible path if we put
enough effort into
it I'll donate some material but that's
all so have have either one of you
shifted in any way from uh your
positions is there is there any movement
whatsoever
yes um yeah I I you know I think I've I
I get um I get where Roman's coming from
I mean I think as Roman pointed out um a
lot of the original disagreement might
also just be in terms of like
definitions right so even just I I agree
that if I Define Doom as as being a
broader set of possibilities right like
a mediocre Hunter gather world if you
count that as Doom then obviously my P
Doom has to go up just by definition
right because I I wasn't previously
counting that as Doom so you know this
idea of like what is Doom what is not
Doom um but in terms of like where I've
actually shifted yeah I think Roman did
say a couple things that made me a
little less optimistic about alignment I
mean I like what he said about like if
this field is is such a good field if it
is analogous to other productive Fields
we've had isn't it still a little light
on the initial breakthroughs right
doesn't it seem like uh crappier than
average field and I think he's got a
point
there complimentary so if I restrict my
time frames to you know three years
instead of infinity and uh no longer
count crappy Lifestyles as do me and
only count complete exterminator of
humankind down to like 15% we're doing
great wow 15% in the next three years
nice okay all right and and let let me
ask you guys a broader question do you
think P Doom itself is a useful thing in
this conversation about AI risk and AI
safety and I I think it is because you
know I'm all about trying to just talk
to regular people about this and there's
a lot of words that have been thrown out
today that will are just over you know
people are just not going to engage with
but like um Doom is a word that is you
know sounds like a movie people are like
okay I Doom like I've seen a bunch of
movies that are kind of doomy I can kind
of see what that is percentage doom and
you're like yeah um so you know like
Elon mosque you that guy you know that
guy is right like whatever you think
about him he's building a system that he
thinks has a 30% chance of killing us
all and I think to the general public
that is a real like oh [&nbsp;__&nbsp;] what do you
mean are you are you serious what the
[&nbsp;__&nbsp;] are you talking about like who
would do that who could possibly do that
so talk just talk for a moment about the
the utility of P Doom as a concept as as
we try to wake the world
up well a lot of times I get invited to
debates and it's always uh Doomer versus
accelerationist or something like that
and I'm like I don't work on Doom I
don't research Doom I'm not the one
creating Doom it's mislabeling I work in
safety safety is safety researcher the
other guy seem take high risk which may
cause some Doom so from that point of
view it's just inappropriate labeling it
confuses completely is it good to put
some numbers to where people stand and
show okay like the optimists are saying
there is a good chance they're going to
kill everyone just to get profits out
that seems valuable if you understand
what those numbers represent like we
talk about me being crazy at 99% 1%
everyone dying in the next decade is
insanely High
absolutely um and I mean one thing to
note is like can people handle the idea
that Doom is high probability I mean
surveys show there haven't been a ton of
surveys but I think there's been a small
handful of surveys and the general
outcome of the surveys is like yeah 70%
of Americans are worried that AI might
extinct Humanity like some crazy thing
like that where sometimes you'll you'll
look at in the technology industry and
you'll think that's such a fringe
opinion but then it's like no Americans
are totally aware of the possibility
that aii might go out of control that AI
is scary and one observation I've
pointed out is like we in the
technosphere we tend to just be higher
IQ we tend to be an intelligent group
ourselves and so when we think about AI
we're like yeah we know intelligence we
have Smart friends we're part of the
intelligencia but the average American
statistically is going to have half of
them are going to have a below average
IQ and when they think about
intelligence they're like man I know so
many people who are like so smart and I
a lot of what they say is like kind of
hard to understand and like the AI is be
like even worse than that right so it's
easier to get an intuition about how
like smart AI is going to be tough to
deal with when you yourself are just
having difficulty like following stuff
that smart people around you are saying
so I I do think the average American is
primed to be worried about Ai and I
think that their intuition happens to be
correct in this case and when we say
yeah look at this p p Doom number it's
very high I that sounds like a good
strategy to make them think like Okay so
let's put some urgency into this policy
so I guess I'm for
it problem is they don't really have any
options now you convince someone one
that this is a huge problem what should
they do pretty much all advice is on our
security theat it's not really
mean I mean the posi type policies right
and I mean and unless you want to also
throw in the where Roman get real dark
get so dark in this world then this
world so Roman tell me if I'm correctly
stating your position but like basically
all attempts at
regulation all
uh policy approaches are theater and not
real like the TSA at the airport because
guns go through the airport all the damn
time and all you need to do is get one
AGI gun through the theater and it
lights out is that yes fair but I still
strongly support them they're a great
way to extract uh funding away from
compute towards litigation legislation
lawyers so I strongly encourage a lot of
red
tape yeah same here same here and you
know I agree with what you're saying of
like if we actually built you know like
meta with their llama right this is why
I'm really not liking open source AI
because okay the next version of llama
is like so close to Super intelligence
that just needs a few tweaks somebody in
Russia downloads it you know works on it
in their basement could be a teenage
hacker right like just a really smart
teenager and suddenly it's like oh look
if instead of the Transformer
architecture you use this other thing or
like you use this postprocessing now
it's super intelligent I agree with you
that at that time we can't hope to have
a regulation that it's like okay no
allowed to use that no at that point
we're screwed right it's game over which
is why I we you know now is the time
when it's not quite game over yet um and
and I agree with you that like dragging
things down now is kind of the only
leverage point we can hope
for yeah do you think we're at the this
just crazy question came in my head but
like we all talk about these Bad actors
and blah you know like are there like
James Bond super villains with rooms
with 50 dudes on computers right now
that are you know being like mush make
the make the bad thing make it bigger
make it stronger go like how could that
not be happening right now I mean
definitely I think North Korea has a
room like that where people are up to
Shenanigans right they're like oh let's
go make a100 million in Ransom right and
state sanctioned um hacking basically
are are those types of rooms also trying
to train the next model I think that's
an excellent question I mean you got to
you got to think it's very possible
right that like China or Russia would be
like let's or or Israel you know they
like to be on The Cutting Edge so I mean
yeah it's it's a great question I would
say out of all the different countries
at least one or two probably have an
effort like that and that sucks I have
zero knowledge I'm sure there are secret
government programs trying to get latest
technological Edge and how about
non-government actors just you know bad
guys crime crime law crime Lords uh
right I think
State yeah I so I think being on The
Cutting Edge of the next model I'm not
sure that they'd be trying to do that
compared to just because so I guess the
issue is just that it is expensive right
now right so the way that people get a
more intelligent model is dump a lot of
money into it because we just found this
Paradigm that if you dump 10x more money
you'll probably get a little bit more
intelligence out so when I think about
that the question that comes to my mind
is just that like what you said before
of okay so why doesn't China just
anticipate the next 10x right because
the problem is open a right now we
always think of open AI as spending so
much money to train open Ai and
Microsoft but so much money is like a
billion dollars why doesn't China race
ahead and be like okay here's a hundred
billion doll right like so I wonder if
they're doing that underground that that
might seem like a good guess
it's possible we Tred to control their
chip access so maybe it makes it a
little harder for them to get that level
of Compu even with
money sure sure all right let's let's uh
let's bring this this debate to a to a
close uh we we are all uh dads here
everybody's got a bunch of kids in this
in this Frame so P Doom is a horrific
thing to consider when you're thinking
about your family your future all these
kinds of things um
a lot of people watching this uh
struggle with the same things I'm sure
the three of us do which is just sort of
the weight of the subject matter and um
you know we're having a ludicrous
conversation here today absolutely
ridiculous if You' had told me two three
years ago I'd be sitting here with you
guys having this conversation I tell you
you're [&nbsp;__&nbsp;] nuts that's crazy
um how do you find the mental fortitude
to press ahead every day to smile at
your family kiss them out the door tell
them everything's gonna be okay and have
conversation super easy humans have
build with this bias we ignore our own
pee of death right all Dy and with 100%
probability my parents my children my
neighbors my friends everyone I know is
getting closer so we kind of build to
ignore that fact completely and go on as
nothing is happening be happy which is
doing it at the level of humanity
now yeah I think that's a great answer
so for me me personally I mean it's
there's an irony to this because you
know I live in San J California it's a
safe area and we're you know the fridge
is stocked right the house is is safe
the the chance that my kid safety is
going to be in Jeopardy for the next
year is very very low right so some of
the safest conditions that have ever
been created in human history I'm
getting to enjoy um and so I you know I
consider myself lucky because when I
think about most humans throughout
history trying to tuck their kids into
bed they had all these worries right
like are we good on food for the rest of
the week are we good on safety is the
other tribe going to come attack us are
criminals going to come steal our stuff
and I'm not worried about any of that
stuff but at the same time rationally
I'm like well as Humanity as a whole not
just my kids in particular but just
everybody may only have a decade or two
right so it's on one hand I'm like way
better off than most of humans try to
tuck in their kids on the other hand you
know I I have this big worry that
potentially outweighs everything else
but all you can do is as Roman said
right just like well just take it one
step at a time right I mean try to focus
on the problem and and that's it I mean
I don't have any insights yeah I mean
it's sort of like the mental game we
play is Right obviously we're all dying
ran everybody's dying and and you know
we're all closer every every second but
it's like um if you can you know just
sort of
like feel the difference between no
tomorrow for you no tomorrow for me but
no tomorrow for everybody no more
tomorrows at all is a totally different
thing to handle you know sort of in your
in your
in your gut
because you know like think about people
when they're dying old people when
they're dying they're so into their
grandkids there they they just want to
know that the story continues and and
the idea that it doesn't is something
fundamentally unhuman and um I think
really difficult to
consider yeah I'm I I mean that's that's
true that weighs on me a lot I think
it's it really sucks that the story
won't even continue because that's
usually the last that anybody can take
right even if you're being tortured to
death you're like well at least the
story is going to continue but now you
won't even have
that yeah [&nbsp;__&nbsp;] this was not the hopeful
place I hope to end off damn
it good night everybody
um I will say this so I was at 75 coming
in I'm a weather I I if you if I was
like the uh CNN debate Watcher and you
had the you know electrodes hooked up to
me and you saw like my
uh course through this debate I think I
I was definitely like Roman came out of
the gate very strong I was leaning Roman
I was probably Roman plus two or three
points Lon you pulled me back a little
bit towards the middle but I don't think
I'm all the way back I think I'm I think
I'm gonna end up today at
76 I'm gonna take a plus one for Roman's
infinite time frame which seems in that
perspective it seems hard
to uh not do a little update in that
direction
fair enough yeah I mean it's I think
Roman's doing a really valuable service
getting all these points out there and
and like I wish we had more people like
him my goal is to be wrong and for you
guys to convince me it's zero and life
is
good I would love that I would love that
someday we'll we we need we just need to
get the right person on here at zero who
can convince us all and then and then we
can all just go back to our lives Mark
andreon that's that's his that's his
calling okay friends let me know in the
comments who do you think won this
debate who persuaded you to shift a
little bit to their side for me Roman's
Long View on time makes it really hard
to imagine that he is wrong but I really
really really really hope like hell he
is wrong and even he would admit that
there's a significant nonzero chance
that he is in fact
wrong okay so it's 2024 we don't know
how long we have to live so we live
every day like it could be our last and
I end every one of these shows with
something I call the celebration of life
just something that makes us thrilled to
be alive for today's celebration of life
a little road trip um after my dog Dolly
had her major surgery she's doing
fantastically well I took her to uh one
of my absolute favorite places it's
called getaway house um and they have
them all over the United States there's
more than 20 locations and they're open
up new ones all the time they're mostly
out front of uh outside of big cities um
they should really be a sponsor of this
show I'm going to work on that I go
there for a little less than 48 hours
just like two nights um and I just look
at the forest and I think and I
decompress and I put bad things in my
body and then I come home and it is a a
really restorative ritual that I have
gotten to enjoy this was my sixth trip
to the getaway house here's a little bit
of it hey friends for this week's
celebrate hey dolly for this week's
celebration of life we are going on a
little bit of an adventure one of the
cool things about even in Maryland is we
have mountains like 3 hours to the west
and then uh the Atlantic Ocean and the
beaches 3 hours to the east so today I
am taking Dolly on an adventure to the
West we are going to the mountains to
what I think is about the coolest thing
in the world it's this thing called the
getaway house um and I've done it like
five times this will be my sixth time
super excited just to get away had a
pretty crazy couple of weeks got some
crazy couple of weeks coming up so just
going to take two nights here um to lack
sit by the fire cook some weird [&nbsp;__&nbsp;] eat
some weird [&nbsp;__&nbsp;] and just be by myself be
with Dolly and chill super excited about
it and I'm excited to show you what
getaway house is all about got some
mountains and some graen out here
today uh we are actually heading to
Virginia we are in the state of Virginia
now um and it's about a three-hour drive
it all kind of just looks like this uh
it's pretty not super pretty pretty
enough good to see those Shannon DOA
mountains right
there out here on the East Coast that's
what passes for
mountains all right friends we are here
we are here very nondescript opening
here you see getaway
there come up in here
so now you have the cool thing about
this place it's like 45 little tiny
houses all
situated the absolute perfect distance
from each other where no one can really
mess with anybody see like that's kind
of like what the little campus looks
like so every time you come you get a
different little tiny
house um and the one we are going to
today is called the bear so we got to
find it dolly
is very excited for what is happening
out the window all right so we are on
the hunt for the
bear we'll start to see as we get in
here all these adorable little tiny
houses they're like little
trailers that are
yeah that's what they look like like a
little little baby tiny trailer on
wheels that has been set
down
um and then it has a giant window in the
back that looks out into nature it's so
cool found the bear here's another bear
let's take a look
inside so the genius of this little tiny
house
so
cool The Genius of this place is that
they put the window right up on the bed
and this is just like a
painting so cool if I ever build a cabin
I want to make sure that I have like
trees like feet from the actual glass of
the
window so I'll just keep Dolly on this
little lead here you can see how close
the next one is not too
close there trails and stuff um and so
one of my favorite things about this
place is how I transform it when I move
in here so you can see when they get
here this counter is clean they have
like some nice little stuff here
whatever whatever but um I have gotten
so much candy and junk food and [&nbsp;__&nbsp;]
that I'm going to eat for the next day
and a half I'm normally very careful
about what I eat but I am just not going
to give any [&nbsp;__&nbsp;] whatsoever uh and eat
my favorite [&nbsp;__&nbsp;] [&nbsp;__&nbsp;] for 36 hours wait
till you see what goes on here okay I
honestly don't think it's that obnoxious
got some gummy nerds clusters hot tamali
Junior vs Reese's
Rollo truffles Peppermint Patties this
like lemon
bar you know some moderate chips
pistachios whatever I don't think that's
that bad um coming down into
here we got some fruit got a lot meat
fruit some cheese
water it's about it doly that's not that
crazy right that's not that crazy that
is not that
crazy this is my
spot I sit here
for two
days I think
and that's
it did about 44 hours
here
in
getaway bear it's the name of our little
cabin looked out at these
Woods sat at this
campfire now it's time to go back to the
regular world
H I cannot recommend a day or two at
getaway house enough okay my friends
that's all for this week please remember
that AI risk is not someone else's
problem it's yours and it is mine
quoting Margaret me never doubt a small
group of thoughtful committed citizens
can change the world indeed it is the
only thing that ever has for Humanity
I'm John Sherman I'll see you right back
here next week
for