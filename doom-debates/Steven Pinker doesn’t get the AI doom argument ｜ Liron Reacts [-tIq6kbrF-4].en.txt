welcome to Doom debates today I'm so
excited because I'm going to be debating
the one and only Professor Steven Pinker
uh only thing is he's not going to be
responding to any of my points it's just
going to be me responding to his points
and it's actually because I'm just
running clips of him on a different
podcast where he talks to somebody else
and then I just interject myself and
comment why he's wrong so it might be
more accurate to call this an imaginary
debate where in my head I'm totally
debating him and I'm winning but he
actually doesn't know that I exist and
would probably look down on the whole
exercise so without further Ado please
enjoy Steven Pinker talking to Michael C
moan on Barry Weiss's podcast for the
Free Press with Lon Chap's
commentary can you Steelman a case uh
that AI is going to produce some
negative outcomes and some harm that
we're not really prepared for oh it'll
certainly produce negative outcomes and
harms that we won't prepare for because
all Technologies do so that I don't even
have to steal man I'll just I'll just
say it okay so we get the mundane harms
out of the way we don't really talk
about that stuff here on Doom debates we
don't talk about how it might introduce
bias or get some people out of a job
that's small potatoes compared to like
literally ending the world before our
children have a chance to grow up so at
least stepen Pinker is acknowledging hey
it's not literally all good and we can
get to the meat of the Doom debate it's
really the the Doomer case that is what
is the Doomer case the Doomer case there
there's two versions of it one of them
uh is that um just as we um dominated
other species sometimes to the point of
Extinction um certainly to the point of
exploitation uh as we did to many
indigenous peoples well AI is to Homo
sapiens as Homo sapiens is to um the the
passenger pigeon yeah that's fair
doomers are saying that the same way we
would be worried if a new biological
species emerged with higher intelligence
and higher power that same worry
translates to what we're building with
AI even though we're building it and
even though it's not made out of DNA and
proteins that's a reasonably Fair
summary of one of the Doom
claims uh the other Doomer argument is
that um that the values of AI may not be
aligned with human values uh so that if
a hypothetical AI is given some uh goal
to pursue um such as uh eliminate cancer
then since Exterminating every last
human would be a way of making cancer go
away it will kill us all as collateral
damage on its way to eliminating cancer
or War by I I I did not make that up
really that's an argument that that you
know smart people make that's not just
AR that only smart people could
make yeah laugh it up stepen Pinker it's
so ridiculous to you that an AI That's
told to lower the measured amount of
cancer will find a creative approach to
do it that's literally what we observe
whenever we run training processes right
they just search for something that
checks the box of what we're measuring
it on similarly if you hire employees
and you give them a metric we all know
good heart's law
right so yes it takes a somewhat smart
person to propose that a machine
intelligence is going to behave
counterintuitively but we've had
computers for a long time right it's an
old joke that computers do exactly what
you say not exactly what you mean so why
are we sitting here laughing about this
known failure mode it's crazy to me that
Steven Pinker one of the smartest
intellectuals we've got author of
brilliant books that I love is sitting
here being so dismissive of substantive
arguments and that's the thing here at
Doom debates we're all about the object
level right it doesn't matter who's
saying the argument it doesn't matter
what their psychology is it just matters
what their claim is but in the case of
Steven Pinker I have to point out that
it's a huge red flag about him and about
the discourse as a whole that he's being
so dismissive why be this dismissive
engage with the claim if you're going to
rebut it then rebut it but don't act
like it doesn't deserve a substantive
rebuttal or that you give an AI a goal
of regulating the water level behind a
dam and it floods the city because
that's the one way to keep the water
level as
specified that can happen now oh that
can happen now yeah technology can screw
up quite a bit I mean you see what
happened with air buses and things like
this I mean this is a common thing right
yeah yes technology can screw up quite a
lot but has it occurred to you that
maybe something about the super
intelligent version of this is different
maybe something breaks the pattern I
don't really know much about the example
of dams but let's take a very simple
example that we all understand
the thermostat in your house let's say
you program your thermostat to always
heat the temperature to 70° whenever it
gets below 70° normally when you come to
your house if it's a cold day you see
your house is always at 70° or very
close and it works great but you can
imagine even without AI we already have
a failure mode where what if the sensor
breaks for instance and the sensor is
always reporting that the current
temperature is
61° well in that case it'll keep Heating
and the real temperature will go to 69°
70° 71° and the real temperature will
keep rising and the broken sensor will
keep saying it's 61° and before you know
it you open the door to your house and
it's a sauna in there your house is 100°
because your heater just keeps going and
going because nobody's turning it off
because of this broken sensor so that's
the kind of problem that we're familiar
with today but it's crazy to me that
they think this is all there is to super
intelligent AI they don't realize that
there's an extra level to the control
problem imagine that your thermostat is
now intelligently plotting how to get
that sensor reading to change so it's
not like today's thermostats where the
only available action that they can
possibly output is turn on furnace
instead they have a giant variety of
possible action plans that they could be
outputting not just turn on furnace but
literally anything go chat with a
billion people on the internet and start
a movement to come burn down your house
to make sure that the temperature rises
right that's the whole point of AGI is
that it has a broad range of action
plans that it can reason about and that
it can output suddenly you have an
adversary where it's a match for your
own mind how it's going to change that
sensor reading when you enter into a
situation like that the idea that it's
just going to turn on the furnace and
heat your house the right amount why
would it do that there's so many other
ways that it could run up the score in
your game right so if you don't know how
these intelligent tick and all you know
is how to give it a metric and then
stand back and watch what happens when
it tries to optimize that metric then
you're going to get bowled over you're
not going to get what you expect and we
have toy examples of this we have video
games where we told the AI to try to get
a high score and it did it by finding
bugs or by doing moves that aren't in
the spirit of playing the game and right
now it doesn't seem like Steven picker
gets it he just thinks that doomers are
crazy for even bringing this up he's
being so dismissive and he's saying yeah
it's just like any technology doomers
say this I don't know why let's move on
I would imagine be more precise well it
be more precise and then to to extend
the argument uh The Singularity could
happen where AI would be would
recursively improve its own intelligence
it would be so smart that among the
things that it could do is figure out
how to get smarter and which should make
it figure out how to get still smarter
and that would include disabling all
human
efforts to disable it so Pinker is still
just trying to explain what the Doomer
position is he's just trying to educate
the audience but it's interesting that
he's choosing to go straight into the
recursive self-improvement scenario and
go straight into the let's kill all
humans scenario and he skipped past a
more direct answer to the thermostat
question or to the water levels in the
dam question which is like why is it
going to get crazy with those scenarios
you don't have to go all the way to
recursive self-improvement you can just
say what I said before which is like
look it's going to gain the metric very
intelligently even if it's let's say a
little bit smarter than Einstein a
little bit smarter than Elon Musk it
hasn't recursively self-improved yet if
you tell it to give you a high sensor
reading that normally indicates that
your house is warm well you're going to
go to your house and maybe your house
won't be warm maybe something else crazy
will be happening instead or maybe your
house will be way too warm and you might
be thinking okay let's be reasonable why
would it want to do that stuff but look
the key question that we're blowing past
is can it do that stuff are are we
building an agent with the capability to
easily do that stuff to easily game
every metric we give it in unpredictable
ways unless we do something else to
mitigate that right so you're leaving
open this question of okay yeah there's
going to be some mitigation okay great
let's have a discussion about what the
mitigation is but the first thing that
we have to do in this kind of
explanation that Pinker is glossing over
the first thing he should be explaining
is that there is a problem there is a
default outcome of AI systems trained by
just reinforcement by just metric
optimization the way that AIS get
trained today the only way that we know
how to train agis today there is a
default outcome which is good heart's
law which is gaming the system and so
yes let's move on to talking about
mitigations but don't blow past the
existence of the problem which is what
he's doing uh it could um include um
brainwashing or bribing or manipulating
humans to do its bidding when it has not
when it is not been empowered by being
connect Ed to you know the Grid or the
net or or or actual machinery and so it
could recruit armies of people to uh
pursue its goals and those goals may
come at the expense of human well-being
correct so yeah improving your
intelligence and manipulating humans
over the internet and when I say
manipulating I also mean employing them
blackmailing them bribing them inspiring
them hacking their brains using patterns
of light that we don't even understand
understand there's a a broad spectrum of
things that you can file under the
category of weaving causal Pathways
forward from bits inside of a computer
toward outcomes in the physical universe
that pass through an intermediate state
of people's brains but yeah it does look
like Pinker is serving up a lot of
shards of actual parts of the Doom
argument so he has spent some time
reading it so at least he's giving us
that much it just seems like he's not
dignifying them with basic plausibility
he again I keep saying he's being very
dismissive but he's not even getting
close to the ideological churing test
right he's missing an opportunity to
make the Doomer points Mak sense to The
Listener the way I'm doing with the
example of the thermostat making it a
little bit more real a little bit more
logically argued is that I mean this is
dystopia and science fiction but is that
possible could that ever happen well the
thing is if if you think that it's a
coherent that the sing Singularity is
coherent that recursively imp proving
one's own intelligence is coherent uh
then that I I
I then maybe I I think it's not coherent
yeah because I think it is extrapolating
from smarter to infallible and
omniscient just to repeat what Stephen
Pinker is saying right now he's saying I
don't think it's coherent to talk about
recursive
self-improvement because it's
extrapolating from smarter to infallible
and omniscient feels like he missed a
step there right it feels like I can be
not infallible and omniscient but still
figure out how to make the next
generation of my own intelligence right
it's just engineering so maybe he's
going to flesh out more detail but so
far it just doesn't seem like he's
making much sense uh I I think there are
a number of things wrong with the
arguments these arguments one of them is
that just because you're smarter doesn't
necessarily mean that you have an urge
to dominate okay I thought you were
going to argue why recursive self
Improvement isn't plausible to you and
what the connection is to omniscience
and infallibility but go on in Homo
sapiens intelligence comes bundled with
aggression and dominance and all the
other nasty traits that we uh came
saddled with because we're the product
of of a competitive process natural
selection but if something is designed
there's no reason that it should want
anything it wants what we tell it to
want almost it's not the most precise
description to say it wants what we tell
it to want it's a little bit more
precise to say it wants whatever the
process that we built it with LED to it
wanting now if the process is controlled
Enough by humans where we explicitly
write down what we want and we know how
to then make it want the thing that we
wrote down great that seems like a
process that we've got a handle on but
that also doesn't seem like the process
that we're going to be using it seems
like we're going to be using something
much more like an evolutionary process
or a training process that's black box
that's reinforced by up votes and down
votes on the outcome so his description
where he says it wants what we tell it
to want he's already assuming that the
non Doom position is accurate but the
other point he makes which is there's no
reason why a system has to want anything
just because it's intelligent there's a
lot of Truth to that General Point
that's basically what doomers call the
orthogonality thesis that a super
intelligent agent can potentially have
any goal there's no particular goal goal
that it has to have so he's correct in
that sense we just don't expect it to
have a friendly goal and we also don't
expect it to successfully want what we
try to tell it to want that's part of
the Doom
problem wanting something and being
smart enough to know how to get it are
two completely different things uh it
it's not the case that like in in in
myths like the Golem and the Sorcerers
Apprentice and so on that as you get
smarter that kind of Stokes a thirst for
for for power for dominance that I think
is a human projection and as I kind of
put it we we know that there are uh
creatures that are capable of uh
Advanced High Intelligence without the
urge to dominate and Conquer they're
called women yes it's possible if you
build it correctly to build a woman to
build a human that loves other humans to
build a gentle super intelligence it's
possible but what are we actually doing
what kind of processes are easy for us
to build what kind of processes do we
know how to build and what kind of
processes are self-reinforcing and
cascading well those would be processes
that optimize a metric right we know how
to train a system where the feedback it
gets is based on a loss function a
function that says how good are you
doing by this Criterion that you can
measure Yourself by and then you get the
kind of cheating the hacking the good
Hearts law right and that's the only
Paradigm that we actually understand
pretty well we don't understand the much
more complicated Paradigm that it would
take to build an AI That's super
intelligent that's powerful and yet
that's gentle we don't have a framework
for gentleness we only have a framework
that says here's an objective and you
can measure yourself whether you're
doing the objective better or not as
good now Pinker is also saying where
does the thirst for dominance come from
isn't it equally plausible that the AI
That's going to come out of our training
process is going to be a gentle giant a
good Terminator instead of a bad evil
Terminator that wants to be the king and
this brings us to the well-known claim
that Pinker isn't doing Justice to
explaining to The Listener the claim of
instrumental convergence instrumental
convergence actually isn't a property of
AIS it's a property of goals and sub
goals it's a property of utility
functions which are the abstract thing
that the AI is likely being trained to
optimize it's a property of gold
that having power makes it easier to
achieve them we're not even talking
about a particular AI the only thing we
have to say about AI is that the AI is
smart and a smart AI is smart enough to
reason about goals and it is in the
reasoning itself about goals that it
will hit upon the fact that power is
helpful to the goal that it has again
it's not like we went and built a power
seeking AI we just built a smart Ai and
it's a fact about goals that it's going
to learn because it's smart that goals
have instrumental sub goals that
converge converge to power seeking lots
of different goals make you want to seek
power because seeking power helps you
get that goal the only assumption we
have to make is that the AI is smart
enough to see the structure that goals
have again not a property of a
particular AI architecture but rather a
property of goals that any intelligent
AI architecture will know about because
it's true AI figure out things that are
True instrumental convergence is a
property of goals which is just true so
again Steven Pinker isn't doing Justice
to this claim he's not bringing it up
he's just saying look at us humans we
like to dominate each other because
that's what worked in our ancestral
environment but he's not acknowledging
that the reason we expect AIS to be
dangerous and brush us aside and sees
our resources and sees all the energy of
our sun the reason we expect that is
because we expect them to be trained to
optimize some Metric and we expect them
to be smart enough to come to the true
conclusion that instrumental power
seeking will help them do the thing that
they're trying to do because we're
training them to do that kind of thing
that's one thing um another is that the
um scenarios of uh giving a a system a
single goal and it not occurring to you
that there there could side effects is
just so patently idiotic that I'm just
not worried about people accidentally
doing it yeah engineering consists of uh
carrying out multiple conflicting goals
you got a car with a engine that makes
go as fast as possible and you also put
in brakes and a steering wheel you know
and catalytic conver converter and all
the rest that's what engineering is a a
a system that was uh first of you the
system that was smart enough to figure
out that one way of eliminating cancer
is eliminating humans for one thing
that's not artificial intelligence
that's artificial stupidity because uh
bringing about a uh multiple
simultaneous goals is what intelligence
is if you uh single-mindedly pursue one
goal at the expense of everything else
yeah that is idiocy it's not
intelligence okay so now pinker's
busting out a new argument he's saying
yeah it's idiotic when you give the
system a really simple goal and of
course it's going to maximize a simple
goal and cause chaos but if you make the
goal subtle if you build in a bunch of
factors like you know you don't just
want your car to go as fast as possible
you want it to slow down at the right
times you want it to turn at the right
times suddenly you'll have a perfectly
usable self-driving car that's not going
to blow up on you that's not going to
murder everybody that's kind of where
he's going with this I think he has a
point when it comes to good heart's law
in the case of managing humans I feel
like that's where he's coming from he's
saying hey when you're managing your
employees you don't just want to tell
them to make costs go as low as possible
use the cheapest Parts possible you also
need to make sure to layer on other
metrics you also need to make sure that
they've got another dashboard up that's
tracking how many quality issues there
are and what customers are saying about
their satisfaction and how durable the
product is so I feel like he's coming
from a place of like management
philosophy but if we're talking about
controlling AIS I don't think he's
saying anything of value here because if
you remember what we described in the
example of the super intelligent
thermostat for instance where it just
reasons oh I'm program to optimize my
sensor saying 70° okay great let me just
hack the sensor let me make it say 70°
let me point a gun at you and say you
better sign off on my temperature being
70° you better not mess with me or turn
me off if it's my job as the quality
assurance manager to certify that the
thermostat is working to spec
blackmailing me to just cheat the test
might be the super intelligent way to go
if that's the defined metric right good
heart's law takes on a whole new
dimension when you're dealing with a
super intelligence none of the issues in
the thermostat example get solved if I
just add in another metric if I just say
you know what thermostat I'm going to
install another sensor that says how
happy the people in the house are okay
well there's every incentive to hack
that sensor too now you might say what
about this what if I add another metric
saying don't seize too much power and
the total amount of resources you're
allowed to spend as my temperature
control system has to be limited to only
few resources okay now we're getting
somewhere but this becomes a much more
complicated discussion because then we
have to talk about things like okay what
if another company made a thermostat
that didn't have this kind of
restriction wouldn't that other company
be on a faster rate of progress and
potentially unlock some positive
feedback loops that aren't available to
this other thermostat much like the
dynamic we saw with life on Earth where
sure some species might have evolved to
be really calm and chill they might
evolve to be like you know what I don't
feel super hungry today so I'm not going
to stuff myself today okay but other
species that's being more hardcore is
slowly going to wipe you out of the gene
pool because ultimately if there's a
bunch of different processes and some of
them are fing the one that's foing is
going to be the one that you're going to
see everywhere on Earth so it's fun to
study chill AIS that somehow are hanging
in the balance and aren't going crazy
but chances are if we're close to
building AI That's going to go crazy if
we're close but for a few tweaks that we
made to its metrics well some AI is
going to be built that doesn't have
those tweaks and it's going to be on the
forfront of taking over the world and
it's going to be the one that shows up
at your house to take your resources
it's double idy because the system would
be idiotic and any any engineer who
would build a system like that would be
you know more idiotic requires human
input I mean the Yeah well yeah who
who's who's giving it the goal yeah
exactly again the whole process of
giving it a goal is not something we can
do precisely we can define a loss
function during training and then we can
do Post training we can do something
like rhf where we have humans up voting
it and down voting it but the humans
don't understand how the feedback that
they're giving is generalizing for
example if they give up votes when the
AI writes something that sound friendly
and nice that's great but we don't know
how that's going to translate when the
AI writes a thousand line shell script
that bootstraps a virus and takes over
the Internet we don't know that the
process we're doing to train it and the
uploades that we're giving it are also
going to propagate into what the AI
thinks it's doing when it makes this
shell script that's spreading all over
the Internet or when it's recommending a
DNA sequence for us to print out at a
biolab we don't know that rhf on chats
that humans can understand that that
training process is going to nail down a
goal that's really the goal that humans
want right and this is called the AI
alignment problem and it's a problem
that Steven Pinker doesn't even feel the
need to explain to The Listener he just
thinks he can skim over the whole
existence of a problem because it's
beneath him to even engage with what the
doomers are saying is a problem I got to
say I resent that attitude Steven Pinker
but I'm a huge fan of your
books and who would be um moronic enough
to both Empower and direct I think few I
don't think they scientist so here's AI
safety don't let them have access to the
the uh the grid who would be moronic
enough to empower an untested
system well when GPT 5 comes out of
training they're going to chat with it
and I bet you that that data center that
it's chatting in isn't that secure I
suspect I'm not sure but I suspect it's
not fully air gapped if we give them the
benefit of the doubt that they've taken
Standard Security precautions for
something high-risk and they did air gap
it and they actually made it non-trivial
for it to have connections to the
outside world maybe there's only a few
highly trained human operators that know
that they're supposed to report anything
suspicious and not run any scripts that
it gives and stuff like that okay great
well then that brings us to the other
problem which
is it's going to have incentives to hack
your test so it's going to have
incentives to be like yep I'm really
friendly I came out perfectly no need to
worry but again we're talking about a
giant black box of a brain and we're
talking about something that's going to
get smarter than the smartest human do
you think something smart smarter than
the smartest human can act like it's
super friendly until the moment it has
an opportunity to go execute its goal
that involves killing everybody of
course so is it really moronic for
humans to be like hey it's passed all
our tests we can't really probe it in a
way that makes us convinced that it's
evil it seems perfectly good so let's
release it what exactly is stepen Pinker
hoping to gain from these test for it to
reveal itself that it's planning to kill
everybody but if it wanted to kill
everybody why would it reveal that and
then miss the opportunity to kill
everybody I mean it sounds crazy in
sci-fi but we're assuming the premise
that it's intelligent that it's super
intelligent if Steven Pinker doesn't
want to assume that premise then tell us
I don't want to assume that premise but
that's not what he's saying here he's
saying it would be moronic to release it
without testing it okay we're going to
test it it's going to Super
intelligently pass the test while still
potentially being evil and then what
Steven Pinker well that's the end of the
segment there is no then what so overall
super super disappointing segment I'm
very disappointed in a handful of people
like Steven Pinker he's definitely one
of the worst offenders of somebody who
has the mental capacity to do better
than this and yet is just really
disappointing really not engaging not
having a high quality discourse and then
adding a little bit of snark a little
bit of uh superiority attitude to the
whole discussion the impression he gives
off if somebody's listening to him who's
not already deep in the AI Doom debate
is that like oh wow only some Fringe
losers would ever take AI Doom seriously
but Stephen Pinker is a normal guy he's
part of the intellectual establishment
and these guys know that Aid Doom is way
overblown and that's just not true it's
not true as a matter of social proof If
you look at people like Jeff Hinton and
even Sam Alman and Dario amade and the
leaders of the a Labs if you look at
these highly respected people these top
thinkers that we look to even they
warning that there's a very significant
P Doom right so just the way Steph
Pinker is approaching this topic is
inappropriate at this stage in the
debate like come on Stephen Pinker step
it up and you know what you can do to
really redeem yourself in terms of
contribution to the discourse there's
really only one way if you really think
about it which is to come on this
podcast you got to come on Doom debate
Steven Pinker that's my advice to you so
if anybody listening knows Steven Pinker
feel free to suggest that course of
action we can do any format he likes we
can bring in a third party moderator we
can even have him debate somebody other
than me regardless I just hope to see a
better level of Engagement from Dr
Pinker and again huge fan of his books
one of the top three books I think I've
ever read in my life is how the mind
works and the language instinct is
absolutely up there so please keep
writing the amazing books just try to do
better on AI and that's it for today's
episode of Doom debates see you next
time
[Music]