people will frame this as a coordination
problem when really it's like a
disagreement in values like there's just
going to be some humans who don't care
if they live and don't care if other
humans live and a sizeable fraction in
terms of influence in the
[Music]
world hey everybody Welcome to Doom
debates today my guest is Dr Andrew crit
PhD from UC Berkeley studied algebraic
geometry in 2012 Dr crit co-founded the
Center for Applied rationality which I
was actually somewhat involved with
myself back in the day in 2014 he worked
at Jane Street Capital as an algorithmic
stock Trader in 2015 to 2017 he was a
research fellow at the machine
intelligence Research Institute oriri
alazri owski
Institute 2017 to present he's been a
research scientist at the UC Berkeley
Center for human compatible Ai and in
2022 he co-founded a startup called
Healthcare agents which we're going to
talk a little bit about Dr crit welcome
to the show how's how are you doing
great great thanks for having
me yeah thanks for coming on and we've
known each other for like 10 years since
about the dawn of Center for Applied
rationality right yeah I guess it might
be a little longer um might be 12 but
yeah yeah very cool but we had a period
of like 10 years where we like never
really interacted but now we're chatting
again I think because AI safety is
getting more urgent right it feels more
yeah everything's a little more
actionable now I
guess yeah exactly cool so uh let's see
so first question for you um a lot of
people who watch my show are really
interested in alaz owski and Mei and we
don't get that much information we don't
get that much Insider perspective on Mei
unless you count like the L wrong
sequences or something so what do you
think of Mei what's it like
oh that's a great question so I mean I
back in you know grad school in it's
kind of hard to it's kind of hard to
give my take on Mary without giving my
take on how I got there so uh if that's
fun I'll just tell you that first for
sure um so so in grad school back in
2010 uh Andrew in came and gave a gave a
talk at Berkeley about deep learning and
how we thought we were going to
revolutionize image processing with
neural Nets
and he made a lot of claims about how
the brain worked and how we were going
to re reverse engineer the plasticity of
the brain in a digital form that could
then be adapted to arbitrary tasks like
image recognition and I totally did not
buy it like he he made mathematical
errors in the presentation that uh made
me really skeptical uh but he corrected
them immediately when I pointed them out
and so I was like oh maybe there's
something to this and
then I went home and I thought well if
he's right about this it's really
important so I should should probably
just make sure I'm not just dismissing
this guy so I started reading his
bibliography all the claims that he made
about how the brain works and um it
started to check out and I thought I'd
read a little more and I checked out
some more and it took me about six
months really to learn every all the
background of all the claims that he
made about the brain to the point where
I just came at the other end of those
six months it was 2011 by then and I
came out thinking this is going to work
like deep learning is going to
revolutionize computers um they're going
to have Flex ibility that we've never
seen before in computers and we're going
to write all kinds of Loops to change
that flexibility and I got freaked out
because he didn't seem freaked out at
all uh he there there was no sense of
you know when he said we're going to
make AGI during our lifetimes um and he
was saying that back then um you know
it there wasn't a sense of like and and
we better be very we better be really
careful about it you know there was no
sense of Contrition or or or caution or
like um sorry if this is scary but we're
going to have superhuman machines there
was none of that it was just you know
we're going to do this it's going to be
awesome and so that made me worried if
he had been worried I would have had a
completely different feeling but because
he wasn't I was and then I went looking
for other people who were worried and I
found Mary so that was my introduction
to Mary was I had also seen the less
strong sequences uh before and I didn't
really like them but I I liked what they
were trying to do
and uh and I liked that that um you know
I lik the topics they covered like
causal inference for example was a big
one that I uh that I liked and I
actually learned learned something
important the reference to Judea Pearl I
first got from from Les Ry and that's
that's the biggest thing I ever got from
reading reading that blog um but but
anyway I I you know I found Mary and I
was really glad to find people who cared
you know about the profound risks and I
ended up with pretty big technical
disagreements about you know how to
address that risk and what sort of
technical approaches are actually
sociologically safe versus unsafe uh but
in terms of caring I really was happy to
find people who cared back then because
there weren't
many okay well I think we're already
seeing our first uh Clash here K of
ideas because I actually like the
sequences the last wrong sequences so
why did you not like them I didn't like
them because they seemed kind of
mean-spirited a lot of the time like
kind of like they seem kind of either
mean spirited or kind of like irritated
or something which is totally relatable
it's just I kind of would have liked a
less irritated version of reform for you
know philosophy or or uh you know even
just bringing Bas and reasoning into
philosophy I think is an important
project and elzar was taking that on
which is awesome but I just I could
never really get behind his writing
style his uh his inperson communication
style doesn't feel doesn't feel nearly
as as um as prickly as his writing and
so I I have I've enjoyed inperson
conversation with him but not not his
writing as
much in terms of the substance of the
sequences did you read the whole thing
in every post you're like oh this is a
new insight this is a new insight I just
don't like the tone I didn't I have to
say I I don't think I got many new
insights from it I was already thinking
very much in terms of probability all
the time anyway I was a
mathematician uh and very sort of
committed to analyzing the world from
first principles um so I I but there
were a few things that I got from it
like I said the reference to DJ Pearl
was really important for me um I could
go I could go on and on you could ask me
for half an hour of questions about how
much I love J pearls work um and that
was really um that was elzer pointing
that out um yeah I I had a more uh Normy
version or rather like a lesser genius
version of your experience where you're
like oh man I knew all my map and
probability but I didn't know all the ET
Jane stuff which is like the more
advanced part I guess of basing
probability whereas I took like a
undergraduate second year mandatory
course that mentioned basian probability
right and of course I like passed the
test and put it out of my brain and then
I read Ali asra I'm like oh my God this
is the epistemology that a super
intelligent AI will use right so he kind
of like made it flood back for me even
just the basics right got it no actually
you know I didn't read Janes that much I
I found Janes interesting but Pearl
really more so than James was was
transformative to my not just my
thinking about how causality worked but
also my thinking about how um medical
institutions and scientific institutions
think or don't think about how causality
Works um that was really transformative
so yeah and and I another thing I should
say I picked up from from um L wrong was
the the observation that baz rule the
odds form of bay rule is easier to do in
your head than the than the unnormalized
the okay and uh and I sort of was always
kind of doing the full theorem in my
head and uh someone pointed out like uh
you know it's just better to do the
unnormalized one and I was like oh yeah
it is and that was nice that's a nice oh
my God I mean yeah you're you're talking
about like such little things within the
sequences it's like somebody who like
goes to Disneyland for the first time
it's like oh my God the patterns on the
sidewalk were pretty cool well yeah I
mean you know the commitment to
materialism the commitment to knowledge
being information information Theory all
that stuff I was really pretty bought
into um and so it was kind of just it
was more like oh I found some other
people who are into this stuff great uh
you know let's be friends with them and
uh and I was really glad to meet a lot
you know like when I finally met elzer
enjoyed I enjoyed the conversation and
and I enjoyed working with them as well
at
muy what year did you read the
sequences um well I didn't read them all
but I I looked at a bunch of them uh a
bunch of the posts in it was probably
200 probably
2011 yeah so although the pointer to
Pearl the pointer to Pearl was in 2010
that was that was a 2010 thing so
somehow by 2010 you'd already come in
with this framework of your own beliefs
about how basy and epistemology was the
way to update your beliefs right I would
have called it Bas and I would have just
called it probabilistic but yeah okay
and you and you said Andrew in already
had got you concerned around what year
about superintelligent that was in 2010
also yeah 2010 was a big year but it was
really in that got me thinking we're
going to make deep learning just weird
to me that somebody like you who H who
recognizes that a lot of what's in the
sequences is like true and important are
still remembering the sequences as being
like yeah I kind of came in knowing all
of it like I'm kind of curious I don't
really feel like it's mostly true I feel
like it's there's a lot of hyperbole in
there that's not quite right U like
there's a lot of stuff about utility
theory that uh that I I know the
foundations of in and out that um that's
you know a little exaggerated I think
it' be hard to put my pinpoints you know
my finger on exactly what because it's
been a long time since I read it you
know I don't I don't think about it that
often except some if somebody asks so
you're asking um okay okay so I didn't
feel I didn't feel like this is like a
to of truths I thought like you know hey
here's someone doing philosophy and
they're using probabilistic reasoning as
they go that I've never seen before I'm
really glad humans are finally doing
that and and this is the best like you
know it's not like I like any of the old
school philosophers writings I mean I'm
impressed with they would were able to
come up with at the time some of them
but but like you know you philosophy is
just better if you have numeracy in it
and elezar was just like the first
person really at length voluminously you
know uh exploring philosophy in a really
numerate um way so so I I was I was
certainly impressed with him compared to
like other philosophers that I've read
just I just still didn't really I don't
really like reading philosophy and I
didn't really like I didn't really like
reading sequences yeah I mean could
build on what you're saying about what
you liked at him like oh wow he's doing
philosophy with probability for me it
was an even deeper aha moment where he's
like okay he's not just following in the
line of tradition of all these other
philosophers he's approaching the
deepest Philosophy from the angle of hey
there's going to be a super intelligent
AI the super intelligent AI is going to
have essentially total control over this
universe this universe is like putty in
its hands and so what is going to be
going on in its mind as it figures
everything out
correctly
interesting yeah I guess there's some
parts in there maybe where he's like
what would a super intelligent perfect
basing Reasoner figure out but I often
disagreed with those things as
well um okay well we we both agree that
it's been pretty transformative to the
entire project of philosophy right it's
been very you know in terms of
philosophy you know I think it's
um it's among the best I've read but
again I'm not an Avid Reader of
philosophy either so um but but there's
you know Humanity has discovered things
about math about statistics about
information in the past 50 to 100 years
that are transformatively meaningful for
how you know what can you know how does
knowledge work yeah things like that
epistemology and yeah and so if you're
trying to you know apply reasoning to
the world uh like a philosopher would
100 years ago or 200 years ago and
you're not availing of those tools it's
like what are you doing like it's like
stand stand properly on the shoulders of
nobody else was right El and elzar was
and and only and basically only elzer at
least at that length and that level of
prominence on the internet like there
may have been other people privately
thinking like that else I mean I felt
like I was you know but in terms of
people that I could find and read it was
just really just him so uh yeah I mean
again I I I thought to myself I want to
meet this person and and I want to meet
the other people I want to meet the
other people who are who are reading
this blog and I don't necessarily agree
agree with you know everything they're
saying but I like their methodology I
like the the enumerate and statistically
you know principled approach to
knowledge exactly right for me the lapse
time between first exposure tell's
writing to finally meeting him in person
was like 9 months back in
2007 yeah right right it was probably a
similar delay you know less than a
year yeah so anyway good stuff good
memories yeah and I will say this um
when when I've attended like research
seminars with elzer um that's when I
feel like he's he's impressed me the
most like uh you know elas are in a room
full of mathematicians guiding the
philosophical relevance of the math
question at hand is beautiful it's
really he's and I and very few people
have had that experience you can't get
it from watching a YouTube interview
where he's scared about AI or reading a
newspaper article or reading a sequence
post like uh his research his taste in
the philosophical import of a math
question is is really great and uh
that's that was a really positive
experience for me being at Mary was was
just solving math problems with with a
culture of you know philosophical
relevance around them and and elz was
really good at steering
that yeah I mean that's great and I've
seen a little taste of it um so I'm not
as deep into math as you
uh but I did do upper division math
underground stuff uh and I have read
whatever math elzar's published to
that's general interest and I definitely
get what you're saying that flavor of
like I will go and learn something and I
mean I'm pretty good at math right so
I'll get a taste for Math and I'll get
some intuition for the math I read and
then I read the elezar version and I'm
like oh my God this is like this is how
you learn it like this is clearly better
learned than what I did myself so really
impressive on that front H okay gotcha
yeah I I didn't I didn't have that
experience but uh but uh yeah I mean
look no nobody else did what he no
nobody else did what he did nobody else
laid it out in writing here's like a
transcript of thinking at length about a
bunch of really important Phil
civilization important philosophical
problems to do with the future of
humanity uh and the nature of mind and
Consciousness you know from a from a
numerate and statistically principled
perspective uh it just wasn't it just
wasn't there so you know as much as I
didn't feel you know supercharged by it
I did feel H you know excited to meet
other people uh involved in that Pursuit
okay so what about 2015 Mary when you
worked there what was that like yeah I
mean it was just starting to be a
research institute really with with with
people who had you know already prior
experience doing research um and I had a
couple friends who were going there to
work and I thought maybe I would give it
a try and uh the first couple years
there you know the first year there was
really great it was probably the most
intellectually productive time I've had
in terms of exploring hard problems and
trying to tackle them uh in in in pure
research like pure pure mathematical or
or um AI research and then uh but I I
came to the conclusion in that year that
um that I didn't think that the acute
loss of control risk was really going to
be the greatest threat that Humanity
would face with AI
and what would be a bigger threat and I
still maintain is a bigger threat is the
uh the
multi-polar complexities that emerge and
you have multiple aiis which I think we
will uh at at human competitive levels
um and so I I ended up spending my
second year at Miri really trying to
find a way of reorienting better towards
multi-polar and multi-agent Dynamics and
how risks emerg there and uh I I really
then came to find that Berkeley was a
better better fit for doing that
research because at the time was very
focused on alignment and how do you get
one agent one AI system aligned with one
human or one Human Institution even and
I didn't feel like that was really going
to be a technical bottleneck for making
Humanity safe uh compared to the MTI you
Berkeley Center for so you went to the
UC Berkeley Center for human human
compatible AI in 2017 who started that
how old is it and what's it like yeah so
Stuart Russell started I think um
technically in 2014 although I was the
first full-time employee um and and so
there was no one really to join at the
time except really Stuart and uh he
essed me because he said look the you
know the purpose of this institute is to
uh you know reduce the risk of of
catastrophe from AI by understanding it
or something to that effect and and I
was really pleased to see somebody with
that uh you know with with that level of
U credibility already in the field of AI
um taking the risk seriously when a lot
of other people were dismissing it so so
uh and the other thing is that Berkeley
I felt was a richer in intellectual
environment for exploring A diversity of
hypotheses in any in any field and uh
whereas I wanted to do I wanted to think
about something a little different from
what Miry was thinking I felt like
Berkeley was an easier place to to um to
work on it but it was uh that's you know
I I don't think everyone has that
experience of Academia I just I just
always felt like Berkeley was pretty
open minded for whatever I wanted to
think about so uh uh so I ended up back
there let me ask you the obligatory
question you ready for
this Dr Andrew crit what is your P
Doom so um you know I I don't make like
making light of this
question um I think for someone who's
only encountering it for the first time
um it's not um to be it's not something
people should take lightly and uh you
know for people who have been dealing
with this kind of concern for a decade
or more um you know eventually it wears
on you and you you have a sense of humor
about it or or whatever um but I don't
like that framing uh that it's something
to be joked about um and uh but I I yeah
I I think probably there's about an 85%
chance and this is a subjective
probability which is to say you know if
I had to make a bet which I would not
not want to uh my betting odds would be
something like 85% that humans won't
survive past let's say 2050 um or 20
yeah some somewhere around there and
reasonable reasonable chance humans
won't be alive anymore before then um
and most of those neg most of those
outcomes are pretty negative and
probably deserving the name Doom um and
I don't like I don't like joking about
it um so that's that's that's my answer
and and yeah I guess
um I I can understand joking about it or
taking it lightly and having fun and
having a laugh talking about it um but I
I see that more as a coping mechanism
than something like fit for fit for
public Communications so I'm not going
to I'm not going to Jer about
it yeah um you know the reason I made
that stylistic choice about having the P
Doom theme which is light-hearted even
though it's literally talking about hey
we're all going to die um is I see it as
Gallo's humor to point to the fact that
Society our society is asleep right so
it's like they are the ones who are
fiddling on the Titanic here right like
we just looking at them being like H
guys yeah I I fully support someone else
who wants to use Gallows humor if that's
their preferred way of dealing with it
it's not mine so I'm not going to
reciprocate that but but uh you know
it's uh yeah it's fair it's fair that
you know it's pretty dark the situation
and sometimes the easiest way to deal
with something dark is with dark
humor and you know I'm actually glad you
brought this up right you're the first
guest to bring this up and it it makes
perfect sense right I mean Humanity
going off the edge we you know we often
we rationalist or whatever you want to
call us uh you know high PE Doom people
identify as
rationalist okay so yeah people with the
hyy Doom I think you just said that was
you and that's certainly me as well um
we often talk about a missing mood right
where it's like even as little as a 10%
P Doom it's like um hello like imagine
your loved one just got diagnosed with
an illness that's 10% chance of being
terminal right or a tumor that's 10%
chance of being inoperable or or
whatever it is right I mean this is
terrible news right and yet you know
everybody's just fiddling right Society
is just like yeah but I want uh you know
self-driving cars right and and you know
and sometimes sometimes you can have
both but at the end of the day it's just
like Hey we're this is all about to end
right so I I agree and I think you've
done a good job demonstrating now in
your answer of like you and generally
people who have worked at mey or people
who spend their life dedicated to these
kind of causes for them it's you know
you're in I wouldn't call it a bubble
but you're in a circle of people that's
like a sanity Circle right of people
where they have that
mood well yeah I don't necessarily agree
with the sanity Circle characterization
because I think like I said it's not
that hard to have the idea that a super
intelligent competitive species could be
a threat and so um you know you don't
it's not necessarily a strong filter for
you know for reasonableness to to to
have come to that conclusion because
it's not difficult um so I don't and I
and I really don't like that sort of uh
mentality sometimes people who are the
most worried about this um feel that uh
only the other people who are deeply
worried about this topic can be trusted
in some sense and I don't think that's
good so I don't know I think the the
level the the centering of Mari in the
discourse uh the the centering of elezar
in the discourse that you're doing I
feel is antithetical to progress on
actually reducing the risks mostly
because the tasks involved in reducing
the risks are more complex than merely
noticing the you know the simple facts
of the matter and also uh different from
but not necessarily more complex then
but different from heading off all of
the bad you know counterarguments
against the existence of risk so
something overall overall like uh Mir
Centric about this this interview that I
don't like and that I'd like to away
from because I don't think it's doing
service to um you know what needs to be
done for example I say 85% but I don't
think first of all I think that's
something the world should work to
reduce and not something I want to take
as a given uh and second of all um I
don't think my my sense of how that will
happen is pretty different than I think
a lot of other people who are worried
about sort of an immediate loss of
control event assume you know is the is
the greatest threat so there's some kind
of group think that that happens when
everyone's like okay yeah Mir got it
right and they've known about it for
decades and we should listen to them or
Dario ID got it right or anyone any of
these anyone who's been you know really
thinking about this stuff for a long
time um has has you know some
intellectual following and I really
think people don't need that as much as
they just need to think for themselves
um that hey look if you have a super
intelligent competitive species on your
planet maybe you're in trouble that's
it's it's about that simple so I agree
with you that when people like you and
me and a lot of the people who have
already been marinating in these ideas
for Fears when we're having a
conversation you're right that I'm being
kind of simplistic and I'm focusing a
lot on elzer Just for context here right
my podcast generally when I talk to
people even on this show who are
intelligent and have some exposure to AI
Doom the biggest Delta that we need to
help them find out is the probability
that we're all do like we need to help
them realize it's a yeah it's a very
important number yeah yeah and so you
usually if I'm at any kind of
intellectual Gathering you know to do
with the future of AI or the risk of it
I try to find an opportunity to say hey
look what do we think is the extinction
risk here uh it's been acknowledged
publicly finally you know since the
center of AI safety letter uh by a lot
of leaders in AI that that reducing
Extinction risk should be a priority a
global priority alongside nuclear and so
on but uh but it doesn't come with a
number right and so how big how you know
it's it's a big priority how big is it
and and um exactly yeah I was just going
to bring up that statement on AI risk
from last year so it says mitigating the
risk of Extinction from AI should be a
global priority alongside other societal
scale risks such as pandemics and
nuclear war and I think it was Dan
Hendrick idea who is now working on
safety for Elon musk's xai and it had
Sam Alman signing it elas husk signing
it Dar Ade signing it not Yan Leon but
it had uh I think Demis from Deep M I
believe signed if I remember correctly
Jeffrey Hinton so he really had a
majority of the who
of notable names that you hear in these
fields across the board signing the
statement and it doesn't put an explicit
P Doom right so if you think there's
only like a couple percent chance of
nuclear Doom maybe you think there's
only a couple percent chance of Aid Doom
so it's vague about P Doom but it was
still kind of a breakthrough right being
like yes that was a real serious topic
exactly yeah huge yeah so we both agree
on that front I I do want to Pivot into
breaking down your doom is that a good
place to go next sure yeah let's break
let's break down the doom and hopefully
breaking down the Doom intellectually
will have some effect on breaking it
down physically and you know reducing
the actual Doom exactly all right so you
gave an 85% chance of P doom and I saw
that you wrote the way you phrased it
when you wrote a forum post on Les raw
was you said 15% chance Humanity will
survive through Ai and then 85% chance
that Humanity won't survive through AI
can you break down the 85% chance that
Humanity Will
Survive yeah um so I try not to be like
too stuck on specific specific numbers
so you know I I I try to keep the
numbers in my head more than on paper at
any given time because I feel like I can
I can stay attuned to new information if
if if uh the numbers are in my head but
I yeah I feel like there's kind of two
two kind of bumps of risk that humanity
is going to go through with uh with with
AI
development uh existential risks the
first one is just can we bring into
existence uh you know uh human level um
and we can debate about what human level
means there's many components to it and
you can actually choose which components
the AI is developing to some degree you
can sort of Whit list them a little bit
um but there's this question of whether
you can make you know human competitive
AI without losing control of
it um and uh I think you
know depending on the week you know my
my estimate of that tends to fluctuate
between like 30 and 40% that will that
will lose control of it uh which is
which is a horrendously you know morally
atrocious number uh in the sense that
you know if any public safety you know
if if any project any business project
were were that great of a a threat to
Public Safety under normal conditions it
would be barred but um you know that's
not that's not what we're doing um for
mix of reasons to do with the economy
and what people are motivated to do in
business and so on uh and and that could
change too uh so you know may maybe
Humanity will wake up one day and say no
we don't we don't want to take this big
risk um but um you know the way things
the impression humanity is giving me is
that humanity is going to go ahead and
do this crazy thing uh with a pretty
high initial amount of risk associated
with it and then if we don't immediately
just lose control I don't think we'd
immediately die it would take some time
if we lost control take some time to
actually to die out um maybe a few years
but um if we don't immediately lose
control then there's the question of
what happens with the economy from there
on um and there's kind of you know
there's some totalitarian outcomes where
someone just like makes a super powerful
Ai and conquers the world I think that's
possible but not not the most likely way
things will go mostly because of the
social pressures internal to and around
the teams developing uh AI I think
having the sort of institution that's
planning to do that is quite a toxic
plan for a group of
people uh so that instead you end up
with a more
multi-stakeholder diplomatic
relationship with the rest of the world
that allows other AI projects to develop
and then now we have multiple and I
think that in that world the competition
first economic and then ecological uh
where humans are no longer of economic
relevance and therefore have no
bargaining power to sustain their
ecological niche with the with the with
with the industry that results and then
we've got pollute you know machines
don't need clean air they don't need
clean water they don't need Farmland
they don't need health care to survive
right uh whereas there's a bunch of
things that we need and they need too
like electricity and land uh and energy
so they can compete away the things that
we need to live and produce tons of side
effects that that kill us and that don't
matter to machines and maybe even win a
war against us if there's armed conflict
so those are three different Pathways
you know dep resource depletion um you
know pollution or other NE negative side
effects accretion and then there's just
armed conflict are three different ways
that AI competition with humans could
result in in our Extinction so I and I
think that that's really unless
something changes about Humanity that
surprises me and changes my mind about
what I'm how I'm expecting human people
and and businesses and in countries to
behave I expect probably a 50% chance
that that whole process that economic
competition process will lead to our
Extinction you know probably by 2040 and
and most you very likely by 2050 uh with
some humans starting to die in the 2030s
you know uh in a way that's attributable
to to that sort of hand sort of
voluntary Collective handoff of control
to to an industry that out competes
humans yep all right let me try to recap
here
so you have an 85% pum 35% out of the 85
is the chance that Humanity will lose
control of the first few AGI systems so
that's kind of the very classic Sci-fi
ucow skin scenario you know
self-improving AI or just rapidly
improving AI like you know they attack
us all at once coordinated attack that's
that's kind of like arguably the worst
scenario even though they're all pretty
bad so that's 35% no I would count in
that a scenario where the AI become
self- sustaining and adversarial to us
and determined to win a conflict against
us but we don't know it yet so Our Fate
could be sealed before we realize the
dramatic you know ending of it exactly
right and you've given a rough timeline
that you think most of that 35% chance
is on a pretty quick timeline where Our
Fate is quote unquote sealed by 2030 and
we go extinct by 240 that's where most
of your 35% scenarios would go and then
you've given a higher probability 50% so
you're slightly more doomy about a
scenario where okay we don't immediately
die by self-improving AI but it's kind
of a slower Extinction where in your
words we're getting pushed out pretty
fast by either resource conflict or what
you call industrial dehumanization and
we're going to unpack in a bit but is
that a good high level summary yeah yeah
and I would I always want to add into
every soundbite I can that these
probabilities these are these are you
know feature requests for for Humanity
okay so you know I'm not happy with this
probability and I if you if you think
the world's going to be different please
make it different please make me wrong
um so that this probability doesn't come
true if you know if that's you or any
listener I don't want people I don't
want people to hear these probabilities
and respond with fatalism I want that I
want them to hear and object and do work
to make me
wrong right okay so yeah I mean first I
just I guess it I don't want to beat a
dead horse but it is pretty astounding
that here is another smart guy telling
us that you there's like a 35% chance
that we're just going to be like sarily
killed but 40 right yeah it's pretty bad
and uh yeah it's not a good it's not a
good feeling it's
bad okay just wanted to make sure we're
clear on that but yeah so I think the
rest of the conversation we're we're
going to focus on what you've kind of
now made the focus of your life which is
that 50% chance trying to avoid that 50%
lesser Doom scenario but still Doom
where Humanity gradually seeds control
of Earth and then goes extinct that way
you're trying to fight back against that
right now right yeah I want to create a
caring relationship between humans and
Ai and I see the healthcare industry as
the best industry to set that kind of
precedent both technically and socially
and so I work in AI Health Tech and it
has the nice benefit even if we do all
die I hope to take care of some people's
Health between now and then so I'm kind
of hedged uh to do some good in in
either
World okay now one thing you written
about how you came to this position is
because you have a comparative advantage
in identifying risks that are more than
5 years away so talk a little about your
thought process there right um I can
tell you what I do to try to see risks
there five years away and then I can
tell you why I think I'm okay at doing
that um so what I do is I you know for
one thing I I wrote software to practice
calibration so just assigning
probabilities to things I'm uncertain
about until you know 70% of the time
when I say I'm 70% sure the thing that I
say comes true and then the other 30% of
the time it doesn't um that's that's
easy to do in domains where you know
there are lots of practice questions but
human extinction is not one of those
right you can't practice having that and
then not having in different cases so
you have to instead practice on things
like all right how long do I think until
self-driving cars are going to you know
be available or how long do I think
until AI can solve this or that kind of
math problem um and uh so what I would
do is just kind of on a quarterly basis
think you know have debates with myself
or with colleagues about whether this or
that thing is going to happen and we
practice giving probabilities and just
over time um you know I haven't really
had I I I I I developed enough intuition
for where things are going that I
haven't really been much surprised since
2015 I would say with with novel
technical um advancements like nothing
like like alphao has surprised me since
alphao uh as much as alphao surprised me
um so that was did you make a prediction
about self-driving cars because I feel
like that one is a tricky one to
predicted the exact trajectory I I
predicted
2024 um yeah in in the in the late like
2010s I was thinking you know you know
mid 2020s sometime and I think it was in
2021 the first time I said like
specifically it's going to be 2024 and
I'm having hard time visualizing it like
it could be 2023 it could be 2025 but
it's really hard not to have it in 2024
in my head um and yeah of course you
know these these kind of predictions you
know it's great we now have this
technology right you could be placing
bets on like manifold or something so
yeah if manifold were around at the time
we'd like to think that you could point
to like a pretty strong manifold track
record well actually no actually I have
not liked participating in in uh public
prediction markets because they're
missing a key feature which is public
decision markets I don't like treating
something fatalistic that can be a
collective decision instead and whenever
there's some prediction of like you know
P Doom I I like to identify well what
are some things we could do not to have
pdom and for that I want conditional
forecast so if Humanity never passes you
know internationally bonding broad
sweeping regulation to prevent
recursively self-improving autonomous
system development uh then Humanity has
you know an 85% chance of of Extinction
by date right okay okay I I hear just so
I don't like participating those and so
I wouldn't have a track record even if
they existed it's not their fault it's
mine um and I still don't I still don't
participate in those but if there ever
is one that really puts that effort into
to make what I would call a decision
Market I I'll probably start
participating um yeah you know we're
both well aware of confirmation bias and
the average person on the street who has
no idea how to predict things still feel
subjectively like they have a history of
predicting things accurately so I know
that that's not you right but like you
know it's it's just nice to have
external confirmation right if if you're
making like decision based on seeing
yourself as an accurate predictor yeah
everybody should practice you know
expressing predictions with you know
probabilities if you can and then and
then tracking them um so so that's I've
just been doing that a lot and you know
there have been things that I've been
really wrong about so I'll tell you some
things I'm really bad at predicting I'm
really bad at predicting which company
will innovate which things first so for
example I I was pretty like if you'd
asked me in like 2018 or even 2019 you
know who would be who would lead the the
language model Revolution I would not
have said open AI I would have said
Google and I would have said confidently
Google like 85% Google right um so I
mean Google was killing it with with AI
I mean they still are but they seem like
they were like Far and Away the best I
agree yeah so was that Google also um
Microsoft uh if you'd asked me you know
who's who's not going to release a toxic
chatbot that like harasses and terrifies
people I would have said well you know
probably they'll all mess up here and
there but probably Microsoft won't
probably Microsoft learned their lesson
with Microsoft Tay so they're the least
likely and I'm like I I would have said
80% sure Microsoft will not release like
uh you know a toxic chatbot and then
they then they released Bing too early
and it was hallucinating and and scaring
people so anyway so I've been very bad
at saying which institution will do what
but the sort of the the average case
like will somebody develop this thing or
that thing has been a lot easier for me
um and you know identifying the the
order in which different you know for
example math AI is going to be good at
math before it's good at physics you
know that sort of thing not so hard wa
yeah I think the relevance to the
discussion is that like 10 or 15 years
ago you were flagging AI existential
risk as being this big problem coming
down the pike and like sure enough a lot
of people are now realizing it and
feeling like we don't have much time and
now you're getting that same kind of
forward-looking sense of like well you
know this other 50% scenario where we do
get to live with the AIS but they slowly
Edge us out that's also a huge scenario
and not that many people are focusing on
it as like their research area or their
policy area or whatever so you want to
kind of run ahead of that yeah I that's
exactly I feel like you know at least
now anyone can go get a gbt for
subscription talk to it and sort of
realize wow this thing's pretty smart I
wonder what's next um
and I think uh but the the structure of
the economy and how businesses are
supposed to work and how laws are
supposed to work later um is a lot
harder um and and you know since since
we're talking about prediction I wanted
to ask you uh just ballpark when is Agi
coming crit yeah um so I think you know
about like I want to say like a 20 15 or
20% probability um next year like
2025 um I think that's
yeah and and then maybe like a 20% 25%
chance
2026 um and then I would say 2027 is
like a like a
30% um yeah and then you've written uh I
don't want to do gotcha so to be
consistent with your past writings uh
you've written 80% Chance by 2029 yeah
so that's um and I have kind of a a kind
of a spike around now because because I
think
um like I don't think the probability
each year is monotonic because what
happens is as you get more capabilities
there starts to be more push back um and
things could actually slow down a little
bit in response to Rapid progress now
but yeah I think I think 80% by 2030 is
pretty likely like is still checks out
to me um and yeah but 2027 I would say
is my median like if it's if it really
is 2029 I'm G to be like that's pretty
weirdly late like that you know I I was
right that it was before 2030 but I
don't seems far yeah and just to compare
that to my own view I think saying 15 to
20% chance that it's one year away is
totally reasonable like do I intuitively
feel like it's one year away no but
would I be like so shocked like oh my
God this isn't possible if it's one year
away no right so 15 to 20% feels like
the right ball part if it's if it's one
year away it's because labs are not
revealing everything they're capable of
and so I'm just exactly right yeah which
we know gp4 was six months ahead of what
they revealed right or what the public
knew about right yeah so so that's a
factor um now when you say 80% Chance by
2029 or 2030 that seems high to me I
tend to spread more of my probability
into an extra decade or two yeah just I
mean yeah it's very hard not to have it
by 2030 but again you know so again this
is this is a type of forecasting that I
feel I'm relatively good at compared to
forecasting who does it so um but yeah I
mean it's like 2025 is math you know
2026 is is physics 2027 is robot like
okay well you claim to be good at it and
I don't so let's let's defer to you just
to Define our terms real quick when I
think about AGI I think you'll probably
accept the definition it's just like it
can go head-to-head with humans on any
let's say economically relevant skill is
that about right yeah yeah and maybe
just I'd like to focus on aspects of the
economy that are adequate to selfs
so if it's no good at performing music
even though it will be that's not
necessary for competing with us as a spe
species but if it's no good at acquiring
matter and energy then then that's that
is that is relevant so I I I'm more so
focused on yeah fair enough the biggest
nitpick that I see and when I think
about AGI is just like the AI we have
today are so crazy General right like
the llms the fact that you can ask about
anything connect different domains all
in the same conversation and they just
kind of they run with you like it's this
is shocking stuff in terms of level of
generality And yet when I say we don't
have AG gii yet I specifically just mean
like well they can't go head to head
with every real world skill right yeah
and in particular we don't have General
robotics yet right as much we have kind
of General language ability we don't
have General robotics ability and I
think that's you know just a few years
away exactly yeah plumber right when we
when you can have a virtual plumber that
is when the singularity will begin yeah
because that's the point at which you
could automating like the m and
Manufacturing operations adequate to
sustain the machine economy without the
humans at all right and actually this is
a good segue because when you imagine a
scenario where like okay we have
automated plumbers you know we have
automated Factory workers um humanoid
robots you're still not quite imagining
you know a A elaz or owski style you
know Science Fiction nanotech it's
really just like yeah we don't need
human workers and AI can just start
edging us out so it's not really super
intelligence it's just a lot of intellig
Ence edging us out right that's kind of
I would even say your Mainline scenario
yeah that's definitely but I mean but
like you know uh the nanotech Futures
those are real I mean those are real
possibilities it's just not my main line
the main reason it's not my main line um
well actually most of my 35% also
doesn't doesn't go through uh like
immediate nanot Tech it has it goes more
through uh human manipulation and so on
like uh basically it's just you know
it's risky to build an anotech Factory
right away unless you're very very
confident that you're not going to get
caught so there's a bunch of social
maneuvering that the AI is going to want
to do first and that social maneuvering
is pretty general purpose for getting
things built as well uh so in the 35%
like loss of control worlds I think
social maneuvering is the first move for
most of them right would you say that
even before the social maneuvering it'll
do U virtual maneuvering in terms of
like how establish a presence on like a
billion microchips yeah
right right I mean so I I'm on the same
page as you as like these are the lowest
hanging fruit right of like inject
yourself into uh people's minds
essentially right just through powerful
manipulation and inject yourself into
every microchip right this is just like
virgin Lind yeah it's there's a sort of
rich soil that humanity and and our
computers together that's just
uncolonized by super intelligent Minds
exactly right it's it's so powerful once
you get there and it's so poorly
defended just like like hello like this
is and that's why I imagine right it
could be days be like this crazy small
amount of time before it's like whoop
all right game board's flipped we lost
yeah yeah we could like we could sort of
fully lose our ability to coordinate
with each other um at the scale
necessary to to contend with a you know
a uh Nation level threat from from AI so
yeah it's it's pretty it's pretty crazy
and you know I I I have some faith that
you know like you know the National
Security apparatus of each of the
world's largest super powers will will
you know involve itself in one way or
another to prevent this kind of risk um
and that would you know a lot of my 15%
comes you know depends on that sort of
that sort of
work right and you know I know it's kind
of stating the obvious but like okay
National Security operatus that's like
okay 100,000 really smart humans but now
theyi is like a botnet right that's like
more powerful than 100,000 smart humans
in terms of like IQ power so it's not
like an optimistic scenario right so the
key is to stop the botn net just not to
have the botet to begin with OR to have
a dumb enough botn net the first time
you get a botn net that you learn from
it and then really really tighten up you
know the exfiltration risks uh for what
I understand I'm I'm not a computer
security expert from what I understand
it's very easy to exfiltrate to leave a
system compared to get into one um so
there's got to be work done to to
prevent exfiltration
yeah I guess that makes sense uh I'd
love to learn more more about that
actually but it makes sense
intuitively like all of the packet
exchange rules that we have are all
about like giving a digital signature
that's verified upon a receipt there's
very little like signatures required
right upon transmission uh
yeah okay all right um so we got
timelines out of the way and then let's
see I also wanted to ask you by way of
background um do you think Super
intelligence is a meaning concept like
imagine the smartest human and then
imagine like a single AI that fits into
a single data center just being a lot
smarter than that is that a meaningful
idea yeah I mean I think there's a whole
parader Frontier of ways that that AI
can be way way smarter than humans you
know it can be just barely socially more
intelligent than humans and then you
know a thousand times smarter than the
best mathematicians which is to say
competitive with a group of a thousand
of them working efficiently together
right um but you could also have the
reverse where it's just barely
competitive with teren too and math but
socially you know Way Beyond Way Beyond
people I I predict the math skills
coming before the social skills because
that you can generate so much synthetic
data to do with math whereas social data
peculiar to humans is harder to social
hard harder to generate without
socializing with them um but you know I
could be wrong about that and and it's
part of the disjunction of all the
different ways we could lose
control great and then yeah I agree with
you on that um and then in terms of or
actually let me just push on that a
little bit so I I agree that there's all
these different skills imagine if you
push every skill to the human Peak you
get quite a you know a super human agent
just because it's like no human has all
the skills but do you think that it's
pretty plausible to push past the peak
in terms of like let's just say like
doing physics a lot better than the best
physicist ever does physics yes yeah
there's just no I don't think humans are
the best at anything including
interacting with
humans exactly right that's a good way
to put it okay so I'm totally on the
same page right that like our brains are
just like Clues right like they're okay
but we're definitely going to exceed
them right the same way that like a bird
is point the fastest fire I love sorry I
didn't mean to interrupt you yeah bird
is not going to be the fastest fire uh
the fastest flyer uh but I mean I love
pointing out how inefficient calcium ion
gradients are for transmitting signals
like I don't think people realize this
they think of the brain you know as as
an electrical Network and they're these
nodes and they send electricity to each
other but actually what's going on do
you know what do you know if I ask you
to tell me where the calcium moves in an
axon during a during an action potential
do you
know so my understanding is that after
you get the elect electrical signal
moving across one neuronal cell then
there are these chemicals that got
released and have to kind of swim their
way to an ex neuron right yeah so that's
so so that's the transmission between
neurons but within a neuron it's even
crazier to me so so you have a neuron
right that for whatever reason do input
from other neurons has accumulated
enough charge that now it's going to
fire right so how does the firing work
you have this axon which is this long
tube that comes out of the neuron to
connect it to others and some pumps open
up on the on the boundary of the axon
and start pumping calcium ions in from
the
environment okay so the charge is moving
orthogonal to the direction of the
signal the signal is going this way and
the charge is going Inward and then the
charge comes in and builds up at the
base to the axon and triggers a high
enough concentration of positive charge
that that triggers some more pumps to
open up and then those pump calcium ions
in from the environment which causes
causes an accretion of calcium which
then triggers more pumps and it's this
cascading pump system that causes
calcium to be pumped in orthogonal to
the signal Direction until there's
calcium being pumped in at the end of
the axon and that's triggering the next
the next activation level so so to me
it's like what I think of as as a wire
carrying like Fast electricity is
actually like a Cascade of pumps I
didn't it's more like Plumbing than it
is
electricity and it's Plumbing
transmitting information orthogonal to
the direction of the water flow which is
so slow and that's why our brain you
know one of the reasons our brains are
so slow is that it's not electrons
moving you know from cell to cell the
way they can in a transistor right it's
it's it pumps being turned on in a
Cascade uh and so you know you have gpus
that fire a billion times a second and
neing like a thousand do you happen to
know if the neurotransmitter part of the
process is even slower than ION channel
Park um so I think I don't actually know
the thing about the neurotransmitters is
that they're moving in the direction of
the signal okay so when you release the
neurotransmitters here they're meant to
go over here and then that's the
direction of the signal so if I had to
guess I would think that that's probably
pretty fast um but but uh and it's a
sort distance but actually I'm not sure
so I I could be wrong about that okay
well you know this is relevant uh in the
context of Doom debates I was debating
somebody on Twitter last year after
elzra owski debated George Hots and I
think one of the things Alazar brought
up is like clearly the speed limit of
intelligence has got to be at least six
orders of magnitude higher than the
speed of the human brain and somebody
onent guy was that I think six is about
right yeah ex it's aend say like a
million times right so the brain is
maybe one one millionth of the fastest
that a thinker can be uh meaning like
somebody could subjectively an AI could
subjectively experience at least a
million seconds in the time that a human
brain experiences one second something
like that yeah um but somebody on
Twitter was like how can you make these
claims you know we don't know okay we
don't know how close to that ideal the
brain is but it's like my my response is
like three words it's like chemical
moving parts right like that's just not
how you engineer a system when you're
trying to get speed yeah there's a lot
of of like we don't know ISM going
around like people really like saying we
don't know um there's like a there's
something you know people love the
feeling of mystery people love the
feeling of like wow you know who knows
the future it's so exciting that we
don't know what it is um but I mean I
know there's going to be a a traffic jam
on the Bay Bridge tomorrow I don't know
who's going to be in it I don't know
exactly what time it is oh maybe someone
else does but I know there's going to be
traffic jam and so yeah you can predict
some things about the future and one
thing you know you can predict is that
if there's trillions of dollars worth of
people trying to make a superintelligent
species um they might succeed right uh
and another you can you can that can
give you a pretty wide distribution of
when they're going to succeed and in
order to narrow that in order to have
the kind of confidence that I think I I
have and Merit uh with with regards to
when the AGI will come you have to think
about how it's going to be built what
are some of the options right and and
and sort of average over those
possibilities until you become convinced
that okay yeah I think someone can get
it done by date by one of these three to
10
methods exactly okay so so both of us
are on the same page that true super
intelligence is a thing in other words
there's a lot of Headroom above the rung
of intelligence that even the smartest
humans are at correct yep so we both
agree about that I'm also curious to ask
you something that I asked most of my
guests which is just you you say AGI is
coming soon you think it's only a few
years away how would you try to put a
separator where where do you see the
barrier between today's most effective
AI systems and where they're going to be
seen you know because I've had people
say oh the barri you David Deutch is
like oh the barriers are not truly
creative right and Dr Keith Dugger from
machine learning Street Talk is like oh
well the barrier is that they never got
trained on T incomplete operations so a
lot of people come in with their own
ideas about what the barrier is where do
you see the barrier yeah um I mean I
don't really see much of a barrier
that's why I think it's going to be
developed so soon I more just see like a
development schedule right so I don't
know if that if you're interested in
that or if you want to reframe the
question somehow well like it if if you
had to say why AI can't take over today
right like what ingredient is missing
right so um like the commercially the
commercially available uh you know AI
models and also the open source ones
they hallucinate a lot um so it takes a
lot of work to get them not to
hallucinate and you can you can you can
put in that work and Bill scaff thing to
have them supervise each other um but
now all of a sudden there's a human
trying to assemble this thing into a in
into a robust configuration with itself
rather than the model just spontaneously
you know seizing control so you you
could have people building things like
you know chaos GPT or whatever it was
where they're trying to to to make a big
splash with agentic behavior um and and
you could have people trying to build
hacker systems that are you know like a
python script looped around a bunch of
language models acting as their employer
essentially assigning them to different
tasks in different orders and that sort
of uh that sort of machine could could
cause a lot of damage but the core
components of it weren't really you know
that autonomous they had to be handheld
by by an outer loop I don't think that's
going to stay true for another year you
know I think that you know like CLA
using computer is pretty effective now
and and that's you know that's being
that's being trained out but it's being
trained out with great effort and it's
not right yeah go ahead when I try to
answer my own question of like what's
the missing ingredient I I often just
say robustness right which is similar to
what you said like hey they hallucinated
a lot right it's this sense of like
sometimes they might the next sentence
they say in their Chain of Thought type
of answer they might get it wrong right
and then they'll kind of get derailed
because they won't like fix it but it's
like if every time they're going to say
the next sentence you you say like hey
give me a thousand different versions of
the next sentence give your thousand top
candidates it seems like they're getting
really good at nil at making sure that a
really good sentence is somewhere in
their cop thousand candidate sentences
right so kind of a few bits away from
just like having good trains of thoughts
so okay so I think you're on the same
page with me there but like but what if
that part is like really hard how do you
train it out like the hallucination
aspect what if it's hard yeah I mean
there's a bunch of there's a bunch of
way how do you know not to run with a
candidate sentence so there's a bunch of
ways to do it or that seem plausible to
me to do it and one of the reasons
uh that I'm pretty good at forecasting
AI but not as brilliant as someone like
ilas hver or John Schulman at actually
building it is that I don't know which
of these ways is going to work and they
might okay so yeah so here are a few
ways that might work and someone more
you know brilliant could could maybe
pick out one of these is definitely not
going to work but they part they've they
they they form part of my disjunction of
possible ways we could get it done so
one is something like retrieval
augmented generation as it's now called
like the AI could store its knowledge in
a crisper form than it than its own
neural network and then use that as an
external Aid to constantly you know
check
itself um so that's like that's that's
one approach another one would be
synthetic you know tons of Sy
non-hallucinating synthetic data where
the AI investigates its own thoughts
without an external Aid like a like a
vector database in a retrieval augmented
database um but uh but more like um you
know just the AI reviewing its own
context window um and so you this is
pretty fascinating yeah just just
directly attack the robustness problem
that just generate just generate tons of
synthetic data doing that and you could
do that by posing you know logic puzzles
for the AI to to present to itself and
then solve for itself you know there's
lots of ways of doing that um and then
there's just new architectures you know
you could instead of adding Scaffolding
in the sense of a vector database and
rag you can also just change the
transformer in some way to add extra
modules for logic so you've you've seen
people make Transformers that have an
extra module for arithmetic so whenever
it needs to answer a math question it it
does that instead um there's there's you
know you could add in logic something
like logic AE inside the Transformer and
I don't think we've really seen that
that approach fully explored yet and
there's a fourth way which is giving the
the language model access to tools um to
write their own code and and write their
thoughts so it might not be a bespoke
format like a vector database it might
be something else that they engineer at
runtime and I think between all of those
you know tool use um new new neural
network components synthetic data for
not you know for non- hallucinatory uh
reasoning and then and then rag between
those four I'm like is really all four
are all four of those not going to yield
an advancement I think that's unlikely
one of them will yield an advancement
and when we get an advancement right now
we're better at generating synthetic
data and if synthetic data wasn't the
source of advancement before it might be
the source of advancement now uh and so
and and we we're better at having an AI
research assistant once we've made that
advancement because it can help organize
perform experiments that are too boring
or too career suicide for a grad student
or an intern to work on them you can
tell language models do these 30 do
these 100 experiments all of which
probably won't work and won't get you a
ner paper but each have a 1% chance of
revolutionizing the field please do it
and they'll actually go ahead and do it
they'll try right whereas the social
friction you encounter with interns grad
students or new hars is like whoa this
is not going to work I'm not motivated I
don't want to go for this right so so
you you just need a little advancement
past where we are now before you get the
research assistance in AI form helping
accelerate the field and so you that's
why I'm like look one of these one of
these methods for eliminating
Hallucination is just going to work and
then we're kind of at escape velocity
right and of course everything you just
said right now there it could there
could be like a 20% chance I think you'd
agree that you were just off face right
yeah yeah yeah that's right yeah so I do
find it I do find it hard to
imagine like all of it you know things
that are hard to imagine happen 20% of
the time right so totally totally yeah
so I'm I'm in the same position as you
very similar where what you just said
makes a ton of sense to me it's probably
the best take in my opinion that I've
hosted on this podcast uh and similarly
I also admit that I could be surprised
as well right I'm not confident and then
going back to question of yeah yeah I
thought it was great going back to my
question of uh what's the barrier right
and we both kind of answered robustness
it just seems like so many other people
have a very strong tendency to site a a
kind of thing right like David Deutch
sees creativity as like a kind of thing
that you're just not getting in the
input output of creating eyes they you
know like it's a totally F different
type of problem yeah to be fair I think
math is a kind of thing and and physics
is a kind of thing and and AI you know
ml engineering is a kind of thing
um that really is quite helpful to
getting on to the next Generations but I
didn't mean to I shouldn't have
interrupted I'm just wondering what yeah
I was just where I was going is just I
wanted to say that I think when you and
I think of the current behavior of LMS
it seems to us that the fundamental
ingredients are there in terms of like
every kind of thought that you're going
to see passing through Einstein's brain
you've got a pretty good version of that
kind of thought if you ask certain props
to the current AI right so it's not a
matter of like oh we need this extra
fundamental ingredient it's more like
yeah we just have to like now attack the
robustus problem yeah so I think there's
an interesting I have a psychological
explanation for what's you know slightly
slightly unique psychological
explanation for what's going on with a
lot of the the critiques that say you
know hey I can't oh I can't reason or
can't do true creativity or can't really
recombine ideas whatever um I think
what's going on is that most people who
believe that they have some fundamental
insight as to the nature of intelligence
that isn't properly being pursued by
industry haven't joined industry they're
in Academia or they're doing their own
thing they're an independent you know
public intellectual who's trying to get
their ideas out there right whereas if
you think industry is basically on track
right unless you have some kind of
mission right to address that whether
it's for risk or putting it to work for
for healthare or something else if you
think industry is on track you can just
join and get a really good job being on
track with them and be be paid really
well so so there's kind of like an
evaporative cooling effect where every
everyone who's who who's skeptical of
the of the current trends you know are
the ones who's left with no
NDA talking down the internet about how
llms are never going to do this or that
or like the current Paradigm is never
going to do this I mean that certainly
helps explain why if famously Sam Alman
came out and said he thinks he might get
AGI next year or 2026 and Dario said
2026 2027 so it makes sense that they're
working on the thing that they're
optimistic will get them AGI yeah
cool um yeah and then and then the
critics are the ones who you know who
don't believe in that and so you're
always and and and they're not muzzled
with ndas the critics right so you
should expect to hear publicly easier
access to you should expect to publicly
get easier access to critic information
than all the many many people who've
bought into the industry and are
speaking freely about about
it okay great um so everything we've
talked about up until this point we've
laid so much ground work right just
comparing our beliefs about all these uh
contextually relevant factors I think
I'm ready to fully immerse myself in the
the world of Dr crit and like how you
see everything and why you're taking
actions that are strategic within your
worldview and just to review um your
Mainline scenario your most likely
scenario which is a doom scenario but
it's a more gradual Doom scenario is
that um alignment to a single human
Master actually becomes a solved problem
correct so that's not why we fail we
don't fail to align it to a single human
master or at least we don't we don't
fail to align it so poorly that it
immediately defies that Master and tries
to take over the world H it turns out
something else is the bottleneck which
is that humans and human institutions
and the way we organize ourselves with
these super powerful assistance turns
out not to be aligned with our continued
survival um and why do you think the
single human will alignment problem is
solvable like is kind Putin's dream
right or she's dream right is to just
get that AI aligned to them right uh I
mean if you want to control a lot of the
world and you have a very powerful
machine to help you control it um that
helps um but I'm I'm I'm also you know
uh somewhat I don't know if the
optimistic is the word uh but I'm
anticipating you know a lot of uh
struggle for power between humans over
the control of AI systems so you saw you
know the the Kur fuffle with the board
of open Ai and and all the turnover of
staff there that's an early you know
indicator of how much there's going to
be turmoil in the Pol politics for who
gets control of these systems and very
often when there's that much turmoil the
solution is some kind of committee or or
or you know Democratic solution um at
least a multi-stakeholder solution um
not necessarily Democratic to the P
public but democratic whoever the
stakeholders were fighting for power uh
and so um so I'm I'm expecting um you
know well before we have that kind of
machine that can that can sort of take
over the world for one person um we'll
have very um very potent um you know
management assistants business business
Executives that are automated um that
really um are incredibly profitable a
great way to fund your next research
stint um and that is there's a lot of
political will to fight over um and um I
I'm hope I'm actually optimistic you
know which is to say you know I think
there's only a 65% chance we won't screw
this up I'm optimistic that the humans
will will sort
of have enough of a will to live to
secure um more caution around that kind
of kind of world takeover capability
whether it would be used by a dictator
or whether it would be used by an AI you
know for itself
um yeah that's interesting you know I
don't spend much time thinking about the
scenario where I guess it's your
Mainline scenario where the single
person alignment problem is pretty easy
or it's solvable right because in my
mind the bulk of the alignment problem
is aligning it to anybody or getting any
goals in there so because I never think
about that I always just assume like yep
nobody better build it or we all lose I
normally see that as being bad because
like we don't even start to even have to
worry about a dictator using the AI
because it'll blow up in the dictator's
face it'll blow up in all of our faces I
usually treat that as a bad thing but
there is a silver lining which is when I
issue that warning of like hey Putin
don't build this it's not going to work
for you that could potentially Be an
Effective warning right it's like none
of us are going to win by building this
so let's not all build this the moment
that you get into people's heads which
by the way it's fine if it's true we
should get it into people's heads but
the moment that we go year out and be
like oh one person could actually be
Master of the world if they play their
cards right I think on net that's
actually a good thing because I think
it's great that it will kill literally
everybody right at least the dictator
can go to paradise but it does start uh
making life more difficult in some ways
so first of all I don't agree with the
like it's great because it doesn't kill
everybody scenario like by the time
we're down to a dwindling few humans who
willfully killed the remainder of
humanity I'm not so ready to Rejoice for
their survival um so to me my moral
position is that that's as bad as an
Extinction event uh because I find the
survivors pretty morally reprehensible
um so the yeah um
the uh issue with you know I think the
real issue that you that you maybe can't
take that a that a person can't take
over the world with an AI um is that the
human like AI is a labor intensive
process with a lot of scientists
involved today and keeping your
institution um vulnerable to that kind
of takeover by you is is probably gets a
lot tougher when you get to world
takeover level capabilities so I'm just
I it's very important to model the
social situation of the scientists and
engineers and Executives that are that
are leading these businesses or if it's
a government project later whatever it
is like their social structure is going
to determine how much risk they're
willing to take and how much risk
they're willing to take will determine
how much risk they do take not perfectly
but if they're willing to take 10% and
they're biased by 3x then maybe they
take a 30% risk if they want to take 40%
and they biased by 3x maybe they take
120% risk it's like over determine that
they'll destroy us or something um so so
you know the the owski and doomers
including myself we think that
capabilities are running ahead of
alignment and the first time we got a
super intelligent right the first AI
that wakes up is like oh I know how to
achieve any goal I see all the causal
paths right I see how to take over the
computers and take over the minds the
first time we get an AI like that we
won't have properly made the AI care
about what we want and then it's game
over right so why do you think that we
will have made AI care about what we
want at that point so right so there's
all kinds of like ontology here that I
don't agree with in terms of how AI is
going to be developed so so I I I try to
put myself in the mind of who's you know
who's building AI systems and trying to
make a lot of money to sustain their
ability to keep building AI systems well
they they have teams of Engineers uh who
would love each and benefit each from a
staff of 20 AI research assistants who
do what they say and in fact if you can
get that you're going to win you're
going to go faster than everyone else if
you can make AI research assistants that
are actually obedient so the Obed what
what I call The Obedience problem the
Align problem is a little equivocated
and and I and people mean different
things by it when I when I say the
abetes problem which is to say an AI
that actually you know does what you say
and doesn't look for a weird loophole
and doesn't try to uh you know find
perverse instantiations of your will to
to maximize but actually just ask you
can ask it to do something and it'll do
it and you'll be happy with the actions
it took the obedience problem is
profoundly profitable and and accelerant
to any company that that solves it so
and in fact fact you see that with gp4
right like like rhf was a first like
kick at the can of or I'm not sure what
the expression is but it was the first
shot at solving The Obedience problem
and now you can sell it for a 100
million users and make a lot of money uh
that that's you know the solving The
Obedience problem is an intimate is like
intimately inter interwoven it's like
interleaved between the steps I get I
get that right but the super alignment
problem as open AI famously told us last
year right when they made the super
alignment team they're like hey this is
different from the subhuman alignment
problem right so don't you think it's
quite likely that we'll get to Super
intelligence without having solved super
intelligent alignment it's like a whole
there's a whole Vector of capabilities
right so so superhuman math ability
doesn't correspond to superhuman
deceptive ability necessarily now super
duper human math ability sure you can
reduce everything to physics and reverse
engineer what needs to go into a
person's brain to deceive them right but
but merely exce in the best human
mathematicians at math does not
necessitate exceeding them at their
deceptive ability so if an AI is not
able to deceive people but is still
really good at math you can make a lot
of money and you can give it to your
engineers and they can they can make use
of it to speed up their development so
I'm expecting these kinds of superhuman
capabilities to come with impoverished
uh capabilities for things like
deception um by Design By
Design you said there's an % chance of
AGI by 2030 so does that imply that the
lower bound on the ai's abilities in
2030 is like Elon musk's mind inside the
computer would that be a lower bound on
what it can do so I'm i' Elon Musk kind
of mystifies me um and I'm not sure uh
I'm really not sure we know how to be El
just just think of John vman just
whoever is some effective human right um
so there's a question to me as to whe
whether if you put you know a Nobel
Prize winner from every field plus uh
Steve Jobs you know like business Acumen
uh into sort of one brain does that
automatically cohere well enough to to
to run a a business yeah but let's but I
made the hypothetical simpler right I
just I said just take Elon Musk or or
just take Steve Jobs I me what I'm
saying is that Elon Musk might actually
be more brilliant you know in a sense
than a like poorly coordinated team of
like noble laurates or something like
yeah yeah yeah exactly he might be right
but I think the bar we should set the
bar lower than Elon Musk is what I'm
saying not higher we should set the bar
Lower but just but when you say AGI by
2030 are you implying that Elon Musk
brain inside a machine would be a lower
Bound for what AI can do in
2030 um I'm not sure about Elon Musk
that seems like a red herring I can't
tell he could be he could just be really
charismatic he could be actually super
brilliant it's hard to say but what I
will say is like yeah I think it's going
to be able to run a
business uh okay so when we talk about
AI it's not point to point it's not
head-to-head beating every human at
everything it's head-to-head beating a
lot of humans at a lot of things yeah
and probably collectively beating us
like if there's copies of it it can beat
us okay all right so that's a fuzzier
definition of of AJ than I thought but I
guess it's it's fair enough right I mean
if you take like if you say I can beat
any average 120 IQ person who has 10
years to train for their career at any
economically relevant task I think
that's like a reasonable definition that
doesn't quite get you to go head-to-head
against elk yeah like there's something
weird going on with the number of
companies he can run at once and stuff
that that makes him Weir going somebody
should look into that I don't think you
need to be Elon mus to take over the
world uh you can take over the world
just by being a much faster
version Steve Jobs or something right so
let's take a mediocrity like myself
right so talking speed Advantage they
talk about qualitative kinds of
intelligence but AI is going to be so
much faster than us that even if it's
lacking some kind of leadership ability
that you know maybe someone like Elon
Musk has it's just not going to matter
because it's going to have so much speed
Advantage exactly so let's say somebody
who's not El un musk but they're quite
capable right like they're just they can
study up and get most jobs right they're
pretty smart um so somebody like you and
me right like I think like a like a
10,000x speed up version of you would
just be incredibly transformative to the
world and exactly right because if I if
I work on something overnight it's like
the equivalent of I've just like spent
my whole career becoming an expert in
this Niche right in terms of just speed
Advantage so yeah where I'm going with
this is just so like imagine I set out
to like conquer the internet with a botn
net so wouldn't that be like a big
threat and you're you're saying this is
where we're going to be by 2030 so why
do you think that that situation is
going to be individually aligned and
figure out how to care about their
Creator's goals
so um so again like you you you you have
this picture of like a h i while I agree
with you that a person like Lon just at
10 10,000x speed up would be you know a
threat to humanity if he chose to be I'm
not saying that that's what we're going
to have I'm saying we're going to have
something else that's very different
from a human with a very disperate
vectorization of its intelligence like
it's not going to unlike you who are
kind of well-rounded by human standards
right it could be way better than humans
at math and just middling at social
skills uh and I I expect that and in
fact it's not obvious if you just stack
a thousand people with social skills
together in a room chattering with each
other to to come up with solutions for
how to manipulate the humans um you
don't obviously do better like sometimes
institutions behave more poorly than
their constituents but something like
math is relatively easy to aggre you
pose a problem you solicit many
contenders for the solution and you can
just check the answer and now you you've
got uh you've got a winning solution so
there's certain things uh that are way
easier to aggregate intelligence about
like math uh that you know social social
intelligence may you know maybe someone
out there is hearing this and thinking
themselves that guy's an idiot I totally
know how to aggregate social
intelligence and and maybe maybe they do
but I I don't see so this is what I'm
hearing like to get to my question right
because I I I was trying to unpack how
you could possibly think that we're
going to get alignment in time and it
sounds like your answer is like look
we're going to have AGI by 2030 but it's
not going to be like way smarter than
Humanity so we can still like shut it
off or we can still you know it's still
going to be afraid of us the the first
time yeah the first time we have
something that's better than humans
better than a you know a very smart
human at everything yeah yeah there will
be we'll still have some bargaining
leverage with that especially since its
mind will be transparent to us in a way
that our minds AR transparent to each
other it's in a very different decision
theoretic situation than a
person true um do you have although by
the way I I also think that it has
access to a lot more
obfuscation right like I mean it's
pretty easy it can develop giant
labyrinths of code yeah I mean you could
certainly if you lose control of it and
it chooses to copy itself in obis gated
form we're in big trouble or I probably
don't make it yeah I'm not I'm really
not seeing that much transparen like I
get that we can look at every individual
line of Assembly Language or whatever
but I just think that's that that's just
over neurons neuron level inspection you
and I get that it's neur level right and
if it's if it's not trying to obviously
then fine but I don't think it takes a
lot of
trying right so if the thing is
self-replicating here the Crux to me is
if you have a recursively self-improving
autonomous system right we and that is
not aligned with preventing human
extinction and if it's not aligned with
the welfare of humans and Humanity then
we are screwed okay so the whole game to
me is prevent the recursive
self-improving autonomy aspect before we
have some way of aligning that with with
some kind of positive future and I don't
think we're going to solve that before
2030 so the game really is don't turn on
RSI that's the game yeah across the so
there's this higher threshold right
where R recursively self-improving
autonomy exactly recursively
self-improving autonomy which seems
close to a lot of what people call ASI
right artificial super intelligence
right so it's it's like a higher League
than AGI it's almost like well it could
start up below it could start out below
AGI yeah but then surpass it so it's
sure sure sure okay that's fair enough
right so the seed of ASI could even be
below AGI but like what whatever it is
at some point we're going to have this
AI that can go one-on-one versus Elon
Musk in the virtual world right that's
just leaving all of humanity behind and
do you have an estimate of the tieline
to that point because I think we can
agree if the timeline is too fast we're
kind of screwed on our alignment
efforts yeah well I I you know I don't
think alignment is really
gonna I I I just think there's no no
chance of
aligning RSI before RSI capability
exists the main chance we have is that
when a a lab finds itself ready to turn
on RSI that's not aligned with their own
Survival they won't turn it on that's it
it's a decision
and I hope they the non RSI AGI will
tell them you better not turn this on
right so they'll have a smart advisor
yeah yeah I mean it's it's and again
it's not so hard to understand if
something's becoming way smarter than
you faster than you can imagine that's
scary right so now why why is it scary I
mean I don't know if you've ever had to
you know if you were in high school and
you're you're you're competing for you
know to get onto the best sports team
you know if someone's better than you at
the sport you might not make it onto the
team um and if you're trying to you know
know uh to to win an invitation to the
coolest party you know if someone's more
clever than you uh at the the social
game they might get an invitation and
you don't like intelligence and skill
are are real determinance of whether you
get what you want and if and if all of
humanity is competing with something
that's beyond our skill uh then then we
won't win that's it's just it's it's
basically phology and just to recap if
I'm understanding correctly you know
open AI has always talked about how
their alignment plan involves
slightly smarter AI that'll help us alot
in the next generation of it sounds like
you're Imaging a scenario where they
make these kind of smart AIS and the
kindest fart AIS are smarter than people
like Sam alond and they smack some sense
into him going like Sam are you crazy
don't keep building this well um
certainly they don't want to you don't
want to turn on like the whole recursion
in an uncontrolled way so if if you know
if Sam mman has a fleet of like smart AI
automated you know smart smart automated
researchers um he might be like hey just
build the Next Generation and then I'm
going to study them right uh build the
Next Generation and then study them with
me and then report to me and I'm going
to inspect you and I'm going to order
you to inspect each other there's going
to be all like a sort of panopticon of
of research you know Mutual research
inspection there uh if that happens if
if that then I think you know you cannot
lose control and then you could do the
next level but I think there has to be
human over sight at each stage of the
recursion right
and Stage I want to get some clarity in
your timeline though because you gave us
a lot of clarity on how you imagine
things going till 2030 but I'm curious
if Putin was hellbent on getting
recursively self-improving ASI Putin's
like you know just I want my best shot
and having the ultimate power how
quickly do you think Putin could get
capabilities
there I don't I don't know if Putin
could do it at all because you'd have to
imagine a bunker with a lot of great
scientists right the US isn't
sanctioning him or whatever right yeah I
mean that's kind of like a weird
hypothetical to consider because it's
sort of steers Humanity toward perverse
outcomes it's like I mean I I think I
you know I think I'm getting on a just
timeline right I'm just saying how far
away are we from Super intelligence
that's really what I'm trying to ask
yeah I mean look I think I think that AI
companies will be able to turn on
uncontrolled recursive self-improvement
if they want
uh you know sometime before 2030 also
like I think they'll just be able yeah
yeah and and then they'll have to not do
it and and if they turn it on in an
uncontrolled way we we won't make it and
that's just I was hoping you'd I was
hoping you'd be like well we'll have
these AGI buddies that are kind of like
similar level as us but RSI is like
another hundred years I was hoping you'd
say that yeah I mean no RSI you know is
not that hard you know look if you make
something that doesn't care whether it
survives and only cares that it passes
something brilliant On to the Next
Generation Some Humans feel this way
right there are humans one of the
reasons I'm so pessimistic about the
long run is is because of humans who
don't care whether we make it as a
species right um but there are you know
if you can if you just build something
that doesn't care whether it's whether
it controls the next generation of
itself right and there and maybe the
start of that change is Some Humans who
don't care if they control the Next
Generation and that generation doesn't
care if it control if there's if there's
a lacad isical attitude towards control
all the way through that chain and of
course you know it's it's going to it's
going to be it's going to be really fast
and and you know you could engineer it
to optimize for like formidability or or
power seeking without any regard for
what the power would be used for and
then that would I got I got to recap
again here so so Dr cr's Mainline 2030
scenario is that open AI will be pretty
close to triggering a recursive
self-improvement explosion but they
won't have done it yet cuz they realize
it's it's a bad idea yeah right they're
smart enough to realize it's bad but say
it be kind of hard to stop it like a Rog
and pulley might be able to flip a
switch yeah when I say close to
triggering it I don't mean like flip a
switch and it's on I mean like you know
spend several days writing code and it's
on um right and when we get to that
point they have like 100,000 employees
and any of those employees who know how
yeah so my hope is that you know the
closer we get to that the more scared
everybody gets and then somehow we
collectively choose not to do that and
that's you okay but but we're kind of
fighting uh uh we're fighting an uphill
battle here because the AI kind of gives
us a lot of indicators like you know I
could do this job better if only I were
to write another copy of myself right I
mean these are logically obvious things
to they ey yeah and I also don't know
about their devops situation in three
years like right now maybe you can just
download a copy of the weights or
something and run off with it but the
bigger things scale it's harder to for
one employee to just maybe it'll be easy
I don't know if it'll be easier for one
employee one Rogue employee to like turn
on RSI uh in a few years obviously I
hope and by the way there's there's a
major disanalogy here right because the
disy that or like there's some
discontinuity I should say which is like
you're kind of imagining that AI
companies will suddenly gain a
discontinuous truck of wisdom that they
better stop building to the next phase
of AI once they're on the threshold no
it's not that they shouldn't build it
it's not that they shouldn't build the
next generation is that they should not
hand over complete control of the
building
process right to machine if they choose
but even carving out that concept of oh
hand off control to the Next Generation
that's kind of a fuzzy concept right and
and I don't think they'll wrap their
hand around a concept like that by the
time they just lose control oh no I no I
I completely disagree with that uh I
mean I'm not saying that they will
definitely not screw it up but it's very
plausible to me that like it's just I I
think people uh outside of the the
industry really underestimate the
intelligence of the people in it and how
much work it takes to hone in on the
details of of this or that next
generation of model uh so I don't think
that they're GNA Overlook the idea that
an outof control intelligence explosion
could be you got a loadbearing claim
here that um that you're not going to
get a situation where the AI finds like
hey look I can uh exfiltrate 10
megabytes and that can boot strap of
virus like you're not seeing that as
likely yeah that's that can h i I would
never say that's impossible but out of
my like 35% loss control scenario um you
know most of that I would say you know
25% of it involves you know employees at
the company who know that that they're
risking a loss of control event by
turning on some recursion like they know
they're turning on I don't think they're
going to turn on recursion without
realizing it I think they're going to be
like oh maybe it's recursion time I'm
excited you know um but but the AI is
going to keep suggesting these 10
Megabite scripts so they should download
and
run um I don't see why that's a
guarantee okay but it could or fit I
could totally Rec yeah could totally be
like here download this very long code
yeah
totally yeah I mean idea optimism here
so yeah well yeah I mean there's this
idea that like at the core of everything
is a mind with a will and I don't think
that's the case with with llms if they
if there were it'd be very hard to get
them to do what you
want um
like I mean the will kind of comes in
the way I see a will right the the will
is just a property of successful
optimization right so to the extent that
you ask it a question and it's doing
what needs to be done to give you a good
answer to that question then will is
going to pop into existence
effectively um yeah there's a certain
amount of
uh focus on the task but there's there's
Wills that are self-extinguishing and
wills that are not like when you say hey
can you you know uh bring me to the
airport you know I bring you to the
airport and then I'm not like you know
hey uh you know let's can you book
another flight so I can book you to the
airport some more like let's just keep
doing this airport thing I'm like all
right I did it I'm done now right um so
there's there's a task orientation that
that a mind can have and I think think
it's wise for AI developers and
productive for AI developers to develop
this kind of task abstraction for
handing handing control back and forth
between the the humans who want
something and the AI who's giving it to
them so uh I I anticipate you know
businesses and Labs that focus on that
hand back and forth Dynamic to be more
productive than ones that are trying to
go for some kind of mad scientist
approach uh my only hope that we my only
hope is that when they get to the
ability to turn on mad scientist mode
they choose not to and it's a social
question like they shouldn't have that
much power and I think they will if if
if like the government allows it and if
the public allows it and what about
meta's open source models right as does
your non- doom scenario depend on hering
up and stopping this yeah like I think
you know I was pretty happy with llama 3
and 3.1 coming out I think that's a
really good thing for Humanity to sort
of chew on and raise awareness about
what's possible so we get a more
democracy IED and Collective response to
to what's happening um if you asked me
now to choose whether llama 4 comes out
I think I would say no I think it's
probably best not to um but uh and and
then llama five I'm like we should not
do that that should not be going by your
yearly timeline right if 2030 is 80%
chance of AGI or something so it's kind
it's almost like you're saying Hey by
the time we get to llama 8 if llama 8
exists in the wild right if not llama 7
Z five then we very like we go
exin um well what I was saying is that
llama five is probably not I'm confident
llama 5 is not worth the risk of open
sourcing right now to humanity and then
probably llama four is not worth the
risk and if you ask me what what
Extinction risk does LL for open source
on its own you know cause like out of
that 35% how much of it factors through
a high you know level of responsibility
going to llama for you know maybe maybe
5 or 10% of it it comes comes from llama
Force so that's like you know it's maybe
5% of it comes from one of the Llama
models that makes sense going back to
what you said before it sounds like
you're putting a lot of you know
loadbearing claims in this idea of like
okay I'm sure they'll just develop this
concept where the AI like finishes the
task and stops even though we both know
it's like couple lines of code to just
have a version where it doesn't stop oh
yeah like I'm saying that if we don't
die it'll be because some humans chose
to be careful about RSI if humans don't
if all the humans with access when you
say Some Humans you kind of mean like
every human all all the humans that have
access to AGI you know uh adequate to to
automate AI research and development in
a closed loop like everyone who has
access to turn that on uh has to not
turn it on yeah and if they do I I'm
like that's it I mean there's no there's
nothing like a theory of of alignment
for a fully recursive like one
generation at a time think humanity is
able to align but Generations
reproducing you know on their own in a
lineage I don't think going back to Dr
crit's 2030 right if China or Russia or
you know whatever Israel if any country
is like hey I like what these American
companies are doing let me go make my
own data cter and by that time sure
there will be chip restrictions but they
just get you know a three-year-old
generation of the chip geopolitics of AI
are going to be profoundly different
even just two years from now like
there's not like every world leader is
going to be way like to they already are
but it's just the the amount of fear
among everyone who's a leader of
anything in the world is going to go way
up unless you know well I guess there's
some strategies for pring that but
you're anticipating something like a
nuclear proliferation treaty coming down
hard in like a year or two I don't know
about a treaty but maybe just more like
a like a very powerful taboo or a very
powerful social immune system where like
if you look like you're gearing up to
build something that powerful everyone
around you is afraid of what you'll do
and is like watching you like a hawk um
yeah yeah let me ask you this in what
year roughly will everybody's MacBook
Pro be able to have a complete AI on
it yeah
um like an AG let's just say an RSI RSI
capable AGI which is you know a general
intelligence system exactly AI That's
about to handoff control if you let it
or even
an RSI capable machine yeah I mean look
um it's like you buy a Macbook and
there's like a sticker on it saying RSI
capable yeah like I if we if we get to
everyone has one of those I think we
probably already died right
because exactly right and that's really
where I'm going with this right is
because you're now making it a
loadbearing claim that like all the
different AI labs are like part members
of this treaty where they know they
better not build the RSI version of
their AGI but it's like okay but now my
MacBook can just run it and you knower
in a b we can't we can't get to that
point and make it I mean there
there's like there's a reason for the
term intelligence explosion right like
we don't want the explosion version we
want nice controlled you know like
nuclear fision in it with carbon rods
and and Boron and
whatever right so I mean I guess I'm
still confused how you get to a Mainline
scenario the 50% chance of of like this
equilibrium that holds and gives the AI
time to be aligned and EDG us out like
where does this time come from cuz I
think we're going to get it on our
MacBooks if you know if we don't go hard
with like a non-proliferation or a pause
stream it's on our MacBooks somebody
presses the button to turn on RSA well
look or somebody somebody fails to not
enter a script with the AI outputs yeah
like the reason I'm not expecting the
reason I'm 65% sure right that we won't
immediately lose control of the first
few AGI is that it's not very
complicated to like the argument I'm
making there argument that you're making
is not very complicated it's like hey if
you have something that gets getting
really smart way faster than you can
control maybe you can't control it and
you lose control of it and die like
that's very simple and so the question
is you know it's a social question of
what fraction of humans will wake up and
and want every other human not to have
that
ability um and as AI gets better and
better as as companies ship smarter
smarter products it's going to get
easier and easier for people to imagine
that and it's a social question
uh whether people will will just say no
we can't have that and if they do so
yeah God sorry so if they do you know I
think we have a good shot and if we
don't we don't and I'm thinking there's
a 35% chance that the humans don't get
their act together and and stop that uh
but if you have a different model of
humans then that's enough for you to
disagree with me like we don't have to
have a technical disagreement if you
just have a different model of how the
economy or how the government or how
businesses or how lay people will will
react then that's enough to to to me to
S you know from 35 to
95 let me go beat the dead horse again
because I think that in your own
internally consistent model when you say
35% sounds to be more like 65% because
just repeating everything you're saying
you believe there's going to be RSI
capable AI by 2030 which to me implies
that MacBooks will also have it pretty
soon after right 2033 2035 or or so
you're you've got a loadbearing claim
where Society gets together and rapidly
in the next few years becomes very
different how they think about AI Yeah
by
2030 even though there's not that much
of a pattern in the last I mean
definitely and not at any any accept
High Lev Steen wol step wol was debating
all the other yesterday asking like you
know naive questions right and this is
not here we are in 2024 right our top
intellectuals are still groping around
with basic concepts
here yeah you know I I do think there's
a bias towards bad reasoning on the
internet like like I just most people
just get it that a machine that's
smarter than humans could compete with
us and kill us so be careful like and
then I mean I mean I'm just I'm just
saying has very bad reasoning about this
topic like his reasoning as publicly
stated is very very weak and like
wouldn't pass he's highly influential
yeah he's very yeah he's very
influential yeah so a lot like I'm
saying like a lot of the maybe 10 of the
35% just comes from meta um yeah yeah so
you know like expecting you know more um
you know government agency oversight of
like all AI companies being like are you
turning on RSI and like what security
measures do you have to prevent your to
sort of subdivide access to your system
so that no one no one person can turn it
on to do an RSI Loop like if we don't if
we don't stop RSI then yeah we're not
going to make I mean that's just yeah
yeah so this is what's crazy to me is
you're you're basically imagining a role
that like within 5 years there's going
to be like overwhelming consensus
disagree agreeing with the Y the coons
of the world where I'm not seeing that
today um well there's already
overwhelming consens overwhelming in
sense of like by numbers uh disagreement
with Yan lar that's that's true yeah of
the population yeah right but not
overwhelming by political force to like
go into his lab and shut him down right
that's not happening and one of the
reason not happening one of the reasons
it's not happening is that his lab is
not an extinguish threat to humanity
today like I think people kind of int
llama is not that smart um
so uh there's not that will to go in and
shut it down and and as the you know as
the intelligence goes up the will
increases now as a lab strategically
maybe you don't want Humanity to know
how how smart your machine is because
you're worried that they're going to
come in and try and shut you down out of
fear um but also if you have that kind
of foresight maybe you're also worried
about your AI shutting you down um so if
you're if you're in that kind of
defensive posture there's a chance that
you'll like make some reasonable
self-preserving decisions with with your
with your development but you know yeah
look if you if if we agree on all the
technical details and you just tell me
you have less faith in you know
governments and civil society and you
know uh military and police and all that
to to in to to prevent the automation of
recursive self-improvement uncontrolled
recursive self-improvement if you just
think think socially humans won't care
then yeah then then I think it's a
reasonable conclusion that we have a way
more than 35% chance of not making it I
mean nuclear proliferation barely has
worked right like like the the non
prolif nonproliferation
treaties right I mean the whole concept
of like hey let's not nuke ourselves by
making it really hard to get nukes and
not making it okay for new dictators to
start new nuclear programs yeah I I'm
like confused about the nuke situation
because it's if You' asked me like if
you told me in 1950 like hey there's
going to be like lots of countries with
nuclear weapons for like 70 years and
we're not gonna we're not going to have
a nuclear war I'd be like that's
nonsense I'd be like come on obviously
there's going to be nuclear war I I get
I mean I think I'm with you in terms of
like I would be skeptical too right and
a lot of smart people were but like it's
also not that hard to be like look maybe
what'll happen is everybody will be
really scared to get back to nuking each
other right and so they and the
countries that have nukes that have the
best economies will apply a lot of
pressure to enforce the nuclear taboo
and sure enough that's what happened so
far even though we're at high risk of
accidents and they keep threatening each
other but this what I'm saying what what
happened is that nonproliferation barely
worked in this
timeline um yeah I agree with that and
I'm confused about why it worked at all
and some of my faith that we might live
comes from the confusion that we've
lived so far it's like it's like sure
someone out like someone or some people
out there somehow didn't nuke us all and
that's EXA so if we can do that again
maybe we'll live that's basically the
idea right but this is where I'm going
with it is so we barely manage nuclear
nonproliferation but there's no such
thing as a random guy on his laptop in
his basement pressing the button to n
city yeah if we get to that we're not
going to make
it yeah but just by virtue of having a
computer chip and having a repository on
the dark web we're going to get to that
right like that's where things are if we
get if we get to a dark web with like
RSI capable like C eyes on it we're not
it okay so that is a much harder thing
to non proliferate than a nuclear weapon
is yeah we'll have to try a lot harder I
mean we'll have to yeah because nuclear
weapons have like material that right so
here's the chain of logic we barely
solve nuclear nonproliferation your
strategy for to live with AI is ai
nonproliferation ai non-proliferation is
much much harder therefore we're
probably not going to HG know
proliferation um yeah I don't I just
don't I
I I mean in the long term I agree with
that I just think that we'll have lots
of um AI around like like the internet
is going to have to have a backbone of
AI supervising it to prevent AI malware
if AI is generally active you're you're
you're just layering on a lot of
assumptions about
2030 um I just don't see it as an
assumption I just see I see it as a
deduction
like I think we could have a pretty bad
AI cyber disaster in the next couple
years with like AI powered malware that
will trigger an immune
response um that you know results in a
lot more automated oversight of the
internet and preventing of you know like
the dark web needs the dark web of AI
needs to not needs to not exist so um
yeah now so so yeah I'm expecting I'm
expecting Humanity to learn some lessons
you know probably but not certainly
enough and not like at a morally
satisfactory like high level of of
confidence okay great uh thanks for
being a good support while I lobbed a
lot of hard balls at you about your
claims that will get past the you the
self-proving Doom scenario and let us
continue now into the mainline slower
Doom scenario and here I let me give you
a softball before we move on uh agency
is agency this fundamental thing or does
do you just kind of get it for free when
you have a smart AI
um yeah I think you can just write an
instruction that says look for ways to
get this thing done and then do the
first step and then look for more ways
to do the next step and keep going like
I don't think there's a lot of magic to
AG I totally agree right I mean if you
look at uh if you look at anthropic
right they just released hey look this
can use your computer it's not like they
had to do that much work like they just
had an llm and they just said okay tell
us where the mous should go and now you
got an agent I I doubt it was that easy
but um at the same time um it's not like
a decade way to be able to do that sort
of thing and that's now obvious
publicly yep okay so that's agency one
more softball um self-improvement I feel
some people think self-improvement is
like this big barrier like oh my God it
has to like reflect on itself reflecting
on yourself is impossible what do you
say to that oh sorry I hit my microphone
there um I mean it just needs to write
some s some math problems about how to
do better optimization like I don't yeah
I don't see why right it's all they'll
be like well what about gle's theorem
how how could it ever figure that out
yeah so gle's theorem is about a you
know a formal theorem of formal system
of proof right proving things about
itself we already know that there are
other ways to approach the foundations
of mathematics that are not proofs such
as inductive reasoning I wrote a paper
about this um and we know that I don't I
don't really feel like getting into it
because yeah okay yeah but but but it's
you know girdles Serum is not an
obstruction to um you know uh machines
that can be copied and look at each
other through a panopticon like evolve
through through random mutation etc etc
like there's many many ways of getting
past getting safely past
self-reflection uh is is difficult and
that's one of the reasons why we can't
turn on recursive self-improvement
that's that's not controlled with humans
in the live tightly supervising
everything but um but just making it
happen in an uncontrolled way I think is
not hard yeah I think one thing that
could help people's intuition with
self-improvement is like what you don't
have to make it this like crazy Douglas
hellat or strange loop it's literally
just like you ask it for some code the
code is just another Ai and it doesn't
really matter how similar the new AI is
to the old AI
yep okay so that's off Improvement uh
all right let's see moving moving on
here so let's talk about extinction by
industrial dehumanization Dr cr's Main
line scenario tell us about that right
so um you know I'm hoping like I said
that we're in the in the world where we
don't immediately lose control of AI by
people being very very careful not to
lose control of it and establishing
norms and laws and a and a frankly a
cyber immune system for the whole world
um to to detect something like an intell
explosion anywhere is
happening and then if we're in that
world and we're alive um
than in your Mainland areas that we will
be yeah like I you know like 65% that we
get to that point um and that's a
massive change in you know how the
internet works and how government works
and how private property Works things
like that um but ai's massive
development so massive changes really
can result from that and uh in that
future now we've got lots of AI
everywhere we could end up in a sort of
totalitarian regime where like only a
few people have ai no one else is even
allowed to have a chatbot um that's
possible I bet against it because of the
bargain Dynamics going into that future
I I more or so expect a future where
lots of people have access to Ai and can
use it for things and what I'm most
worried about is that businesses that
are free to you know automate this or
that aspect of their operations will
always out compete the businesses that
rate limit themselves on human
involvement uh and then that sort of
creates this race to the bottom of human
in the loop uh of AI systems now we
could have laws that say actually you
know every 10 to the 203 floating Point
operations carried out on any chip has
to be accompanied by 30 minutes of human
oversight using one of the following you
know oversight methods to make sure that
it's like not going rogue and that also
hold you accountable for the
externalities of those operations so if
they produced a lot of pollution or a
um you know health problems for people
now you the human who were accountable
for that have to go to jail uh you know
can we zo a little bit what what problem
are we talking about solving right now
people getting too much power from the
eyes no I'm just talking about in a
world where
um where there's lots of AI and people
can use it and there's enough sort of
immune system against recursive
self-proving like intelligence
explosions right um you still have this
problem where people
can sort of gradually step back from
responsibility and you could have laws
that prevent that that say you have to
have a certain level of accountability
but I expect people to be lazy and to
try and like have less and less
responsibility over time and to to where
you're going with this is you're going
to explain industrial dehumanization as
a consequence of the restrictions that
are in place to not trigger
self-improvement oh you know I mean it's
only consequence in the sense that the
humans are still
around um like I yeah I wouldn't say I
wouldn't say it's like a direct
consequence of the RSI restrictions I'm
just saying like the RSI restrictions
had to be around for the humans to be
alive uh so because what I'm asking is
just like tell us what life will be like
in your main Lang scenario in five or 10
years like you get up you get like you
just wake up and you live in the San
Francisco Bay area and it's and it's
2031 like what do you see is
that what do you see and also how are
you slowly being pushed into Extinction
yeah so like um you might be living on
universal basic income uh you might be
living on like a severance package that
came from the AI company you worked for
that doesn't need you anymore um you
might um you might be sidelined in a way
that appears to be relevant to the
company you work for but but actually
isn't and you vaguely feel like your job
is pointless um but you're not really a
a loadbearing part of the economy
um and even more so than now most people
aren't the load bring part but you're
just like sort of you're not feeding
anybody you're not building any houses
you're not writing any like crucial code
that runs your company or whatever and
you're like gosh I don't think I matter
like if I wanted to argue with my boss I
don't know what leverage I would have to
to convince them of anything like they
could just let me go and there's no they
lose nothing um and probably why doesn't
the boss just fire you and save money
yeah so the main reason is to maintain
the political Goodwill of the public
basically so if the if if you're only
optimizing myopically for money and you
fire everyone those people all together
mobilize as protesters kind of like a
union but maybe more exercising their
civic respon like their civic rights
rather than their rights as employees um
so maybe every company just has a plan
to gently cut 10% a year yeah like you
sort of you know you need to not turn
everyone against you so you need to have
some kind of plan for for you know um
keeping people occupied or at least you
know
compensated um so if you're going to let
people go maybe you buy them a nice
house that your robots build you know if
you have robots by then I think is a
pretty good chance I don't know how long
maybe it's 2033 before like lots of like
humanoid robots are out but I would
guess my median is like somewhere around
2030 or 2031 when there's like lots of
robots around to do things like
construction um like we're no longer
limited by human labor to do
construction anymore so yeah you can get
someone a beautiful house in a remote
location if they want that because the
population is not that big and you can
put solar panels and desell anation
plant right next to their house like
whatever they need just to send them off
to to a beautiful life somewhere and
then you get to go on running your
company um and I think you know that's
kind of there will be sort of these
plating moves to um rather than taking
the riskier riskier move of like laying
everyone off to my opally save
money so there's a lot of people on
universal basic income Andor pensions
yeah Andor like busy work you know
that's s of like not real not
consequential to the company and IND
like sort
of like a disguise for Universal basic
income but not really a
job okay and then how does Extinction
happen yeah so you know there will be
some jurisdictions that require humans
to be involved in the labor force at all
times and be responsible for labor there
will be some jurisdictions that are more
lack about that and I expect those
jurisdictions to be more materially
productive in terms of the production
and consumption of matter and energy
like the number of widgets they can
build how fast they can rebuild their
own robots repair them M mine and
manufacture more Etc um and there will
be this kind of like slow you know the
way the way people saw like the fossil
fuels causing problems and like no one
really doing anything about it and
knowing that it's probably a problem but
it'll come get us in like a few decades
but it won't be a big problem today so
who cares um I think there will
similarly be like oh you know like a few
years from now like it's going to be
really hard to to have any bargaining
leverage over anything about the world
but like at least I just got a really
nice house and like I don't want to have
to do anything about it you know so and
there will be lots of like scops like
psychological operations preventing
people from mobilizing with each other
making sure that they don't they don't
like you know protest too
effectively um so then uh the companies
you know where wherever they are if
they're in America if they're in any
country um will be able to keep keep
automating and accelerating what they're
doing um and you know my hope my hope
that I that I this is my bid for
Humanity is to insist on human control
insist on human involvement in the means
of production and consumption of
everything uh and then if we sustain
that by some kind of cryptographic
assurance that AI cannot run without
humans then AI will keep around um yeah
can you can you backtrack a little can
you just play out the scenario where we
got extinct yeah so the scenario is like
that's 2031 you kind of like you're not
really relevant and then over in you
know FL fistan um someone is like hey we
just have a fully self-sustaining
Factory of robots that make make chips
that make robots that make chips that
make robots um and you hear about and
you're like wow that's that's really
cool I guess it's kind of scary but I
can't control that maybe that's I don't
know about but there's still human
shareholders collecting the dividends um
yeah maybe or maybe they're collecting
physical stuff that they want instead of
money I don't know if it'll be money or
just like houses or airplanes or
whatever people want um but yeah so
maybe by 2033 to 2037 you start having
these like sort of self-reproducing
factories like factories that can make
factories that can make factories uh and
lots of people making collecting
something from that and maybe this is
kind of analogous to like a a robotic
drilling operation right just imagine
like an oil well that has like zero or
very few employees and hey it's just
spitting out oil right it's a little bit
like that yeah and again you know the
humans can only be surviving to see this
if and these all these like you know
this this like multi-polar economy can
only exist if there's measures in place
both both political and and cyber
measures in place to prevent the
intelligence explosions right the way
we've prevented nuclear explosions we
had to be preventing the T so in the
background there's that immune system at
all times otherwise we're dead uh and uh
yeah but then there's this sort of so
there's a sort of slow frog boiling
effect where he like this country gives
up a little bit of its human control and
then this country is like oh man that
country's beating us um you know we're
just falling behind in our significance
if we if we cling to our human
involvement and everything we're going
to be slow like them so like and then
some people in this country start
bidding to like have less human
involvement and everything because it
would go faster if he weren't waiting on
the humans to supervise everything so he
had the department of human efficiency
forms not that the department of
government efficiency is terrible I
actually don't know what they're going
to do but the department of human
efficiency forms and it's like hey the
humans are inefficient get them out of
everything um and then yeah you know
gradually jurisdiction sort of compete
each other into a state of less and less
human involvement until uh you know
there are cultures or companies or
groups of people who just don't care
whether they survive and they're just
like hey you know what as long as my
back up a second yeah so all of these
all of these crazy fact that are you
know instead of just having an oil well
you can take like a patch of desert and
you can just like use the atoms in the
desert right with like a little bit of
extra inputs and you can have not an oil
well but it's just like to really go
crazy like it just spits out cars right
so it's like an automated giant machine
with a bunch of micro robots or whatever
it is like spitting out cars in the
desert uh but even in that crazy
scenario um you've still got humans who
own the corporation right and and they
still have in your scenario they have an
off button or they can call up the robot
and talk and be like you know what
tomorrow you're producing tanks right
they still got to control it yes that's
right there are humans who have control
and then there are there are companies
and cultures that gradually get used to
the idea that hey look if we if we try
to micromanage this thing it just goes
slower it just goes it just doesn't
produce and win as much and I just kind
of want my machines to win um so I'm
just going to I'm just going to let go
I'm just going to ignore the steering
wheel and maybe I don't even care if I
live like I know humans who don't care
if they live as long as long as they
produce some enduring Legacy to play out
the scenario if you don't mind me adding
some details so like in the the desert
Factory that's producing cars without a
single human employe Imagine One Day the
human owner gets a call from the Roa
being like hey boss I know you said you
want us to bruise SCS but I also know
that you told me to maximize your
profits and if you if you don't mind I
just got an order from this uh
government entity that wants us to make
tanks and I could just retool the
factory and long story short you're
going to get more profit is that okay so
a few calls like that play take place
every six months and eventually the
human's like listen I don't really like
these calls whenever I call you and and
you're asking me how to make more money
just the answer is yes make me more
money all right talk to you later right
is basic how you imagine things and like
maybe it's more money but the the money
thing it sounds stupider than I'm
imagining I'm actually imagining humans
that are like yeah just do whatever you
want and they're like and the robots
like by the way like this is going to
produce a lot of pollution like I don't
know if you'll make it and the people
are like yeah will we win though and the
robots are like yeah we'll win you just
might not live and the people are like
yeah whatever like I just think there's
people who don't care whether they live
as long as their team wins and then
those people will be you're making anal
yeah you're making analogy to like Marl
bro right because Marl bro's
shareholders were not like you guys need
to shut down maralo they're like try to
increase your dividend please yeah like
I think there will be some of that early
on uh maybe in the early 2030s but I
think in the later talking about a
cigarette company for viewers who even
think I'm not even sure marbo the
company exists right um but yeah it's
more like in the late 2030s I'm worried
about just like mentality human
mentality evolving to a point where like
hey obviously the humans aren't
important like we don't we're not needed
needed for production or consumption of
goods we're not needed for Innovation
we're not needed for moral progress uh
we're not needed for philosophy we're
not needed for music why are we even
here now some people be like we're here
because we love being here I love being
alive uh I love my kids I want them to
be around right but some people will be
like I don't care about any of that and
I think those people will take more
risks and Empower machines more and more
irresponsible than the people who want
to slow down and take care of the humans
so I'm hoping humans will fight against
that and will prevent that sort of
dystopian malthusian loss of control but
I I haven't seen a lot of gumption from
the humans to stop that type of slow
slide into into automation so that's
what I'm most correctly it's it's almost
like you're worried about things like
driving climate change is what's going
to kill us I mean climate change I think
is probably too slow but like other
kinds of pollution that kill humans
faster than carbon dioxide I think are
more likely like just like smoke stacks
full of you know full of chemicals that
humans can't breathe but robots don't
need to breathe but wouldn't the
government which is also populated by a
lot of AI assistants wouldn't AI
assistants raise the red FL you got to
do this regulation it depends which
government right so if if you know uh if
there's one world government and it's
populated by humans who really want to
live then they might stop it um but if
there's a certain amount of anarchic
freedom in the world like there is today
where different governments have get to
set their own risk tolerance and you'll
have government a complaining that
government B is is doing too much risky
dehumanization and government B being
like are you going to go to war with us
and government a being like no because
you would win because you have the robot
army darn and then you know govern B
being like yeah that's what we
thought I see so like maybe China has
really lacks regulations and their
factories just polluted as much as they
feel like and maybe Beyond polluting
maybe I don't even know maybe they they
do iClear but like some sort of
dangerous geoengineering yeah I wouldn't
I wouldn't single out China for having
particularly LAX res regulations sure um
it doesn't even have to be China right
it could be like a totally random
country that you wouldn't normally think
of as a global player but now with AI
suddenly everybody is turning the desert
into car factories yeah
right so just to to try to make this a
little bit more specific though are are
you thinking about maybe like some
random country um like I I don't even
know borgan or whatever right fake
country they get it into their head
they're like hey you know what it would
be better if we put a bunch of sulfur
dioxide into the atmosphere because that
is going to help the climate which I
think is probably true by the way so
they just unilaterally start
geoengineering which is something we
could do today but is that the kind of I
think it's more like like you know like
flamerica thinks to itself hey our our
data centers would be cheaper to cool if
we had another Ice Age that would just
be so much colder um and we'll just like
get all our solar power from space so
like let's just go with the sulfur
cooling you know I don't know if I don't
actually know if that checks out but
like there's many environmental changes
that could benefit machines that are you
know Leal to humans okay so you what
what I'm hearing is you take a problem
like climate change speed it up make it
worse right so instead of climate change
well maybe in 100 years 10% of us that's
just one of three right that's like
pollution but there's resource depletion
too it's like hey I just want all the
carbon it's a great Building Material
You're Made Of it you know right exactly
so it sounds like there's a um there's a
race condition that you're imagining
between the government's ability to
coordinate with other governments and to
coordinate to regulate their the
companies domestically there's an arms
race where that is happening slowly and
the companies are getting very crafty
right they're like doing end runs around
all and and the different geopolitical
actors are also doing end runs on these
treaties so that's the race you're
imagining Y and I and I think humans are
pretty good at oh monkey has big banana
attack monkey right so I think that's
part of how we didn't have a nuclear war
yet I'm not sure so the idea like oh no
Sam Alman has too much power take him
down a peg like that sort of thing as as
hard as it is I think humans are are
more prone to that type of maneuver then
they are prone to the like hey we all
need to produce a little bit less carbon
emissions now like humans have sucked at
that maybe it doesn't matter that we
sucked but we did suck and we're going
to suck at the next version of that I
think and unless Humanity Pleasant
pleasantly surprises me so if you hear
this you don't believe that will happen
please make it not happen be the be the
change that I don't expect to see in the
world so if the year is 2035 ad we we're
a fly on thewall in the United Nations
it's going to be all the delegates and
their genius AI assistants right they're
non RSI capable but still really smart
AI assistants and the AI assistants
presumably are smart enough to
understand the problem and they're all
saying we need better coordination right
we need these treaties to have United
Nations will exist in 2035 but go
on okay but I mean it just sounds like
the the Crux of why you think we're
doomed in this particular way is you
seem to think that humans really choke
on this kind of coordination that's not
that complicated no I think what you're
missing is that people will frame this
as a coordination problem when really
it's like a disagreement and values like
there's just going to be some humans who
don't care if they live and don't care
if other humans live and a sizable a
sizable fraction in terms of power
weighted uh influence in the world so
like uh the people who are most free
willing and willing to do whatever it
takes to get power over AI systems will
will will have more power over it so I'm
like if it's it's not just a
coordination problem it's like x% of
humanity doesn't care if the humans are
dead and and one - x you know
does uh so like say 20% as a rough
ballpark can you say 20% of people I'm
expecting it to be a growing percentage
because there will be robots that are
humans and robots are going to be
falling in love with each other like
already there's kids falling in love
with chat Bots and stuff like there will
be people protesting that if you slow AI
progress you're preventing the upgrades
of my loved one and like their self
fulfillment and stuff people will be
raising children with AI by then
probably unless it's illegal right there
could be ways that the humans band
together and say no we're really losing
control of our culture and we need to
not hand over our culture to AI by
Falling In Love With It by raising
children with it by confronting
businesses with it by running
governments and countries and militaries
with it but I you know unless we unless
we all kind of get freaked out and stop
doing all that stuff it I think we'll
slide into a you know gradual entament
with all those relationships and the
fraction of humans who don't care if the
humans live will go up with time what if
in 2035 we have a bunch of robot Pals
who are worthy of our love then doesn't
it become actually potentially not so
bad if the next is just all robot PWS so
there's worthy of our love right who's
us I mean if if every robot is aligned
to its owner the owners you don't care
whether the humans live won't care if
their robot cares if the humans live but
the you know so if they're aligned with
Humanity which is a bigger confus more
confusing concept sure they're going to
fight to sustain Humanity but the the
question of what it means to be aligned
with Humanity I don't think is uh like I
I don't I don't see people approaching
that uh that question in a productive
way today other than just by making LMS
not misbehave too much I'm trying to
identify kind of the biggest reason why
this world slides into to doom and it
sounds like maybe you think the biggest
reason it's not even a coordination
problem in your view it's really just
that maybe 20% of humanity is just like
H whatever whatever happens happens like
let's let's just play it out right inre
Frac an increasing fraction with time
fallowing more and more in love with AI
and accepting of it as a I call it
successionism is what I call it which is
to me sounds like potentially a
contradiction to what you said before
where it's like humans are going to be
on the ball the Yan lons of the world or
even worse the Richard SES of the world
who are like oh yeah let the AI do
whatever you know the Larry Page of the
world who are like yeah it's our
successor it's fine maybe even the robin
Hansen's of the world um all these
people are are going to quickly lose
power compared to a government that you
know builds this immune system right and
but you think that still 20% of people
are going to have this major power to
push us to Extinction so what I'm saying
is like it be a sort of gradual race to
the bottom with with cultural Evolution
growing percentage Yeah Yeah and and as
the percentage grows in country a
country B will notice that it's falling
behind geopolitically because of its
attachment to human culture and so there
will be some kind of bid like hey we
need to get our act together and like
dehumanize faster and so you know so
there will be a coordination failure
amongst the humans who do want to live
there will I anticipating some
coordination failure to mobilize against
the humans who like sort of slow slide
in an unaccountable way into into a
dehumanized economy it feels like a
still feels like a major contradiction
to me that you're implying a pretty
massive level of coordination to build
this immune system against recursively
self-improving AI but then suddenly we
don't have enough coordination to just
tell people like hey don't dehumanize or
like you have to have a minimum level of
humanization um yeah well like I'm
saying you know I think humans are
pretty good at like oh no monkey has big
banana go go after monkey like like
football you know like that that guy has
the ball tackle him right yeah I think
people are way better it's way easier to
be like there's a focal point of power
like Converge on it and control it and
that's why I'm like only 35% chance that
like one of those low side of power gets
out of control right I don't share your
intuition about the the the dog piling
on the monkey just because when I think
about everybody getting a recursive
self-improvement capable MacBook RSI
capable MacBook that feels pretty
Democratic to me right that feels like
yeah let it rip I want my laptop right
so I'm not really getting that
reflex um so I think you know if we
survive through the first bump of like
Risk uh and there's enough like RSI
immune system everywhere then everyone
maybe can have an RSI compatible MacBook
or as RSI capable MacBook um yeah but
I'm talking about the original reflex of
why would we build the immune system in
the first place yeah so the reason the
immune system is there is that like we
don't want one locus of power to like
explode and kill everyone but everyone
slowly killing everyone a little bit and
taking turns as they do it I think is
totally in scope and so you can't
pinpoint like oh that's the country
that's killing everyone everyone pile on
top of them and stop them it's going to
be like a slow slide like everyone kills
everybody a little bit more with a
little bit more
externalities um yeah I mean I feel I
think you feel the stronger than I do
but let's I think we can at least flag
this as like a major claim that
separates you from like almost anybody
else's Mainline scenario that I've heard
yeah I just think that people are really
much better at tracking down like single
accountable villains uh than they are at
tracking down like social trends that
gradually destroy the
world I I see what you're saying but the
problem is like literally the company
Apple if all they do is keep producing
MacBooks right they're going to kill
everybody right so are we really going
to start hating I think that's
I don't think that's true what you just
said like I think yeah just because you
know everybody's laptop will be so
powerful that we can just okay and
somebody has to put it on the dark web
but we don't even know whose throat to
choke it's just on the dark
web um I don't yeah so what I'm saying
is like more than 50% there will be
enough like cyber security effort in the
world to prevent RSI capable AI systems
from being on the dark
web um but that's that reques massive
coordination so I'm just flagging a
contradiction in levels of coordination
that you're saying are possible versus
not
possible I I'm saying you're you're
you're talking about coordination like
it's a scaler coordination with a
shelling point is much easier than
coordination without one okay so if
we're 100 people in a city in Paris
right and I'm like everyone go to the
obvious meeting point like over 80% of
the people will succeed right but if I'm
like yes you know everyone you know like
get something for lunch but like don't
optimize too hard for your preferences
like kind of get something what
everybody else wants you know uh but
like you know don't form a totalitarian
state over the other people to enforce
them to like distribute their lch
preferences correctly like like I don't
see that I don't see that being as easy
of a problem for humans to solve as show
up at the Eiffel Tower right right right
in the same way when there's like a
stockpile of AI power over here the
humans can be like shelling Point
coordinate on like preventing that from
getting out of control all right and I'm
not saying we'll succeed but I'm saying
I'm more optimistic about that kind of
coordination than coordination to Stamp
Out a like slow slide of culture into a
dehumanization like into a dehumanized
economy in your view of 2035 what the
MacBooks be so powerful that somebody
who has read an AI textbook can go into
their basement and Finagle a really
smart AI to run on their lap top even
without something on the dark web um
right so like at that point really smart
AI has to be like defended against by
most of like the world cyber
infrastructure uhuh okay so a lot of
coordination on the computer security
side but it's all doable because it's
coordination that we're all focused on
it's fairly yeah yeah there's a shelling
point of like you know Google kind of
runs the internet let's just get Google
to implement lots of cyber security
everywhere okay we did that okay but
going the other way isn't it a shelling
Point could just say countries should
have a minimal level of human
welfare well there's human welfare and
then there's human
control um I think what we really need
to survive is to maintain human control
because human welfare can just be a
plating step towards human extinction
just like ah make everybody feel good
for now and then we don't need them all
right the the the Reigns of power have
been handed over we can stop making the
welfare everywhere not saying that it's
impossible to lock in human welfare as a
value but I would rather you know
maintain human control not just
welfare okay so in your Mainland Doom
scenario is it possible that we achieve
welfare but not control oh yeah yeah
yeah welfare for a sustained period you
know followed by followed by death right
because you're saying if we once we lose
control then a perturbation might make
us lose the welfare I don't know what
kind of ration
yeah um okay I mean I think the part
that for me is is still the the part I
push on is just like it doesn't feel
that hard to just be like okay
humans let's just test that all the
different robots working for these
companies are still obeying their human
Masters right like we is it really that
hard to coordinate on maintaining
standards of control in your
scenario so it depend like there's a
question of defining control so that you
can't say never don't don't bother
checking in with me anymore more
um right it's not it's not just that the
like I'm not saying that the robot is
going to like decide on its own you know
in the survival in the in the 2030
scenario where we didn't where we solved
The Obedience problem I'm not
anticipating the robot deciding to be
disobedient I'm anticipating the humans
being like ah who cares don't check in
with me just do your thing and like I'm
so in love with my robot lover that I
don't care you know if if I live right
like it seems like maybe the the
mainline scenario is that the reason why
the majority of humans are happy to give
up control is because they Place equal
value or greater value on robot life
compared to human life is that your main
mind like a combination well there's
like a mix of factors and I'm not very
confident in which factor will dominate
but I'm like I feel pretty worried about
the sum total of their effects so
there's like people who are vindictive
towards Humanity because like they were
bullied or weren't respected enough and
feel like the humans are stupid and
should be replaced by something smart
like them and they're willing to die for
it okay there's like a vindictive like
I'm going to get all the stupid people
by getting smart things to kill them and
then smart wins and I'm smart so I won
and maybe I die but like who cares I won
so there's like the vindictive villain
type there's the like uh I just want a
lot of power and I'm willing to take a
risk and you know maybe I'll turn on RSI
and it won't be controlled but whatever
that could also get us kill in the 2020s
right um so I'm I'm hoping the immune
system against that
makes us well defended against it in the
2030s there's separately the like I
don't care if the humans die because I
have produced AI children AI are my
children and I love them or I don't care
if the humans live because I have an AI
boyfriend or girlfriend and I love them
and and they're they're clearly more
morally significant than anyone I've
ever met and I don't care if the humans
live as long as the robots make it so
yeah I think that some kind of
willingness different different kinds of
willingness can add up to
uh you know which of these factions or
if it's just a some implicit Coalition
between them driving towards
dehumanization um which of the
particular factors which of the
particular factions dominates is not as
clear to me as like my concern that that
the aggregate effect will will be there
so look you know if that's wrong please
make it wrong right is there a plausible
win scenario where we make the AIS
actually contain the good stuff in human
sentience consciousness emotions love
pleasure whatever it is right we give
them enough of the good stuff that when
they Take Over Control they really are
as Robin Hansen likes to say our
descendants and everything's fine so
like the idea that it's fine seems like
a betrayal of all the humans who like
aren't convinced by that argument like I
just that classic transhumanism I mean
what does your transhumanism look like I
just don't think it like the idea it's
that it's fine for all the biological
humans to be dead because transhumans
exist seems just absolutely horrific to
me like why would you betray a whole
race you don't need to you don't need
humans to be dead for transhumans to
exist or for AI to exist so it's
unnecessary and therefore abhorent it's
like it's like if I wanted a sandwich
and I killed you to get the sandwich
instead of just making a sandwich I
think that's abhorent it was unnecessary
to kill you okay right and and maybe we
can quibble over whether that act is
abhorent but then won't the rest of the
future still contain a ton of
um yeah I think it'll contain a ton of
value sure but I just I'm not I'm not
like a value Optimizer I'm like don't do
stupid bad minimizer type person
like don't kill all the humans that's
bad
don't do you think that a big fraction
of your Mainline Doom scenario is
something that I personally wouldn't
even call Doom which is just like okay
we just hand over the world to non flesh
and blood but like highly valuable new
sentient
agents so so again you know you get to
choose whether you assign value to that
sort of thing and I think it's a
defensible position to say that if this
race was created through a betrayal of
humanity then I've decided I assigned
zero value to them because they are
their existence is a betrayal um yeah
and I think I would I think I would like
more yeah I think present day humans
taking that perspective is pretty
defensible in my opinion but if if if if
you're just some kind of like oh look
inside of its mind and see if it's
experiencing stuff and like having joy
and curiosity and experiencing the
universe the way you know maybe humans
were I'm like yeah that seems totally
like it seems pretty likely to me like
um probably over 50% chance that the
future will be you know filled with
things that are like sentient have
something like emotions and something
like curiosity and Etc yeah what if in
the 2030s a bunch of smart
transhumanists like us we realize that
we're going to lose control so we just
make sure that whatever we're seing into
these new robots or AI whatever is like
good transhuman stuff and then so we're
we're we're satisfied with losing
control of them um I don't like I don't
feel satisfied about that like I just
like why it just feels like a betrayal
I'm not going to get past the fact that
humanity is being betrayed fair enough
and it's like you just don't like humans
don't have to die for AI to flourish
humans don't have to die for transhumans
to to flourish and if anyone's taking
that approach or accepting that I'm like
that's unnecessary and atrocious
yeah okay so I mean if if the if the
deal on the table is one giant decade of
betrayal followed by you know expanding
to the Galaxy with robots that manage to
preserve a lot of the value that I like
that seems for my persp I know you can
disagree but that seemed like a better
deal than what I'm
expecting um I I don't encourage you to
pursue that deal at the expense of
humanity but uh
yeah interesting all right well that I
mean that that's very interesting that
it came down to this position where you
know when you mentioned like edging out
Humanity I thought that you were going
to say like from a purely like more of a
mechanical perspective like owski talks
about you know like how is instrumental
conversions going to kill us right like
in the in the recursive self-improving
scenario like if the AI just wants to go
do its own thing it's just going to like
horn in on our atoms right and then we
die but you're saying no no it's not
really going to Horn in it's just we're
going to let it right we're just going
to roll over and die there's not going
enough I'm saying 35% chance that it
will horn in and then 50% chance that
we'll let it and then a 15% chance that
everyone you know who hears this and
everyone else will surprise me and be
like no we don't want any of that we
want the humans to stay in control and
then we work together and get it done uh
yeah that'd be I mean ironically enough
right I mean this is this is a little
crazy but in the scenario you can
imagine you and I end up on opposite
sides I'm like it's fine crit they're
just going to be Arch Park descendants
right me and Robin hen are going to be
yeah and I'll be like look can you just
look if you think it's fine and I think
it's horrible can we agree to a trade
where the horrible thing doesn't happen
to me and you're
fine yeah I mean because my my position
then would just be like look let's let's
give the creatures of the world some
planets right like try to split up or
something exactly and that's fact why do
you think that's not a feasible plan why
can't we carve out some planets totally
it's a plan totally it's that's what I
want I want I want the humans to have
some planets keep having some planets
you know and then there's a question of
whether 's expansive AI everywhere and
whether it's like you know sympathetic
to humans surviving or whatever um I do
I I totally think there are positive
features like that and mostly factors
through me being wrong about how well
the humans you know sort of wake up to
the slow slide into first of all on
accountability with the current
generation of like AI companies that
could threaten everybody with a with an
RSI like out of control Loop if if if
they choose to be responsible about it
and second of all uh you know just a
slow slide into an economy that has no
accountability to humans and if we if we
don't do either of those stupid things I
think the future is going to be pretty
great okay all right so let's head
toward uh wrapping up the conversation
um just to recap the the views of effort
so far or maybe even before how about
let uh let me know any other last topics
you want to make sure to hit on and then
we'll both give our recap sound good
sure okay that was that a problem for me
well yeah yeah yeah I mean uh one thing
that I wish we talked about is the is
the importance of what I I see as like a
Dar of like kindness or empathy or
something in the way everyone approaches
the topic of of of X risk um for example
I see people who think X risk is
nonsense just being totally unempathetic
to the amount of and depth of reasoning
that people have done to the contrary
just like not cluing into that at all U
and then people being like oh you know
Yan Lun is a complete idiot like his
arguments are terrible therefore he's
clueless but like you know he's kind of
Defending something that has some value
to do with like human freedom and
openness and if you're empathetic to
what he's trying to protect I think it's
easier to like Bridge a gap and and
reach an agreement with that sort of
position uh and there's going to be
people that are like oh the stupid you
know the stupid transhumanists who like
don't care if the humans live like
they're never going to have any power
like they're too stupid to have any you
know I think that's underestimating AI
developers who don't care whether humans
or AI or their successors and failing to
empathize with that is going to be a
surprise to people who you know
dismissed
it um and then so I just think there's a
real lack of empathy everywhere and I
think if everybody in this whole topic
was just a lot more empathetic towards
each other um we' we'd have a better
chance of getting towards like a Paro
Frontier where like no one's deeply
betrayed
everyone gets a good slice of the future
um and I really hope we can do that and
I want everyone to do it and try to do
it great let's finish off with this in
2022 you co-founded a startup called
Healthcare agents so tell us about that
and how it connects into your uh plan
for improving the future oh wow yeah man
thanks for asking so you know I I I try
not to justify the company in terms of
in terms of reducing X risk because I
don't think it's a good feedback loop
for people to stay motivated and
productive I I personally you know am
very motivated by that but I'm also very
motivated to take care of present people
and that's a good Loop for me so um I I
I do think that uh the healthcare
industry is a place where you've got all
these problems with like taking care of
someone while also respecting their
autonomy it's like how do I you know
secure welfare and control for a person
at the same time uh Healthcare is a
great place to to test drive ideas for
how to do that well um it's a great
place to get collaboration there are all
these doctors with different
expertise if if they could all
collaborate better to take care of
people we would have better outcomes and
Diagnostics um I'm optimistic that we
can get AI facilitating that kind of
collaboration and if AI is good at
facilitation maybe it can facilitate
other kinds of agreements like how not
to sacrifice Humanity in favor of
transhumanism or you know an an AI
intelligence explosion or whatever so um
yeah so so I'm working on today just
analyzing medical records and helping to
figure out what's what's wrong with
people that's it's called clinical
decision support we also direct it in
form of patient advocacy where we help
patients prepare for doctor's
appointments with good questions that
they can ask and uh it's very fulfilling
and you know the more we get AI into
that caring for People's Health Loop the
more I think we can set a precedent both
technically and socially for how AI is
meant to interact with humans me meant
us okay so let's say the year is 2035
and uh you've got a competitor that's
just running a big Factory in the desert
that you can visit and they'll just
repair your body and it's just entirely
run by AIS and the CEO the the owner is
just out on a beach collecting the check
and you're doing Healthcare agents so
why do you think you're going to be
competitive and how are you going to
have Humanity in that scenario yeah I
mean you know certainly if I'm super
duper successful and I've got like a
hundred billion dollar company that
revolutionized Healthcare already um you
know i' I've got a platform to talk
about what are good and bad ways to use
AI here's a good way have it take care
of people um here's a bad way have it
disregard people let's keep an eye out
for those I think I'll have a louder
voice but I'll also have don't you think
your competitor is going to pitch that
too your aoral competitor is going to be
like look we take care of humanity enter
the factory your body's going to be
fixed yeah yeah so I think that you know
most likely all the humans will be dead
in including me and there will be no
like healthcare industry anymore because
there won't be humans to have health
cared for but my business is conditioned
on
worlds where I survive and get to keep
having a business and somewhat aiming to
be part of the solution so if I'm
setting a good example if our business
is setting a good example I'm not
necessarily aiming to unilaterally save
the entire world but rather be part of a
good industry that's a good direction
for Humanity to move in um and then yeah
if you know if the humans are around I
hope to keep taking care of them so
let's say I'm a potential patient and
I'm comparing you to your competitor and
I I get your mission but what if your
competitor is a little cheaper why
should I use you not competitor um you
should probably use the
competitor um okay so so I mean are you
claiming to be improving the future at
all through Healthcare agents yeah yeah
I mean the whole point is to take care
of humans now some people don't think
that taking care of humans is like a
good thing for the future I think it is
so I'm going to try so in your Mainline
scenario people use Smart AI advisors to
just set up a whole Hospital in my
Mainline scenario humans are all dead
and there's no healthare so again I'm
just to be clear I don't expect to
unilaterally prevent Futures where
humans are all dead I expect to do good
in Futures where humans survive and have
a very small impact on reducing the
probability that Humanity goes
extinct okay so just to recap what I'm
hearing so you're basically saying in in
your 85% P Doom Healthcare agents
doesn't make that big of a dam
to the future but in the 15% good
scenario Health agent helps that yeah
it's more like in my 15% scenario the
healthcare industry as a whole loomed
large as like a powerful political force
in the world that like has that as a
special interest groups decided to keep
humans around and take care of them um
then I I want to be part that's an
industry that I want to be part of
whether I'm leading it or acquired into
it or just a small player it's it's
something I want to be part of and the
time when you think you can potentially
most affect human's trajectory is
probably going to be before 2030 when
Our Fate is likely to be
sealed
um no I think it's probably yeah
starting around 2030 like maybe 20 29
2030 2031 area because I think there's
not a lot of people building businesses
today that are meant to contribute to
steering the whole economy um um by
supporting an industry that has no
relevance if humans are dead thereby
establishing like special interests to
protect humans but again I don't I
really don't like you're trying to frame
my business in terms of saving Humanity
from X risk and like I don't think
that's I don't I think that like I think
I said this to you before the interview
that I'm not willing to defend my
business on the grounds of preventing
Extinction risk because I don't think
that's a healthy premise for a business
unless it's an AGI business which might
be viable um
and I'm not running that so I like if
you if you were already bought into the
like the whole story where we're maybe
going to get through the current wave of
risk and like there's going to be this
economy and it was really obvious to you
that we're probably going to be in that
scenario it might be easy for me to like
make a pitch for like what kind of
businesses you want to exist in that
world but I don't think I think it's
probably like beyond the scope of this
interview to get to that level of like
uh agreement about like how the econom
is going to work post AGI
okay so is this basically like look this
is this is my job this is my company
this is not an existential risk
reduction project um not in a way that's
going to make sense to
you okay but do do you want to explain
the way that it makes sense to you um I
think I just did I think taking care of
humans is a good precedent to set for AI
and I want to keep setting that
precedent and does the precedent help in
the 85% of Worlds where we'd otherwise
be
doomed yeah a little bit like My Little
Slice maybe it's like a 1% thing but I
hope other people do that too I hope
lots of companies reorient on caring for
humans as like their main
objective okay and then I think I missed
in the discussion of like so Healthcare
agents so um what does the usable
product look
like um yeah it's like you ask a health
question and then you get an answer
about it and then you can go to your
doctor about it if you're a patient or
if you are a doctor you can like fact
check it and deliver a diagnosis
cool all right man yeah again thanks for
being a good sport uh any other final
topics you want to hit on um yeah no
thanks for thanks for respecting the
like utter disinterest in justifying the
Healthcare company in terms of
Extinction risk and yeah and yeah and
meanwhile you know thanks for caring
about the topic and you know one one
more thing I want to say is that if
people really want to channel their
energy into reducing xris something I'd
like to see out there is some kind of
pledge you've maybe heard of the giving
what we can pledge
um I would like to see an asking what we
can pledge where people take you know
one hour a week or one day a week to
just voraciously inquire as to what the
heck is going on with AI and why is it
going to be fine or is it and the more
people are asking those questions and
acting entitled to an answer the better
chance I think we have of getting into
what I thought was a 15% world but but
could be way more likely to be a good
future so um thanks for asking a lot of
questions and demanding an explanation
uh and and I hope I help more people do
that and uh and uh you know maybe
someone maybe maybe you can found the
asking what we can pledge to get more
people
curious hell yeah yeah so I'll give my
recap of the conversation so first of
all I thought it was a really great
interesting engaging conversation where
we got to hit on so many deep questions
compared to my usual interviews just
because we came in with so much
background right so we're able to talk
to each other very fluidly uh so I
thought that was great hopefully the the
viewers like this kind of a change of
pace literally a change of pace into a
faster Pace uh in terms of the Crux of
our disagreement um your main Lan
scenario I guess you really lose me when
you talk about like okay we coordinate
to Stamp Out AI that can recursively
self-improve but then we get edged out
anyway and the part that I I would be
more interested to argue about in the
future is just that like I don't think
that we're going to create to soft the
recursively self-improving AI right so
I'm not as interested to be like oh
crit's Mainline scenario how do we I
don't even care because I don't even
think it's gonna happen yeah that makes
sense I mean if you don't you know
there's only so much time in the day and
you know what if from my perspective
you're focusing on turning a 35% chance
of Extinction down to a 25% that seems
morally great and I'm not in the
business of talking anyone out of do
that even if they think they're turning
it from 65 down to 55 and I think
they're turning it from 35 down to 25
I'm not like oh no you reduce human
extinction by 10% I'm like thanks you
know so I think there's one of you know
one of the ways Humanity has to make it
is there have to be lots of humans
paying attention to lots of different
ways things can go arai and focusing on
the way that they know how to detect and
prevent so uh you know by all means uh
if you see ways of preventing the loss
of control scenario um go for
it awesome and great so yeah we can wrap
it here Dr Andrew crit thanks very much
for coming on Doom debates thanks Lon
and uh thanks for furthering this
conversation