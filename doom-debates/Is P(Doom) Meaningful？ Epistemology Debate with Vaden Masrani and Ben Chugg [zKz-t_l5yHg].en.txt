if you want to know my subjective belief
about something I will absolutely tell
you if you want to know how strongly I
believe that tell you that too what I
won't do is put numbers on it because
putting numbers on it allows for facius
comparisons between things that should
not be
[Music]
compared welcome to doomed baits today
I'm speaking with Vaden masani and Ben
chug they're the hosts of their own
podcast called the increments podcast
which has a lot of overlap uh in terms
of talking about uh epistemology and
occasionally AI topics in this debate
next couple hours you're going to hear
Vaden and Ben challenging me about my
basy and
epistemology I'm going to challenge them
about David Deutsch's AI claims and Carl
Popper's epistemology and we're going to
talk about the risk of Extinction from
super intelligent AI so guys welcome and
please introduce yourselves at a time
and tell us a little bit about your
background awesome yeah thanks for
having us excited to be here uh I not
sure exactly how much you want me to say
but yeah my background is mostly
academic uh studying math and computer
science I currently I'm doing a PhD in
statistics and machine learning had a
brief stin to the law school pretending
I knew something about law but yeah
mostly in uh mostly my background's in
math uh yeah I'm stoked to be here um
yeah so my PhD was in machine learning I
was working in a basian um machine
learning lab that uh on the website is
all about building super intelligence so
um I kind of in that space started
reading a lot about popper and Deutsch
and um have uh a lot of positive things
to say about basanis with regards to
statistics and Engineering I think it's
amazing and a lot of negative things to
say about basanis with regards to
epistemology and beliefs um and so kind
of like to walk that difficult tight RPP
and defend it to some people and then
attack it to other people um and then on
the podcast Ben and I have been doing
that for about four years and we've been
um old buddies growing up in Calgary and
started the podcast as um Co began just
as a beans of continuing to argue and
learn and talk to one another and um we
explore a multitude of different topics
uh yeah poer and de come up a lot but
also things like Recycling and things
like um uh the patriarchy and things
like AGI super intelligence and
everything in between so we try not to
limit ourselves to just a few topics but
um because both um because Ben was
coming from an EA background and I was
coming more from a popper background
that tends to be um kind of the locus of
stuff that we talk about but the future
is open and we have no idea what the
podcast is going to be in a couple years
that's us sweet yeah everybody check out
increments podcast uh it's a ton of
interesting content I'm enjoying it so
to set the stage uh we're going to start
by talking about epistemology and as
viewers probably know my own background
is I'm a software engineer I'm a small
Tech startup founder I'm a lifelong
student of computer science theory of
computation that kind of thing and I'm
an AI Doomer since reading elzra owski
in 2007 pretty much got convinced back
then and can't say I've changed my mind
much seeing how things are evolving so
that's my background and we're going to
kick off talking about epistemology and
just to get the lay of the land of your
guys's position I guess I would
summarize it as kind of what you said
where you're not really fans of basian
epistemology you don't think it's very
useful your epistemology is more like
Carl poer style more or less and you
just think the AI Doom argument is like
super weak and you're not that worried
is is that a good
summary with the exception um there's a
lot of things about AI to be worried
about autonomous weapons uh face
recognition technology um that kind of
uh stuff I am worried about and I think
it's a huge problem um and like other
forms of technology uh it absolutely
needs to be worked on and if we don't
talk about it it's going to become very
problematic so I'm not naive that there
are certain um huge difficulties that we
have to overcome the stuff that I'm not
worried about is super intelligence
paperclips bostrum
simulation broose basilisk all that
stuff that to me is all just um science
fiction nonsense basically however the
caveat is um I haven't read all of yuny
and at some point in the conversation
I'd love for you to just take me through
the argument as if I hadn't heard it
because it could be that we're operating
with asymmetric information here and so
I'm completely open to having my mind
changed and uh we don't have to do it
now but it's some point I'd love to hear
just like from the step one step two
step three what the full argument is
because I could have just missed some
stuff that youi has written that would
change your mind so that's the that c
okay this is a question I like to ask
every guess here it comes
Doom what's your doom Ben what is your P
Doom uh I'm
almost I'd almost be unwilling to even
give you a number because whatever
number I gave you would just vary so
wildly from day to day and would just be
based on some total gut hunch that um
I'd be unwilling to defend or bet on and
so I think it's much more fruitful in
these conversations to basically just
talk about the object level
disagreements that we have and not try
and pretend knowledge that we have about
the future and come up with random gues
and put numbers on those guesses and
then do calculations with those numbers
as if those numbers had some sort of
actual epistemological relevance so I'm
sorry to break the game here but uh yeah
I I'm it would be silly of me to even
say I
think Aiden want to add that yeah um I
completely agree with everything Ben
said uh yeah I have a deontological
principle to not put numbers on my
beliefs however if by P Doom you simply
mean to like what do I believe then I
would categorize it in the same place as
um The Rapture or the Mayan apocalypse
or roko basilisk that's my number what
if we zoom out and we're not just
talking about AI right so like nuclear
war pandemics uh even asteroid impacts
although those seem unlikely in 100
years um but just yeah looking at
everything together just the probability
that Humanity goes extinct or gets
reduced to cavemen in the next hundred
years
anying meaningless question I won't give
a number I don't think that we can know
the probability uh if you want to know
my beliefs about stuff that's a
different question so I can tell you how
much I believe but I won't give you a
number would you tell me if you think
that it's more than one in a million
chance numbers are meaningless Al also I
mean so it's I I can I can yeah I can
compare it to other stuff though so
that's I will give you a comparative
thing so the reason why people ask for
numbers is because I want to compare um
so I can give you things to compare it
against and one thing with compared
against is Roo's
basilisk I mean Roo's basilisk is a
pretty obscure topic right so and the
question is pretty straightforward of
humanity is going extinct so maybe we
can compare it to like an asteroid
impact right so compare the chance of
humans going extinct to the chance of a
large asteroid the size of the the
dinosaur one coming in The Next Century
so yeah I think I think it's much better
to just take these one Topic at a time
right so when we talk about asteroid
impacts this is a a very different class
of event than something like AI or
nuclear war in particular we have models
of asteroids right we have both counts
and we have physical explanations of how
often uh asteroids uh enter our orbit
and you know uh we have a sense of our
deflective capabilities with respect to
asteroids so there's lots of like
there's lots of knowledge we actually
have about the trajectories of the
asteroids and uh and then we can use
some statistics to start putting numbers
on those risks that's completely unlike
the situation of geopolitics for
instance we have no Goods TOA models to
model the future of geopolitics yeah no
I hear you okay I'll just explain where
I'm coming from with this kind of
question so as I walk through life I
feel like I'm walking on a bridge and
the bridge is rickety like it's very
easy for me to imagine that in the next
100 years like the show is going to end
right it's going to be game over for
Humanity and to me that feels like a
very Salient possibility let's call it
Beyond 5% probability that's how I would
normally talk about that and then to the
reason I'm asking you guys is you know
we don't even have to get that deep into
the epistemology I'm really just asking
you like hey
in your mind is the bridge like really
really solid or is it
rickety solid so yeah I would argue Ven
and I might disagree about certain
object level things right there's very
there's geopolitical risks that I'm
certainly worried about and you know I
think nuclear war uh is a possibility in
the next hundred years and I'm worried
about nuclear deterrence and I'm worried
about uh the US getting involved in
certain geopolitical conf conflicts that
increase the likelihood of nuclear war
so all of that we can talk about when
you say the words put you know what's
the probability of this you're already
bundling in a lot of assumptions that
you know we're operating in some
well-defined probability space here
probability is this technical tool that
we use that you know mathematicians use
sometimes to solve certain problems that
has a domain of applicability uh machine
learning is one of those domains right
we use statistics a lot to reason about
algorithmic performance and reason about
how to design algorithms to accomplish
certain goals uh when you start talking
about the probability of nuclear war
we're totally outside the realm of
probability as a useful tool here and
this is you know now we're sort of
getting to the heart of the matter about
the critique of basan epistemology it it
views you know it has this lens on the
world where everything can be boiled
down to a number and those numbers can
be compared uh with one another in
coherent ways and those are premises
that I and Vaden
reject whole yeah I mean you guys you
guys are being pretty quick to throw my
question back at me but I feel like I'm
asking about something that you can
probably interpret meaningfully
for you perhaps easily Ian V saying that
he the next years are solid in terms of
uh probability of human extinction or in
terms of fear let say subjective fear of
human extinction
right okay so it's solid in some sense
that maybe you could describe as your
subjective sense right when you say
solid it's right the sense that you
answer that but but to be clear
subjective sense is different than
probability yeah okay fair so I can make
the question um perhaps either even more
meaningful by saying like hey imagine
this is the peak of the Cold War crisis
peak of the Cuban Missile Crisis and
people are saying like man this blockade
the us is going to do the blockade
around Cuba the Soviets have threatened
to respond they might even use their
missiles before they lose them so
imagine it's like that evening right
where a lot of people like I sure hope
they don't launch those missiles during
that evening if I ask you hey does the
next Century of Humanity's future seem
to you solid or rickety would you still
give that same answer solid
not in that
circumstance okay and so from our
vantage point today could you imagine
that sometime in the next few decades
like what's happening right now with
Ukraine and Russia or Israel and Iran
could you perceive yourself entering
another type of evening like that when
you're like oh maybe I don't feel solid
anymore I can imagine all sorts of
things for so generally when we're
Imagining the future and we're thinking
about the past and we're just we're not
sure what's going to happen that's
generally a time when a lot of people
would be like well there seems to be
some significant probability
that things are going to go really bad a
lot of people would but we don't totally
that's the you would rather dismiss
those people and just be like nope my
answer is solid no near misunderstanding
the claims that we're making um I don't
dismiss those people I ask for their
reasons for the number because the
number itself is next to meaningless
it's not entirely meaningless but it's
close to it um if you want to know my
subjective belief about something I will
absolutely tell you if you want to know
how strongly I believe that tell you
that too what I won't do is put numbers
on it because putting numbers on it
allows for fallacious comparisons
between things that should not be
compared so we can talk about subjetive
I'm just trying to into yeah but I won't
put if you want me to put numbers on it
then we're going to stalemate here but
if you want to have something else then
we can go yeah right now I'm just
pushing on your answer when you said
solid do you want to perhaps revise
solid or are you still want to go with
solid no no I'm an optimist um yeah I'm
an optimist about future I think that
there's definitely things to be worried
about but there's also many things to be
excited about um technology is awesome
um we can talk about Ukraine and Israel
and Iran and those are things to be
worried about we can also talk about um
the mitigation of poverty we can talk
about uh getting to Mars we can talk
about the amazing things that
um a diffusion models are making and how
that is going to but none of those
things are directly irrelevant to my
question of the risk of something like
the Cuban Missile Crisis really coming
to aead
Quest that wasn't the question if I
recall it the question was about do we
think we're standing on a solid Bridge
or a rickety bridge and then you use the
Cuban Missile Crisis as a example right
so if a lot of good things are happening
on the bridge right like there's a canul
land happening on the bridge but the
bridge still might collapse that's
really more of what I'm asking about is
the risk of
collapse yeah I don't think we're going
to
collapse okay all right um so yeah you
guys mentioned a lot of diff different
topics that I definitely want to uh
drill down into um I think yeah a good
starting point is like to zoom all the
way out and like let's talk about
epistemology right epistemology is like
the study of how people are allowed to
know things or is that basically right
sure yeah the study of you know how how
we know what we know I think is usually
how how it's phrased yeah yeah yeah
knowledge about knowledge knowledge
about knowledge why is epistemology
important and what are the stakes for
getting epistemology right Ben yeah so I
mean epistemology is at the center
perhaps of this mystery of how humans
have come to do so much in so little
time right so for most of even human
history let alone world history or let
alone Universal history not much was
going on and humans weren't making tons
of progress then all of a sudden a
couple hundred years ago we started
making huge leaps mounts of progress um
and that is a question of epistemology
right so now we're asking questions how
are we making so much progress why do we
know that we're making progress can we
actually say that we're making progress
we seem to understand the world around
us much much better we're coming up with
theories about how the world Works
everything from you know cellular
biology to astronomy um and how is this
Myst unfolding and epistemology is uh a
key question at the center of that right
to to be able to say um how and why
we're making progress and also to start
analyzing uh the differences between how
people goow about making progress and
how that differs maybe across cultures
are there better and worse ways to
generate progress are there ideas that
stultify making progress uh you know
these are all important questions when
it comes to Future progress and you know
just human human welfare in
general great one thing maybe add to
that is I like sign of on everything
been said I'd also say epistemology is
like the grand unifier so if you like
science and you like um literature and
you like um journalism and you like art
and you like thinking about the future
epistemology is the thing that underlies
all of that um which is why our podcast
just keeps branching out into new
subjects every other episode because
epistemology is the the center of the
ven diagram so for right that reason and
for reason yeah I like it science was a
major breakthrough in popular
epistemology right this idea of like hey
if you want to know what's true instead
of just like arguing about it and
getting stoned and deferring to whoever
is higher status why don't we go outside
and conduct an experiment and let
reality tell us what's right right
exactly
um yeah so epistemology is is powerful
in that sense and then also as we stand
today I think we argue over epistemology
as it relates to how we're going to
predict the future right I mean you saw
it a few minutes ago and I'm like hey is
the next century are we all going to die
and it sounds like we're kind of on the
same page that we can't even agree on
whether or not we're all likely to die
because of a conflict that's going to
trace to our epistemologies right 100%
exactly yeah okay great so I just wanted
to set that up because I think a lot of
viewers of the show aren't epistemology
nerds like we are but now we we raised
the stakes right so the rest of the
conversation is going to be more
interesting uh okay so my first question
about your pisty is would you describe
yourself as paparian right in the style
of Carl
popper reluctantly yes yeah um I only
say reluctantly because I don't like
labels and I don't like a lot of
obnoxious freaking paparian on Twitter
who also identify as paparian and every
time you label yourself now you are
associating now other people with that
label their bad behavior or their
annoying Tendencies maps onto you so
that's why I don't like the label but I
have to just yes I'm a paparian through
and through he's the one who's
influenced me the most um and every
other utterance of mine either has his
name cited directly or is just
plagiarizing from him so yeah a paparan
definitely I think he said on your
podcast you spent like hundreds of hours
studying all of popper or is that your
background yeah um that was what I was
doing while I was in a basian machine
learning reading or machine learning
research group yeah um so it was basian
in the day and popper at night and that
was exactly yeah
yeah F how about you um yeah probably
more reluctantly than Vaden if only
because I don't know popper stuff as
well so my knowledge of popper you know
I I've read some of Popper's works and
in great detail uh and argued with Vaden
almost endlessly about much much of
Popper's so you know it be cheap to say
that I
don't understand poer uh uh but you know
I haven't read all of his work and I've
become extremely allergic to labeling
myself with any particular view but yeah
if you press me at the end of the day I
would say that I think popper and his
critical rationalism makes the most
sense of any sort of epistemology that
I've come across uh previously so I'd
have to adopt that label okay and and
you came from a ea background I think
that's important for the listen to to
know it's not as if you totally neutral
and they should listen yeah they should
listen to our first 10 episodes because
that's where the battle began and so you
were familiar with the EA stuff and
there's through a long slow battle which
this two three hour conversation is not
going to resolve at all um but hopefully
the conversation will spark some sort of
interest in the viewers who and those of
um and those who want to explore this
more can listen to our 70 plus episodes
where we gradually explore all of this
stuff so no minds are going to be
changed in this particular debate which
is why I don't like debates too too much
but if it Kindles some sort of interest
and people actually do want to explore
this slowly then there's a lot of stuff
to discover um so great okay and uh as
people may know I'm coming at this from
the basian side uh people who read less
wrong and elzra owski that whole
framework of rationality and Aid Doom
argument it does tend to come at it from
basian epistemology and it explains why
basian epistemology is so useful from
our perspective and in this conversation
I'll put forth some arguments why it's
useful and you guys will disagree right
but so that's kind of where we're going
with this is kind of a popper versus Bay
epistemology debate is that fair V let's
do it and then be before we jump B
further when I think about popper today
I feel like David Deutsch has really
popularized it in the discourse so I
feel like most people myself included
haven't read almost any popper directly
but we have read or seen indirectly a
good amount of David Deutsch and when
David do was on your podcast I think he
said he's not an official spokesman
right he's not a popper surrogate he's
just somebody who's read a lot of popper
and been highly influenced by popper but
he doesn't claim to speak exactly like
popper would speak but from your
perspective isn't David deuts very
closely aligned with
popper uh yes if you don't know Popper's
work very well if you do know Popper's
work very well then you start to see
major distinctions and differences
between the two um so from an outsider
perspective I think understanding
deutch's work is a great entry point
it's more approachable than poer for
sure but um but there's no substitute
reading Deutsch is not like so actually
let me stick one step back um for about
five years I read the beginning of
infinity and fabric of reality and I
just thought to myself ah you know what
I basically get conjectures and
reputations I get the point wrong you do
not get the point you have to read
conjectures and reputations there is so
much more in that book than you have
learned in beginning of infinity and it
is not like a surrogate at all you have
to to read uh conures and reputations at
least um great to start to have the
picture filled in fair enough and from
my perspective uh I've read enough about
Bay that maybe there's some deep stuff
that you guys don't get yet right so
maybe we'll bring out some of the deep
stuff in Stars so just to add so um in
logic of scientific discovery and
realism in the aim of science about
three quarters of both of those books is
discussing probability and Bas so it's
math and its equations and um everything
that I know about Bay comes from popper
and that's not in deuts so if you want
to really understand baz and probability
then you have to read popper I it's not
enough to read Yosi because Yosi is
coming from the chains line um so ET
James is the famous basian and and so
James is yukos's popper but um James G
gives one glimpse into how probability
Works um and so if you actually want to
understand it at the root you can't just
read um otsky or po James you have to go
down to popper and then Branch out from
there um so just add that and just to
tie David Deutch into this argument a
little more directly I heard when he was
on your podcast you were talking about
how you're not a fan of Bays these days
and you spend a lot of the time on your
podcast telling people why baz is wrong
or the arguments are weaker than they
look and David deut was really nodding
along I think he gave you like an adab
boy for so he basically supports your
mission of being kind of anti- baz right
our mission was because of like one page
in beginning of infinity um and that
then that got my little cogs turning and
then being in a basian machine learning
reading group like research lab um
coupled with reading popper is what made
the the whole argument um starts to
become very interesting to me um but
yeah yeah so our debate today it's a
little bit of a proxy two on two where
you've got this team of Carl popper and
now David Deutsch who's actually still
alive and well and then on my side we've
got uh you know the Reverend Thomas Bay
or you know who the the group who
actually invented uh Bayan reasoning um
and and alas VI Kowski right who's been
highly influential to a lot of people
like me teaching us most of what we know
about baz so yeah so elzer um as a
successor to baz versus if de as a
successor to poer all battled through US
random
podcasters sound good with with the
caveat yes sure I mean yeah there's
always a bit of trepidation I think at
least on my part and I'm sure on vaden's
part as well to speak for anyone in
particular I mean David de has his own
lines of thought and you know I I would
be very hesitant to label myself uh a
deuts or popper expert and so you know I
always prefer it if we just keep the
maybe debates at object level um of
course in the background there's going
to be these basian versus deian paparian
Dynamics and you know that's inevitable
given how we've all been influenced but
just to put it out there I'd be I'm I'm
uncomfortable saying that my views uh
comport precisely to someone else's
views yeah just to clarify for the
listeners um
the Reverend Thomas Baye is not
equivalent to basanis and the guy Thomas
Baye is legit and he's fine and that's
just like where base theorem came from
or whatever but basian I think of as ET
Janes and IJ good and Ellers Zukowski
and so these are the people who um I
would put on the opposite side of the
Ledger great and also the the other
correction I would make is that I think
Pierre Simone llas is actually the one
who publicized uh basian methods and he
kind of named it after
so yeah you know and this isn't really a
history I don't really know what
happened but it it just is what it is in
terms of philosophy yeah okay all right
so to kick this part off uh Ben how
about just give us really briefly um
like explain paparian reasoning and like
pitch why it's valuable uh sure so I
think the way I like to think about
paparian reasoning at a high level and
then we can go more into the details is
just trial and error right so it comes
down to how do we learn things you know
if how you if you ask a kid how they
Lear learn how to ride a bike or how
they learn to cook or how they learn
anything you try stuff and it doesn't
work you learn from your Stakes you try
again and you slowly reduce the errors
in your in your uh thinking and your
habits uh and then popper just takes
that same attitude to epistemology he
says okay um how do we learn things well
we conjecture guesses out the word about
the world how the world Works whether
it's politics whether it's science um
and then we look for ways to refute
those guesses so this is where the
critical experiment comes into play for
popper in the realm of science right so
we have a theory that theory makes
predictions about how the world works it
says certain things should happen under
certain conditions uh and that gives us
something to test right so then we go
out we run that test and and then again
follows his famous falsification
Criterion right so if that test does not
succeed we say okay uh Theory falsified
uh and then we come up with new guesses
um and so there's of course a lot more
to say but it's really the method of
trial and error at work in the realm of
epistemology and so popper really does
away um with the notion of seeking
certainty so at you know he was
operating at the time of the Vienna
Circle and people were talking a lot
about how do we get certainty out of our
science right or how and how do we
justify our certainty um and also
talking about demarcations of like
meaning versus uh meaning meaningfulness
versus meaninglessness um and popper
basically takes a sledgehammer to both
of those traditions and says these are
not uh useful questions to be asking and
certainty is not achievable it's not
attainable so let's just subvert that
whole tradition and instead uh we're not
going to search for certainty um but
that doesn't mean we can't search for
truth um and that doesn't mean we can't
get closer and closer to the truth as
time goes on but there's no way to know
for sure if something's true so we can't
be certain about truth um and then this
also starts to subvert certain Notions
of basian ism which wants to they you
know basian want to approach certainty
but now via the probability calcul
um and so you know that gets us perhaps
farther down the line but that's maybe
uh the the scope of the debate and then
I'll let Vaden correct any thing I've
said wrong
there great um just one thing to to add
is um what popper says we don't do is
just open our eyes and observe evidence
getting beamed into our skulls such that
the probability of a hypothesis goes up
up up to some threshold and then bang
you know true and that's how you get
knowledge it's not about just opening
your eyes and having the evidence beamed
into you it's about conjecturing stuff
and then actively seeking evidence
against your view trying to find stuff
that falsifies your perspective not
opening your eyes and observing stuff
that you want to see um so we'll
definitely get into that so me and the
Basin we don't have a problem with
taking in a bunch of evidence and then
updating your belief on that evidence so
I guess we'll talk more about that that
does sound like an interesting
distinction let me give the Quick Pitch
for what basm is what it means uh so
basian ISM says that you go into the
world and in your head you basically
have a bunch of different possible
hypotheses some mental models about how
the world might be working right
different explanations for the world and
then you observe something and your
different competing hypotheses they all
say oh this is more consistent with what
I would have predicted this is less
consistent than what I would have
predicted and so then you go and you
update them all accordingly right you
make a basian update you the ones that
said hey this is really likely the ones
that gave a high prediction a high
probability to what you ended up
actually observing they get a better
update After You observe that evidence
and eventually once you keep observing
evidence you hopefully get to a point
where you have some hypothesis in your
head which is really high probability
compared to the others and you go out in
the world and you use the hypothesis and
it steers you in the right direction
like it turns out to keep giving a high
probability to things that actually
happen so that's the model of basanis
and it sounds like a lot of what bism
tells you to do is similar to what
popper tells you to do I mean baz and
popper they're not like night and day
right they're not enemies and they're
arguably more similar than different I
mean there's major differences we're
going to get into but like when you guys
said hey there's no certainty right
there's just like doing your best I mean
I feel like that fully Dov tales with
what baz would tell you right because
you're not supposed to give like a z% or
100%
probability okay ask a question there
what do you mean by update so um what I
do is I change my mind all the time when
uh stuff that I think uh turns out not
to be true or I see new evidence that um
either confirms my view or disconfirms
it so I'm changing my mind all the time
um but you didn't use that phrase change
your mind you said update and somebody's
curious what the difference is between
updating and changing your mind yeah so
when you talk about hey what's on my
mind like what do I think is the correct
hypothesis um like uh maybe a good
example of the election even though you
know politics is such a controversal but
I'm just talking about predicting who
will win let's say Trump versus comma if
you ask me Leon who is going to win and
I say um I don't know I saw the latest
poll I guess kamla and then tomorrow
like another poll just moved the moved
the win probability 1% and nowc like I'd
guess Trump but it's not like my mind
has changed from K to Trump it's like I
I was always very much entertaining both
the hypothesis that says tomorrow Kam
will win and the hypothesis that says
tomorrow Trump will win and when I
update their probabilities I'm just like
okay yeah if I had to bet I would now
bet slightly higher odds on one than the
other so that's what I mean by changing
my mind it's very much not
binary no I didn't ask what you meant by
changing mind I but you meant by update
um so update is the same as changing
your mind or is it different um so I I
don't really have such a thing as
changing my mind because the state of my
mind is always it's a playing field of
different hypotheses right I always have
a group of hypothesis and there's never
one that it's like oh this is my mind on
this one every time I make a prediction
I actually have all the different
hypotheses weigh in weighted by their
probability and they all make the
prediction together where did you get
the hypothesis from wait let's
okay yeah can we no just want to have a
conversation like um I I just don't
understand your your answer right but
Ben had a question first uh yeah like
maybe let's just make this concrete um
so when if you're designing a satellite
uh you're going to send the satellite
into space right uh you're not going to
base the mathematics of that satellite
uh on some combination some weighted
combination of theories of physics right
um you're going to base it on general
relativity hopefully otherwise it's not
going to work uh and so in what sense
you know you're not assigning a
probability one to general relativity
because we also know it's wrong in some
fundamental way right specifically it
doesn't count for uh certain very small
subatomic effects that we know to be
true so yeah in what sense is like you
know you're taking a decision there uh
it's not a weighted average of physical
theories so what's what's going on
there great question if I'm going to go
and invest $100 million on an
engineering project it's because
whatever combination of hypotheses are
going in my head are agreeing with
sufficiently high probability on
predictions that my engineering is going
to work so I don't have a hypothesis in
my head that has more than 1%
probability saying you're going to
launch that satellite and some other non
neonian non- einsteinian force is just
going to knock it out of its trajectory
I don't have a hypothesis like that
that's getting sufficiently high
probability so this is a case where I
feel very confident so my hypothesis
about how the physics is going to work
has already established itself with more
than 99%
probability I don't understand that but
Ben did you understand uh yeah I mean I
okay that's that's fine I I just I
think yeah we can we can move on I mean
I I I don't think this is actually
what's going on in your head I don't
think you have these explicit theories
and you're actually signing
probabilities to them I think what's
going on is you've been swayed by
arguments that if you send
satellite into space it's going to work
relativity so I think basanis in this
way is both descriptively and
normatively we'll get into that later
false um but you know I can't sit here
and examine the context contents of your
mind if I understand I think this this
is an interesting point you're making uh
you're basically saying look the you
kind of reconed right you retroactively
described yourself as using basian
epistemology to justify why you funded
the satellite project but realistically
you never even thought of that you're
just retroactively pretending like
you're Bas is like basically your
criticism but hold on though because
Ben's question wasn't about if you have
$100,000 and you need to allocate it to
different engineering projects it's if
you were the engineer and we don't know
how to make a satellite yet how are you
going to do it that's a different thing
right so we're not talking about
assigning probabilities to which
Project's going to be more or less
successful we're talking about like how
do we get a satellite in the sky um and
to do that you need to understand
general relativity and quantum mechanics
and these two things are mutually
exclusive so if you sign probability to
one you have to necessarily assign less
probability to the other under the
basian framework however that isn't how
scientists make satellites because as we
get more evidence for the quantum
mechanics that doesn't take away what we
know from general relativity because we
have to use both to launch the freaking
satellite into the sky and so he just
kind of answered a question that was
adjacent to but not the same as the one
that Ben was
asking to make this specific is there a
particular prediction like you're
basically saying hey how am I going to
resolve the conflict between these
different theories but can you make it
specific about which part of the
satellite engineering feels tough to
resolve for you well it's uh it's it's
just more when you were saying like how
do you reason about the world right
you're not you're not tied to any
specific hypothesis it sounded like your
world view is not like okay uh for the
purposes of achieving this I'm going to
assume General general relativity is
right that's anatha to the basian right
the the analogy there is assigning
probability one to gen to general
relativity you're not going to do that
because we know general relativity is is
false in some important way um and so
you said what you're doing know what
you're thinking and the actions you're
taking correct me if I'm wrong of course
are some you know weighted average of
these hypotheses that you have about how
the world works but that just doesn't
comport with like if you know if you
were to be an engineer in terms of how
you're actually going to design the
satellite and send it up into space um
it's not you know you're not relying on
a mish mash of physical theories to get
the job done you're relying on general
relativity in this
case I mean there there's specific
things that I need to model to get the
satellite to work and I don't
necessarily need to resolve every
contradiction between different physical
theories I just have to say what are the
relevant phenomena that need to follow
my models in order for the satellite to
perform its function and not fall down
to earth and I probably don't have major
conflicts between different theories I
mean if I'm not sure whether Einstein's
relativity is true right if I'm not sure
whether time dilation is a real thing or
not then I as a basian I don't think
that bism is the issue here right if
Engineers launching a satellite didn't
know if time delation was going to be
the issue I think even as a poar and
you're like uh oh well they better do
some tests right I think we're in the
same position there yeah for sure sure
can I can I can I go back to a different
thing that you said earlier maybe the
satellite thing is getting us a bit
stuck um you said that you never change
your mind because you have a fixed set
of hypotheses that you just assign
different weights to First is that
accurate sub about you dud I don't want
to if you want to drill down into I
wouldn't call it a fixed set of
hypotheses and in some sense it's a
variable set of but it's always there a
community of hypotheses right and
they're they're all getting weights and
and then they're all weighing in
together when I make a prediction so
when you said you never changed your
mind just maybe flush out a bit more
what you mean by that cuz I don't want
to if if I mean if I walk into a room of
strangers and I say guys I never changed
my mind I think that's very much sending
the wrong message right totally totally
which is why I'm not trying to strmen
you I so may because on the contrary
right the takeaway is really more like
no basian I'm a master at the dance of
changing my mind right I I don't just
see changing my mind as like oh hey I
have this switch installed that I can
flip no no no I see myself as like a
karate
Sensei where I can like exactly move to
the right new configuration of what my
mind is supposed to have as a belief
state so so does that answer your
question um so I I guess why did you say
you never change your mind in the first
place totally understanding that that
what I me yeah I I when when I say I
don't change my mind what I meant was
that when you use that terminology
change your mind it seems to indicate
that like somebody's mind has like one
prediction right or like they've picked
like this one favorite hypothesis and
then they threw it away and took a
different way and and I'm just saying
that's that doesn't describe my mind my
mind is always this community of
different hypothesis yeah gotcha yeah so
yeah so that's actually a nice
distinction between like a paparian
approach and an a basian approach so for
me once I have enough disconfirming
evidence I do exactly what you said the
basine doesn't do I take that hypothesis
and it's gone now I don't a sign less
probability is just it's dead up until
the point where there's another reason
to think that it's no longer dead and
then I'll revive it again but um but so
that's just a just distinction between
how my thought process works in in yours
I guess um but I'm curious another thing
though which is um where do you get your
hypothesis from in the first place uh
because I understand that Under The
basian View you start with hypothesis
and then you just assign different
weights to them but um but I'm just
curious before that stage before the
reting where did the hypothesis come
from in the first place uhhuh that's a
popular gacha that people like to throw
up the basins right they're like hey you
guys talking about question no no no I
know I know I know people like to you
know BAS to keep updating their
probabilities but if you don't start
with a really good probability in the
first place then you still might get
screwed up for example like if my a
priori probability that like Zeus is the
one true God if it's if I start out with
it at
99.99% then even if I see a bunch of
evidence that the world is just like
mechanical and there is no God I still
might come out of that thinking that
Zeus has a really high probability so
you know this is just kind of fleshing
out your your kind of line of AR no you
misunderstood the question you
misunderstood the question I'm not
talking about um how do your prior
probabilities where do they come from um
I'm CU when you talk about Bas you have
your likelihood and your prior um so P
of e given h p of H over P of e yeah um
so we can talk about the probability for
p of H and that's what you were
describing I'm not talking about that
I'm talking about H where does H come
from so before the probabilities of
scientist where do H come from so this
is not necessarily a point of
disagreement between us I mean just like
you can generate hypothesis I'm also
happy to generate hypotheses and
consider new hypotheses so the short
answer is you and I can probably go
Source our hypotheses from similar
places the long answer is there's an
idealization to how a basian can operate
called Solon off induction are you guys
familiar with that at all yeah yeah yes
yeah so Solon off induction just says
like hey there's a way as long as you
have infinite Computing resources right
it's an idealization for that reason but
there is a theoretical abstract way
where you can just Source from every
possible hypothesis and then just update
them all right that's the ideal so I do
some computable approximation to that
ideal but but the approximation that's
where the that's where the details are
hidden right you you you don't have
every you're not running every possible
hypothesis in your head right at some
point you're coming up with new ideas
like sometimes you wake up you have a
creative thought that you haven't had
before um you know basanis can't really
account for that all in and you know you
want to get into the math if really
complicates things cuz now all of a
sudden you're you're you're working with
a different probability space right and
so like what happens with the
probabilities that you assign to some
other fixed hypothesis class now it's
like okay now I have a new hypothesis
everything's got to get rejiggered um
and so it's it's it just doesn't account
for idea Creation in a satisfying way so
this is um this is how I perceive the
the current state of the conversation
I'm basically like hey my epistemology
has an uncomputable theoretical ideal
that I'm trying to approximate and you
guys are like well that's so fraught
with Peril you're never going to
approximate it like what you actually do
is going to be a shadow of that whereas
I would make the opposite criticism of
you guys of like okay well you guys
haven't even gotten to the point where
you have a an uncomputable ideal so I
feel like I'm actually farther along to
be in this because approximating an
uncomputable ideal we do that all the
time right this whole idea of like Hey
we're going to go do math well math is
actually uncomputable right the general
task of evaluating math monical
statement is UN so in in all areas of
life we're constantly approximating
uncomputable ideal so I I'm not ashamed
of approximating uncomputable Ideal but
we do it on this side of the aisle too I
again if you want to set this up as a
debate then we can I guess do that um
the turing machine is an uncomputable
ideal that we approximate with our brain
so we have that on our side too if
that's what you're looking for
right and how does that relate to
paparian ISM um it doesn't totally
because paparian or popper so it does
relate to deuts so the um deuts CH
Church Turing thesis is where he gets
his Universal explainer stuff and I we
can maybe go into that if you want but
um but but in terms of popper it doesn't
at all but in terms of giving you what
you said we didn't have it does because
you saying that on our side of the aisle
we don't have a
uh incomputable ideal that we
approximate but we do because I'm
talking to you on it right now which is
a MacBook Pro and that is approximation
of an incomputable ideal so okay got it
so so when I ask you guys hey where do
where do paparian get their hypothesis
you're basically saying well we do at
some point consider every possible train
machine as I no no no so this this is
great so um we don't know so the answer
is I don't know um so popper starts with
trial and error um but the question of
like where do the conjectures come from
where do the ideas come from we don't
have an answer I don't know and I would
love to know and to me answering that
question is equivalent to solving AGI um
so I have no idea how the brain comes up
with its conjectures in the first place
popper starts there it just says there
is some Magical Mystery process that
will leave it for the neuroscientist and
the psychologist to figure out we're
just going to say dno but that's the
foundation the trial and the error um so
that's the answer from our side yeah I
wanted to ask you another question that
I think you might have answered now
which is so poer talks a lot about
explanations right like good
explanations it sounds like you're
saying that when you think about an
explanation uh you can formalize what an
explanation is as being a turing machine
would you
agree then what do you think uh no I
don't think so I mean if we if we knew
how to program a good explanation
presumably that would allow us to
generate them computationally right if
you understood them deep enough at that
level and also I suspect something like
that is impossible because then you
might be able to litigate what is a
better and worse explanation in every
circumstance and I highly doubt that
that's possible right this is like the
realm of argument and debate and
subjectivity enters The Fray here and
like you're not going to be able to
convince everyone with with an argument
and so I don't think computation is the
right lens to have on something like
good explanations and and just to add a
metaphor to to that so um it's kind of
like saying uh could you automate um the
proof process well in some sense
absolutely no no like this is what go
like the incomplete stuff is is about
which is that like for different kinds
of mathematical problems you have to
entirely invent new kinds of proof
techniques such as like Canter's
diagonalization argument right um that
was completely new and you can't just
take that formula and apply it to all
sorts of new kinds of problems that's
what mathematicians are doing all day is
coming up with entirely novel kinds of
proofs and so if you grock that with the
math space so too with explanations I
think that different kinds of phenomena
will require different modes of
explanation um such that you can't just
approximate them all with a turning
machine now in the mouth space I think
we're now at the point where you know
we've got set theory and formal proof
Theory and I think we're at the point
where I can say what a mathem what do
mathematicians do they're approximating
this ideal of creating this mathematical
object which you can formalize within
Pro Theory as a proof like we we
actually have nailed down the ontology
of what a proof is but it sounds like
you're saying okay but we haven't nailed
down the ontology in epistemology of
what an explanation is so but now you're
saying we'll compare it to math but I
like MTH is farther along can so um just
jump in here for a sec Ben which is that
uh Ben will never say this about himself
but listeners just type in Ben chug
Google Scholar and look at the proofs
that he does they're brilliant so you're
talking to a mathematician and so not me
Ben um and so I just pass that over to
Ben because he is absolutely the right
person to answer the question of what do
mathematicians do all I'm just more
curious about what you mean by we've
solved the ontology of proofs um as a
genuinely curious question because this
might make my life a lot easier if I
could appeal to some sort of book that
will tell me if I'm doing something
right or
wrong let's say a mathematician grad
student goes to his professor and he
says hey I'm trying to prove this I am
trying to write up a proof once I figure
it out well that thing that he writes up
these days almost certainly is going to
have an analogous thing that could in
principle might take a lot of effort but
it could be formalized right purely
symbolically within set theory is that
fair yeah I mean well yes I mean okay
I'm I'm I'm confused I mean once you
have the proofs the point is that it is
a it's it's logic right so you should be
able to cash this out in terms of yes
like going down down to like ZF set
theory for instance right you should you
can cash this out all in terms of like
certain maximums with respect to math
you don't tend to uh descend to that
level of technicality and every proof
you stay at some abstract level but yeah
the whole point of a proof is that it's
written in tight enough logic that it
convinces the rest of the community that
doesn't mean it's certain that doesn't
mean we're guaranteed truth that just
means everyone else is convinced to a
large enough degree that we call this
thing published and true okay great the
hard part is coming up with what the
hell the proof is in the first place
right so once you have a proof yeah we
can start doing things like running
proof Checkers and stuff on it the hard
part is you know the thing in the first
place you're right so the reason I'm
bringing it up is you know I don't even
want to talk about the hardness of
finding the proof yet I just want to
talk about the ontology right this idea
that when you ask a mathematician what
are you doing the mathematician can
reply I am conducting a heris search my
human brain is doing a computable
approximation of the uncomputable ideal
of scanning through every possible proof
in the set of formal proofs and plucking
out the one that I need that proves what
I'm trying I know a single mathemati he
would say that and you just asked a
mathematician and his reply wasn't that
is is a hypothal I'm not making a claim
about what mathematicians say right I'm
just making CL about this the antology
of what is right so so an informal
informally written English language
mathematical paper containing a proof
maps to a formal object that's all I'm
saying sure yeah yeah I mean you're I
mean map map is a formal language so so
far as proofs are about manipulations in
this formal language then sure y
so the reason I brought that up is
because when I'm talking about B and
you're asking me hey where do you get
hypothesis or what is a hypothesis I'd
oh a hypothesis is a touring machine
that outputs predictions about the world
if you also encode the world in bit
right so I have this ontology that is
formalizable that grounds basian
reasoning but when you guys talk about
paparian reasoning it sounds like you
haven't agreed to do that right you
haven't agreed to take this idea of an
explanation and have a formal equivalent
for it false analogy because a
hypothesis is in natural language not in
a formal language so the analogy doesn't
work because the ony matical paper right
so is a research
paper step
outside go to go to physics or chemistry
or
something yeah like I'm just saying that
the stuff that you're just asking about
the ontology of a mathematical proof
using that as um an analogy to the
hypothesis the H in B theorem the analog
is broken because a hypothesis is some
natural expression it's not a formal
language so just the analogy just
doesn't work is all I'm saying yeah I'm
I'm not saying that a hypothesis is a
proof what I'm saying is when I talk
about a hypothesis using natural
language or when I'm saying hey my
hypothesis is that the sun will rise
tomorrow there is a formal there's a
corresponding formal thing which is hey
if you take all the input to my eyes of
what I'm saying and you codify that into
bits and you look at the set of all
possible Turing machines that might
output those bits my hypothesis is about
the sun rising tomor is one of those
machines sure I mean okay so let me just
let me just try and restate your
critique of us just so I make sure I'm
on the same page I think you want to say
you know in theory basanis has this way
to talk about the generation of new
hypotheses right as abstract and
idealized as this is we've put in the
work in some sense to try and formalize
what the hell is going on here you
paparan are sitting over there you know
you're critiquing us or making fun of us
you haven't even tried to put in the
effort of doing this where are your
hypotheses coming from you can't
criticize us for doing this you have no
you don't even have a formalism for
God's sakes you just have words and
stuff you you know is is that kind of
that's kind of where you're coming from
without the snar I added that it's yeah
it's roughly accurate because I do think
that formalizing the theoretical ideal
of what you're trying to do does
represent epistemological progress but
how only if the theoretical um
philosophy assumes that a formalism is
required so part of Popper's view is
that formalisms are useful sometimes in
some places but most of the time you
don't want to have a formalism because
having a formalism is unnaturally
constraining the space of your
conjectures so the theory on our side is
that formalisms are sometimes useful in
some places not always useful in all
places and so I I totally accept your
critique from your view because your
view is that a form formalism is always
better and we don't have one thus we're
worse but our view is that formalisms
are sometimes useful in some places not
always in every place what would be the
problem with you just saying okay I can
use a tring machine as my formalism for
an explanation because when we look at
the actual things that you guys call
explanations it seems like it's pretty
straightforward to map them to Turning
machines the and yeah I guess you go
ahead then well I I think it just
doesn't help you try and figure out the
question of like really where these
things are coming from right so if
you're interested at the end of the day
of trying to figure out uh
philosophically and presumably
neuroscientifically how humans are going
about generating hypothesis mapping them
to the space of all possible touring
machines is not helpful like sure the
output of your new idea could be run by
some touring machine great the question
is you know what you know there's an
entire space of possibility as you're
pointing out you know like vast
combinations endless combinations in
fact of possible ideas the human mind
somehow miraculously is pairing this
down in some subconscious way and new
ideas are sort of poing into our heads
how the hell is that happening I don't
see how the touring machine
formalization actually helps us answer
that
question well it's because we're talking
about the ideal of epistemology it might
help to think about hey imagine you're
programming an AI starting from scratch
Isn't it nice to have a way to tell the
AI what a hypothesis is or what an
explanation is the ideal of your
epistemology is that a formalism is
required not our epistemology
right but so what I'm saying is okay
let's you're saying a formalism isn't
required but let's say I take out a
white sheet of paper and I'm just
starting to write the code for an
intelligent AI right so when what you
say is formalism I say is like hey I
have to put something into the ai's head
like how do I tou the AI this I mean I
agree like this would be awesome if you
could answer this question but I I just
don't I don't think you're answering it
by appealing to like one thing I don't
quite understand about your answer is
you're appealing for a process rather
okay let me say that again for a process
that is taking part in our fallible
human brains as an explanation you are
appealing to this idealized system by
definition we know that can't be what's
going on in our heads so how is this
helping us program an AGI which I
totally take to be a very interesting
question and I and we you know we'll get
into this when we start talking about
llms and deep learning I don't think
this is the right path to AGI and so in
very interesting question from my
perspective is what is the right path
like if we could have some notion of
like how the human brain is actually
doing this I agree that we you know once
we figure it out we could write sit down
and presumably write a program that does
that uh and that's a very that's a very
interesting question I just don't think
we we know the answer to
that yeah so I I agree that just because
I have a formalism that's an
uncomputable ideal of Bas epistemology
doesn't mean I'm ready to write a super
intelligent AI today uh and by analogy
uh if you know they understood chess
when when the first computers came out
it was pretty quick that somebody's like
hey look I could write a chess program
that basically looks ahead every
possible move and this is the ideal
problem uh program it will beat you at
chess it'll just take longer than the
lifetime of the universe but it will win
so I agree that your criticism is
equally valid uh to me as for that chess
computer my only argument is that the
person who invented that chess computer
did make progress toward solving uh you
know superhuman chess ability right that
was a good first step yeah yeah that's
fair can I um can I just pivot slightly
and ask you to clarify whether you're
talking do you think basanis is true
descriptively of the human brain or
you're making a normative Claim about
how rational agents ought to
act right yeah yeah you said this a few
I'm glad we're getting into this because
this is definitely one of the key points
so and remember like what you said
before like okay you're telling me now
that this engineering program you used
basian reasoning to say you were 99%
confident of the theory but it sounds
like you're ronning like that's kind of
the same the same style of question Sor
what's what's roning what's R it's like
retroactive basically right like totally
B in okay cool sorry I didn't know that
term sorry sorry interrup you no that's
good yeah um so okay it's totally true
that like as I go about my day right
like why did I open the refrigerator
what was my community of 50,000
hypotheses right that told me different
things were going to be in my fridge or
there wasn't going to be a black hole in
the fridge right what were all the
differentes and the answer is look I
just opened the fridge right because
it's like muscle memory right like and I
was thinking about something else right
so I'm not pretending like I'm actually
running the the basing algorithm what
I'm claiming is that to the exent that I
can reliably predict my future and
navigate my world to the extent that my
brain is succeeding at helping me do
that whatever I'm doing the structure of
what I'm doing the structure of the
algorithm that I'm running is going to
have B structure otherwise it just won't
work as well uh okay so descriptively
you're saying or you're saying
descriptively if you do something else
you'll fall short of perfect rationality
like you'll have worse outcomes what I'm
what I'm saying is like sometimes my
muscle memory will just get me the can
of Coke from the fridge
without but to the that
true it dovetails with what basian
epistemology would have also had you do
like basan epistemology is still the
ideal epistemology but if you're doing
something else that ends up happening to
approximate it then you can still
succeed according to
you um yeah it's not based law it's
based theorem first of all uh but sure
yeah um that's that's that's the
worldview that we are saying we disagree
with but sure yeah I mean look Lally to
you guys right like when you're opening
your fridge right you're not you don't
have the the one paparian model with a
good explanation and what's that right
you're just like thinking about
something else most
likely are conjecture that you're
thirsty or you got a little
like I I guess I don't entirely know
what the question is if if you're asking
what is the paparian approach to getting
something from the fridge it's pretty
simple it's you have an idea that you're
hungry and you go there and you open the
fridge and you get it um but if the
claim is something deeper which is like
does The paparian View say something
about baz being the ideal etc etc then
it definitely says that is not the case
um so we can go into reasons why that's
not not the case but in your answer is
assuming the very thing that we're
disagreeing about is the point yeah okay
a couple thing no I was just yeah just I
we're somewhat in the weed so I just
maybe wanted to say to people how I how
I Envision the basian debate often is
like there's two simultaneous things
often happen
one is like the descriptive claims that
you're making about like how humans do
how brains do in fact work and that
they're doing something
approximating uh basian reasoning um and
B and I both think that's wrong for
certain philosophical reasons um namely
you know we can get into empiricism and
stuff but I don't think observations
come coupled with numbers and that those
numbers aren't being represented
explicitly by your brain which is
updating by a basis theorem um so
there's this like whole descriptive
uh morass that we've sort of entered and
but then there's there's you know where
rubber really meets the road um is like
the normative stuff right so basian want
to because they want to assign numbers
to everything uh like you want to do at
the beginning of this episode right
you'll assign new numbers to
geopolitical catastrophes and you know P
doom and and then you'll compare those
to numbers that are coming from you know
robust statistical models backed by lots
of data and I think faden correct me if
I'm wrong I think fayen and I's core
concern is really with this second
component of basanis right I think the
descriptive stuff is philosophically
very interesting um but it's sort of
less important uh in terms of like
actual decision making and real world
consequences like if you want to sit
there and tell me that you're doing all
this number manipulation with your brain
that helps you make better decisions and
like that's how you think about the
world then you know like honestly that's
that's fine to me but what really you
know when the stuff starts to matter is
I'll just steal vaden's favorite example
because I'm sure it'll come up at some
point which is uh you know Toby or's
book of probabilities in in the
precipice right so he lists the
probability that humans will die by the
end of the century I forget correct me
if I'm wrong um and he gives this
probability of one six where does this
six comes from it comes from
aggregating all the different
possibilities that he's that he analyzes
in that book so he does Ai and he does
um uh biot terrorism and he does Volcan
so he does all this stuff and this is an
illegal move that from Vaden and I's
perspective and this is the kind of
stuff we really want to call out and
that we think you know really matters
and really motivates most of the basian
critique and sort of goes beyond this
like descriptive level touring machine
stuff that we've been arguing about now
um so anyway I guess I just wanted to
flag that for the audience like I think
there's more at stake here in some sense
than just
deciding how to open the fridge in the
morning which is is fun and interesting
to talk about but I just wanted to maybe
frame things yeah may may I just
yes I just want to add something to what
Ben said beautiful exactly right I think
it's so important to continuously remind
The Listener the viewer why we're
arguing in the weeds so much and we're
arguing so much about this because of
exactly this high level thing that you
said which is um it is illegal it is
um duplicitous and it is misleading the
reader when someone says the probability
of super intelligence is 110 and they
compare that to the probability of
volcanic Extinction which is one in one
million because you can look at the
geographical geological history to count
volcanoes and make a pretty Rock Solid
estimate but you are just making up
when you're talking about the future and
then you are dignifying it with math and
a hundred years of philosophy and so why
Ben or I can't speak for you actually on
this one but why I like to and needs to
argue in the weeds so much is that I
have to argue on the opponent's
territory and so when I'm getting all
annoyed by this one and 10 to one in one
billion comparison to argue against that
I have to go into the philosophy of the
touring machines and the this and that
and the whatever and we get super in the
weeds um but the reason I'm in the weeds
there is because Toby or has been on
multiple podcasts and probably blasted
this number into the ears of over 10
million people if you can fairly assume
that Ezra Klein and Sam Harris who both
swallowed this number um uncritically uh
if their listenership is somewhere um I
think it's one and six for the the
aggregate of all extinctions and then
one in 10 for the super intelligence one
if I'm remembering the precipice
correctly and that was compared against
um I don't remember the numbers for
volcanoes and supernovas and stuff but
one in one million one in 10 million
that that that order of magnitude yeah
yeah and then so you're making the case
why we're getting in the weeds why
epistemology is so high stakes because
basically the upshot in this particular
example is that Humanity should be able
to do better than this basian guy Toby
or because it's kind of a disaster that
Toby or is saying that like nuclear
Extinction for instance might have a
probability of just to oversimplify what
he actually says something in the
ballpark of 10% right which gets to what
we were discussing earlier so you
consider it kind of a failure mode that
people like myself and Toby or are
making claims like hey guys there's a
10% chance that we're going to nuclear
annihilate ourselves in the next Century
you think it's a failure mode because
you think something better to say is hey
we don't know whether we're going to get
annihilated and nobody should say
quantify that no um misunderstood the
the the claims um so I didn't use
nuclear Annihilation intentionally
because I think that is also in the camp
of we don't really know what the numbers
are here I used volcanoes and I used
supernovas and I used asteroids I did
not use nuclear that's what he I think
we're all on the same page that those
things are unlikely in any given Century
right but so so why don't we talk about
the the thing that's like the more mey
claim right the thing about but but but
my claim is is not that it's we can't
reason about nuclear Annihilation I
think that's very important I'm just
saying that if I talk about the
probability of volcanoes and then I talk
about the probability of nuclear
Annihilation when I say the word
probability I'm referring to two
separate things I should talk about like
probability one and probability two or
probability underscore s and probability
underscore o or something they're just
different and we can't use the same word
you might label it frequentist
probability or would that be a good
use no no frequentism yeah frequentist
is a philosophical interpretation um
I've been using objective probability
but just probability Based on data
probability based on on Counting stuff
data but frequentist is not
right okay yeah yeah maybe you could
call it statistical
probability um let's just call it
probability that's Based on data like
csvs Excel Json yeah fine for the
purpose of this this conversation
honestly um and yeah just to just to
maybe answer the question you you asked
ven a minute ago it's certainly not that
we can't talk about the risk of nuclear
annihilation right what we're saying is
let's skip the part where we all give
our gut hunches and like scare the
public with information that no one can
possibly have uh and so I would just
turn it on you like so if you know say
you're very worried about uh nuclear
Annihilation you give a probability of
one over 10 in the next 50 years then
someone comes up to you some
geopolitical analyst say John mimer
comes up to you and he says my
probability is one out of 50 okay uh
what's your next question you're going
to ask why is your probability one out
of 50 and he's going to say why is your
probability one 10 what are you going to
do you're going to start descending into
the world of arguments right you're
going to start talking about
mobilization of certain countries their
nuclear capacity their incentives right
you're going to have like a a
conversation filled with arguments and
debates and subjective takes and all
this stuff uh you're going to disagree
you're going to agree maybe you'll
change his mind maybe he'll change your
mind great uh and then at the very end
of that the Basin wants to say okay now
I'm going to put a new number on this um
but B and I are just saying the number
is totally irrelevant here and it's
coming out of nothing let's just the
number part and have arguments right and
that's not saying we can't think about
future risks we can't prepare for things
it's not throwing our hands up in the
air and you know claiming that we yeah
we absolutely can't take action with
respect to anything in the future it's
just saying let's do what everyone does
they disagree about things let's take
arguments very seriously arguments are
primary is a way to say on our world
view numbers are totally secondary
numbers are secondary and only useful
when they're right they're they're right
for the problem at and they're not
certainly not always useful yeah
yes typically when you have a data set
is when it's useful to use numbers yes
imagine none of us were basian and we
just had the conversation behind closed
doors about the risk of nuclear
Annihilation and we come out and we're
like okay we all agree that the
likelihood is worrisome it's uh too
close for comfort it's still on our
minds after this conversation we didn't
dismiss the possibility of being uh
minimal right so that that' be one kind
of non- basian statement that normal
people might say right yeah
okay and or alternately you can imagine
another hypothetical where people maybe
it's in the middle of the Cuban Missile
Crisis and people walk out of the room
which I think actually something like
this did happen in the Kennedy
administration where people walked out
of the room saying like I think this is
more likely than not like this looks
really really bad so where I'm going
with this is I think that there's you
could bucket a number of different
English statements that people normal
people often say after leaving these
kinds of meetings and it's pretty
natural to be like okay well in the
first place where they said too close
for comfort maybe the ballpark
probability of that is 1% to 20% hold on
hold on that's the move that's the move
that I want to excise so I think it's
completely legitimate at 100% to bucket
degrees like strengths of your beliefs
um I think that this is done all of the
time when you answer survey questions so
like a 1 to 10 scale is very useful um
how do you agree with this proposition
uh sometimes it's like strongly disagree
disagree neutral agree strongly agree so
that's um a five point scale that
indicates strength of belief uh
sometimes it's useful to go to 10 I I
think for like certain mental health
questions I do that all great I'm so on
board with that that's important where I
say hey hold on people is calling it a
probability okay you don't have to do
that you could just say you could just
say how strongly do you believe
something um and then as soon as you
start calling it a probability now we
are in philosophically dangerous
territory because the arguments to
assign probabilities to beliefs and then
equating probabilities that are just
subjective belief gut hunches with like
counting freaking asteroids that's where
all the the the difficulties come so I
am totally in favor of quantizing
discretizing um strengths of belief and
I think it's about as useful as um a
10-point scale but that's why doctors
don't use like 20 point scales very
often and only when I'm answering
surveys from like the less wrong people
or the freaking fostr people do they
give me a sliding scale 1 to 100 it's
the only time I've ever been given a
survey with a sliding scale is when I
know that they want to take that number
because I'm an AI researcher and turn it
into the probability of blah blah blah
blah blah but most people don't think
that um granularity Beyond 10 is very
useful that's why doctors use it I mean
I think we're on the same page that you
don't want to introduce false Precision
right but it's uh it's surprising to me
that people get really worked up about
this idea that like yeah we're just
trying to approximate an ideal maybe if
if there was a super intelligent AI that
AI might be able to give really precise
estimate as humans we often say
something like hey Anroid impact we've
got a pretty confident reason to think
that it's like less than one in a
million in The Next Century because it
happens every few hundred million years
statistically and we don't have we don't
have a particular view of an asterid
that's heading toward us so roughly
that's going to be the ballpark and then
I can I can't confidently tell you the
probability of nuclear war in the next
Century right maybe it's 1% maybe it's
5% maybe it's 90% but I feel confident
telling you that nuclear war in the next
century is going to be more than 10
times as likely as an asterid impact in
The Next Century am I crazy to claim
that let's just descend into level of
back to the weeds of philosophy for one
second what do you mean by approximating
ideal what's the ideal here like is the
world thank you yeah and who ideal who
ideal cuz it's not my ideal but no not
even not eventive idea when you say like
you know am I C you're okay correct me
if I'm wrong you're saying is a right
probability and I'm trying to
approximate that with my degrees of
belief so there is an x% chance for some
X that there's a nuclear strike on the
US in the next hundred years do you
think that yeah I mean Solomon off
induction is going to give you the ideal
basian probabilities to make decisions
based off okay okay that's different
okay so that's that's a claim about
rationality I'm asking you is there a
probability attached to the world is the
world like is the world stochastic in
you for
you no probability is in the mind of the
model maker right so um the universe you
might as well treat the universe as
being deterministic because you don't
there's actually no ontological
difference when you build a mental model
there's no reason to take your
uncertainty and act like the uncertainty
is a property of the universe you can
always just internalize the uncertainty
into your model which is one of the
which is one of the good basan critiques
about frequentism that I like so I we I
totally agree with you
that that that the world is different
deterministic nonstochastic and
Randomness doesn't actually occur in
nature I
agree we there there's just no epistemic
value to treating the universe as
ontologically fundamentally
non-deterministic and the strongest
example I've seen of that is in quantum
theory like the idea that Quantum
collapse is ontologically fundamental to
the universe and like the probabilities
are ontologically fundamental instead of
just saying hey I'm uncertain what what
my Quantum point is going to show you
know to me that seems like the way to go
and by the way I bounce this off elzar
because it's not officially part of the
elzar cannon but elzra says he thinks
what I just said is probably right yeah
nice um I think yeah so for the purposes
of this I think we're all come to an
agree and the world's deterministic so
yeah so now the question is when you say
ideal now you're you're appealing to a
certain uh normative Claim about how
rational agents ought to behave R and so
now we need to descend into like by
whose lights is it rational to put
probabilities on every single
proposition yeah um but I just wanted to
just wanted to cuz when it sounds like
your it sounded when you were talking
like you were saying you know there is
an x% probability that some event
happens we're trying to figure out what
that X is that's not true right so the
you know the world is some way fair
question the
ideal it's the ideal Bas and Reasoner
right is what the ideal it means let let
me give you more context about the Bas
in worldview or specifically the Solomon
off induction worldview so the game
we're playing here is we're trying to
get calibrated probabilities on the next
thing that we're going to predict and
this ideal of Sol solof induction is I
take in all the evidence that there is
to take in and I give you a probability
distribution over what's going to happen
next and nobody can predict better than
me in terms of like you know scoring
functions like the kind they use on
prediction markets right like I'm going
to get the high provably get the highest
score on predicting the future and
that's the name of the game and remember
like the stakes the one reason we're
having this conversation is because
we're trying to know how scared we
should be about AI being about to
extinct us and a lot of us basian are
noticing that the probability seems high
so the same way we would if there was a
prediction Market that we thought would
have a reliable counterparty we would
place like a pretty high bet that the
world was going to end okay good we're
getting into a meat of it sure um I just
have a a historical question is solof
induction tied to the objective basan
school or the subjective basian School
uh or do you not know um I I don't
really know right so so this is where
maybe I pull a David deuts and I'm like
look I don't necessarily have to
represent the bans right I think that
I'm Faithfully representing yud alaz
owski I think you can consider me a
stochastic parrot for his position
because I'm not seeing any daylight
there but I I don't I can't trace it
back to you know what alzer wrote about
Solon off induction he indicated that it
was uh part of it was original so this
could just be alaz only at this point
yeah that wasn't supposed to be yeah
Salon of induction
is it's um it is induction like
philosophical induction the stuff that
we've been railing against um except
with a basian uh theorem interpretation
on top of it so all of the critiques
that we've made about induction just
curious because um you know there are
two schools of basanis the objective
basian and the subjective basian James
comes from the objective school um and
Solomon off induction oh he comes from
the objective that's what the ideal
rational agent is about like he thinks
there is a correct prior there are
correct probabilities to have in each
moment and it sounds like s but that's a
subjective School in bism which is still
a subjective interpretation of
probability there there's an objective
there's or call it logical versus
subjective asianism these are different
things right so subjective Asians I
think wouldn't sign off onto Solomon off
induction this is a total paning you can
cut this out if you want but they I
don't think they'd sign off on SOL off
induction because they're they're like
for them probability is completely
individual and there's no way to
litigate that I have a better
probability than you because it's
totally subjective then the log the
logical or objective basian want to say
no there is a way to litigate who has a
better uh a better uh Credence in this
proposition but they're both still
basing in the sense that they're putting
um probability distributions over
propositions and stuff right like
they're still yeah gotcha gotcha thanks
anyway sorry keep for me yeah okay keep
that you know Ray solomonov came a
couple centuries after llas I think so
there was a long time when people like
hey basing updating is really useful but
where do the pyes come from I'm not
really sure but if you have PES this is
a great way to update them and then
solomonov came along and is like hey
look I can just idealize even the priors
right I can give you I can get you from0
to 60 of having no belief to having the
provably the best belief I see okay yeah
so probably the objective as schol yeah
can I say for the listeners all this
like ideal provably blah blah blah it
all rides on coxys theorem and so just
Google my name and just type in the the
Credence assumption and then you can see
the three assumptions that underly coxys
theum the first one the second one and
the third one are all something that you
have to choose to assume and this is
what Yuki never talks about and when he
talks about laws and you have to be
rational blah blah blah all of that is
only if you voluntarily decide to assume
the Credence assumption I don't because
that assumption leads to a whole bouquet
of paradoxes and confusion and nonsense
about super intelligence and yada y yada
um but just for the listeners when you
hear that there's Bay's law and the law
of rationality all of that is only if
you voluntarily want to assume the
Credence assumption and if you don't
like myself then none of this applies to
you so just take that maybe we'll get
into that um but I I got more for the
listeners than for than for you for sure
yeah um where I'd like to try next is so
you guys uh just put in a good effort
which I appreciate uh zooming into some
potential nitpicks or flaws of basis own
so let me turn the tables let me zoom
into something in paparan own that I
think I might be able to collapse a
little bit uh let's see so okay so we
talked about how okay you're not in
you're not really liking the idea of
let's formalize the definition of what
an explanation is it's just like look we
do it as humans we kind of we do our
best right it's a little bit informal um
one thing paparan say about explanations
is that better explanations are hard to
vary right certainly deut says that do
you want to like elaborate a little bit
on that
claim um yeah so that comes from um
Deutsch that's one of the things that he
um kind of built on uh poppers stuff
with and all he means there is um that
just consider two theories for why the
um sun rises in the morning Theory one
is that there's a God which if they're
happy that day will make the sunrise and
another theory is the heliocentrism
where you have a sun in the center of
the solar system and the Earth rotates
around it and the Earth is on a bit of a
tilt and the Tilt the Earth rotates um
as the um the Earth itself is rotating
around the Sun and this the rotation of
a spherical Earth which causes the
um sunlight to rise the next morning so
the first explanation the gods one is
completely easy to vary and arbitrary
because you could say why is it when the
God is Happy why is it one God why is it
six gods and just whatever you want to
justify in the moment can be justified
under that theory so to actually with
super intelligence we'll come to that
later um with the uh with the
heliocentrism theory that one is very
difficult to vary because if you change
any detail in it so why spherical let's
switch it to um cubic well now all of a
sudden the predictions are completely
different because the Sun is going to
rise in a different fashion um and so
it's uh that's what deuts is is getting
at with the heart to vary stuff um some
critiques of this though is that it's
not like if I give you a theory um it's
not like you can just naturally
categorize this into those which are
hard to vary and those which are easy to
vary um and so I'm assuming you're about
to say something like um well it's this
is a different degree not in kind um
because everything is um kind of easier
or hard to vary and you can't um
naturally bucket them into one camp or
the other to which I'd say I agree that
is true you can't um the heart of very
Criterion uh I think is rather useless
as a critique of other people's theories
you could try to tell astrologers and
Homeopathy people and all these people
that their theories are hard to VAR are
not hard to vary andless it's wrong
they're not going to listen to you it's
not a very good critique for other
people it's a great internal critique
though and so if you this on yourself
and you um in subject your own thought
process to is my explanation easy to
vary here like is the explanation that
the super intelligence can just create a
new reality whenever it wants is that
easy to vary is that hard to vary then
you can start to um uh weed out
different kinds of theories in your own
thinking so um so I just adds to to what
dech said which is that it's um it is a
a degree not a kind and it's a kind of
useless critique on other people but
it's a great internal critique um I
don't know if you'd want to add anything
to yeah maybe the only thing I'd add is
that while this might sound uh perhaps
like philosophically uh in the weeds a
bit this is precisely the kind of thing
that people do on a day-to-day basis
right if you drop your kid off at
kindergarten uh you go to pick them up
there's many theories that you know they
could have been replaced by aliens while
they were there now they're a different
person or they've completely changed
their personality over the course of the
day like many possible predictions you
could make about the future what are you
doing you're saying those are totally
unlikely because like if that was to
happen
you know you have no good explanation as
to like why that would has happened that
day so this also just comports well with
like how we think about um reality dayto
day like why do I not think my tea is
going to all of a sudden start
levitating like yeah precisely for this
sort of reason even if people don't
really think of it like that I think
that's sort of what's going
on and maybe a little plug for our
conversation with Toler Summers because
we go into this in much greater detail
um and so just for people who want a
more flesh doe version of what we just
said check out that episode
okay so personally I do see some appeal
in the particular example you chose like
I I think there it's you know I I get
why people are are using it as a
justification for their epistemology
because like if somebody is like reading
it's not a justification for the
epistemology just to be clear it's it's
more of a consequence of the
epistemology it's a it's a heuristic and
a Criterion not a justification for it
but yes do you think it's a coral are or
do you think it's one of the pretty
foundational rules of thumb on how to
apply the epistemology no it's not
foundational no it's um it's a COR yeah
interesting because I I feel like
without it you might it might be hard to
derive from the rest of Pap arianism
nothing is derivable in papism and it's
not a
foundational but you're saying nothing
is derivable but you're also saying it's
not foundational oh sorry sorry uh if by
sorry good good claim if by derivable
you mean like formally logically
derivable then no nothing is derivable
it's it's conjectural conject if by
derivable you just mean like in the
colloquial sense like oh yeah I drive
the yeah yeah so just to be clear there
just because the formal natur
distinction yeah seems to be important
in this conversation I haven't gone that
deep on peronism so I am actually
curious like so this this rule that
Deutsch brings out a lot or heris or
whatever it is right that that a good
explanations are hard to
vary did Deutch infer that from
something else that popper says and if
so what's the inference I yeah B correct
me if I'm wrong here can't uh doesn't
this come somewhat from Popper's notion
of content like imperal content of
theories right if you want theories with
high empirical content that's naturally
going to mean like you want things that
are hard to
vary is related I just want just because
the um there is a important distinction
between just the way that Ben and I and
you think about stuff which is uh formal
systems compared to natural language so
words like derivable infer Etc I just
feel like we need to to to plant a flag
on on those because there's going to be
translational difficulties there so just
because of that um yes it is absolutely
colloquially derivable from his uh
theory of content absolutely but um
deuts just kind of renewed it um so it's
consistent with popper for sure but it's
just like a it's a rebranding it's a
what is the concept handle it's like a
concept handle
nice Ben do you want to elaborate on
that I'm curious to learn a little bit
more because I mean look I I I find some
Merit or some appeal to this concept so
can you tell me more about the the
connection to the content whatever
saying
go he loves
stuff do you do you want to hear a full
thing about content I could feel about
that for like an hour but can you can
you just tell me the part that grounds
the explanation should be hard to vary
claim um yeah so this yeah I'd love to
talk about content so I need to explain
what it is like do you know what poer
stuff content is um I don't think so
yeah okay so content is a really
interesting um concept uh so the content
of a statement is the set of all logical
consequences of that
statement okay okay yeah so um and I'm
going to expand upon this a little bit
because um it's actually going to lead
somewhere it's going to connect nicely
to what we've been discussing so far um
so just to give an example so the
content of the statement uh today is
Monday would be um a set of all things
that are logically um derivable from
that so today is not Tuesday today is
not Wednesday today is not Thursday Etc
um the content of the statement um it is
raining outside would be it is not um
sunny outside there are clouds in the
sky that that kind of thing um so that's
what the content is uh and then there's
different kinds of content so there's
empirical content and there's
metaphysical content so um empirical
content is a subset of all the content
and that is things which are derivable
that are empirically falsifiable so if
for example I say
um what's the content of the statement
that all swans are white
um well
one derivable conclusion from there
would be there is not a Black Swan in
Time Square on Wednesday
2024 um that would be empirically
derivable um uh a claim uh the content
of um a metaphysical statement would be
something like um uh the Arc of progress
bends towards Justice or what's that um
quote from MLK um and then the content
of that would be something like the
future will be more just than the past
um okay if you let me to elaborate a bit
further I promise this is going to
connect to what we're talking about um
so now we can talk about
um how do you compare the content of
different kinds of statements so with
the exception of tautologies essentially
everything it has infinite content um
because you can derive an infinite
number of statements from today to is
not Monday you can just go today is not
Tuesday Etc so it's it's infinite but
you can do class subclass relations so
um the content of Einstein's theory is
strictly greater than the content of
Newton because you can derive Newton
from Einstein so Einstein is a higher
content Theory from Newton precisely
because anything that Newton can drive
you can drive from from Einstein um you
can't compare the content of say
Einstein and Darwin for example because
they're just infinite sets that can't be
can't be compared um so going a bit
further now and where this is going to
connect really nicely to what we've been
discussing so far so let's talk about
the content of conjunctions um so the
content of a conjunction um so we have
two statements today is Monday and it is
raining so the content of the
conjunction is going to be strictly
greater than or equal to the content of
the uh statements uh on there on its own
um the content of a toy is zero if you
want to put a measure on it if you want
to put numbers on it it's zero because
nothing can be derived from a toy the
content of a contradiction is infinite
or one because from the law of what's it
the law of the explosion principle or
whatever from a contradiction anything
can be derived so it's infinite but
because it's infinite you can
immediately derive a empirical um
falsifier that would show that the
content of contradiction is is false
yeah so now we're going to connect so
let's talk about the probability of a
conjunction so the probability of a
conjunction today is Monday and today is
not raining strictly goes down
probability is less than or equal to the
probability of a topology is one the
probability of a contradiction is zero
so if you want in science and in thought
to have high content you necessarily
must have low probability if you want um
your theories to be bold and risky then
they necessarily have to have low
probability so on this side of the aisle
we claim that the project of science is
to have high content propositions
theories that are bold and are risky
that's necessarily low probability on
your side of the aisle you want high
probability so if you just want high
probability just fill your textbooks
with tautologies um if you want low
probability fill with contradictions
from our perspective we want high
content um so we want low probability so
we are completely inverted and I would
claim and Ben I think would claim and
poer this is I'm just ventriloquizing
poer entirely that the goal of science
is to have high content risky bold
empirical theories such as Newton
Einstein Darwin and DNA etc etc and that
means low probability which means that
basanis is wrong done yeah what thanks
for that let me make sure I fully
understand here because in the example
of the you know that I think we talked
about the sun uh going on the earth or
like we see the sun rising and setting
and one person says I think this is
because the Earth uh is spinning right
so we see the sun coming up and down and
another person says I think this is
because I believe in the Greek gods and
this is clearly just Helios and his
Chariot as said in the Greek mythology
uh and you're saying well look we prefer
a a higher content Theory and so when
you talk about Helios because it's easy
to vary that makes me think it's using
fewer logical conjunctions which would
make it lower content am I paraphrasing
you correctly uh yes exactly no that's
yeah great yes that I actually didn't
connect those two um and there's a nice
relationship between um complexity which
is about um conjunctions of statements
and simplicity and what we look for in
science is simple statements with high
content because those are the ones which
are the easiest to falsify um and so if
we have certain statements that a lot
can be derived such as you can't travel
faster than the speed of light um then
it makes a lot of falsifiable
predictions and thus touches reality
much more um and it's harder to vary
because if you change any part of it
then you're falsified you're falsified
you're falsified so there is directly a
relationship there okay but what if the
ancient Greek pushes back and he's like
oh you need logical conjunction and say
let me tell you about Helios okay Helios
rides his Chariot around in the Sun and
he wears these sandals made of gold and
he's friends with Zeus right so he gives
you like 50 conjunction he's like I
actually think that my theory is very
high
content uh yeah and so this is where
there's a difference between content and
um easy to viness right so all the
conjunctions that he just made up he
could just make up a different set and
that's why it's so easy to vary but but
what if okay just playing along I mean
you may be right push the bumper car
here right so what if he's like but okay
but I'm specifically just telling you
all the conjunctions from my text right
and we haven't varied the text for I
think there you want to talk about the
consequences of his view you want to
look in in conjunctions of the content
which are the the the this case it's
supposed to be an empirical Theory so
the empirical consequences so you ask
him okay like given all these details
about your theory that's fine but like
what do you expect to see in the world
as a result of this Theory and there
it's very low content right because it's
going to be able to explain anything
that can happen war no war clouds no
clouds um I don't know I don't know I
don't actually know what chariots
I'm playing Devil's Advocate right I'm
not even necessarily expecting to to
beat you arent but I'm really just push
just to see if I can right so it's not
win or losing trying to learn learn from
each other yeah yeah imagine that he
says um okay but I have this text it's
been around for a thousand years and it
specifically says every day Helios comes
up and then down right and and it can
vary a little bit but it's always going
to be like up and down and an arc
pattern in the sky so I'm not varying it
right and it has like a 50 conjunction
so like why does this not beat out there
the spinning Theory um well so if you're
moving to like induction and stuff and
I've seen it a thousand times in the
past is that where you're going with
this I'm actually still I'm actually
still trying to Pro at this idea of
being hard to vary right Sor so what
exactly is a critique that that the
Helios going around the sun it's not um
so oh I see okay so i' I'd rather talk
about that that's why I think content is
actually sort of like the more Primal uh
concept here or the more primitive
concept rather because there you can
talk about it's not that that has no
predictive power or no content right
that you're as you said it's going to
predict that the sun rises and sets each
day but then you start asking like
what's beyond that prediction like what
else does this say about the world well
the the theory of Helio centralism says
a lot right it says things about Seasons
it it makes very it it it posits a very
rigid structure of the world and we can
go and test this structure like you know
a like tilt theory of the world comes is
related to this I guess um you know and
it this comports with other theories we
have of the world which together make
this web thing of things that like
that's when the hard to viness comes in
all of that together is very hard hard
to vary so it's true that it makes some
uh predictions and has some empirical
content right that's presumably why they
thought it was like a useful predictive
theory in the first place but you ask
okay what does Helio centrism have on
and above that and it's got much more
posits way more content and so we prefer
it as a
theory did that answer your question or
no okay and just to make sure if let me
try to summarize I may or may not have
correctly you're saying like look the
the the Earth spinning model it can also
make a bunch of other predictions that
we can even go test and so just by
virtue of doing that it's it's kind of
like you're getting more bang for the
buck it's kind of like a it's a compact
Theory it's getting all these other it's
constraining the world more but it
almost sounds like heart to veryy might
not even be the main argument here but
it's more like hey look there's a bunch
of different types of evidence and it's
compact I feel like those are the
attributes you like about it oh well so
the heart to VAR again is not like the
core thing that if you refute this you
destroy all of paparian ism right it's
it's a it's a herck is a way to think
about stuff it's related to content
content is a bit more of a fleshed out
Theory uh all of this is related to
falsification so content is part of the
way that you connect to falsification um
and it's related to like oab's Razer and
stuff with the compactness so
compactness connects you to Simplicity
um but again it's it's
not this like uh you got like you got
you man it's it's like yeah sometimes I
think about heart to varess and other
times I think about empirical content
and uh what B said was was beautiful and
perfect which is the the rigidness of
the um the the theory and how it's like
locked and tightly fit on top of reality
and then it gives you extra things that
you can think about that you hadn't
realized that if this is true this leads
to this other thing so for example
heliocentrism leads pretty quickly to
the idea that oh these little
things in the sky that we see that are
lights there stars maybe they're just
far away and maybe there's other planets
there too and maybe on these other
planets there's other people
contemplating how the world works and so
it's not like it's derivable from it but
it's it just you lead leads there
right and that's part of the content of
the theory
so yeah so so my my perspective is you
know if you were to come into my
re-education camp and I want to
reprogram you into bism what I'd
probably do is like keep pushing on like
okay what do you mean by hard de very
what do you mean by like following aam
Razer I feel like if I just keep pushing
on your kind of heris definitions of
things I'll make you go down the
slippery slope and you're like okay fine
Solomon off induction perfectly
formalized as what all our Concepts
really mean it's not making us go down
the slippery slope like Ben is a
statistician he understands Baye I grew
up in a bay lab like we understand this
stuff we read it all I've read a lot of
owski like I know the argument like
there's maybe a here which is we know
your side of the argument but you don't
totally know our side and so it's like
the re-education has already happened
because I started as a basian and I
started as a l wrong person as did Ben
and so we have been re-educated out of
it and so you're talking
abouted yeah you would be I actually
haven't heard my side represented well
on your podcast so so let's see let's
let's see how much you know my side by
the end of this okay yeah sure sure all
right so here I've got another related
question here on on the subject of uh
harder to veryy and I think you
mentioned this yourself that like yes
Technically when somebody says um when
when somebody says that a teos is
Chariot technically or or sorry wrong
way when somebody says Hey the Earth is
spinning on its access axis and that
seems kind of hard to vary technically
it's not hard to vary because you could
still come up with infinitely many
equivalent explanations uh no so like so
what I mean is like okay the Earth spins
on its axis and there's like Angels
pushing the Earth around right you can
just keep adding random details or or
even make like equivalent variation like
build it out of other Concepts whatever
like so there's this infinite class but
the problem is you're wasting bits right
so it's just not compact I no no no no
no no no good sir it's not about bits no
um so the problem with that is yeah you
can take any Theory and then I actually
gave a talk about this at a high school
once and I I called it the the tiny
little angels theories but you could say
take everything you know about physics
and just say it's because of tiny little
angels and the tiny little angels are
doing doing that um the problem there is
not that you have to add extra bits it's
that as soon as you posit tiny little
angels you now positing a completely
different universe that we um that we
would be having to live in that would
rewrite everything we know about it's
same like Homeopathy and stuff like if
if the more you dilute the stronger
something gets that
rewrites right they're just there but
they're
inert uh so then where did the Angels
come from they like how what is their
why Why Not Angels why not devils and
the Very way that you are in varying the
explanation as we speak sh is is what
we're talking about right so it's easy
Earth turns on attacks us but there's
just like one extra atom that's just
sitting there can't I just pait that
right like isn't that an easy variation
but then take take that seriously as a
theory right like take that so what's
that is that extra atom interacting with
anything like if not then what uses it
if so then it's going to have effects so
why haven't we witnessed any of those
effects like where is it in our theories
like you know like um yeah like and also
the heliocentrism theory is not a theory
of there are this many atoms like it's
it's not a theory that level right um
who's counting Adams Theory so I I think
I guess let me let me summarize my point
here I think you guys do have a point
when you talk about hard to vary and I
think it maps to what basian would claim
as like and and alam's Razer would claim
is like let's try to keep the theory
compact like it gets it has a higher I
our prior probability if it's compact if
you just add a million angels that are
inert you're violating aam's Razer which
I think maybe both worldviews can agree
on but if you're saying no no no we
don't care about aam's Razer we care
that it doesn't make extra predictions
or like that it makes you know maybe
it'll make other predictions that are
falsified I feel like now you're you're
starting to diverge into a different
argument right so I I do feel like the
hardto VAR argument kind of seems
equivalent to the aam's razor argument
um Homeopathy probably could be
represented with much fewer bits than
the periodic table but I still prefer
the periodic table even though it's more
complex right so it's not just a razor
and lower number bits um like Quantum
field Theory would take a lot of bits um
there's many simpler kinds of theories
you could use but they don't explain
anything they don't explain the
experimental
results it doesn't
simp valuable the okay well now we're
back to content but I'm saying that if
if the only Criterion is small number of
bits being Compact and simplicity there
are so many theories which are complex
use a lot of bits are not simple but I
still prefer them and that's my point if
you're trying to set up an example
though you have to make sure that it's
an example where two different models
make the same prediction so when you
brought up Homeopathy versus the
periodic table I wasn't clear on what
was the scenario where they're both
making the same prediction um they are
both predicting that if you have my drug
it will make your cold um go away faster
if you have my buy my product at Whole
Foods and they will both address your
cold but in this scenario doesn't the
Homeopathy remedy not work exactly crap
but from theop but for the Homeopathy
people think that they're predicting
that it's medicine right and there's a
reason why people don't use traditional
medicine and go to Homeopathy because
they're both making predictions that if
they take this they're going to feel
better right but I think the type of
example you're trying to set up is one
where we actually have the same
phenomena right like in the other
example is hey the Sun is going to come
up and go in an arc in the sky right
it's the same phenomenon and you have
two theories and we're no the typle
example that's just tring to set up here
is much simpler which is just um
Simplicity and aam's razor aren't
sufficient um they're just modes of
criticism they're useful heuristics
sometimes but they're not at the primary
and if all we care about is small
numbers of bits and simplicity and
compactness then I can give you a bunch
of theories that meet that Criterion
that I don't like very much that's all
this is actually interesting by the way
I guess I wasn't even really expecting
you to say that you basically don't
think aam's Razer is useful or like
what's your position on aam's Razer um I
think aam's Razer is good sometimes it's
it's a it's one way to criticize stuff
but it's not the only thing um times of
theory is super complicated and has a
bunch of Superfluous assumptions that
you need to shave off that's when I'll
pull off aams Razer sometimes they pull
out Hitchens razor Hitchens Razer says
that that which can be asserted without
evidence can be dismissed without
evidence that's also a useful criticism
and a useful heris stic there's a whole
toolkit of different razors that one can
pull out and neither of them none of
them are at the base level they're all
just kinds of Shaving equipment that
shaves off shitty
arguments what do you guys think of the
B view of Arkham's
Razer I think it's as fallacious and
mistaken as bism itself um or and that
that's cheap Ben give me a give a less
cheap answer uh yeah so that I mean so
you want to say that uh theories that
are simpler should have high prior prob
higher prior probabilities right that's
the view of basanis with respect to
right and that's what Solon off
induction does uh right so it just it it
basically orders all the different
possible training machines that could
ever describe anything and it puts the
the ones that are shorter earlier in
ordering which means that if you have
one turning machine that says there's a
million Angels doing nothing right
that's going to be deprioritized
compared to the turning machine that's
like hey there's not Angels right it's
just
simpler so I think a lot can I step just
back like one second and if this is if
if in your view I'm dodging the question
feel free to re askk it but I just want
to like fra frame a little bit and then
please criticize me if it's seems like a
Dodge but um Sol
solov solov induction salom of induction
and basanis and all this stuff tends to
um fiddle with the probabilities enough
to come up with a justification that
paparian are already already have so
there are reasons why we like Simplicity
um one of the reasons is that simple
theories have higher content and thus
are easier to refute so that is in my
view why we like Simplicity however you
could come up with a basian story about
why like Simplicity you could you could
talk about it in terms of induction you
could talk about it in terms of solom of
induction and you could fiddle with the
math and say and this is how we get this
conclusion from Baye and you can do this
all over the place you this is like the
basian epistemology hypothesis testing
stuff where you can come up with a postt
talk story about why B theorem is what
led us to the discovery of the double
helix um but it's just that it doesn't
give us anything new we've already
disced discovered that and then you
could come up with a story after the
fact the double helix is another nice
example of why I like content um and
I'll stop repeating myself there but but
is when you ask these kinds of questions
um yeah you can always tell a story from
Ba's perspective about why we value this
stuff and we're giving you an alternate
story and that I guess is just
ultimately going to be have something
that the listeners are going to have to
decide um but the starting point that
salov induction is right is wrong and we
can argue about that but if you grant
that there sorry if you start with the
assumption that is right and you don't
want us to argue about that then yeah of
course you come up the story from s
lov's perspective about why we value
Simplicity um but it's just we're
talking completely across purposes here
because I reject Sol of induction
because I reject induction and so any
kind of induction just is wrong and we
can and you can listen to us argue about
this for hours and hours and hours if
you like um but you're starting with the
assumption that it's right and that
already is meaning we're not talking
properly to one another yeah and by the
way I do want to hit on hum Paradox of
induction stuff I know you guys have
talked about it I find it interesting uh
so I want to get there uh but first I
think i' I've got a little more um red
meat to you on this topic of you how do
we actually apply um paparian reasoning
to judge different hypotheses that seem
like they could both apply I've got an
example for you uh okay let's say I take
a coin out of my pocket we're just at a
party right it doesn't look like it's a
I'm not like an alien or whatever it's
just normal people right I take out a
quarter it just looks like a totally
normal quarter you don't suspect me
um and I flip it 10 times and it just
comes up with a pretty random looking
sequence say heads heads Heads Tails
heads Tails heads heads Heads Tails okay
so doesn't look like a particularly
interesting sequence um and then you say
okay this just seems like the kind of
thing I expect from an ordinary Fair
coin and then I say I've got a
hypothesis for you this is just a coin
that always gets this exact sequence
when you flip it 10 times you just
always get heads heads Heads Tails heads
Tails Z Tails so if I flip it again 10
times I'm just going to get that exact
same sequence again
that is my hypothesis and I'm like drunk
right so I don't even seem like I'm like
a credible person but I'm throwing out
the hypothesis anyway you think it seems
to be a Fair coin unless I'm doing an
elaborate trick on you so my question
for you is what do paparian think about
contrasting these two hypothesis Fair
coin versus that exact sequence every
time you flip it 10 times which of the
hypothesis seems more appealing to you
in terms of being like I don't know
harder to vary or just better I would
just do it again and quickly find out
like you just flip the going again do
you get the same sequ what what the next
time what what if you have to you have
to bet right what whatever you predict
you have to bet a thousand bucks so I'll
let you do it again but it's just but we
got to gamble on it I would say I don't
want to I just want to do it again and
so like if I'm at a party and someone
gives me a magic coin first I'd be like
well that's crazy how is that even
possible and like obviously a like can
you actually do that man are you doing a
MAG trick or did you just come up with
some like do you can can you control the
your your thumb enough to get the same
sequence so it strikes me as Prima
fascia completely implausible and so
yeah I would take the take the money but
I no I no I wouldn't take the money I
wouldn't take the bet because they're
probably they're asking to bet for a
reason some Tri going on so yeah I would
say imagine this really is just like a
random drunk guy who has like no
incentive doesn't want to bet you like
it really just does seem like somebody's
around and you have no reason to
suspect anything and like there's no
right my question yeah foret forget the
okay so my question for you is just like
I mean brought up like let's flip it
again and I'm saying like well for
whatever reason right before you flip it
again just between you and me right you
just call me up I'm about to do some
paparian reasoning this is fundamentally
a great example of the difference
between paparian and basian and Ben
brought up a great example a long time
ago about this but the big difference is
that a paparian would just do it a
basian would go off into their room and
spend 6 hours writing a 20page blog post
about how they can formalize the
probability space of their beliefs on
this particular circumstance and they
post it to less wrong and then spend
another 20 hours arguing about the
probabilities that's
the would just do it they would say oh
okay that's interesting yeah let's just
do it let's try oh it's wrong okay good
move on maybe to make that answer
slightly more globally applicable and
remove a tiny bit of the snark Sor um I
think a good way to coherently talk
about the differences in worldview which
I think is actually very interesting is
that basian are extremely focused on
having accurate beliefs could decide
exactly how we Define accurate but
accurate beliefs given the information
you have right now paparian are very
interested in generating new hypotheses
and figuring out what we can do to grow
our information so very interested in
like if we have multiple competing good
hypotheses for some phenomenon how do we
go about discriminating between those
and that's where the crucial test and
stuff comes up right so there is this
yeah so so I I get that your mindset is
to just do it and I get that you want to
find your information but like this is
chall I'll stop dodging answer but I I
just I just the reason ven is having
trouble answering your question is
because there is this extreme difference
in emphasis between these two worldviews
so much so that I have recently started
struggling to call bism basan
epistemology because it's not really
about epistemology in a sense of growing
knowledge it's about epistemology in the
sense of like justifying current
credences for certain hypotheses so the
the emphasis of these two worldviews are
different and they're still conflicting
in important ways as as we've seen I
mean Solomon off induction does grow its
knowledge and grow its predictive
confidence right so I I I think you're
going onm to say that basian shouldn't
have a right to the term epistemology no
no that's he said if you want to call
ity that's fine I'm just saying I think
honestly this is me trying to give a Bo
to your side and say like I think we're
often arguing at slightly cross purposes
because the basian are extremely focused
on uh uncertainty quantification right
they want to say like exactly what your
credent should be given the current
information um they're I think they're
less focused on like I mean I haven't
heard many basian talk about generating
these new hypothesis with infinitely
many touring machines and stuff right
like I mean I can tell you I personally
don't spend a lot of time trying to
precisely quantify hypothesis I just
know that when I'm just manually doing
things like hm this seems like a good
move to try when I'm just like thinking
things through roughly I just know in my
head on a meta level that I'm
approximating the numerical ideal and
that's it I just live my life with an
approximation you're assuming in your
head that that's true but sure um but
let me not Dodge the question can you
ask the question again yeah yeah sure so
there's this weird sequence of yeah let
me just answer and say like yeah all
else being equal I think I would say um
I would take the bed or whatever like I
I would say like yeah this is probably
implausible because if if if I can see
how they're flipping it I would say it
seems extremely implausible that there's
a mechanism by which they can control
the coin you know there's no string in
the air or something and like yeah so
the only plausible mechanism by which
this sequence had to happen is um
basically their finger right because
you're seeing the same same coin and if
the coin is like memoryless which seems
like a reasonable assumption it wouldn't
know that it has flipped like ahead it
wouldn't know its own history right so
it has to be the person and by the way
just my intent but where I'm not going
to trick you I'm not going to be like
Psych the guy that's not where I'm going
but no yeah so I would just I'd probably
say yeah it's very implausible that it's
exactly this sequence and I would um
yeah if I was in the mood to bet against
it and I had the spare income to do so
I'm a PhD student so I don't have much
spare income you know but if it's 30
cents then maybe I'll take the BET um
yeah
right so this is my question for you
right is this this to me seems like a
great toy example of like how do you
guys actually operate paparian reasoning
on a toy problem right because you're
saying it's a fair coin but it seems to
me like the HHH always hhht HHH always
that hypothesis that is like a very rich
hypothesis right it has more
and it's harder to vary because when you
say Fair coin I'm like wow you Fair coin
that's such an easy to VAR hypothesis
you could have said it's a 6040 coin you
could have set it to 7030 weighted coin
in fact in my example you got seven
heads so like why did you say Fair coin
instead of 7030 weighted coin you're the
one who's picking a hypothesis that's so
arbitrary so easy to vary put thousand
words you just put a thousand words in
our mouths that we did not
say also yeah let me just I think I can
resolve this quickly and say like you're
right that saying I you can flip exactly
that sequence of coins that is an
extremely strict hypothesis that is very
rigid and has lots of content right the
content says every time I do this I'm
going to get this exact sequence of
numbers what is content good for it's
good for discriminating between
different theories um and so how would
we do that we'd try and flip the coin
again so Vaden like you know he wasn't
trying to dodge the question by saying
flip the coin again content is
inherently tied to like how we okay but
but but for the sake of argument you
don't get to flip the coin again like
you have you have to just give me your
best hold but but the RO like you're
you're saying okay what how would a PO
and deal with this circumstance right
that's that's your question okay and I
get that you really want to flip the
coin again but can't you just assume
that you have to give me your best guess
without fli just gave you a bunch of
reasons you're simultaneously like yeah
like you're you say how would a paparian
deal with this um I say how we would
deal with it and you say okay but assume
you can't deal with it the way that you
want to deal with it then how would you
deal with it okay so you're basically
saying like you you so if this so you
really you have nothing to tell me
before fting in like nothing at
all like where are you trying lead us to
like we would run another experiment or
we would take the bet because it's so uh
implausible that a coin could do this
like either the guy has mastered his
thumb mechanics in such a way that he
could make it happen or there's some
magic coin that somehow knows how to
flip itself in the exact sequence that
is being requested and both of these
things seem completely implausible so I
would take the bet I wouldn't count the
probabilities and then come up with some
number in my head but yeah I've given
you 20 or a couple different answers
yeah so one one reason I wanted to bring
up this example that what originally
inspired me inspired me to make up the
example is to uh to show a toy example
where hard to vary seems to flip you
know is to be counterintuitive right
because I do think I've successfully
presented an example where the 50/50
hypothesis the the Fair coin hypothesis
actually is easy to vary uh you're think
only in terms of yeah hard to VAR you're
thinking only in terms of statistics
though right like the in terms of um in
terms of
like like explanations of like in terms
of explanations of the underlying
physics and stuff um then it's like is
he magically doing this with his magic
thumb right like that's easy to vary
that's that's the part that's easy to
vary the part that's hard to vary would
be uh this is not possible and the guy
is wrong and I'll take the bet because
the easy to vary part would be magic
thumb is he a super being is he a
telekinetic is he are we living a
simulation these are like I can come
with a th do all day and that's what's
easy to vary and I just reject all of
them because I'm just making them up as
I go that's all re and that's how we so
it sounds like the resolution to know
it's Paradox but it sounds like the
resolution has to kind of zoom out and
appeal to like the broader context of
like look we live in a physical world
like coins are physically hard to make
be this tricky right to always come up
in this exact sequence so it as a BAS
and I would call that my prior
probability but how would you call that
being just in common sense just
knowledge about how about how the world
works I just like it's it's implausible
and so I would yeah thank Fair anyone
who doesn't study any like philosophy
would come up with the same answer
approximately that like I think if I
wanted to challenge you more I think I
would probably have to put in some work
where I like invent a whole toy universe
that doesn't have as many as much common
knowledge about like the laws of physics
so so that it's not like super obvious
that the coin is is fair because I do
think there's some Essence that I would
distill as a bean to just be like you
know what I would get out as aan what I
think is like a profound lesson that's
worth learning like you know imagine a
million right imagine a million flips in
a row right so then even if the laws of
physics made it really easy to make
unfair coins the fact that it just looks
like there was no setup and you and you
know it could just be it could even just
be your own coin right your own coin
that you just got from like a random
Kmart 7-Eleven right you flipped it um
and it's like your own coin and it came
up you know 100 times like a totally
random thing and you're like I have a
great Theory it always does this exact
sequence um yeah I mean I I do find it
convincing that it's like look the laws
of physics makes that a prior unlikely
but I feel like the a big advantage of
the Fair coin hypothesis is also that it
uh you know it's a priori much more
likely than the hypothesis of like this
exact sequence like that's kind of a
ridiculous hypothesis like where did I
get that hypothesis without actually
flipping the coin you know like do you
see there may be something there that
could you can always tell aasian story
after the fact yeah it's it's a priority
less likely we all agree on that and
then Ben and I would want to say it's
less likely because it's a bad
explanation so we just reject it and
you'd want to say it's less likely and
okay how much precisely likely is it
less than the other let's come up with a
count let's say okay so there's eight
heads and there's two tails and let's
come up with the problem we just say you
don't need that it's a ridiculous thing
maybe just turn around like how would so
what is your prior probability on the
coin being fair like yeah that great
question that's a great question yeah
what is your problem yeah I mean if it
generally if if somebody does a party
trick and and I don't judge them as
somebody who like wow this guy could
actually be doing some pretty fancy
magic right if it just seems like a
random drunk friend um then I'd probably
be like okay there's probably like a a
97% chance this is just a regular Fair
coin and the other 3% is like okay this
drunk guy actually got access to like a
pretty good magic trick okay so what you
gave is like a bunch of reasons and then
a number right we're just giving you the
reasons yeah and the number is a ballar
exactly when we're just giving you the
reasons and no
number it's such a ballpark that we just
don't need the number like what's the
number for yeah I mean I get that right
so I mean the the standard basing
response is to be like well look if
there was like a market right like a
betting Market or a prediction Market or
even a stock market right and actually
this gets to another section that I was
going to hit you with which is like okay
so expected value right what do you
think of this idea of calculating
expected value oh boy I think it's like
the Pagan theorem that's useful in some
circumstances and not useful in other
circumstances and it's butan
mathematical fact that statisticians use
all the time and for whatever reason
it's also this
like crazy philosophically metaphysical
thing that the Oxford philosophers like
William mccal and Toby ort and Hillary
Greaves and El otsky and some day
Traders too I know what you're about to
say um yeah then that's where the
problems come in and let's let's go we
can talk about this a lot but uh for a
first approximation that's what I think
so in the example I mean the reason I
bring it up is because in the example of
hey we're at this party somebody just
did something with a coin who seems to
be trying to Gaslight me like it's this
coin that always comes up you know these
10 sequences in a row and but then some
Trader overhears the conversation and
walks by and he's not a Confederate he's
just honestly somebody who likes trading
and likes markets and he's like hey let
me make you guys a market on this right
like what odds do you want to give for
this bet and that's where okay yes I
pulled the number out of my butt but
like this guy wants to make odds right
so like you have to plug something in he
does yeah if you're willing to bet I
need to bet well parents would just walk
away we wouldn't participate right but
like at the end of day experiment again
we'd run the and find it no we okay we
would we' run the experiment again and
then decide if it's so ambiguous as to
require us doing the experiment like a
100 thousand times and then collecting
data on it and then building a
statistical model and then using that
statistical model to figure out what's
actually happening because yeah there's
many experiments that are really
challenging to run and you get
differences every time you do it and
that's where data and
statistics um is applicable um but
that's different than just saying the
expected value of this is going to be
big and where you getting this stuff
from you're just making it up and uh
it's useless in most cases unless you
have data and yeah we can talk about the
cultural stuff of Traders and um like
Sam bankman freed like if you read M's
book they talk about his culture at Jane
Street and how they put expected values
on everything and that is a cultural
thing um which people do and we could
talk about the culture there too but
that's just very different than how most
people use it most interesting example
from that book is I mean if you look at
Jane Street and you know the famous
Medallion fund so there are funds that
are placing bets uh you know that they
perceive to be positive expected value
in various markets I mean using a huge a
lot of data and statistics and a
boatload of assumptions about how the P
you know the last five days of the
market reflects something to do with the
next day of the market right
yeah it's completely it's so at the
beginning of this conversation we I
started at least by saying basian
statistics all good basian epistemology
bad boo so the basing statistics part is
what you're just asking about because
you have like 50 years of financial data
and you can run like trials and you can
do simulations with your data and you
can see what gives you a slightly better
return and that is just a completely
different thing than what the long-terms
and the basian like the ukel style
basian are doing so just make sure that
we conversation yeah we're not anti-
statistics because if you have data all
good just make that clear yeah or anti-
expected values because we if you have
data all good like you could do it wrong
of course but I'm assuming that like in
this purpose of this conversation we're
doing it well and like the trait like
yeah Jane Street and all these like big
hedge funds like they are their whole
life is trying to get like slightly
better odds using like
supercomputers yeah yeah so that's fine
yeah for the audience watching the
podcast you guys know I recently did an
episode where I was reacting to say
kapor and I think he made a lot of
similar claims I don't know if he
subscribes to Carl popper but he had
similar claims about like look
probabilities are great when you're
doing statistics but when you're just
trying to reason about future events
that don't have a good statistical set
then just don't use probabilities do you
guys know s Tor and does that sound like
a but that sounds great sounds
like I don't know where he's got it from
but that's yeah totally actually and
then can I can I actually Bri on that a
little bit which is that paparan ISM
bottoms out onto Comm sense so it does
not surprise me at all that someone who
doesn't read popper no popper no deut is
saying similar things because if you
just are
not in so people say poer is a cult too
so we're both in Cults if you're not in
the paper cult or the um youy cult then
you and you just thinking about how the
world works you'll likely just come up
with a bunch of paparian stuff because
it just bottoms out into Common Sense
typically yeah I would also like to
claim that basanis bottoms out into
common
sense that's fair yeah we'll let the
audience decide okay um course two can
play at that game uh but yeah so
what um so a while ago I was saying look
when we do Solon off induction I claim
that that's the theoretical ideal of
what I do in practice which is often
just using my muscle memory similarly
with expected value I would make a
similar claim that like realistically
right in my life I don't make that many
quantitative bets right I'm usually just
like coasting not really doing that much
mouth but I do think expected value is
the theoretical ideal of what I do for
instance I think we can probably all
agree that like if you had to let's say
you had to bet $100 on something and you
either had to bet that like the sun will
rise tomorrow or that comma will win the
election like sun will rise tomorrow is
going to be like a much stronger BET
right so like that would be like some
primitive form of the expected value
calculation yeah it's a I mean we have a
very good explanation as to why the sun
will rise tomorrow
um don't really know too much about the
okay so let me let me ask you about
youro so so let's say I'm just I'm
asking you to to you have to bet $10 um
and in exchange for you know you have to
give me odds at which you'd be willing
to bet $10 to there's probably like with
comma let's say I'm sure you guys don't
have a good explanatory model whether
she's definitely going to win or not
right because it's like too hard to know
but if I said look it's your $10 to my
thousand you know just take either side
wouldn't you just take a side because it
seems pretty
appealing sure 10 yeah sure I mean like
if you're okay meing this when I ask you
this question you're not like no Leon
run the election run the election no
you're not stonewalling right you're
saying sure I'll put down 10 to your
thousand that seems pretty appealing
right so didn't you just imply that you
think the expected value of betting
uncal is more than you know more than
$10 in that situation so you're sort of
defining expected value after the fact
all I'm saying is like I'll take this
bet because it seems like a good deal I
have $10 of disposable income and you
know yeah what I'm not doing so I'm not
doing to Bear seem like a good deal I
think you're approximating the
mathematical ideal of expected
value uh because you you can use that
framework to describe whatever the hell
you want to describe right so the
expected value so okay for for the
listeners so what is expected value so
let's just talk about discrete stuff so
it's a summation yeah so summation of
the probability of a thing happening
times the utility of a thing happening
and then you sum it all up um okay great
so what are you going to put in that sum
well you have to make that up so if
whatever you want to make up you can do
it then you have to make up the
utilities and then you have to make up
the probabilities so what you're doing
is you're taking one madeup number uh
multiplied by another madeup number and
then you're doing this a bunch of times
and then you're adding up all these
madeup numbers and then you're getting a
new madeup number and then you're making
decision based on that new madeup number
if you want to do that and make your
decision based on that go nuts we just
don't need to do that so it is MTI and
coming up with a new madeup number you
could just start with the final madeup
number and you could also just start
with the realization that you don't
actually need these numbers in the first
place because the election is like a
knife edge and if someone is offering
you like a thousand to1 odds or
something then you can take money off of
them because they are mistaken in their
knowledge about what's going to happen
because they're falsely confident about
something and so when say the term knif
Edge the term knife edge is such a
loaded term you're implying that it's
equally likely to go either way how are
you making this loaded you don't know
the future of aen um so because we have
have a data set here which is the 330
million people who are going to vote and
the polls that are trying to approximate
that so polls are a sample of a
population this is statistics this is
what I'm going based off of and this is
why basing statistics is fine because we
know how polls work and we know how
counting works and we have 330 repeated
trials well people are going to do this
this is where statistics makes sense but
there's a dark art where all of these
pollsters are building their models
right because it's the election is so
close that the way you build your model
is going to is going to really Define
which candidate you win and that's a
huge problem yeah I could rephrase that
I could rephrase that problem by saying
there is way too much subjectivity
injected into these
equations it sounds like we all really
agre though in our gut that like the
election is pretty close to 50/50 of
who's going to win like am am I do you
want to push back on that that it's not
50/50 who's going to win um I I have a
pet theory about this but it it's it's
not uh worth taking seriously so I'll
just go with the polls but I think polls
are inaccurate but yeah and I want to
bring up the prediction markets right so
now there's like poly Market Ki manifold
these markets have gotten a lot of
action in recent weeks and months and
it's pretty sweet right because you can
watch these markets and like a lot of
people betting on these markets have a
basian interpretation of what that
fluctuating number means right I mean
how else or or like do you think that
it's crazy to give a basian
interpretation of those numbers as like
good odds that you could use if you're
placing bets yeah I pretty much just
ignore prediction markets but that's my
personal choice I then but I mean what
have you learned from the prediction
markets that you haven't learned from
polls out of
curiosity oh what have I personally um
yeah just what value have you got
prediction market so you haven't got the
polls a lot of times it's redundant I
just I see prediction markets as being a
little bit finer grained than a poll so
when the polls are kind of ambiguous
sometimes I'll look at a prediction
market and I'll see more signal um I
think with maybe with Biden dropping out
that I guess that's not there's no
direct poll about that I mean there was
probably no single representative poll
that was the Biden dropping out poll but
like I guess I just want to in book like
there's times where I see something will
happen where it it's not fully captured
by like the ontology of a poll but like
predic there's a specific prediction
market for it and it spikes and then I'm
and then I really do think like oh man
that Spike seems like I should update my
expectation of what's going to happen
there's a lot going on here
uh let me just touch on a few points so
I think your initial thrust with like
the betting 10 to a th000 odds like
these are really good odds you'd
probably take that and if I keep
decreasing the Thousand you're going to
hit a point where you don't want to take
that bet anymore right um sure so if you
want to then do some arithmetic on this
and come up with like the expected value
that where I think uh I'm willing to bet
on kamla uh versus Trump and you want to
call that expected value uh sure you can
do that um we we just want to emphasize
that this is an extremely different
quantity than what statisticians are
doing when you have a well-defined data
set and you're looking at a well defined
outcome and you're counting things or
making very rigorous statistical
assumptions and calling them both
expected value is unhelpful at best and
actively harmful at worst because these
are not the same sort of quantity now I
don't I don't I'm not disputing that
people you know think think someone's
going to win versus another person uh
more or less or have different risk
tolerances some people like to bet some
people don't like to bet uh they have
different utility functions and so if
you want to you know if you want to
press them on that and make them bad at
certain odds and call that expected
value fine but this is a different thing
than maybe statistical expected value uh
where I'm you know statistical yeah at
the beginning okay fine that's one point
the second point is yeah prediction
markets are interesting as an
aggregate uh an aggregate knowledge
about how people sometimes few people
sometimes many people with money are
willing to bet on an election it's a
summary of information in that sense
right um and again so you can now talk
about different people in this market
expected value uh the whole point is
that their expected values are different
that's why you see differential outcomes
in markets right um and there's not
there's no way to adjudicate that
precisely because it's subjective and
this is again why this is different than
statistical expected value when we have
well- defined statistical models um can
I ask you a question before you before
you respond which is how do you deal
with the fact that you have many
different prediction markets and they
all say different things I mean usually
Arbitrage brings them pretty close
together
no but that doesn't because they are
still saying different things right like
um where there's a persistent yeah I
mean like I know on Trump versus it's
always a plus or minus
3% yeah actually I'll I'll I'll back off
on this and just um let the listeners or
the viewers check because I haven't
checked it recently but if you're
correct that Arbitrage is possible I
mean that's not even necessarily a
statement about epistemology that's just
like weird why aren't peoplebe I mean I
guess that could potentially be a
statement about epistemology right if
you're saying
no fact check myself and also fact like
I would love for the the the commenters
on here to just whatever time they're
looking at it just pull up four
prediction marks take screenshots and
put up underneath and let's see if the
house or maybe a better example here is
just the discrepancy between like mate
Silver's models right and like poly
market so poly Market I think is a way
bigger Edge for Trump at this point like
78 poly Market is allowed to have more
models right Nate silver constrains
Which models he's allowing himself to
use right other people are like oh I
also want to wait this potential model
right so it's not that surprising that
Nate silver hasn't captured every
possible model that you might want to
wait in your prediction but but my um my
question is
is if if you are willing to grant for
the sake of argument that they are
different um how do you decide which one
to follow that's was my question because
if they're all saying different things I
think it's the Crux of our disagreement
right I I think that if you were correct
so I'm happy to uh take a a hit if
you're actually correct that prediction
markets have persistent disagreements in
the probability of something that
absolutely would be and given that they
were liquid right assuming it's easy to
make money by like buying one and and
shorting the other um that absolutely
would be evidence for the
meaninglessness of basan probability
right and then conversely the fact that
I claim that this isn't factually true
that you actually have very narrow
spreads right I think is evidence for
the meaningfulness of B and I actually
have further Evidence along those lines
which have you ever checked their
calibration uh oh boy yeah uh which
Market calibration is so yeah okay yeah
let's go down manifold manifold is the
one I saw so they look through a bunch
of past markets right and these are
basing markets these are not like doing
statistics they're just predicting an
uncertain future about random questions
and when the market says at a given
randomly sampled time that there's let's
say a 70% chance that the the market is
going to resolve yes like whatever it's
predicting right like will Russia invade
Ukraine right like all these random
questions if the market is saying at any
given time 70% they want back and they
check the calibration and uh I can put a
graph in the show notes but it's
ridiculously accurate like for example
the data point for 70% is it's like 68%
this is across a random sample of of
manifold points there's a post on the EA
Forum that says that that's only true
with a Time Horizon of less than 5 years
and it might even be one year I think
you're talking about super forec the
thing so we can ban probability to
predict all events one year into the
future that seems like a pretty big win
for bans no a whole da though because
the whole super intelligence thing is
not one year into the future and let's
talk about okay down this path let's do
it hold let me let me say a few things
um let me say a few things which is if
you want to talk about super forecasting
and Philip tetlock and stuff um you have
to read the appendix of his book where
he says that any predictions Beyond 10
years is a Fool's errand and you uh you
shouldn't even try it and you'll
embarrass yourself if you're going to so
Point number one point number two is
that on the EA Forum someone who is very
sympathetic to basian did analysis of
the calibration of I think it was
manifold um and when you look at these
scores uh you have to account for how
far into the future they are and so yeah
it's totally possible to make
predictions successfully within a year
but the thing that you're predicting
matters a lot so if you're going to
predict that the next year is going to
be kind of similar to this year that's
like a default prediction it's going to
get you pretty high calibration but
that's also
it's not a huge concession at all you're
basically saying as a you know these
basian who think that they can take
something that doesn't have a mass of
statistical data and slap a Quantified
probability on it as given by a
prediction Market yes as long as the
time Horizon is less than one year they
can expect near perfect calibration yeah
I can do that too I predict that in a
year on Christmas there will be a fleux
of flights it depends on the predictions
you're making if the predictions are
simple anyone could do it and get great
calibrations the comp you look at the
preds they're complicated right they're
things like Russ will invade Ukraine
they things that these people are
willing to bet on that they have
differential knowledge about with
respect to the rest of people right it's
not the same people always betting on
you guys are tell me you're basically
saying if there's a market saying Russia
will invade let's say it's you know
January 2022 so we've got like a one-
Monon time horizon or let's right let's
say there's a market saying hey Russia
will invade Ukraine by end of the
quarter right like because I think
that's what there there's not a hypo
there's no hypotheticals like there was
right so in not scenario
super forecasters gave this 15% before
Russia made Ukraine and they gave Co 3%
that there' be over 100,000 cases in the
US by March yeah and remember we're
talking about calibration here right so
I'm not saying the market gave it a 99%
chance and then it happened I'm saying
if they gave it a 15% chance then it
would fall in a class of markets that
were saying 15% and what I'm saying is
calibration data shows us that 15% of
those markets do resolve yes like a
market is generally well calibrated so
it sounds like you guys might be
conceding that with a small time frame
under one year there is such a thing as
a well-calibrated basian probability um
I'll concede that if you concede that
anything after a year is completely
worthless o I mean I just think that
that concession is you know as a in the
context of a debate I do I feel like
that concession is almost conceding
everything wait wait okay sure feel feel
that if you want but it's not I mean
there's a complete difference because
your whole what want
yeah yeah because it didn't by the way
this is unexpected right it's not like
you came in being like okay you basian
are so cocky because you have this
amazing tool called the prediction
Market where you can nail calibration
for things in a year but let me tell you
how bad daisi choke after one year like
that's your I'm just wait no what are
you talking about what are you talking
about here this is zoom out a little SEC
like um you first I'm confused um yeah
I'm just confused about the claim you're
making so what prediction markets are
not is consensus on probabilities so
right so what what they're doing is you
know so you'll have a prediction market
for instance would converge to
50% if half the people thought there was
a 25% probability of something and half
people thought there was a 75%
probability of something what's not
going on is like a bunch of basy and
updating where like you have consensus
of people all updating their prior
probability so I just you don't have to
be aasy yeah so be Bas to play and I'm
not and by the way I'm not using
prediction markets as an example of
somebody running Solomon off induction
I'm using prediction markets as an
example of having a you know say kapur's
whole thing I know you guys don't know
but it's related to your point of like
you guys are basically saying where do
these probability numbers come from you
can't do expected value unless you're
doing statistics it seems like you could
very successfully do basing probability
and expected value calculations if you
simply refer to the numbers being
outputed by already selecting the
phenomenon for prediction Market we know
how prediction Mark Works sorry it's
just it's different but so you're
already selecting for people who are
like choosing to bet on these markets
which are people who think they have
better information than the average
person they think they have an edge
hence they're willing to bet right
meaning they think they have like a good
explanation of whatever you're betting
on okay right do we agree
there okay yes so we're already in a
very restricted class of people um who
are you know they're taking bets for
some reason
um and yeah if you have advantageous
information for something uh that's what
betting is all about you bet when you
think other people are wrong about
something you have an explanation as to
why they're wrong um and so you put
money down on it and then what a market
is is like aggregating all this
information uh and people think other
people are wrong so they bit on the
other side of that uh Etc um I'm a
little confused how this relates to like
a win for each person what basian ISM
says and I think with the claim you're
making is you should walk around at all
times putting probabilities on every
hypothesis that you conceive of right
and constantly updating on new
information uh I failed to see how this
like you know people are betting on very
specific questions so let me restate
something you might let me test your
claim here okay so would you claim that
Humanity as a whole as a team using this
technology of prediction markets
Humanity can be a really great Bas in
because Humanity can just list a bunch
of hypothesis run prediction markets on
them and then plug in those probabilties
as basy update uh no like not for not
anything in long term not like
meaningful like sometimes the future
resembles the past if for very important
things to be precise but it's not basing
updates but it's like for betting right
for expected value for policy and by the
way what I just described I think is
Robin Hansen concept of futarchy right
you make prediction markets telling you
the different probabilities different
outcomes and then you just maximize
expected value you choose the policy
that's going to maximize expected value
according to the probability that you
now know pretty well
um you this interesting thing okay so
your qu your question is if we could get
all of humanity to make predictions
about
stuff uh then do we still have to wait
for the time to pass and then see if it
was right and a prediction will either
be right or wrong and if the prediction
is greater than like a year or two then
all of the predictions are eventually
just going to be 50/50 because we have
no freaking idea about what's let's I'm
happy that you've con one year so let's
just talk about policies that one it's
not a concession because we understand
how this works like so there are certain
predictions that can absolutely be made
within a year time Horizon it depends on
what's being predicted so iort for one
year time Horizons I wouldn't no of
course not fkey sounds insane why would
I predict why would I support this where
does it go wrong because uh so I don't
fut turkey if what you just said is that
you make a decision based on the whole
planet's probability about what's going
to happen in a year is that what we're
doing kind of yeah the idea of futarchy
and we'll limit it to one year anytime
there's a policy proposal that's going
to yield returns within the next year
like let's say you want to make sure
that GDP grows 2% this year right this
and there's different policies right
like would a tax cut improve
GP all all this is telling you it's not
telling you what is going to happen
telling you what people believe is going
to happen and what people believe is
going to happen can be completely wrong
all the time just you know to to flesh
out the scenario cuz you know you you
basically asked me so fut would be like
you know there's there's two different
should we do a tax cut should we a tax
increase should we uh cut interest rates
right so there's a few different policy
proposals where everybody agrees there's
a Target growth rate for the economy
right that's the policy outcome you want
and so futar would say great run
prediction markets on all the different
proposals saying conditioned on this
proposal you know you're allowed to do
conditional prediction markets what
would then be the resulting change in
GDP and that way voters could see okay
this policy is the one that has the the
best change in GDP and then you
implement that and I think you were
starting to push back saying like look
this isn't a prediction but may I remind
you these prediction markets have shown
themselves to be very well calibrated
meaning you could use them as
predictions and your expected value
formula would be very you know yield not
everyone is voting I guess right you're
just restricting it to the people who
like want to bet on these markets cuz
the whole point about prediction markets
is like not every's say you're literally
running it on like Po market right using
the same rules as poly Market where it's
literally just a financial incentive to
participate if you think you know
something okay so you're going to
delegate Democratic decision Mak to just
like an expert class of people betting
on poly Market I would definitely not
support this I mean let's say let's say
we keep the same system but we vote in
politicians who are like look if there's
like a weird emergency I won't do
futarchy but like I get that we we're
all smart people we all get the value of
futarchy so I will be setting up these
prediction markets and you guys can help
advise on my policy that way so you want
to find an Elite Class of super SP
people that will be included into the
prediction markets and you want to get
rid of all the dummies because those no
but but that's a solved problem like
whatever current prediction markets are
doing the data is showing that they're
yielding calibrated predictions so you
just you you just amplify what's working
it's so the you're talking about
predictions in the abstract I would want
to see if if you if you're seriously
talking about this as a policy proposal
I would want to see the set of all
predictions that were made and I want to
figure out okay are these like kind of
trivially easy predictions or are they
like holy that is impressive so
first of all I would want to look at the
kinds of predictions that are being made
and then I want to see like which ones
were right and which ones were wrong and
for what reason um but just to zoom out
for a sec like this is very analogous to
a question of like direct democracy and
if I if my car is broken um I could do
one of two things I can talk to like a
few like um well knowledgeable mechanics
and ask them what they think is wrong
and they can tell me this and I can get
a couple different opinions or I could
average the opinions of 3 30 million
people in the population and just do
whatever the average says and you're
saying that the second Camp just
averaging the opinions of a bunch of
people is preferred to like domain
and I would in every case take domain
knowledge and sometimes that domain
knowledge is going to be in the minds of
um people in the Bay area in particular
who are extremely online and like to bet
on all sorts of different things and
depending on question that may or may
not be a good source of of information
but the there's no like massive ah you
just destroyed the whole paparian
approach because some predictions are
possible within a year it's like we have
to think about what's going on here um
and certain predictions are definitely
possible within a year yeah so so say
you're the president and you ran on a
platform of like I will pay attention to
the prediction markets because I'm
basian and understand the value of pay
attention to Market and then and you're
and you're considering a tax cut right A
Generous tax CR across the board and the
prediction Market says um GDP growth
will increase more than 1% compared to
what it would have been if this tax cut
were implemented markets are saying 70%
chance right and now you're saying just
to repeat back what you just said now
you said okay yeah sure the president
could listen to that prediction Market
but he hired Lauren Summers right or
just like some famous economists right
who's telling him Mr President I give a
30% chance that your tax he would give
an explanation he would give an
explanation as why come with yeah so you
would say because it comes with an
explanation and because this guy is
trusted theid theid should just listen
to him and not the prediction Market
they should listen to the explanation
and maybe get a couple different ones
and see what makes more sense and maybe
get the people to debate a little bit
and also yeah if there's an explanation
as to like why the prediction Market
might be accurate in this case like say
you have like all these expert
economists betting on this on on this
market right so in some sense the market
is reflecting the view of uh some giant
class of people who we have some reason
to expect they know what they're talking
about then yeah I would take that
information on board I'm still I'm
confused about the basian aspect here
right so there are certain questions
where we want to use statistics we've
said that all along right and statistics
is valuable in so far as it helps us
with prediction right especially when
there are huge amounts of data okay so
markets uh prediction markets can
reflect that in some sense um I'm the
the basian picture for me comes in like
at the individual level and at the indiv
individual level I'm super skeptical of
the ability to for people to make like
quote unquote super forecasts right so I
think the L literature there has been
like very overblown so there was this
there was a good review actually written
by um uh I'm GNA blank on their name
Gavin leech and Misha yagodin maybe
right so they like um they and they were
uh I think rationalists of sunf flavor
so very sympathetic to a super
forecasting project um and they took
like a look at Ted Lock's literature um
and found that these initial claims of
like 30% more accurate than uh
expert pundits were way overblown uh
first of all the people were being
measured by different metrics and so
once you correct for this it's more like
a 10% difference secondly this 10%
difference didn't even reach statistical
significance
um uh and so I yeah and oh well I mean I
I have data to counter that right I mean
I think this this is absolutely a Crux I
mean so if I'm wrong about this kind of
data then I I'm absolutely open to um to
downgrading my assessment of the
usefulness of basanis but the data that
I would point to is if you look manifold
markets for instance the one the one
that published the data about the
extremely good calibration there's no
one user in manifold Market who has this
kind of consistent calibration right
it's it's the Market's calibration okay
there's no one us this is good though
this is okay so I think we're getting
somewhere right like there's no one user
with good calibration okay so this is
saying like in doing a bunch of if they
were forced to vote on everything right
maybe there are some users that have
good calibration on the bets they choose
to make C can I just add one thing sorry
the other point just on the individual
thing the actual Brier scores that super
forecasts are getting so like you know
0.25 is like a perfect 50% Brier score
so if you just bet 50% on everything um
assuming there's an equal number of yes
NOS in the answer set you're going to
get 0.25 what sort of Brier scores were
super forecasters getting typically
something around like 0.2 okay this
corresponds to like 60 to 65% accuracy
so what we're saying is super
forecasters who I guess do this for a
living right they bet on stuff um when
they're maximally incentivized to truth
seek right um they can get like 60 to
65% accuracy on questions um if you want
to call that as like a gacha that
they're seeing clairvoyantly into the
future that's fine I'll just acknowledge
that um but I don't view 60 to 65%
accuracy as some huge win for putting
probabilities on everything I basically
view it as like they're running into
hard epistemological limits of how easy
it is to see the future if you have if
you have very good domain knowledge of
an area it doesn't surprise me that you
can beat a coin flip literally random
guessing and expert knowledge in an area
and are are incentivized in the right
way to actually care about outcomes as
opposed to like political punditry for
instance um and so that's where all my
skepticism let me tell you what I'm
claiming here though like why okay why
are we even talking about prediction
markets right let me tell you what I'm
claiming here I so and you bring it back
to like look individual humans an
individual hum human is much weaker than
a prediction Market that's what you say
fine but let me tell you why I'm
bringing this up it's because I think a
lot about Ai and the powers that AI is
going to have if it's programmed
correctly or if we keep on this progress
of putting the right code into an AI
what's possible well a single AI could
take on all of humanity like yes there's
a lot of different humans making a lot
of different models but you could also
just copy the ai's code and run a bunch
of instances of it and have them wire up
to each other and you it literally is in
my mind a question of one AI versus all
of humanity and so for me when I see the
prediction market aggregated across all
of humanities experts the way prediction
markets know how to aggregate
information I see that as a lower Bound
for what one AI if programmed correctly
could do in its own head and come up
with its own basian probabilities so
when I imagine an AI functioning in the
world I imagine it putting probabilities
on things having those probabilities be
well calibrated using the expected value
formula and then placing very well
calibrated bets can I ask a question can
I ask a quick question um what do
prediction markets say about the
likelihood of super
intelligence So currently they're saying
I think AGI is coming at around 2032 I
think that was metaculus last I checked
the probability so that's the
probability of like the scenarios you're
describing that um the super um
forecasters and these markets what do
they assign with
probability uh what's the question
exactly um for uh the Doomsday
apocalyptic scenario that or gives a one
and 10 probability to um that you're
really worried about uh I'm not asking
when super intelligence is going to
arrive because you can Define super
intelligence in a thousand different
ways I'm asking to the do day nightmare
scenario that keeps you up at night what
probability assigned to that so I don't
know which prediction Market I would go
check for that because the problem is
this Market I thought you said they're
all the same or I don't know which
prediction Market even has enough volume
on a question that corresponds to what
you asked because the reason is
prediction markets are a powerful
methodology but they're they do have the
issue of you know counterparty risk and
platform risk right so if you're saying
hey what are the chance that everything
is going to essentially go to zero that
human value is going to go to zero right
how am I going to collect on that if I
if I think it's 90% likely why would I
bet on that I'm just losing my money
today for something I can't collect on I
say so you'll follow their predictions
up until the point that you have a
reason to think that they're wrong and
then you'll ignore them right well this
is a systematic failure right it's like
saying will prediction markets still
work if somebody hacks their server well
wait a minute that's there are some
limitations no right no no right now
there's certainly some prediction Market
that says some apocalyptic student
scenario and I think Scott Alexander has
blogged about this and it's very very
low um I can find the story I think it's
something like 3 5% Ben if you recall
this markets are a way to aggregate
information by financially incentivizing
their participants there's no Financial
incentive to a doom
prediction then why can we be confident
in like your doom predictions or
anything like that like what like why
should we why should we why should we
care do predictions come from just app
yeah yeah I I'm I'm just actually using
Basi andology right so everything we've
been talking about now I haven't been
saying we're doom because prediction
Market T we do I'm saying no I have a
strong epistemology it's called Basin
epistemology it's called approximations
to Sol and off induction you can see how
strong this epistemology is when you go
and look at the calibration of
prediction markets like manifold who are
not using statistics to get great
estimates this helps you see that my
epistemology is strong now as a as a
Reasoner with a strong epistemology let
me tell you how I got to a high PD right
that would be the shape of my arent one
just I just one okay sorry this really
quick so your answer about it doesn't
make sense to bet on these um particular
questions because we'll all be dead if
they turn out to be true so that would
just mean that they're aren't those
questions on these markets right like
people aren't betting on them they just
aren't on there I think people are
putting on the question like for the of
I think that's not true and but all I'm
curious about is there is some number
that they're giving that you have some
reason to ignore because yours is much
higher and why do you um support
prediction markets in every case except
when it disagrees with you at which
point you don't support them anymore
okay so why do I trust prediction
markets besides just their track record
right because it sounds like you're
modeling me as like look if you like
prediction Market's track record why
don't you just extrapolate that no
matter what the prediction is you should
expect it to be calibrated but yeah I do
take a structural fact that I know about
prediction markets I take that into
account for instance if I knew for a
fact that Bill Gates was going to spend
all his money to manipulate a prediction
market right like there are some facts
that could tell me for for some periods
of time or some ins knowledge you have
Insider knowledge as to say that these
prediction markets are wrong and so
you're presumably like leveraging that
and making some money my point is just
prediction it's it's not you can't be
quite so naive to be like okay no matter
what the prediction Market says you have
to try there are some boundaries and I
didn't this isn't an ad hoc limitation
the idea that the whole prediction
Market shuts down under certain under
certain bets you make I mean there's
it's called platform risk like this is a
known thing in trading like you're
you're basically just you know you're
coming at me for for just doing a
standard thing about trading where you
look at platform risk no no you you you
you were saying you have Insider
knowledge here to um for you have
justifications and reasons to assume
that the probabilities that are being
assigned to these particular questions
are wrong um and you should make a lot
of money while the apocalypse is coming
I get that when the apocalypse comes
we're going to be only pays out during
the apocalypse right the presumption is
you make money because when the
apocalypse happens you get paid out
that's like a contradictory model it's
almost like betting like you know
AAL Parx to have a prediction Market can
you just do end of the world can you
just do end of the world bets here like
owski Hansen style where the person but
the the pro yeah yeah so make so there
there is a bet I can make but I I can't
make it on poly Market or or manifold
but I I can make it informally I can
make it with you guys if you want where
it's like if you guys give me $1,000
today so I can have it while the world
still exists I could do something like
in 20 years which is when I think
there's like a 50% chance that the world
is going to be ended then I can pay you
back 2x plus 5% interest or whatever
right it's like you will have made like
a very attractive return over those 20
years if I get to use your money now
because I think that I do want to I do
place a significantly higher value for
your money
today that's it so yeah I I just want to
say for the listeners that I just might
be mistaken about the internal mechanics
of how prediction Market works works
like I thought you could get paid out
before it resolves but maybe that's just
not you can get paid out you you can buy
out if you find somebody else like let's
say that probability of do keeps
creeping out so you could sell your
contract to somebody else and you could
cash out early but the problem is why
would somebody come in with a higher bid
than you even if they thought Doom was a
higher probability right because they're
being stupid because they should know
that they're not going to get paid out
unless they find another sucker it just
becomes a Ponzi scheme
essentially I agree prediction markets
can no that that was a bit cheap yeah
you heard here first guys doers just are
pulling a Ponzi gem on everybody um I I
didn't say it but um uh yeah I think we
should we should pivot off of this
because I I just don't understand the
mechanic enough to um uh to adjudicate
and I'll take your word for it but it
seems like you have Insider knowledge
that you should leverage somehow if
you're right there should be some way to
just make a bunch of Bank you're
conflating Insider knowledge with
platform RIS right these are two
distinct CEST yeah no totally yeah I'm
totally acknowledging that I'm missing
some of the details I love for
commenters underneath too um to clean
this up for us
yeah okay all right all right right cool
so um yeah so I guess we're we're we're
starting to come close to the end of
time um so yeah let me just open it up
to you guys um just if you want to throw
out a few topics to make sure we hit
before we end I can write them down and
we can uh plan the rest of the talk well
so well so we um we've done three hours
on based epistemology and I think this
is a good place to pause and then let's
do another three hours on super
intelligence this has been a blast um we
haven't even talked about super
intelligence yet and um and like this is
kind of why when we had initially talked
about this I'm like let's just extend
the time because we are not going to
make it past the first the first set of
questions all right guys so we've been
talking quite a lot and uh I talked with
uh Ben and B offline and we all agree
that there's so much more interesting
stuff to talk about that we're going to
do a part two it's also going to be
pretty long uh check out some of these
Coming Attractions we're going to be
talking about Ben's blog post called you
need a theory for that theory oh and
we're going to be talking about Pascal's
mugging we're going to be talking about
hume's Paradox of
induction talking about um utility
maximization as an attractor state for
AIS then we're going to have a whole
David deut section talking about his AI
claim like creating new knowledge and uh
a certain capture that I invented based
on David de's arguments we're going to
talk about what is creativity how will
we know when AIS are truly creative talk
about intelligence what is intelligence
can we talk about general intelligence
what separates humans from all other
life forms is there much Headroom above
human intelligence is Agi possible
eventually what about in the next 100
years how powerful is super intelligence
relative to human intelligence can there
be such a thing as thousands of IQ
points
what's a fundamental capability that
current AI doesn't have and then also AI
Doom topics we're going to talk about
agency the orthogonality thesis inell
convergence AI alignment and maybe even
unpack some of Elon musk's Claim about
AI so all of sounds like we might need
10 episodes but a most of those I think
we'll hit on in part two so how about
let's let's go through everybody and
we'll just summarize uh where do we
stand what do we learn about the other
person's position did we change our mind
about anything uh starting with Ben sure
yeah so I'm slightly worried we
verbosely circled the disagreement
without precisely getting to the key
differences perhaps between paparian ISM
and basian ISM but hopefully I'm just a
little uh I'm being a little negative
and the differences did Shine through um
I think to be fair to you I think the
biggest challenge to paparian ISM comes
in the form of betting if people are
doing like you know significantly better
than random what the hell is going on
there right and is probably if
probability is the only way to do that
then presumably that justifies some sort
of probability
um epistemologically speaking um I
remain skeptical that that's true
because at the individual level I just
haven't seen the statistics that super
forecasters like I said are doing much
better than like 60 to 65% which I think
can be explained with incentivizing
truth and limiting uh bet uh thinking
about questions where you have very uh
good main expertise um but that would
definitely be I think that's like a good
Crux to label maybe if I see and I think
actually Ven and I discussed this in
some episode this is sounding familiar
as it comes out of my mouth like if you
know if we start to see super forecaster
accuracy really just keep going up over
time and start hitting 70 75% 80 85%
then I'm totally going to that's going
to start uh verging on falsifying my
claims right if people just become like
more and more omnicient with more and
more uh if they just become smarter and
and better better able to prict why do
you need an individual can I just
clarify here so does when you say they
have to get more and more accuracy do
they specifically have to give like 99%
probability something because normally
we look at calibration right like
they'll say 60% chance and it happens
60% of time so you're talking about
calibration accuracy as well so
calibration is one metric but accuracy
is another completely valid metric to
look at right when you say accuracy do
you mean like confidence like high
probility so any machine learning person
who's listening to this will know what
I'm talking about you can look at
comp but also just have of questions and
or not they happened um and then you can
just count the numbers of successful
predictions and yeah like if I see Bri
scor outputting a bunch of probabilities
okay so Brier score does depend the only
way you can have a good Brier score is
by often having high probabilities as
your answer right you can't just Punt
and be like Oh 51% really but that's the
epistemologically relevant thing right
if if you're if really you're using
probability to reason about the world
and updating your probabilities um in
such a way as to really be able to
predict the future then yeah you're
going to be predicting the future with
high confidence that's the claim right
the the whole point about my 0.25
comment was that you can do you can get
a low quote unquote prior score quite
easily by just predicting 50% so that's
not interesting right what you want to
do is start pushing but the universe is
chaotic right if if I give you hey
here's your test it's 100 problems of
like three body problems right like
super chaotic stuff then you're going to
fail the test even if if you're a really
in other words the universe is
fundamentally there are epistemological
limits about how much we can know you're
saying you're never going to be
convinced what you're saying is probabil
you're getting me a false test here no
no what I'm saying is probability is not
the best tool to reason about the future
precisely because the future is chaotic
and unpredictable right the best thing
we can do is just like argue about
details not put random probabilities on
things that buy your own lights it
sounds like you just admitted are
inherently unknowable so when or says
things like there is a one6 ility of
some crazy event happening in the next
100 years yeah I want to I want to
appeal exactly like you said to the
chaotic nature of the universe to say
this is a totally unjustifiable move to
make and it's it's doubly unjustifiable
to start comparing this to the
probability of asteroid
Collision okay but what if what if
everybody what if the current manifold
prediction for asteroid impact in the
next year let's say and for some reason
that wasn't a world- ending event so
like a small asate impact right a
non-world ending asterid impact
happening in the next 11 months right
what if the prediction Market was saying
1 and six you wouldn't think that one
and six was a trustworthy uh probability
Cali need to look at a prediction Market
in that case like we would have a theory
of like this asteroid is coming towards
Earth we' talk to astronomers like there
this is not the place for yeah but to
the extent that that theory was good
wouldn't the prediction Market I'm sure
wouldn't it precisely be because we have
a good theory right this but this is the
whole disagreement between us we're
saying yeah sure prediction markets are
useful sometimes they're not useful most
of the time especially in the far future
because there are things that are inher
ently unknowable in those Realms
probability is a totally
meaningless uh mathematization to put on
tronic quantify ignorance the basian
position um and maybe you're not trying
to AR argue this I'd be surprised if so
but the basian position is you should
always put numbers on your uncertainty
for close events and far events we're
just trying to say these are not the
same thing when you're predicting what's
happening in the next 5 10 15 years from
predicting like you know the election
tomorrow um can I like you you were
starting to give us a good test of what
would change your mind but then the test
proved to be like kind of impossible
right like the test like what do you
need to see from prediction markets to
change your mind yeah I would that the
accuracy absolutely proves like that
things get better over time but can we
ask why isn't your standard calibration
why is your standard accuracy because
accuracy is impossible if we if we put
questions that that are hard to have
higher than 51% confidence on then for
sure you're never going to get high
accuracy right so you know you're giving
an impossible reason like you're begging
the question there's a reason it's hard
to get high 50 that okay but you're that
you got to admit it's already a good
fith test if you're just saying this is
logically impossible so okay no let me
let me rephrase it so um if this is a
hilarious closing
round clearly indicates we have much
more to discuss about um which which is
fine and which is good but let's just
let's just try to wind wind things down
and and we'll leave the leave the
audience with um with a teas that we
clearly have much more to to discuss but
um okay let's just use calibration fine
let's say that that it gets more and
more and more and more calibrated over
time and for and for more and more
events like we on everything say yeah
for more and more B
it surely unless you want to just handle
the the boring case where it's just the
calibration is 50% if you're getting
more and more calibrated then that
should improve your accuracy as well
right they won't be exactly the same but
you should get a better accuracy because
why not just say the stand this just be
like hey I'm I'm going to filter
prediction markets out for only the the
data points where there's more than 70%
or less than 30% probability I'm only
going to use those data points and then
I'm going to measure the calibration of
that and if it stays High then I'm going
to keep being impressed that basan
epistemology has a lot to offer because
we aren't impressed and we aren't going
to keep being impressed we're talking
about that which would falsify our view
and force us to be impressed because the
standards for ourselves are different
than you and we're just trying to say
like like I would be super impressed if
the accuracy started going up because
the calibration started going down and
it wouldn't have to be like perfect
accuracy just like showing that over
time people get more knowledge and then
they can predict better and there's one
falsifiable test that you don't need
because you are already convinced but
the question is what would change our
mind and and let me concede like
actually something B said earlier like
if if you just did more and more EV that
like if you had prediction markets for
everything and we were predicting you
know the like predicting everything from
the weather to like uh who's going to
get a pluses on their tests to like is
you know like if we're predicting
everything and there's calibration we
have perfectly calibrated like
everything's perfectly calibrated all
these markets are perfectly calibrated
that would be amazing I claim that's
impossible as especially as the Fidelity
of these events gets like more and more
precise smaller I don't I don't know
what word I'm looking for here but um
anyway I think that was not a very
coherent closing statement but you you
understand what I'm trying to say like
we use prediction markets literally
predict everything and they were always
perfectly calibrated label me impressed
and I'm going to I'm going definitely
going to reformulate my thought I'm
still slightly confused about the your
the relationship in your mind between
basian ISM which is an individual thing
for me and prediction markets but I
think we're not going to resolve that
right now so maybe we'll relitigate that
next we should say that for next time
yeah yeah
sweet all right well I agree with summ I
agree with everything Ben said um I the
only comment I want to make is for the
listeners who are um overwhelmed right
now because there's a lot of various
things the like the way that I think
about learning the difference between
paparian ISM and basian ism is um do you
remember like in elementary school where
you put like a leaf under a piece of
white paper and then you take charcoal
and you keep doing passes and then over
time the image underneath starts to
become clearer and clearer and clear but
any one particular pass doesn't totally
give you the resolution that's the
metaphor I have with regards to the
difference in these two kinds of
methodologies because any one
conversation will be interesting but
it's not going to fully explain boom
here's the difference it's it's more
about listening to a set of
conversations with different people um
and listening to our podcast but
listening to other podcasts as well um
and just seeing the difference and
seeing the difference I only say that
not in a self- serving way but also
because this is where like a lot of
stuff is
being put into direct um comparison but
over time you'll just start to see
different methodological differences
different emphasises emphasi different
um cognitive tools for How to Think
Through problems obviously we disagree
on a lot of object level things but the
underlying difference is just how we
think about the world um and that's not
going to be um made clear in any one
particular conversation it's something
that's going to gradually become clearer
and clearer over time as with the leaf
and the charcoal um and so just to The
Listener who's like w there is a lot of
stuff and I still don't totally
understand what the differences here are
you're not expected to it's not possible
to understand it in one conversation but
over time you'll start to see
differences in approach and methodology
and that's what I want to
say awesome yeah thanks for the summary
guys and you guys have been such great
sparring partners you know as fellow
podcast hosts right you're old pros at
this so and it shows I think this is a
really fun conversation I think you know
we didn't pull any punches right we were
both going after pretty strong which I
you know I think we all enjoyed it a um
yeah you know it's all good nature I
mean um like I you know there's like no
hard feelings right just because we dis
this is so much fun well for the
listeners every time we go off pod every
time like there off pod or cut there's
like just great Vibes we're just exactly
that's great yeah it's it's it's not
like tribal or whatever um and also I'll
take a quick stab at being Interfaith
right I'll probably won't work but I'll
try to do a compatibilist solution here
what if we say that uh Sol off induction
is like a nice theoretical ideal the
same way that you know the the chest
player that searches every move is a
good ideal but as humans right when
you're occupying a human brain and you
just have to be like totally limited you
can't even get close to approximating
solof induction if you follow Popper's
recommendations then by your own metric
of trying to approximate solov induction
then you're going to do well how's that
nope Pap arianism was barbarianism was
born by the fight against all induction
of which Solomon of induction is one so
if you want to understand paparian ISM
read literally any of his books with the
exception of like maybe the all of life
is problem solving and every one of them
has some attack about induction and
induction is a much deeper concept than
solom of induction and so once you kill
induction you kill any um derivations or
derivatives of induction so for that
reason we will leave the lever listener
on a cliffhanger okay
with tler Summers from very bad Wizards
where we talk about for two hours my
last ditch effort to try to broker a
ceasefire has failed so this is so we're
going to have to Contin fart too yeah
induction saying induction was like
don't you dare yeah induction is a
crypto y okay great so yeah listeners
just stay tuned hopefully next few weeks
part two is coming and yeah stay tuned
we got other great debates coming up
right here on Doom debates thanks a lot
man good complete blast this is awesome