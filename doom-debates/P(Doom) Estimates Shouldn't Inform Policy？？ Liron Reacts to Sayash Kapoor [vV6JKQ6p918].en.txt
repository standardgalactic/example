now what subjective probability
estimates do is that they sort of feed
into our cognitive biases to take
quantification seriously and so when
someone says that the P Doom or the
probability of Extinction from AI is 15%
we might take it more seriously than we
vote claims about um I don't know the
risk of AI going rogue being high what's
the alternative to reasoning with
probabilities if I don't say hey I feel
like there's a 50% chance of Doom what
am I supposed to do
instead welcome to Doom debates today
I'm reacting to last week's episode of
machine learning Street talk with Sayes
kapor Sayes is a PhD candidate at
Princeton in computer science he's the
co-author of an upcoming book called AI
snake oil what artificial intelligence
can do what it can't and how to tell the
difference
and last week he published a pretty
popular blog post on the AI snake oil
blog uh together with co-author Arvin
Narayanan the title was AI existential
risk probabilities are too unreliable to
inform policy and it caused a bit of an
uproar among the doomers because we
basically don't like being lectured
about not using basian probability we
think that the other side really has it
wrong where basian probabilities are
actually incredibly productive and
they're kind of of foundational to
epistemology so it is actually we who
should be schooling you and this podcast
is kind of me expressing that
perspective so let's get into it we'll
take it Point by Point here is the first
clip from Sayes how seriously should the
government take the threat of
existential risk from AI uh so the
threat of existential risk I think is
sort of a once in a-lifetime event in
some sense and to some extent I think
we've seen policy makers adapt to claims
about existential risk both from
prominent AI researchers but also from
communities such as the AI safety
Community the effective altruist
community and so to some extent I think
the response is reasonable um it's a
event that can only happen once so we
only have one chance to get it right to
some extent and in the article we uh try
to go over what the arguments for
existential risk are and uh and how
seriously we should be taking these
arguments um and we in particular look
at one of these arguments which is the
probability of Doom or P Doom which has
become like ramp not just in the Tech
Community but also in the policym spaces
in the last couple of years interesting
choice of language to refer to P Doom as
an argument as if this particular issue
is being argued by bringing up a
probability it's confusing to me to
approach it that way because every issue
should be argued by referring to
probabilities right so why is he
surprised that this particular issue is
being discussed using probabilities as
if probabilities are like a tool that's
specific to this issue and if if you
listen to how he frames it he's saying
like oh there's all these different
arguments but one of the arguments is p
Doom okay weird way to begin but let's
continue so you made the argument
essentially that these risk um you know
probabilities are too unreliable to
inform policy can you elaborate on that
yes um so when we're coming up with
probabilities for any event I think
there are basically only three ways to
go about it we can come about it
inductively that is we look at past
events of a similar sort that have
happened we can come about it
deductively that is we have a theory for
how the world works and then we apply
the theory to make predictions or um as
has become the norm in the AI Community
we can come about it subjectively wait
what we use basian reasoning basian
reasoning means that you have a bunch of
different hypotheses about how reality
works and they all have a probability
and then evidence comes in and you
update the probabilities of your
different hypotheses and at any given
time somebody can ask hey what's the
probability of this hypothesis and you
can tell them because you're always
keeping them updated like this is how
Basi and reasoning Works according to
many mathematical theorems this is just
how intelligent agents have to operate
to the extent that it looks like they're
operating using some other framework
that framework will only work if it's
equivalent to basian reasoning scash is
just bulldozing right past the
methodology of basian reasoning as if
this is like a fresh idea for him like
how does one use probabilities does one
just make a sequence of events in the
past and try to pattern match that
sequence no basian reasoning is a thing
it has a ton of credibility like how do
you just dismiss it so early in your
talk I don't get it now let's set that
one aside for the moment and let's dive
into inductive and deductive
probabilities first um so in terms of
inductive probability um you can come up
with great probability estimates for the
risk of let's say car crashes by looking
at um like a person of a certain type
who drives a certain type of car lives
in a certain neighborhood Etc insurance
companies regularly price uh products
like insur car insurance premiums and I
think more or less that seems to work
well I I mean there are some nuances
about bias and risks of discrimination
but apart from that you know like let's
say if a car insurance prediction is 20%
off uh that seems all right right um we
cannot do that for AI risk simply
because there is no past evidence for
this type of work we don't have a
reference class so to say and so any
probability numbers that we come up with
based on past events are extremely
speculative now to be clear this
reference class is a spectrum so certain
things can happen multiple times many
millions of times and we have extremely
um precise reference classes if you will
so like when you toss a coin uh we have
a very clear reference class and we can
predict that um the probability of a
fair coin will be let's say 50% uh
Landing as heads and 50% Tails this is a
common type of argument that non basian
makes they're saying look at the gold
standard of probability the gold
standard is basically frequentism if
you're familiar with that term it's when
you can just just look at an event
that's happened before a bunch of times
and you can use a very natural
similarity metric like hey I flipped
this coin a th000 times and so the next
coin flip is surely in the same
reference class as the previous thousand
coin flips so this is the gold standard
of probability if it came up heads 500
out of a thousand times and I'm going to
flip it again I'm totally justified in
saying that I have a 50% chance of
having the next flip via heads and a 50%
chance of having the next flip via Tails
but wait a minute if we're talking about
a epistemology what gives us the right
to know things what gives us the right
to make sound predictions about the
future it's not like this kind of
frequentist attitude really is the gold
standard you know there's the famous
example that Nasim TB talks about about
the turkey right the turkey who's going
to get slaughtered for Thanksgiving well
every day 365 days in a row the turkey
gets fed breakfast lunch and dinner the
farmer wants the turkey to grow nice and
fat Thanksgiving comes along the turkey
gets slaughtered but wait I thought the
probability of getting slaughtered was
Zero out of 365 I just got fed every day
for a year right so what do frequentists
say about that is that the
epistemological gold standard to just
find a sequence and then just use the
ratio of that sequence no it's
absolutely just a shortcut frequentism
is just a shortcut when basian ISM gets
into a certain Groove into a certain
pattern basian ISM might license a
frequentist approach basian ISM might
say don't worry I don't have a model
that you're part of a farm my model
really is that your finding your own
food that your food source is stable and
so go ahead and use this reference class
of how many times you've been fed
without that larger perspective without
actually building a Model A gears level
model of what's happening in your
Universe you can't just expect to
extrapolate the pattern and and always
be robust right so I just wanted to
undermine that kind of claim in general
like it's the gold standard of the
epistemology of probability when you can
put together a sequence of events now
you might think Thanksgiving turkey is a
contrived example I don't agree there's
all kinds of examples where naive
frequentism is going to fail look at the
stock market a lot of people get the
idea that they have a winning strategy
in the stock market when what they're
really doing is picking up pennies in
front of a steamroller so every day they
might sell a call option and the call
option pays them a penny and profits but
the downside is that if the market were
to ever crash Beyond a certain range
that they're not expecting suddenly
they're on the hook for like $100 so
it's like every day it's like oh nice I
picked up a penny I picked up a penny on
this call option this call option is
great it's just printing money a little
bit every day and then like 2 years pass
and then suddenly one day you're on the
hook for $100 and it wipes out all of
those Penny gains that you made over the
entire last two years and that's how
people lose in the stock market because
they get comfortable they're like oh I
know what I'm doing and then what I seem
until I might call a Black Swan wipes
him out right something that undoes all
of those things that they thought were
so stable so that's just another example
of when naive frequentism being like
look I have a pattern of making money
it's just another example of when naive
frequ ism is going to fail you um we
might have reference classes that are
somewhat more wague like the car
insurance example and then in typical
probability estimation of events um the
extreme end of the spectrum is things
like um geopolitical events where you
have the risk of War um and the
reference class is um past events of a
similar nature so past Wars that have
broken out um past famines that have led
to political instability and so on now
compared to this entire spectrum of
reference classes AI risk is maybe
completely outside the Spectrum or at
like an extreme end of the spectrum to
the extent that a reference class just
does not exist now people have tried to
come up with reference classes and I'll
I'll share some of them just to so show
how absurd these reference classes can
be one example is the risk of various
animals going extent um another
reference class is past um sort of
changes to the Earth's atmosphere past
waves of Extinction that we've seen and
hopefully it's clear how very different
all of these types of risks are compared
to the risk of us losing control over AI
or power seeking Behavior emerging in
super intelligent systems or all of the
other hypothesis just to repeat what
he's saying if I were to ask you to
predict whether war is going to break
out between Israel and Iran or North
Korea and South Korea or India and
Pakistan well you could pick a natural
reference class of previous instances of
political instability
or you could compare it to previous Wars
that have broken out and you could try
to set up a fraction with a numerator
and denominator of like okay the times
that things F felt tense war broke out
with this frequency and in his mind
that's pretty Justified to do that to to
set up that particular frequentist ratio
but he thinks that that's totally
different from what you could do with
existential risk where if somebody asks
hey what's P Doom from AI well compar
that to the reference class of times on
earth when species have gone extinct he
just thinks that's crazy that is not a
good choice of reference class because
the similarity metric that he's
intuitively using for him it's just not
that similar to look at other Extinction
risks throughout history to other types
of species that's not similar but if
you're looking at this new war breakout
event that you're predicting well that
is similar to previous war breakout
events and of course the devil is in the
details of what exactly you're going to
use for the numerator and denominator in
that frequentist fraction I think it's
really not frequentism because it is
open to a ton of judgment what you're
going to use in the numerator and
denominator and the numbers could come
out very differently depending on the
way you actually Implement what you
think is an obvious reference class so I
think he's already on very shaky ground
here making the point he's trying to
make I guess I will concede that like
okay I guess it's a little more similar
war between humans compared to war
between humans versus you know human
extinction from AI compared to like the
dodo going extinct or like a super
volcano making 98% of species go extinct
like I guess that's slightly more
similar but it's just basy and reasoning
the way that proper intelligence is
reasoned about the universe doesn't
depend on these qualitative subjective
similarity metrics and then setting up
these naive fractions that's not how you
reason when you want to actually get
results right that's already a very
oversimplified bad explanation of how to
put probability of things but like I
conc that if I were a frequentist and I
thought that this whole framework made
sense then okay I see what he's getting
at but I think it's a terrible framework
so it is on this basis that we reject
the idea that we can come up with
reference classes and therefore we can
come up with inductive probability
estimates um of the risks of a doom so
that leaves us with deductive estimates
of probability um deductive estimates
are those where we have a theory that we
have validated in some real world
setting and we seek to apply it to this
other real world setting um and we have
faith in the theory simply because of it
being validated and us making
assumptions about the scope of this
Theory he's using this term deductive
estimates of probability that's
non-standard language when we talk about
epistemology like how do people know
things how do they shift their beliefs
so that those beliefs have a high chance
of being accurate how do humans do it
how are we going to program AIS to
effectively do it how does cognitive
work yield results right we're talking
epistemology here and it doesn't seem to
be a strong suit because that
description that he just gave is quite
clumsy his description is deductive
estimates are those where we have a
theory that we have validated in some
real world setting and we seek to apply
it to this other real world setting and
we have faith in the theory simply
because of it being validated and us
making assumptions about the scope of
this
Theory okay if you want to describe how
people know things it's because we come
up with hypotheses that compress the
data that we get the hypothesis is
validated by the fact that all of the
data that we get has a much more compact
representation when somebody has that
hypothesis and as long as the compressed
hypothesis actually matches the data the
data makes the hypothesis accurate then
that hypothesis gets higher evidential
weight it gets multiplied by an odds
ratio and then you also have to consider
your AR priori probability How likely
you thought the hypothesis was before
you got this new information but that's
kind of a detail in basanis because once
you get enough evidence it's just going
to be clear Which hypothesis compresses
the data the best and it doesn't really
matter what your a priori beliefs were
because they get overshadowed by this
huge likelihood ratio that your
hypothesis has earned so anyway right
off the bat I'm just noticing that scash
is just not explaining how cognitive
work makes your brain correspond to
reality your brain is an engine where
data comes in and it says which of the
different things I know are the best
explanation in a formal quantitative
sense the sense of compression when
scientists are creating theories when
Einstein is creating his general
relativity he knew it was accurate
because it was a compact theory that fit
a bunch of paradoxes about how the speed
of light can be constant in every
reference frame in every direction once
Einstein hit on that theory he already
knew that it had a very high probability
of being true just because of the ratio
of how few bits it took to describe what
he found and how many known cases it
applied to that didn't have alternative
explanations so Einstein was already
very far down the path of validating it
Theory I disagree with this language
that say is using when he says us making
Theory we don't have to make assumptions
there's actually a formal procedure that
makes your theory get validated and the
formal procedure is if you have evidence
that is
compressed the description of all the
evidence you're getting is compressed by
the theory that you have effectively and
accurately then your theory deserves a
lot of Base points you deserve to update
toward a high probability of having that
theory so ultimately all I'm doing is
I'm just pointing out that there's this
framework of how people have accurate
beliefs and say is kind of giving this
ad hoc version that doesn't explain it
very well which is a red flag for what
he's going to say next now this is all a
little bit abstract so let's get
concrete one example of X risk that's
not from AI is existential risks from
asteroid impact so we might be concerned
that um when an asteroid of a
sufficiently large size let's say
collides with the Earth uh there would
be enough energy energy generated that
it could sort of clog up the atmosphere
and block off all agricultural
production now how can we come up with
the deductive theory for this well we
have evidence from a large number of
smaller asteroid hits as to what happens
when an asteroid collides with the Earth
um and based on this Theory we can sort
of extrapolate to what would happen um
when a large enough asteroid hits the
Earth and again and drawing from other
theory about what would happen and how
much energy would be needed to sort of
block off the production of agriculture
in the world uh we can then come up with
an estimate for how big an asteroid
needs to be um and how often these big
asteroids to fly by close to the Earth
to see what the probability estimates of
U or the P Doom of asteroid impact is I
want to nitpick the relationship between
observing what a small asteroid does and
predicting what a large asteroid would
do because just reading back what say
said he said we have evidence from a
large number of smaller asteroid hits as
to what happens when an asteroid
collides with the Earth and based on
this Theory we can sort of extrapolate
to what would happen when a large
asteroid hits the earth I don't know if
he misspoke and he's just talking about
the probability of those events but if
he's literally talking about predicting
what's going to happen when a large
asteroid hits the Earth by extrapolating
that's not necessarily true right the
physical consequences of an event don't
necessarily scale linearly for example
we can just have lots of small asteroids
hit the earth and be fine but then when
a sufficiently large one hits the Earth
we won't be fine because there's
nonlinear effects right there's
thresholds where it's like oh this one
blocked the atmosphere it didn't just
kick up some dust it blocked out the sun
right things like that but anyway I
think the more important part of what he
said is when he talks about predicting
how often these big asteroids do flybys
close to the Earth and using that to see
what the probability estimates and the
pdom of the asteroid impact is so yeah
the frequency of when big asteroids come
I mean maybe we can extrapolate some
Trend in the data like hey some small
asteroids come some bigger asteroids
come maybe some bigger one maybe you'll
get like a power law distribution or
some something might just fit really
neat
into some mathematical distribution and
so that hypothesis might be the best
thing you've got compressing the data of
course that's not even true right I mean
Newtonian mechanics forget general
relativity just the idea of like hey
gravity makes things fly around using
certain forces and so these are the
kinds of patterns you can expect and if
you zoom out and you look at the whole
galaxy you might expect this um I would
not feel at ease just looking at a power
law of all the different asteroids that
we've seen fly by the earth I would
really want to understand
what makes stuff fly around because I
would really want to understand like hey
what are the chances that something from
a nearby Galaxy that's just not on our
radar at all looking at the historical
data what are the chances that some
other force is about to come down on us
right I wouldn't just use naive
frequentism I'd be like what's going on
in the universe how do these things work
is it possible to get any confidence
that things that have never happened
before will still not happen for some
actual reason instead of just drawing a
graph right that would make me a lot
more confident and luckily we do have
some results like that for example the
speed of light limit tells me that if I
look around to the outer reaches of what
we can see and I don't see any threat
coming then that means I probably
actually am safe for like a few billion
years right because I'll probably start
to see threats coming before they can
suddenly appear on my radar right so
there there's things like that where you
can use actual understandings of stuff
to help shape your probabilities now in
the specific case of asteroids I mean we
have the chick salub impact crater of
the asteroid that killed the dinosaurs
and that would probably kill Humanity
too leaving only well it would kill a
large fraction of humanity probably
leaving a few survivors in caves but
anyway like an asteroid like that or
bigger is indeed like a very big threat
so is this a success case for
frequentism yeah I guess right I mean
there's like a standard estimate that
like yeah in a 100 million years or so
we're going to expect another species
killing asteroid and like that is in
fact an existential risk it's easy to
predict and you don't see a lot of
asteroid doomers raising the alarm
acting like a really big asteroid is
going to come in the next hundred years
right like nobody's saying that because
we all agree that this is a case like
yep um the universe just slings around
these asteroids and they all have these
predictable trajectories and there's no
particular reason to expect something to
come in 100 years rather than 100
million years so it's precisely because
we understand that asteroids are simple
and they just follow simple mechanics
that you can model as Newtonian
effectively it's because we know that
that we're then comfortable just using
frequen statistic so I don't have a
major disagreement with what he's saying
I just wanted to point out that one
nitpick we have no such theories for
existential risk um people have tried to
come up with theories one theory is that
you know once the number of computations
um in in AI system equal the number of
computations in the human brain that's
when we'll have transformative AGI but
that tells us nothing about whether this
AI is like likely to lead us to sort of
the lack of control scenarios that are
talked about this strikes me as perhaps
not a logical non Seer but certainly a
very sloppy way to unfold his argument
he starts by saying hey we don't have a
theory
of super intelligent AI extincting
Humanity I mean we may have a theory but
we don't have a theory that has this
giant set of data like asteroid
trajectories that can make us feel
confident drawing a graph and at least
checking out that if the patterns hold
then we're good we just don't have that
level of quantitative data that
everybody agrees on is a totally
relevant pattern to extrapolate so
that's fair right and a lot of theories
are just like one-offs like that right
like the theory that says hey eukaryotic
cells have mitochondria because at one
point the mitochondrian was an
independent organism and then they
merged well we don't have a bunch of
data points proving that right it's just
like a really plausible theory that has
like a bunch of strong arguments and
most people agree it's true and you can
keep arguing for it but you're not going
to get like a bunch of data points of
like repeatable events because it
probably happened once in evolutionary
history it was like a pivotal thing that
happened so the part where he says hey
we have no such theories for existential
risk he's correct that it's not one of
those super quantitative extrapolation
ations like hey an asteroid is going to
kill us in 100 million years because of
this data um which I'll agree but that's
true of many great theories right like
the theory of mitochondria endosymbiosis
that's like a very important theory in
biology and it also doesn't have a
similar graph you can draw and plenty of
theories are like that now the red flag
for me is what he said next where he
started getting into the theory that
some people have I don't have that
theory but I guess Ray curtz has that
theory when he mentions uh the number of
computation in an AI system equals the
number of computations in the human
brain that's when we'll have
transformative AI again it's not my
claim but the fact that he dives into
that claim as if that's a loadbearing
claim of the AI Doomer claim that's not
how the AI Doom claim is structured the
AI Doom claim is a conditional claim
that once we have super intelligent AI
then it'll be uncontrollable and then
it'll cause Extinction so the fact that
he's already getting into the Weeds
about some particular person Ray curtz
some particular person's Claim about
when we're going to get super
intelligent AI he's just being sloppy in
how he's structuring his argument and I
may be nitpicking but I feel like I'm
going to be nitpicking him a lot because
I feel like his whole argument against
doomers is accusing us of being sloppy
so I do want to point out anytime I see
him as being quite sloppy because
ironically I think he's guilty of what
he's accusing me of but more importantly
this assumption itself is extremely
tenuous we don't understand as well
right now whether this is precisely what
leads to intelligence whether there are
other factors in fact I would say many
neuroscientists would vly disagree with
the fact that the number of computations
is what leads to intelligence again
that's not part of the Doomer claim to
the point where it's kind of a noner
that he brought it up this early in the
conversation like make your actual
argument please um and now this brings
us to sort of the final part the part
that I set aside for the moment so
lacking either inductive theories um so
a reference class for making inductive
probability estimates or deductive
theories about how AI risk might come
about what the community has largely
turned to when forecasting the risk of
AI Extinction are subjective probability
estimates I disagree that I don't have a
quote unquote deductive theory of how AI
existential risk might come about so
intelligence studies is a real field of
study when a go engine is able to beat
you at go we can say that it's more
intelligent than you are over the domain
of go when it can beat you at a math
Olympiad it's more intelligent than you
with respect to that math Olympiad
you can try to shrug it off and say no
no no it's just doing this trick but it
doesn't matter what that trick is doing
because the definition of intelligence
is actually relative to the problem
being solved so anything that's capable
of solving a particular problem in a
particular domain is intelligent with
respect to the domain so anyway I'm just
explaining a deductive Theory a
deductive theory is saying hey an engine
that can beat you in this domain you'll
be powerless in that domain right so if
you build a better chess engine than you
and you were hoping to determine what
the end state of the chess game board
was going to be my deductive Theory
tells you that you're not going to be
able to do that you're going to have a
loss of control with respect to chess if
you build an AI That's better than you
at playing chess and the shape of the
Doom claim in the physical Universe it's
very straightforward deductive logic to
say hey if it's better than us at
optimizing the physical Universe then
you're going to experience a loss of
control in the physical universe so that
part is quite deductive and then it
largely comes down to asking well are
such engines going to be built is super
intelligence a thing that can exist in
the universe or are things always going
to be kind of close to human
intelligence so you can ask that
question but to accuse my model of not
being deductive while you go ahead and
you let the theory of evolution or the
theory of endosymbiosis or the Big Bang
Theory or the theory of evolutionary
psychology all of these other theories
I'm naming are perfectly well Justified
theories even though it's not obvious
which pretty graph justifies them it's
not obvious what the numerator and the
denominator are in setting their
probabilities of being true because the
world doesn't factor into these
frequentist patterns that you apparently
think that it does so the fact that
you're not going after all these other
highly Justified Theory and you're
singling out the theory of intelligence
and the argument for AI Doom as if we
somehow have bad epistemology that's
your arguments to make because I
personally think that I have good
epistemology I'm explaining my
epistemology and you're coming out with
sloppy arguments that are intended to
prove that I have bad epistemology and
when you say the community has turned to
subjective probability estimates it's
almost always quote unquote subjective
probability estimates if you want to go
purchase a business like Warren Buffet
would and you want to find the fair
value of the business using the formula
for discounted cash flow well that
formula every variable in that formula
is going to be subjective if you want to
do Gap accounting every every variable
in accounting is subjective if you want
to be a super foraster and forecast
anything unless it's the trajectory of
an orbit that you can predict very
precisely right Quantum experiment that
we have to many digits of precision
usually you have all this math you have
all these formulas but the variables in
the formula have a little bit of
subjectivity and what the subjectivity
means is you take the full complexity of
the human brain which has a a subtle
model about the universe that's more
subtle than what we can explicitly
describe and an AI could have the same
thing but with a higher ability to
explicitly dump it out but anyway you
take all that subtlety and you use it to
tweak your subjective estimates but
anyway this idea that making pred
based on strong models is going to have
subjectivity that is standard so I
really resent how he's singling out dors
and being like oh wow these guys are
turning to subjective probability yeah
it's called basian probability that's
how the world works and look around okay
cuz I feel targeted now what subjective
probability estimates do is that they
sort of feed into our cognitive biases
to take quantification seriously so I
think um all of us have many cognitive
biases quantification bias is the
tendency to take um events that are
Quantified more seriously that ones that
aren't and so when someone says that the
P Doom or the probability of Extinction
from AI is 15% we might take it more
seriously than we vote claims about um I
don't know the risk of AI going Rog
being high I've never seen a single AI
Doomer try to exploit quantification
bias by using more digits of precision
than they have I personally say that my
P Doom is 50% that's not
50.0% right how many significant figures
are in that probability less than one
when I say 50 I really mean what Yan Ley
formerly of open AI famously said
formerly he was their head of AI safety
he said 10 to 90% right is it really 10
is it really 90 is it really 50 it
doesn't matter what matters is that if
you say it's only 3% that's clearly a
crazy bad probability to give cuz it's
too low and if you say it's 97% you're
also probably not thinking straight it's
too high so I can tell you with high
confidence that it's ridiculous to call
it 3% or 97% if you call it 60% I think
that's fine if you call it 70% I think
that's fine if you call it 12% I think
that's fine although I think you have
too much Precision right so that's what
I say when I say p Doom is 50% a lot of
people are very quick to laugh that off
and they're like oh my God they on
saying nothing no I'm not saying nothing
50% specifically one thing that it means
is it means it's definitely not .01% %
right so can I tell you if it's 50% or
40% it doesn't matter I could easily
shift between that on on any given day
but would I ever tell you it's 0.01% no
0.01% is crazy so I'm telling you
something meaningful when I'm
eliminating the range of low
probabilities if somebody's making
decisions as if P Doom is super low or
super high then I claim that they're
basically they're blind to to this
obvious synthesis of the facts we know
about reality the facts we know about
real tell us that Doom is a very Salient
danger right so now I'm just using
English I'm saying very Salient but
instead of saying very Salient I can
tell you hey something in the 10 to 90%
range just like Yan Ley of open AI did I
think it's very presumptive of sash to
be like oh yeah Yan Ley of open AI is
like trying to fool you he has bad
epistemology because he's using
probabilities it's like no he is doing a
great job when he says 10 to 90% because
just to reiterate he's pointing out that
somebody who comes out saying 1%
probably doesn't have their head screwed
on straight and that's a very meaningful
statement that does not draw on
quantification bias if anything the idea
that somebody would come in and claim
hey P Doom has a 0.1% probability who's
the person drawing on quantification
bias right now is it the one like me or
Yan Ley who's saying 10 to 90% or is it
the one who's saying Doom is so unlikely
it's 0.1% 0.1% that's a
999 to1 odds ratio right that's like my
$1 for your $999 your 9 $99 says that
we're not doomed who's the one who's
trying to draw on quantification bias in
that scenario I don't think it's me
right so I think SAS is just way off
base here accusing doomers of trying to
use quantification bias to their
advantage if we look at all of the
evidence that the community making
predictions about Aid Doom have given us
um the underlying reasons for these
probability estimates are extremely
tenur so one of the most important I
believe uh prediction tournaments that
happened recently was the extinction
prediction tournament
um it was carried out by Philip tlock
and his colleagues and they basically
asked um I think 100 or so AI
researchers as well as super forecasters
that is people who had excellent records
at forecasting events in the past to
participate in a prediction tournament
that is to predict um the possibility of
x- risk um and it turns out that if you
look at the reasons that people gave to
justify their abilities those reasons
are no better than what we might expect
from anyone like on the street who is
thinking about AI risk um reasons
included things like well maybe AGI
would decide to colonize space instead
of the earth and so they'll reduce uh
their AI doome risk and and so on that's
a terrible reason so whoever said that
is just not educated in the theory of AI
Doom right because it's a pretty
significant minority of AI doomers who
would tell you that AI doesn't want to
colonize the Earth before it colonizes
space right I mean that's just almost
certainly wrong right so if a super
forecaster really thinks that it's it
seems like they've taken some license
with not studying Aid Doom Theory and I
don't see how that's excusable when
you're trying to forecast how AI Doom is
going to play out to not actually
understand why it's so important for an
intelligence to use the resources on
Earth before spreading to space like
those resources are right here um and so
what we really take issue with is
dressing up these feelings as numbers
and making it seem as if there is a
precise estimate behind claims that
there uh there is a high risk of
Extinction from
AI so again no is claiming Precision
right so you're accusing AI doomers of
being
precise you can look on the record I
mean again let me give you exact quotes
yon Leakey 10 to 90% Dario amade 10 to
25% Jeffrey Hinton I think it's more
than 50% but based on what my friends
are saying I've updated down to 10 to
20% none of these three statements from
some of the top luminaries in the field
are trying to use false Precision
correct
again the person who says I'm so
convinced that AI Doom is fake that we
should treat it like it has a 0.1%
probability whose probability is more
precise there's actually no way to have
a very very low probability of AI Doom
without that probability being precise
because you could be like oh yeah yeah
just really really low just like below
0.1% it's like wait a minute below 0.1%
so bigger odds than 999 to1 and again
who's the one being precise you're kind
of projecting here so your your article
um argues that there are many many
reasons why these risk estimates might
be you know systematically inflated I
mean can you can you tell us about some
of those reasons absolutely so when we
look at the metrics that are used to
evaluate these forecasters we find that
um they severely
overestimate uh the probability that
events that might not ever happen or
might happen very like infrequently also
known as stale risks in a systematic
fashion so in the blog post we do an
analysis of one such example so let's
say we have two forecasters uh
forecaster 1 and forecaster 2 um
foraster one always makes the correct
probability estimate based on their
prior forecaster 2 makes the exact same
estimate except if the risk is less than
1% they always predict the risk to be 1%
so the relevance of this statement is
presumably supposed to be that doomers
like me go around taking a tail risk and
acting like it has a 1% or more
probability when the real probability
should be like
0.001% but it's just so easy to round up
those tail risk and overinflate those
tail risk so that's the Crux of the
issue then is AI Doom a tail risk is it
just like some unexpected event that
might happen if we're unlucky or is it
more of a Mainline risk right are we
just heading toward AI doom and in my
view it's Mainline because creating a
super intelligence without knowing how
to control it doesn't lead to a good
outcome right that's my Mainline risk
that already suggests a ballpark 10 to
90% type of probability which is why
actual industry experts are largely
agreeing with that estimate right and
brushing It Off by pointing out that
super forecasters overestimate tail
risks I just I don't see how that's
relevant to a situation where the
experts the Doomer experts are just
arguing that this isn't even a tail risk
so why are we talking about tail risks
you you mentioned the the dangers of
utility maximization when it comes to X
risks how should policy makers approach
this problem without falling trapped to
Pascal's wager and if you wouldn't mind
could you explain what Pascal's wager is
absolutely so Pascal Zer is this very
interesting thought experiment about
whether one should believe in God um so
it states that one should always believe
in God because if it turns out that God
does indeed exist and one does not
believe in God there's an essentially
infinitely negative utility function to
that uh to that call because it's like
an infinite Life In Hell whereas so if
there's even a small possibility that
God exists one should always believe in
God um so this is an example of the
dangers of utilitarianism when dealing
with uh making consequential decisions
um because let's say let's take the
example of X risk um what if we try to
estimate the utility the expected
utility of uh existential risk even if
we have a very low probability now some
of us and like reasonable people might
conclude that the cost of humanity going
extinct is essentially infinite there's
probably nothing worse that could happen
and therefore if you have even a very
small probability that is nonzero that
Humanity goes extinct you're
incentivized to always do in everything
in your power to stop existential risk
so if you were to take this seriously if
you were to adopt a purely utilitarian
um expectation maximization um Theory um
and apply it to real world decisions all
policy makers would ever do at any given
point of time is talk about X risk I'm
sorry at this point anybody who ever
brings up Pascal's wager as if it's
relevant to the debate around AI Doom
should be embarrassed we're past that
for the simple reason that doomers don't
argue a low probability doomers argue
generally a probability of 10% Plus at
least 5% plus and what characterizes of
Pascal's wager scenario isn't just that
it's dealing with high utilities it's
that it's dealing with small
probabilities the point of invoking
Pascal's wager of saying oh my God look
at this huge utility the point of doing
that is to compensate for a tiny
probability so if I were to say hey look
I know that AI Doom is only .1% chance
but it's so important to solve AI Doom
because the consequences are so large if
that's what I was saying then you could
accuse me of arguing for Pascal's wager
trying to distract you and have you
focus on the high utility so that you
can neglect the small probabilities but
as I keep repeating nobody's saying
nobody on the Doom side is saying that
the probability is that low we're
generally saying 10 to 90% that's kind
of where the doomers are let's say 5 to
95% to have kind of a big tent right 5
to 90% again you might think I'm crazy
for saying 5 to 95% I don't think I'm
crazy because I'm ruling out the sub 5%
area where Pascal's wager is even
relevant right why are you talking to me
about Pascal's wager when I'm telling
you that the P Doom is more than 5% or
more than 10% I'm giving you such a high
P Doom that even if the utility
consequences even if a mere $1 trillion
were at stake and not the fate of
Humanity's entire Intergalactic future
but just a mere $1 trillion even then
you would want to pay attention to me
when I'm talking about a 5% risk of
burning a trillion dollars or you know
10 trillion whatever some Earthly number
as opposed to some crazy astronomical
number just an Earthly number right the
the probability is already so high 10%
whatever it is it's already so Salient
that even an Earthly number like one
trillion should already get you out of
bed so why are we talking about Pascal's
wager why is somebody criticizing
doomers and using that keyword Pascal's
wager frankly I should never hear that
keyword anymore it's not relevant to the
Doom debate if we were talking maybe 50
years ago and somebody was saying the
probability that we're going to have
uncontrollable AI in the next decade
maybe the probabilities would get small
then but if you talk to ders today we're
not throwing around 1% probabilities
we're all at the 5% 10% 50% 90% level
okay so get with the program don't say
Pascal's wager the one thing I'll say is
you didn't say Pascal's mugging there's
this thing called Pascal's mugging which
is an even more extreme version of
Pascal's wager that we don't have to get
into but accusing doomers of Pascal's
mugging is like an even dumber version
of accusing doomers of Pascal's wager I
hope to never hear it again I suspect I
will because I've already heard it from
multiple smart people but please study
up don't accuse doomers of Pascal's
wager it's just an objectively wrong
accusation of what we're claiming how do
scaling laws compared to other scaling
plateaus you know like air airplane
speeds for example um so there's strong
tendency in like the field of AI in some
parts of the field of AI but more
broadly in the field of people talking
about AI to assume that um exponential
Trends can hold indefinitely um I think
very recently I read this piece called
situational awareness that draws these
trend lines that go all the way up to
the right um and argues on this basis
that we'll get to AGI sometime very soon
now our piece on AI scaling mits was not
a direct response to the situational
awareness piece a lot of people thought
it was but we' actually been working on
it for for a little bit of time um but
essentially if we look back at the
history of what happens to things that
people have thought are exponentials
what happens when we sort of look past
let's say the first few years of this
type of exponential we found that uh
people have largely been disappointed I
think there's a saying that goes every
exponential is a sigmoid looking
backwards um and so this is exactly what
happened with airplane speeds um I think
in the early 1970s or late 1960s people
were booking flights to the Moon based
on the increasing trend of airplane
speeds at the time and then essentially
all of a sudden these increasing
airplane speeds came to a halt uhoh but
I thought they were using a good
reference class I thought they were
looking at the data and using a
frequentist probability estimate I guess
that's not a reliable methodology huh I
guess we need to actually understand
things and make predictions based on
deep theories um we had the saturated
around the speeds that we have today um
and the reasons for that were a little
bit technical a little bit business
related but I essentially it shows the
Perils of uh thinking that trend lines
can continue forever um I think there
are always bottlenecks that arise when a
previous bottlenecks uh when previous
bottlenecks are fixed and that's how
we've also seen progress in AI if we go
back um I don't know 50 years um in the
last 50 years we see that AI progress
has been a punctuated equilibrium so
every time we have a new paradigm
there's a lot of uh like resources a lot
of um intellectual as well as material
resources that are spent in improving
this Paradigm um and this is how
progress takes place until we've
saturated a paradigm uh and once we
saturate a paradigm there is essentially
a period where we're looking for the
next Paradigm and progress sort of slows
down it becomes the sigmoid not the
exponential anymore so I actually do
agree on an important point which is I
don't take the curtz perspective of like
yeah let's just extrapolate Moore's Law
let's naively extrapolate a trend to get
to a prediction about when we're going
to have super intelligence my position
is yeah we're pretty clueless about when
super intelligence is coming I generally
estimate it to be like 1 to 30 years
away the reason I'd be surprised if it's
more than 30 years is something I've
said in the past where it's just like
it's getting hard for me to name what it
can't do I've always had these
convenient bounds of things I could tell
you a I couldn't do like hey it sucks at
chatting right it can't even write me an
essay it doesn't understand English
whereas now it's like okay it kind of
understands everything but it starts
being bad at it once it like says more
than a few sentences but like the next
version is going to get you an extra
sentence it's starting to get hard for
me to draw a boundary about what the AI
can and can't do and so that's why I get
the intuitive sense of like I don't
really know what else is left besides
just like reflecting back on what it's
doing like it's it's hard to explain
right and so that that's why I'm saying
it's it's hard to imagine that 30 years
from now we still won't have cracked it
but I certainly give that a significant
chance right I'm not claiming to be like
we're doomed because I'm extrapolating
this graph no no no that's not my claim
my claim is just I think that we'll have
super intelligent AI eventually if we
don't get it in a year or a decade I'm
quite sure we'll get it in a century
right more likely than not I think
that's a pretty mainstream position my
Doom claim is just if and when we get
super intelligence probably when not if
at that point unless we know how to
control it much better than we know
today today we don't have a clue unless
we know how to control it then we're
doomed because it's going to make
Humanity lose control it's just going to
strip us from our causal influence over
the future and it's just going to have
all the causal influence the same way
that a chess AI has all the causal
influence over the winning configuration
of the chessboard similar Dynamic that's
my whole Doom claim okay so I do think
that we're getting sidetracked talking
about oh naively extrapolating Trends
fine you're right I don't want to
naively extrapolate Trends okay so the
interview with scash goes on for like
another half hour but this is the end of
where he talks about his claims that
doomers are like misusing probabilities
or making claims that are somehow not
epistem sound the title of sc's blog
post that he was being interviewed about
is AI existential risk probabilities are
too unreliable to inform policy which of
course I disagree about because when you
have half the experts in the field
warning you urgently that in the next 1
to 30 years the entire human race is
going to go extinct and their
probability estimates that they're
putting on it is in the 10 to 90% range
even without much Precision I do think
that you want to start shaping policy
around that as opposed to being like H
let's just not shape polic around that I
think that it would be more prudent to
shape policy around that so I just I'm
not moved At All by what he's saying on
an epistemological level and as you saw
I had my nitpicks like it just doesn't
seem like he's getting how epistemology
Works what a Wells supported Theory
looks like even if it doesn't have a
pattern of statistical data supporting
it or conversely I didn't even mention
the case where you might have a theory
that has a bunch of data supporting it
and naively you just extrapolate the
data and be like wow surely this theory
is true for example I could give you a
ton of data showing that Newtonian
physics is just correct it's an accurate
description of the universe and for a
century or two people just thought
Newtonian physics was correct but then
they just started seeing some cracks
like oh wait a minute this could be
self-contradictory when we're dealing
with fast moving charge particles or
wait a minute this contradicts
observations of the planets a little bit
there's like some inaccuracy here but
before the crack started showing you
might have had people be like you know
what this is
99.999% likely to be true because I have
like 100,000 data points of all types
the data points are all fitting and so I
have a lot of confidence that Newtonian
physics is true and anybody who comes in
with this new theory of relativity and
doesn't have a bunch of data points to
present on day one why should I listen
to them I'm so confident so you could
get really confident about Newtonian
mechanics and if somebody came along and
said you know you can push something
arbitrarily hard and it's never going to
go faster than this particular speed
limit the speed of light they might be
like what are you talking about a speed
limit everything's linear I understand
Newtonian mechanics where are you
getting this so it's also possible for
data points to just drive you off a
cliff in terms of making you too far
convinced and once again the takeaway is
just we can't do epistemology based on
extrapolating graphs we have to use a
more fundamental version of forming
hypotheses noting how those hypotheses
compress data and giving them
probabilities accordingly and just doing
bass like this framework is called Baye
I'm sorry that I have to explain the
first principles of Baye because people
like refuse to get it and just kind of
make ad hoc arguments without
understanding vasi and reasoning uh just
one more response I wanted to give to
the piece as a whole what's the
alternative to reasoning with
like there's a 50% chance of Doom I feel
like there's a 10 to 0% chance of doing
if I just don't say that what am I
supposed to do instead how am I supposed
to recommend policies without
quantifying some giant ballpark estimate
of Doom in practice what happens when
people don't use abilities is just that
they use English words to accomplish the
same thing so the English word
significant chance of Extinction is now
just substituting for a range like hey I
feel like 10 to 90% it's the same thing
it's just that the number is a little
bit better because the number doesn't
require everybody to implicitly
translate the number in order to make
policy like now I don't have to clarify
by significant do I mean more than 1%
why yes in fact I do I mean 10 to 90% so
the number actually is conveying
information um they Do not spell out an
alternative way to reason or conduct
policy all they do is basically throw
shade on the idea of using a probability
and it's just like it's just not even
specific to this particular debate right
it's just this epistemology level debate
of like oh my God our basian
probabilties meaningful and the most
recent piece of evidence that gave the
answer a simple yes is if you just go on
manifold markets manifold. markets or
you go on poly Market you just look at
modern prediction markets they have an
incredible track record there's actually
a study that that shows that their
calibration is near perfect so even
though they predict all kinds of things
all kinds of events that don't have uh
inductive or deductive models to use
sash's language even then their
calibration is somehow near perfect so
apparently we as humans can somehow give
calibrated probabilities and you don't
understand how but it's just an
empirical fact that we can so why don't
you inductively extrapolate the
empirical fact that probabilities are
working really well on these prediction
markets and extrapolate that to
concluding hey maybe when doomers say 10
to 90% then we should make policy as if
the probability is 10 to 90% as opposed
to like ignoring that and using some
alternate framework that I haven't
specified okay I think that's all I have
to say about that I will add that
overall listening to sc's podcast I like
him I think he came off as very smart
and informed just not on this particular
topic the topic on this post where he
titled it AI existential risk
inform policy I think he's way off base
and while his points do make sense like
I get where he's coming from I think he
totally lost the plot in this particular
post so I would encourage him to really
reconsider whether he's talking any
sense in this post but I do like a lot
of his other points and I like him in
general and I hope that I can come back
on this podcast and say how something
else he wrote is actually awesome that's
all for me but if you want further
reading I highly recommend a post by
Scott Alexander from March of this year
called in continued defense of nonfree
frequentist probabilities highly highly
recommended post I hope scas checks it
out because it contradicts a lot of what
he's saying besides that if you believe
in the Doom debate's mission of moving
the overturn window so that the larger
population realizes AI Doom is real it's
urgent the arguments for it are strong
the people who are acting like it's
going to be okay don't know what they're
talking about if you support the mission
of building shared common knowledge
about that then it's pretty easy to help
the podcast out just share it in
whatever Forum you're posting on share
it on social media tell a friend about
it and then make sure you yourself are
subscribed at youtube.com/ Doom debates
Doom debates.com for the substack
subscription twitter.com LeRon and also
your podcast player search Doom debates
also bonus favor go to Apple podcast
search Doom debates leave a positive
review there that also helps a lot thank
you appreciate it and I'll see you next
time here on Doom debates