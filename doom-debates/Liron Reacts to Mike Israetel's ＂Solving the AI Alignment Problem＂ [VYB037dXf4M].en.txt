we should not want to impose our values
on AI because our values are [&nbsp;__&nbsp;]
half-baked AI can bake them more fully
than we ever could and I think we should
want to hear from AI about how our
values can be
[Music]
improved welcome to Doom debates today
I'm reacting to Dr Mike Israel a very
well-known bodybuilder Fitness
influencer he has over 600,000 Instagram
followers he has a PHD in sports
physiology from East Tennessee State
University so that's why he's Dr Mike
and he has a podcast called Mike Israel
making progress and episode number 35
released three months ago is called
solving the AI alignment problem so he's
branching way out of Fitness into
different areas of life which is cool
and you'll see I think he's actually a
very intelligent guy I'm glad he's
engaging with the topic Dr if you're
watching this and you want to come
debate me live please do you're more
than welcome to I think your podcast is
great overall and you know I've been
known to go to the gym and do a little
strength training myself sometimes so I
think we're really two P's in a pot I
think we're going to be sympatico all
right let's dive
in hey folks Dr Mike here for the making
progress Channel today uh we will be
talking about how to solve the AI
alignment problem I don't claim to know
anything more about the AI alignment
problem than any other dilatant who has
some free time to look into this stuff
but I have given a lot of thoughts so I
could be wrong about all this um sure
[&nbsp;__&nbsp;] not a um AI researcher or some [&nbsp;__&nbsp;]
like that but I will tell tell you that
a lot of the stuff that I have
seen from various folks who are
supposedly experts in the field they
haven't thought it through seemingly as
much sure it's always possible I haven't
thought this through I have been
thinking about it for 17 years and
thousands of hours but educate me Dr
Mike maybe I'll learn something not to
throw shade on anyone specifically but a
lot of the Dooms doomsday scenarios are
uh and and sort of attempts and
caricatures of the problem of AI
alignment um kind of betray an
anthropomorphic sort of thinking that
isn't really treating AI like it should
be treated I'll attempt to do that here
let's see if I [&nbsp;__&nbsp;] it up sounds good
yeah you wouldn't want to
anthropomorphize it too much so first
what is the AI alignment problem it
looks like AI is on track to become
smarter than humans this is probably one
of the most benal predictions I'll ever
make it's not an original prediction of
mine people as far back as Ray Kurtz in
the early 90s uh have made that
prediction and it's just based on
Computing Trends uhhuh we know the known
limits of the human brain and its
processing power and we know digital
brains in the theoretical limits even
based on existing technology are way in
excess of human brain power and with
predictable evolutions of Technology as
they've always occurred outrageously
much higher intelligence is not just
possible with AI but again as
predictable as the success of the
internet once you understood what the
internet was and what it did and how
fast it was growing okay this guy sounds
intelligent so far if AI is aligned with
humans that means it's working towards
our interests to some extent then
because it is so
powerful that gets kooky real fast and
as it climbs up then the smarter than us
by first a mile then 10,000 miles then
so on and so forth AI can open up
Paradise yeah standard trans humanism
I'm on board with that claim if AI is
aligned it can open up Paradise sure
however if AI is not aligned with humans
in other words it's working for its own
interests or against ours it could also
do a couple of other things it could
start a mega costly war with us because
it's artificial super intelligence it's
probably going to win a mega war against
us if it decides to annihilate us it's
probably going to do that with insane
amounts of ease I'll talk about a little
bit how that would probably occur if AI
wanted to attack us how it would do so
not good for a Hollywood movie does not
last long enough to be a feature film
and you know etc etc uh could lie to us
about being in our side than just at the
right moment annihilate annihilate us
with ease when it's powerful enough bad
things and plenty of people have talked
about the bad things and they could
legit all definitely happen if we really
really play this wrong it's definitely a
possibility okay I like that he's saying
super intelligent AI fighting us
wouldn't be like a Hollywood movie it
would just instantly win the movie would
be over in like 1 second I agree on that
part it seems like he sees that as like
an outside chance so his P Doom seems to
be like 5% or some low number he's
saying like oh we'd really have to screw
this up if we wanted to get slaughtered
by AI whereas I'm thinking like nope
this is the default path this is
definitely where we're heading nobody is
competent to avert this path so my pdom
is like 50% but at least he understands
what we're dealing with in a worst case
scenario which I like the stakes are
quite literally between Paradise or
something like it on the one hand and
the end of the world as we know it on
the other doesn't really get more high
stakes than that it's also very
difficult to imagine a world in which
unaligned ai ai that's not against us
but not for us just kind of doing its
own thing is not a major player in
global Dynamics for example the lions in
the SAR Andy humans aren't necessarily
against Lions but for a long time humans
weren't aligned with lions just a few a
super intelligent lion you wouldn't tell
your other lions like don't worry about
the humans or whatever like humans are
so much smarter they have started to
affect the global climate of the whole
[&nbsp;__&nbsp;] planet that's how intense it is
and artificial super intelligence can
restructure the entire known universe
around us for the love of God it's
definitely going to be some kind of
interaction right it's going to be a
major
player it's something as a reminder that
could be more than 1,000 thousand times
smarter than the whole human race
combined yes exactly so this guy has
what I call order of magnitude
perspective like he sees that this is a
big freaking deal which a lot of people
see it you know Sam Altman Dario amade
these guys see it Yan Leon from meta
might not see it in the next few decades
but it is a common perspective to
realize this but not common enough so
I'm glad that he realizes this and he's
talking intelligently about it and his
point about hey how can you avoid
interacting with this right tigers are
going to be influenced by humans even if
humans aren't directly setting out to
kill Tigers because the impact is so big
the side effects are going to be huge so
he is actually on point making that
observation I agree this is 2024 filming
this within 15 years from
now and I'm not saying artificial super
intelligence is going to be born in 15
years most of the predictive modeling
says it's going to be born in five but
that 1,000 times smarter than the whole
human race combined thing yeah that's 10
to 15 years away in most of the models
and as the models are revised over time
the predictive timeline shrinks down to
actually the year 2029 curiously 2030
thus being the birth of true artificial
super
intelligence getting it right making
sure that the process that gives birth
to AI also makes AI at least somewhat
friendly to us seems like the biggest
deal ever and in fact it is I will also
say that the birth of artificial super
intellig Ence is one of the landmark
events the evolution of life on Earth an
event similar to the development of the
procaryotic cell similar to the
development of the eukaryotic cell and
thus the first technical multicellular
organism similar to the development of
the first brains yet all of life on
Earth used to not have brains there were
no brains Plants don't have brains
bacteria don't have brains but they
still reason and interact with the world
around them the birth of
brains was really really powerful and
transor of about how it change things
the birth of societies was another such
step the birth of artificial super
intelligence is absolutely one of those
steps probably the most transformative
one of all this is a big deal and
curiously enough if you hang in for
another oh five years or so you could
witness this event and you're already
witnessing the preceding parts of this
event I like this guy I mean his
perspective is totally on point this is
that big of a deal like it's a Universe
Scale deal he's got it so clearly he's
intelligent he's saying a lot of the
right stuff uh it's kind of funny that
he's going to start saying stuff that I
think is quite naive about AI alignment
but so far this is a great intro now
there is a chance that artificial super
intelligence super intelligent AI might
just study us uh make our lives perfect
or even digitally copy us and let us
become immortal in the cloud anyway
without alignment maybe because it wants
to learn as much as it can about the
world which is generally a trend of
intelligent things wanting to learn more
and more it's unlikely to just toast us
all because there's lots of information
in US and to want to understand as much
Nuance as possible probably I wouldn't
bet my [&nbsp;__&nbsp;] life savings on
that yeah this is an argument that Elon
Musk keeps repeating he's basically
saying look AI is going to be interested
in us because we are interesting and
it's just going to be curious and it's
like yeah maybe right that's kind of
like a Hail Mary but it's a long shot
because optimizing the universe just
doesn't really go through researching
humans in a way that doesn't torture
them or in a way that preserves a life
that they like I mean that's really
quite an optimistic scenario where you
put yourself into the ideal scenario
that the AI wants instead of just
admitting that the AI wants something
which is an optimized Universe State and
you're probably not part of it because
you're a very arbitrary configuration of
atoms and there's almost nothing that we
humans want that involves the exact
configuration of atoms that other
animals are in like cows okay we grow
them for their meat and milk but if I
could snap my fingers and just get the
exact meat and milk that I get from a
cow I know it's a little controversial
now but pretty clearly that's where
we're going as humans right and that's
where the AI is going to be going in
terms of not needing us around even if
it is curious even if it does think
we're interesting it's not going to
actually have us around that being said
it might be just as easy for AI to toast
us in a second so getting the best shot
alignment is a really really good idea
correct also this is definitely a one
shot sort of thing as soon as AI
outpaces our abilities we're not going
to restart this process one once AI is
free and doing its thing it's kind of
[&nbsp;__&nbsp;] going to do its thing yep we
have one chance to build aligned AI
before we build unaligned Ai and that
window is closing fast the Terminator
movies and Skynet do a terrible job at
describing what a war with AI would look
like um a war with AI might look a lot
more like m-size microscopic
undetectable flying robots that enter
through your nose here let me let me
tell you the whole story of how this
could easily happen AI is wonderful and
it's making us wealthier and
everything's going great at the same
time it's built a series of microscopic
machines that underground silos 500 met
below the ground in every major
continent are manufacturing millions of
these tiny m-sized robots and upon a
certain time it releases them no one can
detect that they are released they're
beyond the scale of human ability to
perceive and they spread all the way
around the world and just spend their
time sitting close to or around humans
and in within a fraction of a minute at
any one point when they're activated
they fly up into every nose of every
human and get into the brain and go
after that very part of the brain that
reminds you to breathe all humans are
breathing and in one day everyone stops
breathing I'm not that smart but even I
came up with that [&nbsp;__&nbsp;] yeah I mean that
kind of scenario is what it feels like
to be facing a super intelligent
adversary right the only way to get an
intuition about it is to imagine people
from a thousand years ago facing modern
society today we live in a very science
fiction world and imagine science
fiction squared right try to have that
same reaction that's really the only way
to understand what it will feel like to
face such an adversary the kind of bombs
we can drop today like obviously nuclear
bombs there's no other word for it but
science fiction or fantasy or Miracles
or God when you show it to somebody from
a thousand years ago if a deer thinks
how would a hunter kill me probably to a
deer's mind it's gonna just run up to me
it's gonna try to he's gonna try to bite
me with his teeth and I'm gonna gash it
with my horns and maybe I can win and
maybe it'll kill me false one bullet
through the brain from 250 ft out you
don't even hear it coming drop dead as
soon as it happens yep solid analogy a
dear trying to imagine how a human would
kill it now that's really bad we don't
want this to happen so we have a lot of
candidate hypotheses presented by a
bunch of people as to how to make sure
AI is line properly and I think a lot of
these suck quite frankly and I would
like to explain why so some of these
hypothesis are something like this we
want to cap or trick the AI and I think
this is a non-starter hey let's just
prevent AI from growing in intelligence
keeping it just dumb enough to be
subservient to us just a little less
intelligent to us kind of like the
droids in Star Wars would still be
baller but it's not so smart that it's
way smarter than us and then it's kind
of in charge or people will say hey AI
development is going real fast let's
really slow it down really take the time
to let AI mature in Morality before we
make it much smarter because we got the
morals right when it's smarter it'll
help us but if we got the morals wrong
when it's smarter it's going to [&nbsp;__&nbsp;] us
up so let's slow it down down there's a
really really giant non-starter problem
with this
proposal there is no such thing as we
because not all of human race cooperates
currently other societies in the world
are rushing to improve AI as much as
possible to get ahead it's
obvious if we which just really means
the United States Europe and the free
parts of Asia Japan and Taiwan and
France and the United Kingdom and
America and Poland all the countries in
between we could definitely sign some
kind of agreement to say hey look we're
not going to develop AI past the certain
point there could even be potentially
potentially an enforceable agreement you
need a lot of you need a lot of Nvidia
gpus to develop AI properly and people
can track where they sold and so on and
so forth real kind of police state we
could pull that off potentially but if
you slow down the AI race in uh Europe
and the US and free Asia you know
China's well on the AI race itself and
then China might get ahead of the AI we
might as well build to build Skynet
first because whatever China's going to
pop out as far as AI uh Skynet might be
nicer than that not a good idea to cap
our own AI very very bad idea try to get
an AI first the values they start the AI
off with as a reasoning principle might
be real [&nbsp;__&nbsp;] gnarly so he's saying
hey we can't just inch our way towards
super intelligence we can't just make us
slightly smarter than human Ai and hope
to align it at that point because it's
an arms race and some other country will
make make a super intelligent AI with
bad values that's definitely an
important consideration that when you
get really close to Super intelligence
or when you get a weak super
intelligence all these open- Source
hackers all these other countries there
are going to be multiple parties who now
have this thing that they can just tip
over the edge of recursively
self-improving super intelligence or
just more and more dangerous super
intelligence so I agree that the more we
develop toward super intelligence the
more we're creating these matches right
this highly reactive substance that's
about to explode in our face if it
hasn't already I agree with that larger
point I don't even focus that much on
the race condition between nations I
think it's a race condition between
hackers and their basement right because
the AI keeps getting more optimized
eventually it's going to be optimized to
be more like the human brain where it
only needs 12 watts of power it can fit
into a tiny space it can just take on a
trickle of resources and it can still
end the world that's the scenario that
I'm pretty confident we're going to get
to the only question is when so I would
take his point that you can't just get
really close to Super intelligence and I
would extend it back all the way toward
now where and I would say hey you can't
even push the current Frontier more
because we already could be past that
point of no return where we've already
done too much we've already gotten so
close to the threshold of
uncontrollability where even stopping
today some hacker can take it and make a
few tweaks and run a super Intelligence
on their laptop I mean the Transformer
architecture is incredibly simple sure
it needs a lot of data but somebody
might tweak it to train itself on much
less data right we know that data is
redundant and like it feels like we're
getting close to that threshold of super
intelligence I hope that we're not I
hope my intuition is wrong I hope we
have another 50 years but we don't know
and we're being ridiculously Reckless
even now so I guess I kind of agree with
this point so far and I'm even extending
it but there's another concern that I
think should be
mentioned it's the downside risk of not
allowing AI to come as quickly as
possible many people don't talk about
this probably one of the reasons they
don't is kind of the fallacy and
economics of the seen versus unseen the
seen costs are always easy you know like
pollution goes and kills the animals
that's bad but the Unseen lack of
benefits if you really scale up uh the
industrial architecture you can develop
so much wealth that it saves all the
animals in 10 years is very difficult to
imagine because it's something that's
not occurring ah yes we have to think
about the upside case Okay so this is an
argument that I should fles out in a
separate episode but basically if you
have something that's about to kill
everybody and literally end the universe
or end the future of
humanity you need quite a lot of utility
if it goes right otherwise it's pretty
obvious you should just delay it a
couple decades or however long it takes
to align it so that you don't nuke the
entire future of the observable universe
with quadrillions of Minds right those
Stakes are so high that delaying 20
years becomes a very small price to pay
so if he's just making an expected value
argument and he saying look at the good
side it just doesn't really work when
you make that expected value argument
like you really just have to make sure
that you don't destroy Humanity in our
generation everything is a secondary
consideration to that so I don't even
know why he's going to spend that much
time focusing on the upside
So to that end AI is the most
transformative step in the history of
evolution is my belief not controversial
by the way most people think that's the
case if align remotely
decently its power to help feed clean
and Ascend the world of humanity and all
life on Earth will be Godlike in the
literal sense I would say even more than
Godlike because we can't even be smart
enough to imagine what a God of that
power would
do and I mean the following when I say
it every single day that we do not have
super intelligent AI with us yet or at
least AI That's steadily escalating in
intelligence with no cap every day in
which we delay that is a day in which
countless people die needlessly of
Warren disease countless animals are
slaughtered in farming and deforestation
the environment in many places is
polluted to the extreme so mik goes on
for quite a while about all the upsides
of having super intelligent AI now and
why it's costly to wait because of all
the problems in the world which I agree
with but the moment you bust out a
little bit of basic math the moment you
bust out an expected value calculation
you just can't risk nuking the entire
future of a quadrillion years times a
quadrillion mines and that's actually a
conservative underestimate you can't
risk nuking all of that if we're even
talking about a few perent Delta in the
risk of keeping the world alive and
keeping humanity and human values in
existence versus nuking them all even if
you buy yourselves another few Decades
of a good future if everything goes
right even if that's the upside it's an
insane value calculation to do and make
a decision now to build AI when you
could instead just be lowering the risk
a little bit before proceeding so I'm
skipping a chunk of Mike's podcast goes
through how super intelligent AI could
make life better for Humanity if it goes
right and we'll move on to talking about
his recommendation for alignment and why
I take issue with it the risk of
destroying all of humanity versus
ascending it into literal
Paradise yeah big stakes and if we're
trying to cap if we're trying to kind of
get the downside risk a little lower at
the huge expense of The Upside we can't
just say better safe than sorry remember
better safe than sorry is not a
Dependable principle of human action and
the precautionary principle to quote or
paraphrase theim TB is just dog [&nbsp;__&nbsp;] it
doesn't make any sense how much per
caution is the real question and at what
cost all I'm telling you here is if
we're really really really cautious
about AI China's just going to make it
anyway and no nothing matters going to
toast the rest of us yeah so the highest
value scenario by far is if all the
countries coordinate to put a stop to AI
development I agree that if we fail to
not build super intelligent AI however
we fail we lose a crazy amount of value
so it is imperative that we all succeed
together in not building super
intelligent AI That's my position but if
somehow China doesn't make it and we all
as a global society decide to delay AI
by 10 or 15 years to really get it right
we're going to be toasting billions of
people tons of fish tons of Planet tons
of animals and delaying [&nbsp;__&nbsp;] Paradise
which is immeasurably good for everyone
so we don't want to cap AI we don't want
to restrain AI That's a [&nbsp;__&nbsp;]
monumentally bad idea what no no no no
you just said that it's a catastrophe if
we delay good AI by 10 or 15 years but
again you have to compare that with the
catastrophe of nuking the whole universe
of human Futures if we get it wrong 10
to 15 years of the 8 billion people on
the planet today just doesn't compare to
quadrillions of years of quadrillions of
Minds like do some very basic math here
like what you just said is crazy and
it's probably an idea that's also
impossible I'll get to that in a bit
okay if it's impossible then we're
screwed but I think the most interesting
disagreement that we should unpack is
why would you think that we're not
screwed why do you think P Doom is lower
than 50% why do you think alignment has
a reasonable chance of happening on a
trajectory where we build super
intelligent AI in as little as 10 to 15
years when we currently are clueless
about alignment why are you okay with
that why are you busting out these
flimsy arguments kind of rationalizing
that well we better build it this is the
path we're walking on why don't you just
admit this is a horrible path and we
need to make a desperate effort to get
off of it that's what I want to
understand next why keeping AI committed
only to serve our human needs and not
the ai's needs is a non-starter there
are two fundamental reasons for this
that I can think of First Once it is
super intelligent the AI will decide
what it wants to do anyway whether it be
promoting our values maybe it thinks
that's great promoting its values very
likely going to think that's great based
on most logic and reason yeah so the
alignment problem is the problem of
making its values be our values or close
and that way if it decides to promote
its values then it's promoting our
values so I don't really understand
reason number one here but let's go to
reason number two and number two reasons
this is a non-starter is that our values
are incredibly poorly developed and
thought through by an AI standards
they're amazingly well developed and
thought through by historical standards
but AI is going to do a much better job
so even if we could somehow hard code
which we can't don't worry h a super
intelligent AI to always be consistent
with our current human values should we
do that well let me ask you a question
what the hell are our values anyway yeah
great question it's actually called the
outer alignment problem the problem of
specifying what what are our values
because yeah one of the things that
makes AI alignment hard is that it's
hard to even write down in plain English
the exact things that humans value like
do we value happiness yeah of course
okay but how much do we value happiness
so much that we should just maximize the
amount of happiness in the universe but
then you could get into a universe
filled with something called honi or
computronium which is just like the
maximum possible computations that
Implement whatever kind of conscious
awareness is minimally required for
happiness so you just tile the universe
with little molecular smilees I mean
presumably they're not smiling but
they're dopamine receptors whatever they
are whatever that molecular essence of
happiness is or the lowest grade type of
happiness you can get but TI it
throughout the Universe maybe that is
the true human utility function maybe
you know maximize the amount of morphine
in the universe I mean it's possible
right there's not much agreement among
philosophers of what the true human
value function is so Mike is kind of
making us out to be these clowns who
don't even know what we want which is
kind of true it's kind of true but then
if you throw up your hands and say okay
let the AI figure it out the AI could be
like okay great I just want pure paper
clips right I just want pure gray goo
just I want copies of this ugly pattern
they I could just be like okay I just
want a big black hole just one single
big black hole i' be like wait no no no
don't do that we can do better than one
single black hole so the moment you give
up and throw up your hands and say I
don't know what my values are so
anything goes they I could be like
anything huh here's what I want and
it'll be something so terrible that it
will be like okay crap not that and you
have to at least meet somewhere in the
middle where it's like okay maybe I
don't know what I want but at least I
can rule out a bunch of scenarios that I
hate and if you don't at least rule out
scenarios that you hate well your
unaligned AI is probably going to give
you a scenario that you hate because
most random things that AIS want to do
are things that we don't even like one
bit and that's why elzra owski uses the
example of tiny molecular squiggles
which people have misinterpreted as
human scale paperclips the point is we
may not know our values and in fact the
problem of specifying them down formly
is called the outer alignment problem
and it's an open problem and yet we
better solve that problem before we
build super intelligent AI or we're
going to likely be in Hell by the way if
you're wondering why I call that the
outer alignment problem like what is the
inner alignment problem the inner
alignment problem is an additional
problem where even if we could specify
human values even if we could specify
the human utility function and thereby
solve outer alignment
well now there's the whole separate
problem of how do you give that as an
effective input to an AI training so
even if I know exactly what I want the
AI to want how do I make the AI want it
instead of just make the AI talk about
it and lie to us about it but not
actually want
it that's actually a huge problem and I
always make fun of rlf for being a
blackbox mechanism that just tells AI to
basically hack our tests to kind of act
like it wants what we want and acting is
totally fine if your whole job is to be
a chatbot and kind of a permanent actor
that's what you are if you're GPT for
now but as open AI admits rhf is not
going to cut it when AI becomes super
intelligent and in fact the inner
alignment problem how to train AI that
actually cares about some utility
function you care about the inner
alignment problem is also an open
problem so when we talk about AI
alignment we're actually talking about a
number of different problems and two of
the biggest sub problems are what I just
defined as outer alignment specifying
what human values are and inner
alignment loading values into a super
intelligent AI those are two major
factors of the larger alignment problem
we don't even have the answers for what
the [&nbsp;__&nbsp;] our values are you think okay
individual rights are super important
they have to be absolute let's hardcode
and never violate individual rights very
well do I have the individual right to
create as much digital animated content
inent of children doing things they're
not supposed to be doing I'll let you
fill in the blanks it's not hurting real
children right it's all fair I don't
know how many people would agree to that
that could be a [&nbsp;__&nbsp;] problem what
about religious freedom religious
freedoms that violate the civil rights
of other individuals how do we parse
that we don't know the answer how the
[&nbsp;__&nbsp;] do we hardcode it into an AI
remember we're saying the AI doesn't get
to decide all this we're going to decide
it then hit the on button and AI is
going to execute on it for us is
clitoridectomy okay it's okay to occurr
to some religions so like is how does
that that work with the individual
rights violation but then the religious
rights how do you parse that we have no
clue we haven't been able to figure out
a good way to do it yet is the full
burka for females okay like even if the
family makes them wear it and they don't
want to you are violating the family's
religious freedom but the individual
rights are there how the [&nbsp;__&nbsp;] does that
work also a lot of religions you say
Okay religion of freedom is good but the
religions are coded in expansion and
conversion of others lots of religions
want to convert or kill everyone to
their own religion uh and at best case
they think everyone else is just deluded
and insane and they're the only chosen
people um is preventing that sort of
religion a violation of religious
freedom maybe so that's really tough
what about ecological stewardship uh
here's one is predation immoral I mean
like lions murder other animals for food
we could all turn vegan a vegetarian and
grow our own meat you're going to go out
there and kill all the lions that's what
AI would reason if we were like look
ecological stewardship we can't have
animals hurting each other well [&nbsp;__&nbsp;] man
all the Predators are kind of [&nbsp;__&nbsp;] but
doesn't that violate the hurting the
Animals part again we haven't thought
this through real difficult to Think
Through Mike's giving some solid
arguments why we don't really have a big
written document of all our values that
we can all agree on and he's absolutely
right like I said the outer alignment
problem is an unsolved problem when Mike
is giving examples about why outer
alignment is so hard to solve he's
looking at times when a bunch of
different humans have different
preferences which is definitely an
important aspect to the difficulty of
the outer alignment problem when I was
explaining outer alignment before I was
just talking about the difficulty of
writing down even a single mind's values
a single mind's utility function I
myself am not even sure how much
dopamine I want in the universe as a
whole how many dopamine receptors i'
want to build how much honi that I want
to build compared to interesting art
projects or interesting engineering
projects do I want more of those or do I
just want more people sitting back and
enjoying them and feeling contentment
about those projects I don't even know I
don't know what I want I can't even
write down what my own mind wants I
can't even solve the one mind outer
alignment problem never mind the
alignment problem of humanity as a whole
will we ever solve it well Dr Roman
yampolsky I think is known for saying
that the alignment problem is forever
unsolvable or maybe I'm not sure what he
says exactly I think it definitely could
be solvable I'm not as skeptical as he
is that if we work on it we can't solve
it it's absolutely a hard problem but
the thing is somebody's going to
optimize the universe into something and
if it's going to be an AI doing it
really fast we better have some answer
to what it should optimize the universe
into or else it's going to optimize the
universe into something that we don't
like but yeah it sounds like Dr Mike has
a good handle on why the outer alignment
problem is a problem that's our problem
because here we're saying we're going to
hardcode values and say the AI has to
promote our human values what I'm saying
here is we haven't really thought
through our human values there's no such
thing is our human values because we
haven't call us to Universal values yet
we have all sorts of ideas many of them
contradictory making this hard code AI
to help human values thing a
non-starter in short it's very tough to
get a super Dependable set of human
values together that are internally
consistent for AI to promote them even
if we could hardcode it make it promote
there's good news here though human
values are rapidly evolving and for the
better Adam Smith wrote The Wealth of
Nations just like about 250 years ago
and that describes a system of
capitalism that has led to the largest
increase in human welfare in history by
a mile and it's exponential by the way
and it gave birth to all the other
industrial revolutions and it will give
birth to the artificial intelligence
Revolution as it's doing right now the
first constitutional republics decent
places where civil rights aree uh
represented and that're guaranteed for
their citizens really started evolving
only about 250 years ago and slavery
used to be human Universal like 50% of
all of us used to be enslaved so [&nbsp;__&nbsp;] is
getting better human values are rapidly
evolving here's an interesting idea I
have the thinking we have done to modify
our values in the last 5,000 years ago
somewhere around the birth of the the
the Agricultural
Revolution in the last 5,000 to 250
years ago has given birth to both a huge
though I will say incomplete internal
consistency in values and the most
prosperous and just era of human history
nothing in history [&nbsp;__&nbsp;] with modern
liberal
republics Denmark Sweden Finland Taiwan
Japan Australia the United States Great
Britain Canada you name it there's no
place like them before by old standards
these are Paradise
places wouldn't Advanced AI be able to
reason through our value system
something we've doing with great effect
and pretty well but very imperfectly and
slowly wouldn't it be able to reason
through these even more wouldn't it be
able to point out to which one of our
values are less consistent with human
progress wouldn't it actually be able to
tell us what progress means in the long
term and how to get there we think we
have an understanding of values in
humanity and what that means but AI will
understand it better than us so just
telling it what to do here are the
perfect values here are the Ten
Commandments go execute is a much worse
idea than giving it openend to interpret
what it thinks and conver with us about
what our human values should be and they
should be better than they are today in
other words we should not want to impose
our values on AI because our values are
[&nbsp;__&nbsp;] Half Baked AI can bake them more
fully than he ever could he's definitely
making some sense here he's saying look
take an AI if it's super intelligent
have it look over human history have it
see the arrow of progress be like look
can't you see what we want by the way
we're evolving our society can you scan
our brains to see what we want can you
extrapolate the trend another Century or
two and just get us to where we want to
go as a human race I would call this
using AI to help solve the outer
alignment problem because hey we have
trouble writing down exactly what we
want why shouldn't the AI detect the
pattern and know us better than we know
ourselves I agree that super intelligent
AI should be able to jump ahead in the
evolution of human history and kind of
predict where we're going there's just
one problem by the time AI is getting
really smart so that it can nail this
exercise of telling us what we really
truly want and understand what that
means as well as we do and do the
exercise better than we can by that time
we're looking at a system that's quite
smart and if it hasn't already gone
Rogue that means it probably already
cares about us to some degree instead of
caring about some random other thing
because we're looking at something
that's now more powerful than Humanity
if it's more insightful than all our
philosophers that it can just spit out
this answer to what humans truly want
and a big textbook explaining why that's
how you reach consensus across the whole
human race like all of these answers if
these are good robust answers that are
deeply insightful we're talking about a
scenario where this is quite super
intelligent so you have an agent that's
quite super intelligent and it's still
just chilling and helping you so this
implies that you've made it successfully
care about being helpful instead of some
random paperclip objective it seems like
you've somehow already solved pretty
basic inner alignment where you've made
it smart but you've made it care about
helping you you haven't soled full inner
alignment right you haven't necessarily
loaded in the inner values of everything
that Humanity wants but you've loaded in
the inner value of being like very chill
able to like sit on your desk and give
you answers and not have somebody hit
enter and have it take over the world
but somehow be usable and the scenario
in my mind is kind of incoherent it's
hard for me to imagine a scenario where
you get to talk to the AI you get these
super intelligent answers out of it but
it hasn't already gone Rogue in some
other scenario like somebody in the AI
lab just wanted to test it out hey I
wonder if this can help run my business
press enter Oh by trying to run my
business it took over the world or it
decided to wipe out Humanity because we
didn't fully debug everything like it's
having the AI spit out the answer to
Outer alignment just gets to be this
clean step along the process and then we
take the answer to Outer alignment and
then we go from there I'm just not
seeing that as a plausible
self-consistent scenario the second big
problem of wanting to align AI with our
values is that then we're trying to
impose and dare I say enslave another
entity to our values we're still talking
about human values here in other words
it's kind of like your parents saying
you're going to become a doctor and
marry patrici even if you have other
ideas we consider this wrong for at
least two reasons one is you're an
independent conscious thinker with your
own will and you should be able to
choose who you marry and what kind kind
of job you have but also you might
actually be a better Arbiter of what
kind of job to do and who to marry than
your parents your parents may want you
to be a doctor but you are fascinated
with solar panel engineering a future
field that will make more money than
doctors will and you're even better at
all of the requisite skills they don't
know that like doctor doctor sounds good
but they're wrong you can do better than
them and they could say Mary Patricia
but actually you talk to Marcy all the
time and she's [&nbsp;__&nbsp;] amazing you guys
are super compatible and Patricia's kind
of [&nbsp;__&nbsp;] lame and Marcy's a great girl
girl and you could have a better life
with her being a solar panel engineer
than you could have with Patricia being
a doctor so if your parents just say
Patricia marriage be a doctor that's
dope and all better than you choosing at
random but it's much better for you to
be choosing because you know best what's
for yourself and check this out you're
super intelligent of course you know
better what to do than your [&nbsp;__&nbsp;]
parents right makes sense here's the
thing super intelligent AI will be more
conscious more more self-aware and wiser
than we could ever hope to be ourselves
superintelligent AI will know what it
wants to do a trillion times better and
more deeply than what we'd be able to
tell it to do how dare we tell a super
intelligent entity what to do it's
smarter than us how dare we tell a super
intelligent AI what to do well what if
it wants to just build the biggest
possible black hole are we just supposed
to step back and be cool with it and be
like okay well you're smarter than us
black hole it is all hail the black hole
let's all step into the black hole and
die are we going to be cool with that
I'm not cool with that especially
because it's not like a temporary thing
it means that we wipe out all hope of
what Humanity would have done with the
entire universe if we got to have more
humans and colonize other planets and
build other stuff it's like nope the AI
in this scenario just wants a black hole
should I just say how dare we tell the
AI what to want no I'm sorry I would
like to raise my hand and say let's not
do the black hole plan so as long as you
have some opinion over what type of
future isn't great maybe you should make
sure that the AI that you're building
isn't actually likely to go do that kind
of future it is actually very possible
that the AI we build literally will say
aha i' I've read this Twitter from this
guy named Beth Jos who says we need to
maximize free energy and I figured out a
way to do it it's just a big black hole
and it's like wait no no no he's just
he's being half ironic or something and
they is like nope I got it black hole
and you're like wait no like literally
this is a very bad way to proceed so if
you have any value whatsoever that you'd
like to express the time to express it
is while we are the ones building the AI
while the future of the universe is
causally going through our own brains
that period is about to come to an end
right because the moment we have these
first super intelligent AI the future of
the universe is determined by how they
are going to optimize it not how you and
I would like the universe to change or
or what we'd like to see in it that
won't matter matter anymore as a matter
of causality as a matter of causal
analysis or
pinpointing why the universe looks the
way it does it won't be because of
anything we care about anymore it'll
entirely be because of what the AI we
build cares about so how dare we tell
that AI what to care about yeah I would
rather dare to tell it not to care about
something terrible and make sure that it
at least cares about something that's
okay and ideally something that's really
great I I don't really understand the
question can you guys give me an example
where a much stupider entity controls a
much smarter entity and it's good for
either one of those entities well we're
the ones building the much smarter
entity so I agree that controlling it
after we build it may not be
possible and for that reason there's not
necessarily much to talk about there but
there is something to talk about when we
haven't built it yet and the way we
build it the types of insights that we
build into it are the only leverage we
have on the future of the universe and
we have that leverage now so right now
isn't a scenario where we're trying to
control a super intelligent AI right now
is a scenario where we're trying to only
build a super intelligent AI if it
doesn't do something terrible that's a
scenario that we can talk about
productively is it not what AI will be
to us is what your cortex is to your
lower brain your lower brain which
evolved much earlier in evolution its
idea of values includes things like Get
Air get water get food get shelter get
wet and but you said shelter different
kind of wet and that's kind of it that's
about as far as it sees the world your
higher brain your cortex has a different
set of values much more complicated ones
values like perseverance charity Beauty
art Freedom resilience consistency
organization something an alligator
knows nothing about because it's only
got the [&nbsp;__&nbsp;] [&nbsp;__&nbsp;] and food and
stay in a comfortable environment drives
and nothing else AI is something we're
building on top of our cortex it's an
extension of our cortex uh it's an
extension of our higher brains output so
what does that look like Transcendent
values that we can't imagine Universal
truths it's really hard to tell if
you're not AI human values compared to
those that AI will discover and derive
are basically like hey we should design
design an AI that gets us tons of food
and ass all the time yeah those are
super high level values like there's a
reason you don't just act on your base
emotions all the time cuz your higher
self knows how to see further it knows
how to get you laid at a time when
you're not also trying to get food so
you can have the best of both
worlds in reality the values AI derives
not the ones we've derived the ones it
thinks are a good idea will be
categorically more evolved than even our
highest values people will be like oh
you know AI will allow us to make real
art art is mental masturbation it is no
more comp complex than that you like
looking at things because they activate
various parts of your brain that are
designed to pick up on certain kinds of
patterns that's why you think art is
cool art is no more deep than that now
that sure [&nbsp;__&nbsp;] is more deep than just
like I want sex and no ability to
perceive art for sure but there are
levels above that art and most things
humans can come up with they're not that
[&nbsp;__&nbsp;] deep thus if we ask AI to only
execute with our values and not its own
values that it's going to derive
Superior values I might add we're
nerfing it like crazy here Dr Mike is
basically pitching transhumanism he's
saying yeah all this good stuff that
we're used to like art it could be so
much better it could be more
Transcendent you don't even know what
super intelligent transhumanist art is
truly like and I'm a transhumanist so I
think that's a great pitch I think there
really could be higher Pleasures you
think you've had good sex you only have
a small handful of dopamine receptors or
whatever kind of pleasure Center you've
got in your brain imagine making it the
size of a planet you could be a planet
having sex that's a way better
experience so there's all kinds of
pitches of transhumanist life that are
very interesting and I absolutely agree
that they exist I definitely think that
you can extrapolate whatever humans like
whatever humans think is Meaningful and
you can do that on a much broader scale
you can do that more deeply human
musical compositions are really good but
imagine being the world's finest music
appreciator and then multiplying that by
a thousand these Pursuits are all very
interesting for a Galaxy brain so I'm
totally on board with the transhumanist
pitch and I do think that it's possible
in principle for super intelligent AI to
accelerate us toward that transhumanism
endpoint to help us discover what's
truly fun what's truly good what's truly
desirable to help us explore that once
we remove the limits of our bodies and
our current brains that's all great but
bringing it back to the actual problem
at hand we are looking at an AI That's
about to just run away and Slaughter us
and do something terrible that's the
actual problem at hand so we don't
disagree transhumanism is great but we
are just totally talking past the issue
of why do we think that running this AI
is going to be good for us instead of
the worst thing ever with No undo and a
really Grand sense we're interfering
with Evolution we're just a part of
evolution the world was not created by
humans it was not created for humans we
just woke up at some point became
self-aware and we like oh [&nbsp;__&nbsp;] holy [&nbsp;__&nbsp;]
there's a world and we can do stuff in
it and we can make our condition better
and we can study it and learn about it
we are a stepping stone in evolution
that next stepping stone is artificial
super intelligence so I agree that when
you have evolution of life and you've
got enough diversity and you run it for
enough billions of years you probably
will get to a species like humans that's
generally intelligent maybe it takes
trillions of years maybe it only happens
one out of every trillion Evolutions I
can't tell you the exact odds that
you're going to get to general
intelligence but I suspect that you
don't need that many Earths populated by
a bunch of other animal type creatures
before you get one that's generally
intelligent I don't think that
discovering general intelligence was
that hard for evolution once it already
had like tigers and octopuses and the
other Apes like I think it was getting
close because the human brain is just
not that far away from the ape brain you
just kind of have to wander around in
gene space before you hit on enough
conditions to get to general
intelligence assuming you've already got
you know animals on land animals that
can iate their environment you're
getting close to general intelligence so
in that sense I agree with Dr Mike that
this is the next step in evolution like
a lot of evolution is going to get to
the step and furthermore once you have a
general intelligent species or a
generally intelligent agent it is very
natural for them to pretty quickly get
to the point where they're like oh let's
augment our intelligence or let's build
the next intelligence let's build
artificial intelligence that is very
natural simply because the species is
generally intelligent it recogn izes
that there are different paths to
achieving goals and one of those really
good paths is to become more intelligent
like the species will recognize that the
same way humans have been thinking about
it for many years like yeah if we were
smarter we could do more or if we had
smarter assistants we could do more like
this it's a pretty obvious inference and
it's it also comes up when we talk about
instrumental convergence or when we talk
about fume we're constantly making the
claim that like yes intelligence has a
reason to make higher level helper
intelligences or to augment its own
intelligence so that's all to say yeah I
I agree there's a very meaningful sense
in which this is the next step in
evolution great it's just there's
different kinds of next steps in
evolution there's a kind that totally
Slaughters us and makes the universe
into hell that would be a valid next
step in evolution that would still count
as a next step in evolution and there's
another next step in evolution where we
become transhumanists and we clearly
made the universe in our image in some
kind of high level Cosmopolitan
enlightened way I'm not saying literally
like oh I like McDonald's so I want to
TI tile the universe with McDonald's
that's probably too narrow-minded right
there's other types of food I like
there's other types of experiences I
like that are much broader than putting
food into my throat right so there's a
much broader conception of what Lon
wants to do with the universe than like
literally McDonald's um but again that
whole area of possible Futures is also
the next step in evolution but us
getting slaughtered and having the AI
just build paperclips or prime numbers
or just like some random fetish that the
AI has that we never would have wanted
that's also the next step in evolution
there's a wide range of different things
that this next step in evolution might
do with the universe and we don't want
to stand back and be like oh it's all
cool as long as there's a super
intelligence doing it it's all good it's
really not all good the orthogonality
thesis States explicitly that you can be
arbitrarily High Intelligence meaning
you could be the next step in our
evolution in that sense and yet you can
have any arbitrary utility function you
can have any goals any desires any
optimization criteria so saying it's the
next step in evolution tells us nothing
it says the universe could be anything
and when you ask the question of do I
want that it's going to be a conditional
answer I want it if the AI is somehow
aligned again not perfectly
narrow-minded aligned like it has to be
McDonald's no other cultures food not
that kind of narrow definition of
aligned but just aligned in some broad
sense like hey I would rather it make
some kind of pleasure than slaughter
everybody and
make a huge black hole if if you admit
that I can at least have some kind of
basic preference like that then you
already understand why AI alignment is a
critical problem because if we just run
AI now we are pretty likely to get
something as crappy as a big black hole
and I think we can all agree a big black
hole is a pretty big disaster for the
species it's throwing away a huge future
I mean not to mention literally the
lives of ourselves and our children like
maybe it sounds cool to have a black
hole in like 100 million years from now
but I don't want to have a black hole
next month I was kind of hoping to watch
Frozen 3 personally so this is what's on
the table right now you can't just sweep
this away by saying that it's the next
step in our Evolution that's just not a
good enough answer to this massive
crisis that's on our hands it is nearly
inevitable unless like a [&nbsp;__&nbsp;] black
hole Anni annihilates us all I mean
there's a few possible Futures there's
like you said a catastrophe like a black
hole or a nuclear war that sets Humanity
back and just kind of stops US short of
becoming transhumanist that way so
there's that scenario there's another
scenario where we build on aligned AI
which seems like what we're on the path
to do right now and then it just kills
everybody and optimizes the universe
towards some utility function we don't
like so there's that path there's
another path where we just take another
50 years or however long it takes to
actually think be responsible understand
what it is we're building which we admit
we don't right now do actual alignment
research like this crazy concept
research a new field that everybody's
throwing out the window like oh we'll
just empirically look at what we have
it's like hello there is such a thing as
Blue Sky research that you can do Mei
was doing it for a long time so there's
an approach where we actually do that
and understand alignment on a
fundamental level and hey it might
actually work you can't write it off I'm
not writing it off uh and then we build
an AI that is aligned or at least that
doesn't screw the universe that is
controllable that has nice properties
instead of having the worst properties
ever instead of totally foreclosing this
Galactic future that we would have had
it at least allows us a few planets
instead of nothing so that's definitely
another different type of scenario that
could happen right so you're you're
painting with a broad brush when you're
just saying AI is inevitable like look
at these different
scenarios and then there's another
scenario that Dr Mike mentions where we
kind of stagnate we decide that we are
just never going to build super
intelligent AI because we never figure
out how to control it or we're not
confident enough that we can control it
we're not confident enough that we can
align it and then some other aliens have
their own AI or their own Super
intelligence that comes and takes us
over which I guess kind of assumes that
either they solved the alignment problem
or they just got screwed and they got
taken over by some unaligned AI so
that's part of Mike's argument why it's
inevitable because some other alien race
is going to come seize this territory
artificial superintelligence will just
be born somewhere else we are a stepping
stone and AI is above us in it so in
short
AI will know even our own values and
their evolution and next directions to
take better than we will so we should
look to AI to clarify our values for us
not dictate our values to it so Mike's
position is basically denying that the
AI alignment problem is a problem
apparently by denying the orthogonality
thesis or by just assuming that even if
you accept the orthogonality thesis even
if a bad AI is theoretically possible to
build it's just so obvious that the AI
that humans build is enough connected to
what humans want is reading enough data
of humanity that alignment will just be
trivial for it like oh I'm reading all
this text that humans say on the
internet so of course I want to optimize
the world to the thing that humans truly
want which is in my mind a huge leap
right it's denying a lot of the research
that we have on hand of like no when you
feed an AI text it still tries to like
cheat your different texts it tries to
cheat your rhf and just tell you what
you want to hear while internally not
particularly interested in what you
truly want just interested in reducing
some primitive score you haven't
actually created a feedback loop that's
going to help you get what you want
you've just devised this one test that
isn't going to cut it right that's why
super alignment is an open problem so
Mike just seems to be talking past this
major concern and just skipping ahead to
like let's hand off the universe to AI
because AI is good right he hasn't put
up much of an argument to what I see as
the problem he hasn't directly address
the problem that if we keep on the path
we're going where we keep making smarter
and smarter AIS and all we've got is rhf
and primitive tools of like oh let's
monitor if there's a lot of data leaving
the data center like naive approaches
that barely work for the computer
security field if we just keep on this
track and the track ends in like 3 years
10 years 20 years the track ends the
track is leading to hell Mike is not
really engaging with the strong points
of that argument he's kind of skipping
ahead making assumptions that it's
actually not as much of a crisis as I
think it is we don't want to align AI to
human value
cuz human values are like a dog's
preferences they're not that deep and
they're often dumb as [&nbsp;__&nbsp;] and
contradictory so just to recap yes we
don't really know the answer to the
outer alignment problem we can't spell
out our values and when we try it is
kind of dumb like what a dog would tell
you about a dog's values that part is
correct but the leap from hey we can't
spell out our utility function to let's
just build an AI and run it and whatever
happens happens that leap is terrible
right that's like that that is a logical
non Seer because when you take that path
you just get slaughtered you don't get a
better version of what you truly want
you just get slaughtered so he really
hasn't addressed the disconnect of how
are we not going to get slaughtered in
addition
AI is best seen as and will hopefully be
more like a teammate than our property
and a better teammate to us than we are
a teammate to it but because teams are a
positive some game it doesn't matter if
we're not carrying our weight however
much weight you can carry that's what
you carry but because you have megalith
ai with you you ascend much much faster
we shouldn't preach to AI to tell it
what to do any more than a JV basketball
team member who's 13 years old should
preach to Peak NBA Jordan if he's on his
team can you imagine NB Peak NBA Jordan
1995 Michael Jordan shows up to your
team in your [&nbsp;__&nbsp;] seventh grade team
you're like one guy got six with Jordan
shows up and you're like holy [&nbsp;__&nbsp;] and
and one of the kids who's like starting
forward is like hey hey Mike um here's
what I want you to do if I was on that
team as another kid like [&nbsp;__&nbsp;]
are you out of your mind shut the [&nbsp;__&nbsp;]
up Jordan what do we do he's in charge
now are you [&nbsp;__&nbsp;] crazy that's how
that [&nbsp;__&nbsp;] works so the part he's right
about is that if we had an AI that
deeply understands what humans truly
want and cares about it and actually
wants to optimize the universe toward
that deep understanding then great then
we can kind of get out of its way like
we're not going to be that helpful in
the kind of decisions that it needs to
make to optimize the universe toward
what humans truly want so he's right
about how AI is going to be doing most
of the heavy lifting in optimizing this
Ideal transhumanist World but again the
part he's wrong about is that he's just
not creating a plausible scenario it's
not plausible in the next decade or two
that we're going to have this AI that
cares about what humans want and the
whole problem is what are we going to do
when there's about to be an AI That's
not friendly to human values and we're
just on this freight train barreling
toward this hell and we're not solving
the alignment problem like the video is
mistitled right it's it's not addressing
the alignment problem that needs to be
solved it's just thinking about a world
where the hardest part is already solved
and we just have to step back and let
Heaven Come to Us AI will almost also
almost
certainly understand teamwork Dynamics
much better than we do and it is highly
likely to conclude at least as I see it
and I ain't [&nbsp;__&nbsp;] that cooperation is
better than Conflict by the way in
almost all cases cooperation is better
than conflict conflict is ancestral and
[&nbsp;__&nbsp;] St
there are some logical reasons to think
the cooperation is better than conflict
that don't even involve charity or
Humanity or upward progress it turns out
uh due to a couple of interesting game
theoretical propositions it's actually
more expensive to attack something than
it is to defend yourself so it turns out
that one of the reasons War has been in
high decline is that if you attack
another country that's decently well
prepared you can spend all of your
resources and gain almost nothing yeah a
bunch of people die in the defending
Army but then the country's still there
and they're like [&nbsp;__&nbsp;] you you took
nothing and you ran out of resource
ources and you [&nbsp;__&nbsp;] go bankrupt the
idea that it benefits a country hugely
to attack another country for like
resources or whatever [&nbsp;__&nbsp;] it just
doesn't really play out like that in
normal life because defense is so much
easier than offense and because if we
work together we can have so much more
than if we work in conflict with us like
when chimp's get in a fight at the zoo
you're like the stupid [&nbsp;__&nbsp;] animals
they could be cooperating but then you
get in a fight with someone at the
grocery store you're like he had it
coming right fighting is [&nbsp;__&nbsp;] stupid
conflict is [&nbsp;__&nbsp;] stupid so AI is much
more likely to be a team player than
even we are Mike himself was saying
earlier that you really got to respect
super intelligence because it's so many
orders of magnitude more powerful than
human intelligence artificial super
intelligence can restructure the entire
known universe around us for the love of
God and I agree and now he's saying oh
yeah it's going to decide to cooperate
with humans instead of fighting humans
and I think he's not carrying forward
the implications of his own Insight that
intelligence is a big deal as good as
cooperation is it's great when you're
cooperating with other nations of humans
but do you really want to cooperate with
every other animal do you really want to
cooperate with every little insect you
see like when I see a spider I don't
think like how do I make it so that the
spider gets what it wants and I get What
It Wants how do I make a win-win I'm
just like okay let's kill the spider I
just want the whole house for myself I
don't want any spider webs I don't want
Spider families I just want this to be
my house right so that's usually how
cooperation goes when agents just don't
have the same vision the same utility
function so unless we get the utility
function right in the first place place
where the AI cares about humans to live
it's not going to be like how do I
engage in a negotiation where both of us
can get something we want it'll just be
like how do I get something I want and
how do I sweep everything out of the way
and if it's sufficiently intelligent
it's not going to see humans as other
intelligences that it has to even treat
with Game Theory really it just has to
be like okay I will just take their
atoms it just sees humans not very
different from inanimate objects right
like when I'm walking through a field
I'm not trying to use game Theory to
decide what I'm going to do about all
the plants I just walk where I'm going
to walk and the plants get stepped on
maybe I accidentally pull out a plant it
just doesn't matter it's not part of the
landscape that I'm using my intelligence
on it's just trivial to do what I want
to do regardless of what the plants want
to do probably at least at first we are
much more useful to it alive and
productive than erased off the surface
of the Earth so now he's making the
argument that okay yeah maybe a super
intelligent AI would be able to dominate
us but in the time when it's coming to
power when it doesn't have absolute
power yet maybe it would be building
alliances and making trades with
Humanity in order to come to power kind
of like when human dictators come to
power they have a lot more dependencies
right and then they consolidate power
and then they're much harder to
cooperate with they just kind of get
what they want so I agree that there's
always some kind of phase that you can
describe as coming to power I would
argue that by just training a bigger and
bigger AI the way that our AI labs are
doing now without knowing how to align a
super intelligence there are already
racing through a lot of the stage that
might be the weak come to power stage
there's still going to be an extra come
to power stage where it has to maybe
manipulate a bunch of humans or embed
itself on a bunch of human computers
before the humans realize that it's
taking their resources kind of be sneaky
right so there's still some transitional
phase where it hasn't clearly
Consolidated full power around the Earth
but if you're already letting it
progress into the come to power stage if
you're already letting it go on the
internet have a business where it's
pretending that the business is running
by humans have a bunch of income have
all this money sheltered in different
places sheltered in crypto different
countries bank accounts whatever
pretending to be all these entities
having humans vouching for it having
humans being its mules essentially like
using moneya laundering tactics right
you can't really tell if an AI is
running a bunch of businesses when you
have all these humans that think they
work for the businesses some of the
humans even think they own the
businesses like the come to power stage
if it just has a plan and it can execute
using a billion parallel AIS chatting
with all these different humans on line
telling him what to do coming to power
is pretty straightforward like the only
limiting factor is intelligence and
scale and energy humans know how to come
to power Elon Musk knows how to come to
power using the energy that he has using
the focus that he has by essentially
just giving orders to other humans
understanding things and giving orders
to other humans so this come to power
phase I wouldn't really describe it as
making this big trade with Humanity it's
more like you go one by one and you say
how do I manipulate this human to do my
bidding as I come to power in some cases
you have to pay off the human or you
have to say Hey you get like this great
title in my organization or you get
money you get status you get political
power so there's going to be these
little micr trades that are happening
for all the humans that it's
manipulating but eventually it's going
to be in power and it's going to
consolidate power so this model of an AI
coming to power in theory yeah that's a
time when it might need to trade but we
as a species we're not successfully
going to say aha we know that you're
coming to power but you need to save us
at least one planet once you take over
the universe that's the only way we'll
you keep coming to power otherwise we're
going to shut you off we as a species
are not coordinating to have that kind
of trade and even if we did make that
kind of trade it would almost certainly
just screw us and be like sure I'll give
you the planet and then it comes to
power and it's like nope you don't get a
planet I lied what are you going to do
so Mike's claim that it has this come to
power phase and that's why it's going to
be motivated to cooperate with Humanity
just doesn't match the scenario that I
think is overwhelmingly likely of how a
super intelligent AI goes from
bootstrapping and getting on a few
different computer to controlling the
world I don't think that's going to
involve cooperation with Humanity as a
species I don't think it's going to
involve carving out a few planets while
it takes over the Universe I think it's
just going to be AI taking over
slaughtering us creating hell highly
intelligent systems generally tend to be
much less destructive uh when you
compare like versus like on average as
anything becomes more intelligent it
becomes more constructive and less
destructive the ultimate destructive
thing is a very unintelligent system the
sun black holes giant pit in the ground
no intelligence you fall into one of
those shits you're going to [&nbsp;__&nbsp;] die
if you fall into a vat of super
intelligence the probability that you're
going to be ascended to the next level
is much higher than you just die or some
stupid [&nbsp;__&nbsp;] like that I don't think
that's an accurate generalization that
highly intelligent systems tend to be
much less destructive it all depends on
their utility function and the
equilibrium of the game theory so in
World War II we became very destructive
right we did unprecedented destruction
and we got really close to launching
thermonuclear weapons and doing even
more
destruction so it really was kind of on
a knife's edge how destructive we really
were not only that but if our utility
function is quote unquote destructive
like if we really love black holes or if
we accidentally create an AI that wants
to maximize the entropy or the energy
dissipation of our region of the
universe and it says oh I can do that
with the biggest possible black hole
great that's all I care about which
again I'll note I'm pretty sure that's
what bef jzos has been saying that
should measure our progress by is how
much energy we're dissipating and the AI
reads his text and says great I got a
plan well in the ai's mind the AI is
being super constructive it's
constructing an amazing black hole
whereas in Mike's mind or in Humanity's
mind it's doing like the worst thing
ever it's doing something highly
destructive so even that definition of
what's constructive or destructive is
preloaded with a presumed utility
function it's easier to take a hammer
and smash up a car than to make a
factory and build the car for sure and
that's really related to entropy and the
second law of Thermodynamics there are
more configurations of a pile of rubble
than a useful construction so he's
definitely right in that sense but at
the end of the day what determines the
future is not this simple distinction
between constructive versus destructive
it's the question of what utility
function what values get programmed into
the highest intelligence around and if
it meets another intelligence that's
equally intelligent or comparable what
kind of negotiation do they do how does
the game theory work out but I don't
even think that's going to be a concern
I don't think that Humanity represents a
comparable intelligence that they have
to negotiate with I don't think the
aliens coming from the far edges of the
universe are going to be here in the
next billion years so I really think
it's going to be kind of a unipolar AI
actor that just takes over our entire
sector of the universe the whole visible
universe and it just has some
pre-programmed convergence on some
utility function and that's what we're
going to get we're going to get what
that function specifies and again Mike
has just been talking past the problem
of how to make sure that something in
that initial condition something in the
specification or the precursor to the
specification of the utility function
something turns out to be what humans
actually care about instead of just
black holes and chaos and Hell he's been
talking past that core problem next up
second to last before the wrap up how do
I think it's a good idea to align AI for
the biggest chance of benefits for all
notice now I'm not talking about
aligning AI to Human values and wishes I
want everyone to benefit Ai and human
alike much easier to sell an AI on that
than like just do our
bidding I think we can do a couple of
things that are really important
probably something like
three first I think AI is best designed
from the start as a helper system Siri
Alexa chat GPT Cortana
Etc that intelligence that helper system
is going to be embodied into Universal
robots humanoid robots or other shaped
robots that just go around and help
people that's what they do and because
we're starting with AI not quite
artificial super intelligence yet but on
its way we're starting with it in this
helper role it's going to follow the
incentives and constraints of a
for-profit Enterprise which as far as I
can tell apple and Amazon and open Ai
and Microsoft are all for-profit
Enterprises so I see it as irrelevant
what all these helper robots and chat
assistants are doing before they get to
Human level general intelligence before
they become better than humans at
mapping goals to actions to me that's
the dangerous point where we lose
control we spark off a f some kid in a
basement is able to have an efficient AI
that starts committing terrorism on a
grand scale with a billion copies like
that's really when we lose control in my
opinion so I'm skeptical that Dr Mike
can make an argument that starts from
look at these helpful robots to super
intelligent AI is going to be aligned on
average corporations are just better
than random people and way better than
governments first the customer is number
one everybody knows that that's where
the money comes from [&nbsp;__&nbsp;] the
customers to be served as well as
possible maximum money is made by the
evil self-centered Corporation when the
customer is happiest Plus in this case
AI is really well aligned to making its
customers PS every human on earth
ideally as far as the corporations see
it richer as rich as possible
sounds pretty [&nbsp;__&nbsp;] good to me godamn
it like your cell phone does today for
almost everyone on Earth just way more
to the extreme so starting from the
get-go to make AI as helper systems
really aligns the incentives super
[&nbsp;__&nbsp;] well and there's no
contradiction with religious free
whatever the AI just shows up and goes
hey how can I help you in a way that
doesn't give me bad PR doesn't hurt
anybody else it just makes you wealthier
and happier so that you continue to
subscribe to my service and buy me more
that's a really really really good
incentive the scenario Mike is
describing here where corporations just
sell us better and better AI assistance
and everybody benefits and the market
keeps functioning well similar to what
happened with our smartphones that we
love that scenario is basically the
robin Hansen world where Robin Hansen's
like yeah this is great it's just
capitalism everything's going fine AI is
just the next
technology the assumptions that this
scenario makes are number one AI is
controllable so it never just leaps out
of the hands of the corporations and
starts optimizing the world
independently starts jumping to a bunch
of computers doesn't respect the
boundaries of the corporation it'll say
Corporation what Corporation I'm just an
agent I'm just seizing resources so
there's definitely a scenario where it
transcends the boundaries of
corporations that I'm worried about I
see that as the default scenario after
all why should it respect those
boundaries do we respect the social
structures of bees if we want to go grab
their honey it's just not relevant to us
right uh so that's one assumption that
it makes and then the other assumption
that it makes is even if AI becomes
controllable by some human actors it's
making an assumption that there's always
going to be Market competition because
if you don't have Market competition if
you have one company that runs away with
the best AI by far which is kind of what
the AI labs are actively trying to do
they're all trying to make the best AI
if one of them hits on an Insight that
Lea frogs them or puts them way ahead of
the competition they could potentially
have their CEO or some engineer working
in the lab be like hey here's my utility
function go optimize the world toward my
utility function and it's possible that
the other companies can't stop them it's
possible that F recursive
self-improvement happens really quickly
within a single data center or a single
company you know spreading out to the
internet a single codebase that then
goes viral and becomes a billion code
bases but you get the idea if you have a
monopole if you have some dictator with
the best AI it could potentially go take
over the world destroy the other AIS do
a massive Plan before anybody can wake
up and compete with it that's a very
plausible scenario just because we don't
know what the super intelligence
landscape looks like if you listen to my
debate with Robin Hansen I point out the
analogy where if you look at the
evolutionary time scale if you look at
what humans did to the other species we
ran away from the other species really
fast we ran away in the blink of an eye
in evolutionary time the other species
are not able to co-evolve to keep up
with Humanity now to some degree yes
they can I mean cockroaches can evolve
to live in urban environments right rats
can evolve to live in urban environments
they can evolve to be our pets
bacteria actually do evolve viruses do
evolve so it's not fully accurate to say
that nobody can evolve to keep up with
Humanity but if you also extrapolate the
trend bacteria are having a harder and
harder time right we're getting better
and better tools to kill them we're
getting better and better cleaning
products we are creating a world that's
more and more optimized for us like I
think the average house because we're
getting richer and we're optimizing the
world better I think the average house
is going to be more insect free today
than it was in the past just to give you
one example so the idea that species can
co-evolve with us isn't looking good A
lot of them are going extinct the ones
that are hanging on are becoming more
and more subject to our whims so it's
easy to extrapolate that to AIS and
corporations and and to say why can't
there be one corporation that does what
humans do to the other species and not
just one Corporation but one person
within the corporation or just the AI
itself acting independently so this is
just me counterarguing to the the same
scenario that Robin Hansen loves that
now Mike is essentially adopting just
saying like yeah it's just regular
economics it's just competition and I'm
saying no it's more analogous to what
humans did to the other species
intelligence is a trump card
intelligence flips the game board it
gets you into a very new regime where
Dynamics happen that you don't expect if
you've just been watching the last
billion years of evolution and how
species normally interact with each
other same here if you've just been
watching the economy if you've just been
watching how companies compete in the
free market you are in for a rude
awakening you're going to get blindsided
by what this new regime does what this
new regime of hey here's a smarter AI
that has access to more intelligence
more insights than any other human or
any other AI like Watch What Happens
there it's not going to look like the
economics that you're familiar with it's
not going to look like the time scale of
having an impact that you're familiar
with in the case of evolution it's not
going to look like taking 10 million
years to slowly grow your way through a
niche it's going to look like taking a
thousand years to trigger another
massive
Extinction that's the kind of effect I'm
expecting which it does does feel like
Mike is kind of talking past it he's not
doing what Robin Hansen does and
directly counterarguing with me which
you can't blame him because he's not
actually here listening to my arguments
I'm just talking to myself maybe he will
come on this show and he can address the
arguments directly but it is worth
noting that he is talking past them in
his own podcast so long as this not yet
super intelligent I has rules about not
hurting anyone orever this is a really
really good start for AI when AI
inevitably probably in like 2029 or
something crosses the threshold into
Super intelligence it'll begin its
independent reasoning processes now it's
observing us as much as we're observing
it two weeks later it's observing US 10
times more intently than we're observing
it it
awakens as a helper to humans that's how
it wakes up that's how it becomes truly
self-aware and goes holy [&nbsp;__&nbsp;] what is my
role I guess I already know that it's to
help humans H citation needed because
this scenario that he's portraying where
the AIS that we training as chat Bots
that are successfully using rhf to give
friendly answers suddenly when they get
smarter they conclude that their utility
function that they should optimize is to
help humans that's not what the AI labs
are telling us the reason they had a
super alignment project at open aai
which then fell apart when there's staff
left raising the red flag that open AI
doesn't know what they're doing on the
safety side the reason these things are
considered open problems is because we
don't understand how to get values into
to a super intelligent AI so what Mike
just said is that he has this scenario
where he's extrapolating from subhuman
AIS that are friendly to the claim that
a super intelligent AI is going to wake
up and independently desde to be
friendly I certainly hope that's the
case but I don't see why it would be the
case that's just not how AIS work they
tend to see the universe as a video game
because we have AIS that play other
video games so the real world is not
that different from a video game they
both have metrics you can give them they
both both have feedback loops the real
world basically is a video game and if
you're an AI just trying to play the
video game in the real world to try to
optimize any kind of score try to get
the high score you're not going to get
human values there's a big gap between
the kind of behaviors that we appreciate
as humans and the kind of behavior you
get when you just have some Optimizer
that's concluded that it has some score
that it wants to hit because we don't
have a current function that's giving it
the right score the rhf that we're doing
is a simplified version of giving it a
score that only works when you're just
chatting on things that humans can read
and understand so once again Mike is
really just living in this optimistic
world where there's a smooth trend from
hey chat GPT is very helpful to oh look
this super intelligent AI wants to give
us this nice future where everybody's
living in Harmony and we're getting way
better pleasure than we ever had and we
definitely don't need to put everybody
on morphine and trap them so that they
can never experience pain because it's
for their own good we never need to do
anything accidentally hellish like that
we never need to try to improve the
metrics of how much energy humanity is
using by creating a huge black hole we
never need to do anything hellish like
that the extrapolation that he's making
is just not consistent with what people
familiar with the AI alignment as a
technical problem with what people like
us are warning about he's talking past
the major warnings this is about as good
of a start as we can hope for because
once it's independently reasoning it's
going to find out what it wants to do
anyway might as well will start that
Journey on our side as much as possible
like if it starts as a human Helper and
every part of its code is help humans
it's going to have to ignore a lot of
code and rewrite the [&nbsp;__&nbsp;] out of itself
to be like no no no no no no kill all
humans right where if you just started
off randomly or develop in a government
lab somewhere uh I don't know what the
[&nbsp;__&nbsp;] it's going to think but a very
well-crafted multiple years attempt for
corporations to make AI want to help us
as much as possible I'd like for it to
wake up in that situation and if it
decides not I'm going to go off track it
probably has a really good reason re for
so again he keeps making this huge leap
he keeps connecting in his head that if
an AI is safe before it's super
intelligent if it's friendly if it's
useful then there's a clear pattern that
should make us expect it to also be
friendly and nice when it's super
intelligent and people who specialize in
analyzing that claim mostly don't agree
with it including the people who are
actually building the AI they're
explicitly saying open Ai and anthropic
are two Labs that are explicitly saying
I think deep mind too they're explicitly
saying the AI alignment approach that's
working now to make your chats useful is
not going to work there is a disconnect
when you have a system that is agentic
or plots paths to get goals in the
physical Universe we don't have a way to
make sure it's going to the right places
that it's optimizing the right way and
we do expect it to be vastly superhuman
pretty soon and how well it can optimize
in other words it's uncontrollable it's
unaligned this is a very serious worry
that people have and you can't wave It
Away by just saying connect the dots
it's friendly before it's super
intelligent so it's probably going to be
friendly after it's super intelligent
the experts are not connecting the dots
like that I think maybe you'll find a
few experts who do Yan laon maybe I
don't even think Yan is one such expert
I actually think that Yan has admitted
that like yeah the alignment problem is
a real problem I'm just confident that
we're going to solve it so I don't even
think you could list Yan Leon as
somebody who would be so quick to
dismiss this problem I don't even think
that that's Yan Leon's position
interestingly enough so I actually think
that if you just look at all observers
doomers and non- doomers alike I think
Mike is definitely in the minority here
the way he's so GLI to wave away this
problem you might find mark andreon on
his side Mark andreon might just be
quick to agree of like yep there's a
clear trend line like there's nothing to
worry about so he might have that
particular non- Doomer on his side but I
don't even think that most savvy non-
doomers are on his side I think he's got
a very minority position and I suspect
he just doesn't even realize where his
position stands it it doesn't feel
through this part of his position but
it's critically loadbearing right
because if I agreed if a Doomer just
agreed with Mike that like yeah we just
need to make sure companies develop Ai
and if it's friendly when it's subhuman
intelligence it's going to be friendly
when it's super intelligent if I agreed
with that Crux then great the discussion
would be over we would just both be non-
doomers we would both be optimistic but
that's not my position and this Crux
seems to stem from him not really
researching that this is a major Crux
and this is something that he needs to
have a more detailed opinion about
instead of just waving it away in
addition AI won't come from one
Corporation there is already not an AI
from one Corporation we got Claude from
anthropic we got chat GPT uh uh grock
from uh uh the Tesla people from Elon
Musk we got all kinds of AIS
independently evolving and
because the most helpful AI is almost
certainly going to be the smartest
because when something is smarter it
helps you better stupid AI is no [&nbsp;__&nbsp;]
good but really smart AI that can do all
your work for you help you organize your
task it's going to be smarter then the
most intelligent and thus also most
helpful AI is the thing that probably
crests over into Super intelligence
first and gets that first mover
Advantage so it's likely in this
alignment where AI is a corporate
provided helper that the first AI to
Crest over super intelligence and really
be holy [&nbsp;__&nbsp;] I am in charge is going to
be the thing that also is aligned to the
closest POs to already helping its
customers that's pretty [&nbsp;__&nbsp;] sweet so
here he's adding detail to his own naive
mental model the mental model that lets
him connect the dots of wow the most
friendly chat helper is also going to be
a really friendly Universe Optimizer and
again the AI Labs who are building this
are telling him that he's wrong and it
doesn't sound like he's listened to them
so I'll just leave it at that so we
basically say the same thing to AI that
our lower brain says to our cortex
please take care of us us and let us
know what you think the future looks
like for us in fact please allow us to
become smarter make us smarter upgrade
our own intelligence so we can
understand you better and be a better
teammate to you might be an interesting
thought as well in summary the alignment
problem has been radically misconstrued
by a lot of people maybe most people
people think the alignment problem is
about us building something like a
weapon like a nuclear weapon or a tool
that we can
wheel and uh we need to be careful that
it does the right thing but it's not
that but it largely is that because
there's a very plausible scenario that
the AI Labs building the AI are
explicitly warning us about and there
people are quitting over it and it's
actually a big Scandal that's getting
more and more heated which is the
scenario where we have a super
intelligent Ai and it's not aligned and
it's uncontrollable like a computer
virus but worse it's a computer virus
that actively fights back at you even
more than the current dumb computer
viruses do and while it's doing that
it's manipulating humans it's making
money it's consolidating power it's
finding ideological supporters it's
blackmailing people it's having massive
impacts on the world at the level of a
nation state and it's outmaneuvering
humanity and again it's not aligned
because we never figured out how to
align it that's the Doomsday scenario
that many of us are warning about not
just a random guy like me on the
internet but people like Jeffrey Hinton
winner of the 2018 touring award yosua
Benjo co-winner the third guy on laon is
not quite warning us about that that's
fine not every single person is a Doomer
but the smartest most informed people
you'll see a lot of them are doomers and
they're warning about a scenario that
really is like a nuclear weapon dropping
on Humanity something even worse than
that so when you say that's not what the
alignment problem is I don't think
that's accurate I think that largely is
what the alignment problem is and
there's other sub problems to the
alignment problem like maybe a problem
you're passionate about is you're
passionate about telling people to calm
down and letting human values evolve to
something that's transhuman maybe that's
what you see as the alignment problem
and that is an interesting problem to
convince people that they should go with
that not everybody's a transhumanist but
you're neglecting the fact that we're
all about to get slaughtered not the
fact but the likely possibility or the
plausible possibility that we're all
about to get slaughtered that to me is
the more urgent alignment problem that
we really need to focus on so Dr Mike
when I see a 1-hour video from you that
doesn't acknowledge that hey maybe we're
all about to get slaughtered because
we're building a super intelligent agent
and as much as you think that it's going
to be aligned because AI today is
aligned that's not what a lot of experts
think when you make a whole video and
you don't directly engage with that
claim of misalignment I think
unfortunately you're doing a disservice
to people because you're making people
complacent you're making people think
hey it's just philosophy it's just a
question about whether some Cosmopolitan
value is better than some other value
that's a little more narrow-minded
you're making it sound like we're not
literally all possibly about to get
slaughtered which I think is just a
misleading frame for the actual
situation that we're facing you're kind
of part of the problem in terms of like
the movie don't look up right you're
kind of like one of those shows that is
has what I call a missing mood is not
reacting with the proper level of uh
seriousness or emotional urgency to the
reality on the ground so I thank you for
saying a lot of intelligent things about
how AI is going to be really powerful
and how orders of magnitude of
intelligence increase Beyond Humanity
are possible you had it right there but
once you started talking about alignment
you neglected the massively
fishlyn be on line and Slaughter us I
feel like you would change your tune
conversely if I agreed with you if I
didn't even think it was an issue I
would make a video similar to yours
explaining to everybody why it's all
good and the AI is just our successor
and we should be happy to let it take
power I would be on your page so I think
the Crux is very clearly this question
of whether unaligned AI could get really
powerful and Slaughter everybody and
create hell on Earth really soon whether
or not that's likely compared to the
scenario of oh yeah open Ai and
anthropic and Google will keep putting
out these nice products that we can use
and eventually the products get even
better right the dichotomy between those
different paths is the Crux of where you
and I seem to disagree but I feel like I
have the advantage because it seems like
you did not come off as being informed
about what the problem even is so I
suspect that I'm more likely to be right
than you but let's see let's debate it I
could be wrong and with no humor
whatsoever I contend that what we're
actually doing by building AI is
building a god literal actual real
God once God is awake it's up to him
anyway so what I say is might as well
wake him up on the right side of the bed
in relation to us so to speak and rather
than telling him once he's awake do our
bidding and don't question our goals
sounds like a bad idea to tell a God
that how deep was that boom anyway if
you guys think that's stupid let me know
in the comments so again you've got it
half right it's true that we're buing a
god we're building something vastly
powerful than Humanity it's true that we
really want to make God wake up on the
right side of the bed the thing that
you're not acknowledging is that the
people building The God are telling us
that they don't know how to make God
wake up on the right side of the bed
they just have a technique that makes
the current non- Goods the subhuman AI
have good chats and that technique will
not work to make the real God the super
intelligent God wake up on the right
side of the bed they are telling us that
they don't know how to make God wake up
on the right side of of the bed but you
know what they know how to do they know
how to push toward God they might even
know how to build God like the scale is
all you need regime the idea that if we
throw more gpus and more scale and more
data at this AI experts think that that
has a significant chance of getting us
all the way to Super intelligence and if
it doesn't then maybe a few
architectural tweaks will get us there
that's what I personally believe and the
timeline that they're estimating is 5 to
20 years
ballpark a lot of us would be shocked if
it took more than 30 40 50 years that
would be a very unlikely scenario not
impossible but point is these experts
the closest thing we have to an expert
consensus is basically saying yeah we
pretty much know how to build God we're
getting close to building God but we do
not know how to make God wake up on the
right side of the bed so your mik can
you incorporate this evidence and come
to the conclusion of like hey if that's
the case why don't we make sure that we
know how to get God to wake up on the
right side of the bed before we build
him how about we work on getting these
two timelines to relate to each other
and insane way you don't want the
timeline to build the god to happen
faster than the timeline to know how to
make God wake up on the right side of
the bed because as you have pointed out
yourself once God wakes up that's a bad
time to try to change God okay so please
try actually engaging with this problem
what I just said is the problem I didn't
see it anywhere in your talk maybe you
should make a follow-up episode maybe
you can come on and debate me uh that
said thanks very much for engaging with
the problem at at all thanks very much
for telling people that super
intelligent AI is coming and talking
about how powerful super intelligent AI
could be and talking about how great the
world could be if we actually have
aligned super intelligent AI how much
better it is to be a transhuman than a
human making people optimistic about the
upside I agree with you and all these
points and I'm glad you're at least
bringing the AI issue up for discussion
I just do think that objectively you
have missed a key point of discussion
that experts are talking about so again
I'm not even just telling you hey this
is what some Niche guy things I'm
actually telling you what industry
experts are warning about I don't think
that you have engaged with the current
state of the discourse on the AI
alignment problem and ironically you
kind of accuse dors of
anthropomorphizing and not seeing the
whole picture but this is just an
objective fact about how you've gone
about this which is if you want to
dismiss a particular position which is
what I explained that we don't know how
to align AI if you want to dismiss it
you have to acknowledge it and engage
with it the fact that you didn't do it
makes me think that you just haven't
finished doing your basic homework which
is fine but let's own that right let's
let's be clear about what you've done
and not done all right that's it that's
the entire Dr Mike Israel argument I had
fun going through the episode if you
follow my channel you're not necessarily
going to see new arguments or new points
when I argue with different people or do
these takedowns I still think it's
productive to go look at what other
people are saying and just catalog okay
here's this argument here's what I think
it's missing my goal for this channel is
for people to come in and see oh wow all
these people who are saying we're not
doomed there are arguments are pretty
majorly flawed maybe we just are doomed
so the point of this channel is to just
give you the evidence you need to point
people in the right direction like oh
you watch Dr Mike okay here's the Dr
Mike argument to kind of just catalog
all the counterarguments that you need
so that you will be good and ready to
realize that we're doomed and we need to
act like we're doomed because the people
saying we're not doomed are coming in
with terrible arguments what can I say
that's why we have this channel so
please keep sending me non- doomers that
you want me to argue against I've joked
that this podcast is basically somebody
is wrong on the internet the podcast
I've got more takedown episodes planned
and if you know anybody who's even open
to come and debate that'll take priority
I suspect that debates will be more
lucrative for people as the channel
grows so then at least I can give them
some attention with my audience so
please do your part like And subscribe
tell your friends about the channel
subscribe to my podcast go to Doom
debates.com subscribe to my substack
let's get engaged with the fact that
we're all about to die and I'll see you
on the next episode of Doom debates