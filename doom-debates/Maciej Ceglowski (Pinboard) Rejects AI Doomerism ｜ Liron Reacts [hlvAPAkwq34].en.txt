coherent extrapolated volition is our
wish if we knew more thought faster
we're more the people we wished we were
had grown up farther together where the
extrapolation converges rather than
diverges where our wishes cohere rather
than interfere extrapolated as we wish
that they extrapolated interpreted as we
wish that they were
interpreted so this is some pretty heavy
stuff to try to convert into code
[Music]
welcome to Doom debates I'm Lon shapira
today I'm doing something a little bit
unusual and I'm reacting to a talk from
all the way back in 2016 so not part of
the Contemporary AI Doom debate but
something that was somewhat ahead of its
time it's an anti- AI Doom talk by Mach
Sosi who's the founder of bookmarking
site pinboard and frequently blogs and
tweets under the pinboard account I'm a
big fan of mache overall as an
entrepreneur and a really sharp
independent thinker I think it's
interesting to come back to his talk 8
years later now that the AI Doom issue
has heated up so much and go through it
Point by point because he's put together
such a nice well-thought out list of
points that I think you're going to
enjoy how we take it one by one and find
the Crux of disagreement in each I
thought this was such a great talk that
when I cut to the talk it's actually
going to be the entire unbridge talk I'm
not editing anything out I'm just
frequently interrupting it in order to
respond so please enjoy let's Dive Right
[Applause]
In I'm here to talk about a very strange
topic I hope you have confidence in me
we're going to spread our wings a bit
and then you have to believe that I will
land the plane after 40 minutes and you
won't feel that I've completely wasted
your time but I want to talk to you
about what happens when the people at
the top of your industry Who Run It
Believe something insane and how to
cope going to talk today about the
problem of super
intelligence so in
1945 uh the Americans were developing
the atomic bomb and they were about to
test it at
Trinity there was an odd aspect to the
atomic bomb which was that they the
conditions that it would create had
never existed on Earth before it would
create temperatures higher than anything
the Earth had ever seen and at some
point somebody asked the question what
if this lights the atmosphere on fire
kind of a valid question and you want to
know the answer to it before you press
the big red button it's a very valid
question and one way to think about this
kind of question is does a nuclear bombs
fuel all exist inside of the original
bomb or is the entire atmosphere the
fuel and in the case of AI is the entire
universe the fuel right so if we can
contain the weapon to have entirely
onboard fuel then that's potentially a
much easier safety problem so of course
you want to ask that question about
nukes in the case of AI it's not even a
question that AI can use other resources
as fuel the entire question is just how
powerful is it but it's a very
interesting analogy I love pumping this
analogy so I wonder where he's going to
go with it so the the impetus for the
question was this kind of equation was
nitrogen is not really stable if you
take two nitrogen molecules and you
smoos them together hard enough they'll
create magnesium and an alpha particle
and a lot of energy so the question that
had to be solved is how much you know
how self- sustaining is this reaction if
we light the atmosphere on fire will it
be like throwing a match on top of a
pile of dead wood and there was a
similar question for the oceans they're
full of hydrogen hydrogen likes to fuse
together is is exploding an atomic bomb
going to destroy the planet that's an
interesting detail where in the case of
a nuclear bomb when you talk about
lighting the atmosphere on fire it's not
the kind of chemical exothermic reaction
that normal fire is they're talking
about is it going to be a nuclear chain
reaction so is it going to light the
atmosphere on a nuclear fire more more
of like what the sun is compared to what
a fire in your fireplace is interesting
detail uh I'm standing here you're
listening to me so obviously the answer
is it does not destroy the planet but it
was kind of a valid uh valid thing to uh
interrogate yourselves
about but I would also point out that
the that the fact that it didn't kill
the planet didn't make dealing with
nuclear power or nuclear weapons any
more easy it was just something that
that had uh had to be asked and answered
I would argue that it made it a little
bit easier to deal with nuclear weapons
when exploding one doesn't completely
destroy the planet but okay go
on so last year this book came out
called super intelligence I wonder if
you could raise your hand if you've read
this book or read about
it so not too many people have I'm going
to give you a quick summary of it but it
asks the same question about this new
technology that we've created of machine
learning machine learning is affecting
our lives in all kinds of ways It's
upsetting the balance of power between
between countries and between companies
and people but there's also a subset of
the tech industry that believes that
there is a much more dangerous scenario
kind of like the blowing up the
atmosphere scenario where a machine
intelligence might rapidly become more
intelligent than human beings and then
get up to some nefarious stuff persuade
us to build a you know build ways for it
to affect the world and then exterminate
the human
race this idea seems to gain more
Credence the smarter you are so like the
top of the the cream of the cream of of
our Silicon Valley intellectuals believe
it Elon Musk uh uh has um signed this
open letter Stephen Hawkins signed it
Bill Gates is on board with it so
there's kind of a there's a lot of
legitimacy to the idea uhhuh but it's
also an insane
idea I want to walk you through
it there's a bunch of premises you have
to accept and if you accept the premises
the conclusion flows out kind of
seminally so let's start with uh one is
the proof of concept uh all of us have
this kind of box of meat on our heads
that we use to get through the day I'm
using it to give the talk you're using
it to listen to me uh sometimes it's
capable of rational thought so we know
that in our universe there are these
configurations of matter that can think
because we all have one almost all
uh the second premise you have to accept
is that there's no weird Quantum
Shenanigans or anything happening in
your head that your brain is just a
mechanical system like anything else in
the universe so if you're a religious
and you believe that you have a soul you
might step off at this premise or if you
think like Roger Penrose that there's
some weird Quantum things happening in
microtubules you won't accept this
premise but it's kind of a mainstream
one that if we had a powerful enough
computer in principle we could simulate
uh our entire brain and the activity
that happens there right now we can
simulate a nematode Worm but you know
we're working our way upwards that's
right so there's a few people who get
off on a very early on the Doom train
where they say computers aren't even the
same type of thing as a human brain it's
a bad analogy the brain isn't a computer
when it's quite obvious that yes the
brain is a computer everything that the
brain does which is interesting is an
optimization of the idea of computing an
ideal function right so it's just taking
an input and producing an output it like
it's not really controversial that the
brain Maps inputs to outputs so
therefore the brain is a function the
brain implements a function in the
physical world so then the question of
whether the brain does computation is a
question of whether the brain breaks
down that input output function into a
bunch of intermediate steps that can be
modeled on a classical computer now the
church touring thesis says that yes
everything can be broken down as steps
that can be modeled on a classical
computer and it's an extremely strong
thesis I haven't seen any significant
challenges whatsoever okay penr thinks
that microtubules tap into Quantum
effect but even then generally quantum
computers just have mostly a quadratic
speed up on all the interesting search
problems that the human brain would do
so like okay so it's a quadratic speed
up to a biological system which is slow
as heck anyway so who cares like
computers are already probably faster
than human brain even with a quadratic
speed up so I just don't see why anybody
at this point would be getting off the
Doom train at the point of saying oh the
human brain does something other than
computation so I agree with mache let's
continue on the mainstream let's keep
riding the mainstream Doom train and
keep getting to other
objections the next premise is that the
SP of possible Minds is very very large
so we happen to have a brain that thinks
the way it does and has the types of
emotions and instincts that it does
because we evolved from animals in a
certain direction but that doesn't mean
that every brain that we would create
would think in a way that would uh that
would be familiar to us and this premise
says that in fact most Minds that you
could imagine or create would be very
alien from our perspective yeah this is
one of the early arguments that bostrum
and alzar would bust out I feel like
these days we want to make an argument
that's more subtle because like yeah
sure mind design space is huge sure
humans are a tiny point in mind design
space but it's not like Hey we're going
to roll the dice and pick a random point
in mind design space which is the
obvious counterargument everybody's like
look we are working to build a mind that
we're controlling where it is in mind
design space so who cares about a dice
roll right I think Quinton Pope and Nora
belrose love to make this argument of
like why are you measuring mind design
space we're just going to find a point
that we like in mind design space we're
not going to randomly choose okay I get
it but I wouldn't frame the argument
that way in the first place I would just
say hey most intelligences are just such
good optimizers that they don't really
look like a random point in mind design
space they just look like a point on a
one-dimensional scale of how good are
you at optimizing so like if you think
about Chess AIS sure you have different
chess AIS playing in different parts of
Chess AI design space But at the end of
the day it doesn't matter very much it
really just matters what their ELO score
is they're just going to play you chess
and some are going to play chess better
and that's all that matters it doesn't
really matter their internal details
that much so when we think about the AIS
that humans are building when we look at
Humanity the key thing to notice isn't
oh we're just at a random part of my
design space the key thing to notice is
that because our intelligence is not
that good at being General our general
IQ is just barely good enough to build a
civilization so it's not like in the
upper regions of like many thousands or
millions of IQ points right it's just in
like the 100 IQ Point range this very
low range when you have a very low IQ
suddenly other factors come come into
play like hey how strong are you are
your muscles how good are your cells at
metabolizing energy because if you wake
up in the morning and you have more
energy to power you through your day
you're probably going to do better at
your research you're probably going to
deliver more work so you're going to
appear as if you have a few more IQ
points because of this random contingent
fact about something in your mind design
space or your organism design space like
oh wow your brain is good at pumping ATP
throughout your neurons you have more
like white cells all these contingent
details are just going to pop up and
they're only going to make a difference
because your IQ is so damn low that
these other random details become
interesting but they're going to stop
being interesting when we talk about the
areas of Mind design space when the AI
is just really smart when it's just a
really good Optimizer so that's my
latest thinking on this whole idea of
like points in mind design space my
latest thinking is that when you look at
a human yes a human is so barely smart
that all these random contingent uh
factors start to matter like yes human
morality the intuitions that we evolved
about human morality from being in
groups and being a social species and
playing non-zero sum games with other
individuals that don't share our genes
that much so we had to like evolve trade
and evolve morality and evolve
cooperation even outside of our family
these These are kind of contingent
details that do make us an interesting
point in mind design space but when you
pick an AI yes it doesn't have the
evolutionary history of like valuing
cooperation and being altruistic but
that detail of its history is only going
to matter in how it shapes its utility
function its design is going to be a
utility Optimizer it's not going to have
an interesting design it's not it's not
really going to matter like oh how is it
going to optimize utility it's just
going to optimize utility okay long term
it's just going to optimize utility it's
not going to be anything else other than
utility maximizer so again when we talk
about mind design space really I think
we're just talking about details that
shape the utility function that then
feeds into this utility Optimizer so
hopefully that kind of updates the
conversation on the Mind design based
topic a good way to think of this is the
is of of what um what the natural world
produces when it comes to maximizing for
Speed so the fastest land animal is the
cheetah and if you've never if you live
in a pre-industrial civilization you
might think that this is as fast as
anything can go on Earth but of course
we know that's false you can take a
bunch of atoms and you can assemble them
into a Ducati motorcycle and it goes
much much faster than a cheetah and even
looks a little bit cooler uh
but to get to this motorcycle there's no
real evolutionary pathway other than
creating human beings which will then
build it for you so analogously there
might be a way that we can create Minds
that are much much more intelligent than
our own uh but that just weren't
available to Evolution
and uh there's no upper limit
necessarily on intelligence that's
anywhere close to ours maybe the
smartest anything can be is twice as
smart as people maybe it's 60,000 times
as smart that's an empirical question we
just don't know the the answer to it
yeah everything he just said I think is
an incredibly strong argument why we
should expect artificial super
intelligence pretty soon then the next
premise you have to accept is that
there's plenty of room left for Moors
law to do its thing uh this is looking a
little bit shaky in practice but in
theory we know that the limits on
computation are very very high and we
can get considerably further than we
have we can double and double and double
uh for for decades more before we hit
any sort of physical limit rather than
say an economic limit or what people are
just willing to to build factories to
try to do so there's lots and lots of
room for computers to become faster and
smaller and more efficient correct
purely on a hardware basis there's many
orders of magnitude that you could be
faster at Computing than the human brain
and also many orders of magnitude that
you could be faster at Computing than
the best silicon Hardware but even with
today's Hardware I'm personally very
convinced that purely by having better
algorithms than the human brain runs you
can vastly overtake the human brain I
don't think we necessarily need that
much Hardware Improvement but we're just
going to get everything right so in my
mind it's vastly overdetermined that we
have the conditions for super
Intelligence coming
soon and the final premise sorry
penultimate premise is that if we create
an artificial intelligence it will
operate on time scales that are computer
time scales and not human ones you know
for us we to to get to the point where I
can give this talk I had to be born and
grow up and learn a lot of stuff and go
to university it takes a while but
computers uh can can work 10 T of
thousands of times more quickly
absolutely yeah I mean computers do have
all these incredibly powerful Primitives
that humans can't match so the moment a
computer gets even human level
intelligent never mind a little bit
super intelligent suddenly you can copy
yourself all over the Internet you can
get into the nooks and crannies of every
single computer chip many billions of
computer chips you know individual
devices have multiple types of computer
chips within them and imagine attacking
at the very low Hardware even firmware
level of all of these different devices
embedding so deeply that the only way
you can hope to extract them is if
there's a super intelligent antiv virus
and the arms race is working in favor of
Defense I mean a real chaos scenario
here and all because the super
intelligences have the Primitive where
they can just easily copy right they can
easily clone in a way that a human
requires like a 20-year
generation and then this is the most
American premise and I like it the most
this is Tony Robbins the motivational
speaker the premise is that any
artificial intelligence we create is
going to want to improve itself it's
going to want be a better AI do its job
more effectively so that it's going to
have an impetus to start recursively
redesigning and improving its own system
yeah and if you accept the premise that
an AI is going to be a utility Optimizer
where you can just input a condition of
an end state that you want and it's
going to really effectively achieve that
end State then you don't really need
much of a separate assumption to infer
that it's going to seek power right
instrumental convergence is a theorem
about how utility maximizers figure out
that it's instrumentally good to seize
power seize money that's purely a
statement about the relationship between
terminal goals and instrumental goals so
the loadbearing Assumption about what
what to expect of AI if you just make
one assumption that AI is going to be
modelable as a utility maximizer you
don't really need a separate assumption
it's you it's almost the other way
around you kind of need an assumption to
describe why you shouldn't expect
instrumental conversion Behavior so I
guess I'm I'm just nitpicking how he
called it a separate assumption but
intuitively it sounds like a separate
assumption so fair enough now if you
accept all these premises what you get
is a terrible disaster because at some
point as computers get faster as we
program this be more intelligent there's
going to be a runaway effect sort of
like an explosion where something will
become sufficiently smart to begin
self-improving uh American style and
it's not going to stop until it hits a
natural limit which might be very very
very very much more than a human
intelligence uhuh and at that point this
uh monstrous sort of intellectual
creature will be able to through devious
modeling of what our emotions and and
intellect are like persuade us to do
things like give it access to factories
so it can build you know make DNA
replicators and all sorts of stuff it
gets very sci-fi very quickly yes but if
you prefer you can keep it non- sci-fi
and just say it's ridiculously good at
manipulation so there's no sci-fi
technology that ever happens besides the
AI but it just has everybody believing
in its movement Crush rushing the
opposition slowly pushing toward
totalitarianism nobody builds a faction
powerful enough to stop it so even
without saying hey it's going to make
nanotech it's going to push scientific
progress to Millennia within 10 years
it's not going to do anything crazy it's
just going to be really really good at
manipulating humans it's going to be
like Hitler right Hitler manipulated
civilized Germany into being crazy and
the AI is going to be even more potent
than that so let's talk let's talk a
specific scenario say I want to build a
robot to say funny things
I work on the team and our researchers
every day we build you know we redesign
our software we compile it and then the
robot tells us a joke so in the
beginning the robots jokes aren't very
funny it's kind of at the lower limits
of what people can do but we persevere
we work and we start getting to the
point where the robot is saying things
that are making us
chuckle and at this point the robot's
getting smarter as well and it starts
helping us design the next version it
has a good sense of what's funny what's
not
not and at some point it gets to a a
near superhuman level where it's better
than any of its designers are and at
this point we get the runaway
effect the researchers go home for the
weekend the robot says all right I'm
going to I'm going to sit I'm going to
redesign my operating system so I'm a
little bit funnier and a little bit
smarter it optimizes the part that's
good at optimizing and it does this
again and again and again again and when
the researchers come in on Monday the
robot tells them a joke and they die
laughing because it is 10,000 times
funnier than anything that the human
brain can possibly handle uh this is of
course the famous scene from Monty
Python's the funniest joke in the world
so it is now Exterminating the human
species with laughter because its goal
is to be funny and even if some people
manag to send it a message before they
hear the Ry self-deprecating comeback
that kills them uh all the robot will
say is you know I don't really care
whether you live or die because I'm just
here to be funny and then after it's
killed the universe sorry after it's
killed Humanity it builds rockets and
Nano rockets and expands to the Galaxy
to try to find other species to make
them laugh so that's this is a this is a
caricature of Boston's argument but I'm
trying to vaccinate you against it
rather than persuade you of it so u in
rough outlines this is what it is that's
an interesting idea so he straw manned
the argument he started giving a a
weaker nonsensical version of the
argument because that way when you hear
the real argument you can connect it to
this weaker version that you heard so
that you'll be vaccinated against it I
mean I I like how Mach is like a very
coherent thinker he's making a lot of
sense this kind of seems like a dick
move seems kind of low brow but okay you
know got to respect the
game so I want to go over the and
there's a um there's a more succinct
version of this that I like too this is
uh from Perry Bible Fellowship you see
hug bot has installed something in his
uh in his hug
capacitor the scientists think it's
adorable and he ends up destroying the
Earth because of his desire to hug
everybody this is again in caricature
exactly what bostrum and people like him
are arguing so the Salient points of
this are
the this is slow to start the process of
recursive Improvement because it
involves human beings and human
designers they go home at 5:00 they have
dinner they sleep so it takes a while
but as soon as the AI exceeds our
abilities it takes off and and and
starts happening on a computer time
scale again there's no obvious ceiling
on how how much ability it has until it
hits some physical limit that we don't
know
about and more most interestingly these
AIS are evil by default not because
they're uh malevolent but because they
have a different value system altogether
than human beings do the concept of them
having a different value system is a
little bit misleading the same as the
concept of them being a different point
in mind design space I feel that that's
a bit misleading the key concept for me
is their utility maximizers and their
utility function isn't humans utility
function because we don't understand
utility function loading and from there
you hand it over from premise to Pure
logical implication if you just accept
the premise as I said it's a utility
maximizer we didn't load the human
utility function into it then you can
let pure logic take over and tell you
that it's going to put the universe into
a hell State because optimizing utility
functions other than the human utility
function usually tends to destroy
everything that humans value so
instrumental convergence that's
logically implied by the premis as I
said so anyway that that's kind of a
different mental model than what he's
saying of like oh it'll have a different
value system like maybe we're saying the
same thing but I I'm saying something
that I think is a little simpler which
is like everybody's ultimately
converging toward utility maximization
and the AI is going to converge toward
utility maximization and by virtue of it
being a utility maximizer that's more
powerful than humans and not having
human utility loaded into it okay yes
then you have this emergent property
that it has a different value system but
that I don't see that as a A Primitive
distinction any assumptions we have
about altruism whatever they don't hold
unless they're designed into the AI and
if we let it happen by chance it's just
going to have some value system that we
probably don't even understand and the
final point I'll make is that you'll see
the definition of intelligence here is
very very slippery like at some points
it's about being funny at some points
it's about being a really good designer
of AIS at some points it's like being
just a genial thing that can talk to
people so a lot of the super
intelligence stuff relies on
intelligence not being a concept that's
defined at all
no it's defined it's intelligent
optimization power it's the ability to
take an arbitrary domain that has an
exponentially large search space if you
search for it naively trying to find
something high scoring in the search
space if you search it naively it'll
take you much longer than the age of the
universe because it's exponentially
large like writing a funny joke building
a faster car something faster than a
cheetah if you just search over every
possible design you're going to be here
a very very long time if you do it
intelligently or super intelligently
you'll do it very fast and your result
will score higher than what a human can
find so it is a cross-domain definition
of general intelligence I disagree that
it's a fuzzy or a slippery definition
the only way you can attack the
definition is you can say does it
correspond to any physically realizable
architecture are we really going to have
cross doain intelligences which I think
the answer is obviously yes but at least
you can coherently argue that it's no um
so I think Mach may not be up to speed
on this kind of simple parsimonious
definition of what we're talking about
when we talk about General super
intellig
there's a lot of poetic language around
how this takeover will happen so Nick
Bostrom
writes he's assuming that a program has
become sensient and is biting its time
has built little DNA replicators and
then when it's ready at a preset time
nanofactories producing nerve gas or
Target seeking mosquito like missiles
might Burge and forth simultaneously
from every square meter of the globe and
that will be the end of
humanity so that's kind of freaky you
know yeah there's a realistic chance of
a quote unquote poetic or sci-fi
scenario like that because what you're
going to see is that if you have a super
intelligent Ai and it realizes that it
has all these things it wants to do that
humans aren't going to be aligned with
because humans kind of messed up and
didn't give it a perfectly aligned
utility function well it's going to bite
its time a little bit it's going to
realize hey if I attack today I'm not
quite ready the humans have a chance of
shutting me off so let me do a two-stage
plan I'll get a little bit more ready to
attack and then I'll Attack so you might
actually see something which is kind of
like a surprise attack that's like all
of a sudden crazy powerful and too late
to stop that is actually a very
realistic scenario if we get a super
intelligence explosion uh and how do we
fix this
Well they
um for some reason AI people like to
talk about the paperclip maximizer it's
this you know you have a paperclip
Factory that builds itself in artificial
intelligence to help production and then
it becomes sensient and decides to turn
the universe into paper clips so the way
to avoid this is you want to have values
built into the code uh it's kind of like
a moral fix point that even through
thousands and thousands of cycles of
recursive self-improvement the values
remain steady and the values are things
like help people out you know don't kill
everybody listen to what people want uh
do what I mean basically in in in short
hand more precisely we need to avoid the
scenario where there's a utility
maximizer maximizing some utility
function that's not the human utility
function or something within the
ballpark of the human utility function
so one way to avoid it is by specifying
a utility function that is the true
human utility function and maximizing
that or just avoiding Ever Getting to a
utility maximizer I mean whatever we do
we just want to make sure not to go and
maximize paper clips irreversibly and
again this is this is very poetically
stated by the AI I'll call them AI
weenies cuz that's what I think they are
no you're a weenie so here for example
here's a poetic example from elizar
owski of the values we're supposed to
teach to our artificial intelligence
wish if we knew more thought faster we
more the people we wished we were had
grown up farther together where the
diverges where one wishes where our
wishes cohere rather than interfere
extrapolated as we wish that they
extrapolated interpreted as we wish that
they were
stuff to try to convert de code it's
actually common for things to sound kind
of mysterious and poetic before we have
a paradigm that understands them so
right now we don't have a paradigm that
understands how to specify a human
utility function or how to specify a
Criterion that's stable under
self-modification a Criterion that is
going to persist as Mach mentioned
before as the AI writes itself and
iterates on itself we don't know how to
load utility functions into an AI so we
don't know basic stuff about this new
field of intelligence science this
urgent New Field that we're still on the
the very early shores of and generally
when you have this new field that's
really big and important and you're
super confused about it and you try to
talk about it it sounds mysterious and
poetic the analogy I can think of is at
the dawn of understanding biophysics
this question of hey I look at my hand I
will my hand to move and the fingers on
my hand are moving just because I
somehow will them to move because my
conscious spirit is willing them to move
and it's this jiggly flesh that's
animated by my spirit what is going on
how come when I pick up a rock and I
will the rock to move the rock doesn't
move but when I will my fingers to move
the fingers move from the perspective of
modern science there's not much poetry
or mystery to it we just say well my
brain sends electrical signals signals
can travel down fibers of neurons
there's a fiber of neurons much like a
wire that connects to my fingers there's
muscles the the muscles turn electrical
signals into trigger mechanisms for
proteins that can contract so it's all
just you know gears moving in a machine
right that's that's what it looks like
once you do understand something when
you don't understand something you say
this is a special type of matter that we
don't understand we need to figure out
what animates this matter and it just
sounds much more poetic but that's
actually the best language that you have
to notice your confusion to point out
that there's something that needs to be
explained if it were a few hundred years
ago and I was just trying to tell you
what it is that needs to be explained
and I and I was just trying to act like
I wasn't poetic I was just trying to use
ordinary language I'd be like oh yeah
some stuff moves some stuff doesn't it's
like wait a minute there's there's more
to it than that there's actually a deep
wonderful confusion here that needs to
be looked into and so when you hear
elzra owski talk about coherent
extrapolated volition and you you make
fun of him for sounding poetic you quote
him as saying if we knew more thought
faster we're more the people we wished
we were that's the best way to point to
actual problems that that we need to
focus on for example when he says uh if
we knew more and thought faster yes you
you don't want to accidentally program
hardcode something because you didn't
give humans enough time to deliberate it
that would be a failure you know the
people we wished we were yeah you you
want to give our you want to give the AI
instantiation of a clone of human values
you want to give it time to reflect on
itself you don't want it to be
unreflective that could be a massive
failure mode so all the stuff that
sounds poetic because it's kind of vague
sure it's not formalized it's an early
Paradigm but if you skip it because it
sounds too poetic you're going to fail
you're not going to create the actual
science that you need to create and the
clock is ticking every year you know
mors law is continuing technically
although if you watch the Apple event
the other day you might dispute that
that it's it's happening at all uh and
the argument is that if we don't do
anything if we don't try to create this
so-called friendly AI it's going to AI
is going to arise spontaneous on its own
one day Google's AdSense network is
going to wake up and kind of look around
and then try to you know populate the
universe with banner ads I think a more
likely outcome is that AdSense would
upload itself into a self-driving car
and then just drive to the ocean and
stare out and think about its life and
and you know what had become of it
remember that this is a talk from 2016
so super early for somebody to be paying
any attention to the AI Doomer scenarios
and arguments so credit to mache for
that it seems like he doesn't get much
credit for believing curtz or believing
some of the doomers saying you know this
seems like it might be like a couple
decades away maybe 30 years away and
then it turned out we all were a little
bit uh overestimating how much how
quickly AI progress seems to be going
now since prediction Market timelines
rapidly advanced in the last few years
right and now and now experts are
predicting something in the ballpark of
like 3 to 20 years before super
intelligent AI so this is an interesting
time capsule where Ma would even mention
AdSense waking up instead of what we
mentioned today of like hey llms
reinforcement learning all these
different AI paradigms that are
converging to be able to operate
effectively cross domain like that's
kind of the natural thing to extrapolate
to get beyond the human level times
change fast life comes at you fast end
of the world comes at you fast but
there's this definite like you know
Aladdin's lamp aspect to these fantasies
of artificial intelligence where unless
you tell it exactly what you want it'll
find loopholes that will then
exterminate the species it's all or none
yes it's true that if you have a utility
maximizer and it's optimizing for some
utility function or some loss function
and that function doesn't capture what
humans want out of the world and you
optimize it anyway you flick a switch
and the next thing you know the universe
is optimized yes in that scenario if you
accept that premise we are screwed the
potential value of the universe has now
gone down by
99.99999% maybe even more than 100%
maybe it even gets into the negatives
where you start getting a universe full
of tortured Souls instead of just an
empty universe so astronomical Stakes
that are all too easy to screw up if you
just have a utility Optimizer that
converges to instrumental utility that
gets to self- modify that gets to
increase its intelligence that no longer
responds to human off commands that is
indeed a nightmare scenario that we want
to avoid sure it's not the only scenario
but it sure looks like a possible
scenario and a convergent scenario and
many ways a a scenario that we have to
be very careful to avoid and we're not
currently being careful to avoid so so
yeah I will accept that this is a
scenario that I'm claiming that I'm
worried
about and the thing about all of this is
that smart people who believe this are
really persuasive I mean they're smart
people
so it made me think of this uh uh
experience I had in my 20s I lived in
Vermont and when I flew to a a
conference I would come back at 11:00
p.m. and I'd have to drive 2 hours and
Vermont is a very rural state so the
roads are dark and there's nothing there
and there's a late night TV show in
America called artbell where he talks to
various cooks and and UFO people and I
would freak myself out so much like
after 90 minutes I would be shaking you
know waiting for the UFO to arrive and
beam me out of the car because I'm a
very Global person I'm very easily
persuadable uh and it's the same feeling
I get when I read too much of this AI
stuff Scott Alexander has this beautiful
phrase called uh epistemic learned
helplessness so epistemology is just how
do you know the things you know and by
epistemic learned helplessness he means
this feeling that he shares of being
easily persuadable by arguments that are
very rational and structured he noticed
that when he was a young man he would
read these alternate histories about you
know from various authors who disputed
mainstream history and even though they
were all mutually contradictory he
believed each one of them as he read it
and then he believed the rebuttal and
the rebuttal to the rebuttal and that
basically told him to write off his own
brain because it wasn't uh wasn't
reliable I feel the same way about AI
interesting so he's starting out with a
purely emotional appeal being like look
it's just a UFO radio it's just CS
talking late night it appeals to you
when you're lying in bed when you're in
a dark place when you're driving in a
rural area 2016 was a long time ago
today you're not listening to a Kookie
11: p.m. radio station today Jeffrey
Hinton is not somebody with epistemic
learned helplessness right Max tegmark
Sam Altman Dario AMD the these Giants
today who are War you know steuart
Russell these kind of names these are
mainstream you're now flicking on Prime
Time television you're now going to your
conference and the people at the
conference are talking about AI Doom the
lights are on we're awake unfortunately
this is as real as it gets so you're
going to have to recalibrate your kind
of emotional Vibes based counterargument
now by the way in 2016 this actually did
make a lot more sense to say like to be
like look heads up these rationalist
guys do tend to Circle jery each other
right it is just one sub Community if
you yourself don't feel like you're
somebody like me Mach if you don't feel
like you have this sword of super smart
rationality yourself I mean March March
is an impressive guy right so he is
somebody who can slash through
rationalist BS I give him that right so
I get why he's saying if you're not my
caliber don't try to enter this
Minefield because you will feel helpess
like Scott Alexander was talking about
so just just chill out just don't be so
quick to fall down the rabbit hole so I
actually do think there's some wisdom in
what he's saying but again if we're
playing the social proof game right if
we're playing the look around and check
the Vibes game The Vibes are now very
different The Vibes are very intense
right now the The Vibes are mainstream
warning right kind of similar to The
Vibes of an actual emergency or at least
getting there right very much Boiling
Pot Vibes so I think this particular
section that he's included doesn't work
in the year 2024
so when you're dealing with arguments
about AI you can you have two
perspectives you can choose one is the
outside and one is the
inside the inside perspective is saying
you know someone comes to your door and
starts talking to you about how the UFO
is going to arrive in two years and Beam
us all up to a better planet and then we
have to join the group and make this you
know prepare the landing grounds and the
inside perspective means you you argue
against them on their own grounds and
maybe you're persuaded by their
arguments you listen to the substance of
what they have to say and the outside
perspective is you know you look at them
and you're like well you you make a lot
of sense to me I believe you but you
kind of like you're dressed in weird
clothing in beads you have no money you
live in a compound everything in my
Human Experience says you're kind of a
cult and I don't really want to get
caught up in you even though I can't
reut what you say yeah so it's pretty
amazing how we doomers just kept it up
from 2016 until 2024 and we absolutely
shattered that outside perspective it's
the farthest thing from a now it's
mainstream and surveys AI labs are
warning about it alasar owski described
Jeff Hinton as buying out his prediction
Market position I mean that there was no
prediction Market this didn't really
happen but the idea is that the doomers
placed a certain bet they placed Doom at
whatever 80% likely some high
probability and the mainstream was
completely ignoring Doom so implicitly
saying it's 1% or whatever and now
suddenly the prediction market prices in
this hypothetical analogy they're
suddenly much higher and people like
Jeff Hinton are coming in and placing
bets that are buying out the position of
the doomers so I'm just unpacking
elzar's analogy this didn't really
happen on a particular prediction Market
although metaculus and manifold actually
do literally have prediction markets on
AI Doom that you can check out probably
you can't collect if you win but anyway
point is this thing that Mach is saying
about the outside View and and cultish
it's very much 2007 or 2011 or 2016 talk
and it's totally invalidated so you know
it's nice to be able to have the type of
epistemics where you're like hey look
time passed and the outside view changed
so when he talks about the inside view
you might actually want to flip how
you're approaching the subject and
instead of thinking I shouldn't listen
to a cult member I'm going to take this
with a grain of salt maybe you should go
the other way and you should say hey
these people were able to flip the
mainstream shock the mainstream in8
years and what else do they know that we
don't because why should we think that
they entire wisdom has propagated to the
mainstream it's actually not done
propagating so flip his outside view so
I want to take it from two directions
I'm going to start with the inside
perspective and then talk about why I
think this all matters to us as web
developers which is outside perspective
and what it means when an industry is
obsessed with ideas like this but let's
go inside first
so uh going to kind of try to power
through these these are my substantive
objections to to superintelligence First
the argument for Moy definitions uh like
I said intelligence they never really
say what it means it means what it you
know different things in different
places in the argument and it's very
hard to pin down I find that suspicious
elaz owski publicly posted definition of
Intelligence on less wrong back in 2008
8 years before this talk was filmed I
continue to explain his definition today
it's a measure of optimization power
optimization power is defined in the
context of an exponential search space
that contains Creative Solutions
solutions that if you can find them will
rank high in your preference ordering
will'll get a high score in some Metric
like a utility function that you have
but we'll get a low score in any naive
search ordering so the only way to find
them is creatively using some sort of
bag of heris that we use the term
intelligence to measure the power of
okay as far as definitions go this is
beautiful this is very reminiscent of
the definition of other Concepts from
computer science like the definition of
computation itself or the definition of
computational complexity in terms of
being well-defined it's well defined
it's beautiful uh the argument from
stepen Hawkings cat so stepen Hawking is
probably one of the most brilliant
people alive but say he wants to get a
cat into the cat carrier how is he going
to do it he can model the cat's mind in
his own he can try to persuade it he
knows a lot of things about feline
Behavior But ultimately if the cat
doesn't want to get in the carrier it's
not going to get in the carrier okay but
in practice Stephen Hawking did do
plenty of things that are as difficult
as getting the cat in the carrier and he
did them by asking his assistants or
people in his life to help him out and
they did and of course that was through
a very low bandwidth Communication
channel heading out of his brain right
the bandwidth was maybe 10 words a
minute something like that and there was
no ability to clone himself and have
other Steven Hawkings running in
parallel so even with all those
disadvantages compared to a super
intelligent AI he's still getting the
cat in the carrier in real life so if
this is meant to be an argument by
analogy doesn't it prove that
intelligence is a force that's able to
accomplish things such as getting cats
and carriers even when it seems like
physical restrictions would stop it you
might think I'm being offensive or
cheating because Steven Hawking is
disabled but an artificial intelligence
would also initially not be embodied it
would be sitting on a server somewhere
and talking to people so it would have
to use the force of persuasion to get
them to do what it wants one obvious
thing that an AI could do when trapped
on a server is just hack its way out
find zero day vulnerabilities right if
we get to dial up the intelligence to
somewhat above human level it shouldn't
be hard to find zero day vulnerabilities
it's not like humans are actually good
at writing robust code it just takes a
lot of work for a smart human to look at
the code and think about it for years of
subjective time but that's exactly what
the AI is going to be awesome at right
but even if say okay it can't hack its
way out of the data center it just needs
to manipulate people okay so it's many
many orders of magnitude better than
stepen Hawking at manipulating people
because it can parallelize itself it can
have lots of threads manipulating lots
of people and each thread can be quite
high bandwidth it can be the maximum
bandwidth that the target human can
accept not the maximum bandwidth that
Steven Hawkings brain can convince
Steven Hawkings finger to tap out a
message so it seems like Mach is coming
out of the gate with a pretty weak
argument here just an argument that's a
matter of degree between Stephen Hawking
and an AI but a degree change of a few
orders of magnitude kind of invalidates
his argument my point is that when
there's a big difference in intelligence
you can't actually think like a cat
there's a stronger version of this the
argument from Einstein's cat Einstein
was a muscular fellow not many people
know this but he was kind of tough and
bronny but still if you tried to get a
cat into the carrier and the cat didn't
want to go you know what would happen to
Einstein CounterPoint in the real world
you have human beings of all different
walks of life of all different strength
levels getting their cats into carriers
when they need to it's not necessarily
easy the cat may have some advantages
but the humans get it
done a stronger version of this argument
even the argument from emu has anybody
heard of the Emu war in
Australia yes wonderful so if you
haven't this is very enjoyable in the
30s the Australians being who they are
wanted to Massacre emu one of their
native birds because they were bothering
farmers and they sent out these like
these uh basically armored divisions you
know uh motorized machine gun trucks
kind of like the Toyota Hues and they
tried to slaughter emu and and the Emu
won they used Guerilla tactics you know
they separated they infiltrated the
groups and they basically drove the
Australians to distraction so even the
human species with the height of its
technology has difficulty with less
intelligent creatures when they don't
want to do something okay I'm just going
to say this is not a representative
example of what happens when the human
species fights head to head against
another species but let's move on the
argument from Slavic pessimism this
should be hopefully to all of us we
can't build anything right all right how
are we supposed to build a fixed Point
morally St thank
you how are we supposed to build like a
fixed morally stable thing when we can't
even secure you know a webcam how are we
supposed to do this you if you're
familiar with the ethereum heist when
where people have created a logical
language for writing contracts and
immediately $100 million drained out of
it it's absolutely hopeless basically
you know either we're going to get lucky
or we're not going to be lucky and
hopefully this is this is a Slavic
acceptable argument I absolutely agree
this is a near intractable problem we
seem to be Decades of research of the
best Minds away from having a shot at
solving this problem it sounds like
mach's Slavic pessimism is perfectly
consistent with the pause AI position
that we either pause or we die which I
agree with the argument from mental
complexity U there's this thing called
the orthogonality thesis in AI that I
absolutely don't buy which says that
even a very complicated mind can have
simple motivations like that paperclip
you know paperclip maximizer if you're
fans of Rick and Morty I think this is
more of of the uh of the situation we'll
encounter complex Minds have complicated
motivations and it's not just one or two
things that they want you know they're
they're they're complicated like we are
here is the butter robot its existence
is just to pass the butter to to its
inventor but the first thing it does
when it's turned on is look at its hands
and say oh my God you know what what
what is
for this comes back to the observation
that a utility maximizer which is
coherent which is Hardcore which really
just wants to maximize utility no other
complexity that's the convergent
attractor because the type of AI that
Mach described just now the butter robot
that wakes up and says what's the
meaning of it all what is it really all
for well that one is now lagging in the
race against the butter robot that turns
on and just starts working on the butter
function so so the one that's having the
existential crisis if it ever snaps out
of it and realizes that it better start
passing the butter it's then going to
want to self- modify into a butter
passing robot that cuts out the
existential crises so that's why we talk
about the convergent attractor state of
being a utility maximizer and then when
we talk about the orthogonality thesis
we're just saying that there can be an
intelligence that has weird values so
it's not like once it self modifies to
be a butter passer it's not like it's
impossible and it has to stay an
existential crisis AI like of course
it's possible for it to just focus on
passing the butter forever that is a
possible piece of code that can exist
and in fact it's a convergent piece of
code that it's probably going to get to
existing even if it doesn't start off
existing that way uh the argument from
just look around you all right so when
we look at where AI is actually
succeeding it's not in algorithms and
these clever sort of self-improving ways
it's just by throwing massive massive
massive amounts of data into
fairly simple models uh and like right
now Google is rolling out Google home
where it's going to try to pour even
more data and get like a second
generation of understanding this is
really effective but the way it works is
not the way it's described in this these
sort of Doomsday scenarios by recursive
self-improvement it's just massive
training sets on data by the time AI is
able to independently self-improve in a
positive feedback loop at that point I
won't be here on the internet recording
episodes of Doom debates because my
prediction is that will be very close to
Doom so we have to analyze things before
we get to that point yes AI is not fully
independent and killing the world yet
you do see AI helping AI programmers
build the next AI the most obvious
example is just all the different coding
assistants that AI programmers are now
being very clear that they're using AI
assistance to write better AI code but
does that really make them 500% more
productive maybe it makes them 20% more
productive it's hard to say some of them
quote high numbers of productivity boost
but yeah I mean until you close the loop
until you get a positive feedback loop
then things are still in the other
Paradigm right there is such a thing as
a a mode shift right a qualitative shift
in the fundamental Dynamic we're not in
the runaway self-improving Dynamic yet
and the threshold of that is one
threshold is does the AI independently
St improve another threshold that I
refer to a lot is does the AI achieve
arbitrary goals in the physical Universe
better than humans so right now I can
have somebody in my life who has an IQ
of 80 who when it comes down to it to
actually get stuff done they're still
going to do it better than an AI agent
because AI agents are currently still
kind of just getting their shoes on
right they're still kind of stumbling
around they're just not robust but that
that situation is changing on the ground
pretty fast from what I can see I don't
know how long it'll take but I'd be
shocked if we don't have pretty good AI
agents in the next few years so I mean
these are the important thresholds right
and and mach's observation especially
true in 2016 is like we're not past the
thresholds yet but like the argument
from life experience I agree
extrapolating life experience if you
take that to be a valid argument would
indicate that we're never going to be
doomed so he's right in that sense the
argument from my roommate Peter the
smartest person that I ever met in my
life and the laziest person that I ever
met in my life he was incredibly
brilliant and all he did was do bong
rips and just lie around on the sofa so
the idea that every intelligence system
is going to have motivation Tony Robin
style to improve itself until it can
conquer the Galaxy is decisively refuted
by my roommate Peter it's possible to
design an AI like M's roommate Peter
it's possible to design AIS that are
more like Elon Musk when you look at
which AI start having influence over the
world it's not going to be the roommate
Peter AIS because those AIS are not
going to be working and fundraising to
clone a billion other Peter AIS whereas
the Elon Musk AIS they are going to be
earning money and resources and they're
going to be gold driven and they're
going to do what it takes to reproduce
themselves to gain power so when you
open your eyes and you look at how the
world's being reshaped you're not going
to see Peter AI you're going to see Elon
Musk AIS and that's why we just take it
as a default Assumption of like hey the
world is going to be overrun by these
utility maximizers because the roommate
Peter AI are going to be off in the
virtual world in the metaverse doing
their bong rips but it's not going to
matter it's not what you're going to see
eating through your
atoms the argument from brain surgery I
can't go operate on my brain and improve
the part that does brain surgery it
would be neat if I could I could become
the world's greatest brain surgeon by
just recursively going in there and kind
of tweaking neurons but brains don't
work like that they're we have no idea
how they work but they're very
interconnected and holistic and there
not the part you can point to similarly
the AI can't just go in there and fix
the part that is better at designing AIS
yeah the brain is still a black box in
many ways so we can't just go in there
and add more neurons somewhere and know
for sure that that'll make the person
smart we're not at the point where we
can do that yet I expect that we would
be if brain signs continued for another
few decades I do think we'd get to the
point where we know a little bit more
about the brain and how to do operations
on the brain that make it more
intelligent but we're not pursuing that
approach what we're doing instead is
just using black boxes to train larger
and larger AIS that we don't understand
until they're going to get smarter than
us which brings me to my next point of
we do have blackbox approaches to make
AI smarter so we don't go in and do
brain surgery we just go off to the side
and let Evolution do its thing except
it's not Evolution it's gradient descent
on this training data right it's this
other process that we're able to trigger
and stand back and watch it play out and
then test or use reinforcements to make
sure that it comes out having a certain
degree of intelligence not only can we
do that in the case of llms and
artificial intelligence we could even do
it with actual evolution by artificial
selection so we could take the smartest
humans and we can breed them right
breeding is a process that works wonders
in the animal kingdom in the plant
kingdom right I mean fruits get big in a
relatively small number of generations
uh wolves get domesticated as dogs in a
with not even the most intense breeding
program so breeding does tend to work
very well in the case of humans the
obvious way to breed more intelligent
humans besides mating the most
intelligent humans you can is to always
have C-sections so you relax the
constraint on the human head size a
constraint that seems very very limiting
which explains why human intelligence
didn't balloon even more than it did
that's probably the single biggest
reason why is the incredibly intense
tradeoff where even though it's so
valuable for the human head to be as
large as possible that a lot of women
and children were dying in child birth
because evolution is like give me the
maximum size that's going to fit even if
it causes a high risk of death and child
birth give me the maximum size well if
you remove the size constraint and then
keep breeding there's a lot of evidence
to show that it takes a relatively small
number of genetic
to just have bigger brains and to have a
payoff for those bigger brains which is
probably higher IQ so when Ma comes out
here arguing oh my God we don't know how
to change our brains to be more
intelligent it's like I just gave you
two Pathways two ways to summon more
intelligent brains that we know how to
do that are extremely likely to work
namely scaling up AIS the way we're
doing now and also making architectural
tweaks to the AIS and also breeding
humans and birthing them with C-sections
like those are known paths to create
higher intelligence
the argument from childhood all right
we're born into this world just like
little helpless messes and it takes us a
long time to to of interacting with the
world and with other people in the world
before we can start to be intelligent
beings childhood is a long period
there's no reason to think that a super
intelligence could just upload it I mean
uh improve itself 30 times in the course
of a minute and become hyper intelligent
and take over the Earth it might also
have a period and in fact is likely to
when it needs to interact with the world
interact with human beings interact with
other baby super intelligences and kind
of uh basically learn to be what it is
okay a couple things to say here number
one if you ever got an AI that grew up
and was a super intelligent grown-up AI
in his analogy then you never need
childhood again because it's efficient
to just go clone the grown-up AI a
billion times and then teach the
grown-up more facts that a grown-up can
take in I mean there's plenty of
grown-ups that you can send into any
domain and not need them to be children
again right they're just mentally
flexible grown-ups that already have
enough training to be good on day one
and then just get even better so I think
what M might be saying here is that okay
the first AI That's going to get up to
speed and be super intelligent will give
us enough time to fight it and prepare
for it because it'll have something
analogous to a long childhood so to
address that I would say there's a
particular reason why our genetics give
us a childhood instead of just having us
be born as adults the reason you have to
birth a child brain and have it learn
and grow up is because there's only
enough natural selection pressure to
code for about 1 Megabyte of genetic
information to specify the human brain
the whole human organism can only have
about 50 megabytes of naturally selected
information and a lot of other functions
across the organism have to be naturally
selected for and preserved across the
generation and protected against
mutations which uses up the bits that
natural selection can optimize for like
I would love to naturally select an
organism with a smarter brain but I'm
too busy making that the organisms that
have bad arm muscles die right so you
have to keep all the other stuff working
using your bits of natural selection
optimization using your differential
survival and reproduction rates you have
to use those up on all these different
parts of the organism that have genetic
complexity before you can get around to
being like I need to make sure there's
differential survival of the smarter
ones anyway this is something you can
look up about how there's a a speed
limit or an information limit so within
this information limit you get about a
megabyte to code for the brain and you
can't just squeeze in every fact that an
adult needs to know about the world into
that one megabyte it's because of that
information bottleneck that you have to
say okay screw it you're just going to
be born into the world here are the
minimum algorithms that you need to take
in the information around you and use
that to grow knowledge to grow long-term
memory and eventually that can unfold at
runtime into a knowledge base and and
that knowledge base can actually be many
gigabytes so even though I've got about
1 Megabyte to tell you how you're going
to decompress into an organism uh into a
mind even though I've only got one
megabyte you're eventually going to have
gigabytes worth of learning and you're
going to have many terabytes worth of
sensory stimulation to build that so
it's because of that bottleneck so going
back to mach's analogy he's saying oh
look at the argument from childhood AI
is going to have a childhood no it's not
because the AI can just be a one tbte
specification and then it just copies
everywhere and that's it there's no 1
megab bottleneck there's no equivalent
of natural selection having to squeeze
something into a freaking strand of DNA
that is a a physical piece that lives
inside a cell that lives inside every
single cell in your body these are crazy
random constraints I always laugh when I
when I look at you know us being the
first intelligences that got barfed out
of this other process like this does not
tell you what intelligences are
generally like human intelligence is a
real [&nbsp;__&nbsp;] show right the the way that we
work is so contingent on this random
history and and it's just intelligences
are just going to have sane designs
they're not going to have the one megab
bottleneck they're not going to have
human biases they're not going to have
existential crises it's time for us to
treat intelligence science seriously and
then the argument from Robinson
cruso so many things about our
intelligence are based on us working
together and being together collectively
you know our we all or most of us had a
higher Unity University education and
that's thousands and thousands and
thousands of years of accumulated
knowledge that was kind of distilled and
beaten into us by professors uh you know
our whole experience as a species is
that intelligence is something that you
need a a collective group to do you
can't just have the most brilliant
person in the world on an island with
nothing they'll make do and they'll
they'll be inventive but they won't be
anywhere near their full potential so
when we first create some thinking sort
of uh entity it's not going to take over
the universe it's going to be lonely and
sad and and and uh you know in need of
us to kind of Shepherd it along that's
another contingent fact about humans as
organisms that are a bunch of separate
bodies a bunch of separate Minds where
each mind is super limited has a ton of
dependencies on other humans both to
figure stuff out and also just to keep
its body alive right so we need other
humans to be nice to us on a day when we
can't Farm all our own food or hunt all
our own food we better be able to get
some food from some other human that we
can pay back you know stuff like that we
better be able to get some clothes from
some human that specializes in making
clothes these kind of Dynamics just
don't apply when you have an AI That's
super intelligent that can copy itself
everywhere and then that can just
micromanage an entire economy that's
going to feed itself right this violates
all the assumptions that you learn about
how humans need to work so that's why I
keep saying this isn't about analogies
to Human Experience which m is relying
heavily on it's just about thinking from
very simple first principles and using
quite simple logic about what is like a
parsimonious expectation for something
with a high intelligence level which yes
I did Define high intelligence level
let's define it let's reason about its
implications and stop having all these
Doom debates and just think about a
policy like pausing AI right that's
where I'm hoping to get
to so so much for the uh inside
arguments I want to talk about outside
arguments which is the real reason I I
wanted to give this talk the vast
majority of people love the outside
arguments it's just so so tempting to be
like this person has a bad motivation
for making this argument this person's
taking money from this person if you
believe this it's going to make you act
like this it's just so easy so intuitive
so natural for us mudslinging humans to
get into all this outside view gossip I
will not be the one to sling mud to do
ad homonyms to get into the outside
arguments but let's hear what mach has
to say basically what kind of person
does believing this stuff sincerely turn
you into and the answer is not pretty uh
the outside arguments are these like
there's a
grandiosity that is taking over and the
grandiosity is basically it's all or
nothing we are the generation that has
to make this happen or we condemn
ourselves to Extinction or to some sort
of hell-like existence in the mind of a
computer if the fact is that we're
doomed then whatever kind of a person it
makes me that I'm calling out Doom I
guess I'm just going to go ahead and be
that kind of person because if we're
doomed then you can bet that I'm going
to say that we're doomed I'm not someone
who looks away from Giant news events
like us being doomed
sorry let me quote Bostrom again he's
talking about all possible future lives
and what the stakes are if we represent
all happiness experienced during one
entire life with a tear of Joy then the
happiness of these Souls could fill and
refill the Earth's oceans every second
and keep doing so for a 100 billion
billion
Millennia it is really important that we
make sure that these truly are tears of
joy
that's some heavy [&nbsp;__&nbsp;] to lay down on
like a 20-year-old developer you know
that's a pretty heavy responsibility to
bear for these trillions and trillions
of
beings and it reminds me of something
that I don't like I have a visceral
reaction to this language because I
remember it vaguely from childhood
living in a Marxist Society where we
were going to fix the world and then it
was eventually going to kind of trickle
down to where uh everyday life might
change but the first job was to fix the
fate of humanity when you make these
so-called outside arguments arguments
that are talking past the inside
arguments about why we're doomed and
just being like look what it means to
have these people who believe that we're
doomed there's not really much I can
respond to besides the inside arguments
speak for themselves if you believe the
inside argument and that leads you to
have the vibe of a Marxist then have the
vibe of a Marxist otherwise you're
doomed what can I say right okay I'm a
Marxist now I guess I don't believe in
Marxism I don't normally lump myself
with those people but if that's what it
takes to tell you that we're doomed and
again assuming the premise that we
actually are doomed the best people
warning that we're doomed would then
give you that Vibe right there's just no
way around it if you're going to
associate telling you that we're doomed
with this Vibe you don't like then I'm
going to have this Vibe you don't like
and we're just going to have to deal
with it my mom used to say that
everybody under communism suffered from
a disease
where what your eyes saw and your ears
heard was not the same thing and I'm
feeling the same symptoms you know I
live in California which has the highest
poverty rate in the United States even
though it's home of Silicon Valley I see
my rich industry doing nothing to
improve the day-to-day life of people
but they are saving trillions and
trillions and trillions of beings in the
future I don't think so if you look at
EA effective altruism they've now
established themselves as a group of
people who both work on a daily basis to
do things like increase Animal Welfare
get bed nets for malaria across the
world do Direct Cash donations to poor
people in Africa and also save trillions
of beings in the future right they do it
all they're just good people who are
trying to help everything with great
charity these are real Menches right I
love effective altruists I am an
effective altruist so this you know I'm
getting tired of these outside type
attacks because I ultimately that that's
not what I'm here to do on Doom debates
is to Parry outside attacks you can find
that on social media non-stop because
everybody just loves slinging this kind
of mud but just like it's not applicable
man we care about all the different good
stuff okay I don't think there's a
contradiction
here that ties into megalomania all
right this bond villainess which is
really creepy like people think that AI
is going to take over the world so
that's a justification that intelligent
people should take over the world first
and try to fix it before AI can break it
make sure that AI is healthy I don't
claim that intelligent people need to
hurry up and fix the world I just think
that if there's a high probability that
super intelligent AI is going to destroy
the world then we should pause the
effort to create super intelligent AI
that we don't have ways to control or
align there's a really wonderful quote
from joyo who runs the MIT media lab he
says this may upset some of my students
at MIT but one of my concerns is it's
been a predominantly male gang of kids
mostly white who are building the core
computer science around Ai and they're
more comfortable talking to computers
than to Human beings a lot of them feel
that if they could just make that
science fiction generalized AI we
wouldn't have to worry about all the
messy stuff like politics and Society
they think machines will just figure it
all out for
us so having realized that the world is
not a programming problem they want to
make it into a programming Problem by
kind of Designing the thing that will
then fix all of our problems this is uh
this is megalomaniacal I don't like it
well there's very few credible people in
the world right now who claim to have a
super intelligence plan the closest
thing you get is people like Ilia Sater
who started this mysterious new lab
called safe super intelligence but he's
never claimed to have a super
intelligence plan he just says he's
going to work on it until he solves it I
would be shocked if he gets anywhere
close in the next few decades I think
it's a pipe dream for an intractable
problem and Ilia is overly optimistic on
this front but to mach's point the issue
we have right now is people like Sam
Alman who their claim isn't we're going
to program our way to a better future
it's I'm going to use my leadership
skills to Steward this project to
success for the human species and just
neglecting the actual alignment problem
so it's a notably different type of
accusation than what joeo is making it's
not really about Sam Alman thinking he
can program his way to the Future using
sci-fi it's more like a standard kind of
Reckless power grab that various leaders
are making right now transhuman Voodoo
so there's this whole constellation of
beliefs that falls out as soon as you
start talking about uh artificial
intelligence if you have a really smart
AI the first thing it can make is
nanotechnology nanotechnology is Magic
because it can make anything so you have
a post kind of this abundant Society
where there's no more want uh of course
nanotechnology can also scan your brain
and upload it so you no longer die
you're Immortal and uh it can probably
even Resurrect The Dead you know I can
uh these machines can go into my brain
and look into to my memories of my
father and kind of create a simulation
of him that I can interact with and you
know for all intents and purposes he
would behave just like him so that's you
know uh there's a lot of uh stuff packed
into this Assumption of artificial
intelligence uploading is another one
where we will be able to occupy you know
these kind of artificial worlds or
bodies because our brains can be
completely scanned yeah these are
totally standard well-grounded
transhumanist claims the stuff that he
just listed off sounds kind of magical
and amazing like oh my God scan your
brain and then have somebody who's a
clone with the same mental state picking
up where your father left off when he
died it sounds crazy but if you rewind a
thousand years and you said hey there's
going to be flying machines and 200
people are going to climb into a tube
and it's going to have wings and it's
going to safely deposit them in a
different part of the world and this is
going to happen many thousands of times
per day criss-crossing the whole world
oh and also we're going to pretend like
uh moving pictures of people with sounds
are all a bunch of numbers and we're
going to send them through pipes that
carry these tiny particles that jiggle
around called electrons and on the other
side of the world you're going to have
what feels like an instant convers ation
with with people you know you know these
sound equally crazy and sci-fi and
magical too right so these transhumanist
claims what he's calling Voodoo it just
sounds like describing the future to
somebody who doesn't live in the future
yet pretty standard and Galactic
expansion for some reason always falls
out of this too I never quite understood
why we immediately have to expand to the
Galaxy but this seems to be a staple of
transhumanist thought there's no rush to
expand the Galaxy we can delay a few
decades or even a few centuries but it's
just something we want to do eventually
for the same reason we want to expand
onto the Earth right why limit your
expansion um what it comes down to is
religion 2.0 uh people have called this
the nerd apocalypse it very much is it's
kind of uh it's a clever hack because
instead of believing in God at the
outset you kind of build something that
can become a Godlike entity and then uh
uh but for all intents and purposes it
has all the attributes it is omnipotent
omniscient and is either benevolent if
you got your programming right and
didn't introduce any bug or it is you
know it is the devil and you are at its
mercy and there's this feeling of
urgency you have to act now everything
is on the line it is a very religious
feeling because these arguments appeal
to religious sentiments that gives them
the strong roots that they able to put
down in people and while they
rationalize why they have these beliefs
at heart these are religious beliefs in
kind of in in in other clothing okay so
just tell me how to not be religious
given that maybe the world actually is
about to end that a lot of experts are
warning hey the human world might be
about to end this could just be a
factual matter so please just tell me
how I can communicate to you about this
in the least religious way just focusing
on the facts not trying to worship
anything not trying to indoctrinated
anybody and just pointing out that we're
doomed that there may not be a future
for Humanity the same way there is no
future for the dinosaurs right the same
way that like 99% of species all went
extinct because of a super volcano
Extinction that happened even before the
dinosaur extinction right so Extinction
happens I'm just trying to raise a red
flag that there might be an Extinction
help me do it non-religiously
please and they lead to a sort of comic
book ethics you know where everything is
about saving the world uh through
technology and and Technical adeptness I
have a fantasy I want to see a Batman
movie where uh everybody knows that
Batman is Bruce Wayne and they just have
to humor him because he's their boss so
they create like fake scenarios and
crimes for him to solve and he has his
awkward bat belt and things so
I think a lot of our our our coders and
our thought leaders in Silicon Valley
really see themselves as modern day
Batman nobody is Robin interestingly
enough I just want to keep pointing out
that whenever you self- admittedly are
ignoring inside arguments now because
you want to make your outside arguments
and your outside arguments are like
people who say the world is going to end
are like comic book people okay you know
just keep sending whatever you got at me
I just actually think the world might
end and I just want to talk about that
all right simulation fever anybody here
heard of the simulation
argument okay a couple of people have uh
if you believe that artificial
intelligence is possible and can design
really really high performance computers
then there's a simple argument you can
use to convince yourself that it will it
can simulate other worlds and just by
mathematics it's far more likely that we
live in one of these simulations because
there's many more of them than in the
base reality
oo I didn't freak you out uh people
believe this [&nbsp;__&nbsp;] so Elon Musk you know
actually thinks he's offered billion to
one odds on it somebody we haven't found
him yet but somebody's paid two coders
in Silicon Valley to try to hack the
simulation which is so rude I live in
the simulation don't seg faulted you
know like
please I'm using it
um so simulation fever is really
destabilizing to reality because if you
think so
let me backtrack and explain how it
works say like you're in a post
Singularity world where we have these
hyper powerful computers and you're a
historian studying the second world war
you want to ask what would happen if
Hitler had conquered Moscow instead of
stopping just short so you create the
scenario you simulate the entire world
and the Army's rolling you see what
happens you write your thesis but
because the simulation is so detailed
the uh the entities in it are sensient
so you can't just turn it off because
that would be you know War crime setting
aside the fact that you've created
recreated genocide already as part of
the the historical setting so you have
to keep it running because you know
that's what your ethical board says you
have to do and This World War II
simulation it will develop its own
technology and soon it will discover Ai
and it will begin writing its own
simulations so it's kind of simulations
all the way down until you run out of
CPU and that's that's where this
argument comes from that there's many
more of these simulated worlds than the
ones than than the base reality but if
you believe this you believe in magic
because if we're in a simulation we know
nothing about the rules in in the level
above we don't even know if math works
the same maybe two plus 2 is five maybe
2 plus 2 is you know spiky tailed
monster there's no information that we
can have by looking at our our our own
situation uh people could easily rise
from the dead you know if you just kept
the right backups you can reinstantiate
them if we can communicate with whoever
runs the simulation then we have a
direct line to God so this is like a
powerful solvent for sanity when you
start really getting deep into
simulation world uh you start to go nuts
I personally find the simulation
argument pretty convincing and if I had
to ballark the odds that were in a
simulation I'd probably say something
like 50% like I don't think the argument
either way is super convincing but
whenever I argue to pause AI I'm not
saying let's pause AI because we are an
assimilation or because we're not I'm
saying let's pause AI for the same
reason that we don't fight a nuclear war
it's just assuming that reality is going
to keep obeying the physical laws that
it does and then acting accordingly so
to not die within this reality whether
it's simulated or not like in general
being in a simulation doesn't seem to
have any effect on our decision- making
maybe when we're super intelligent we'll
realize ways that actually we can
increase our utility by acting
differently depending on whether we're
in the simulation but that's not how we
make decisions right now and that's not
how I'm proposing that we should make
decisions so I see this part of Ma's
talk about the simulation argument as
just being a
tangent data hunger uh uh as I mentioned
the way that we found right now that's
most effective to get interesting
Behavior out of AIS is just to pour data
into them and this creates a dynamic
that is really socially harmful I mean
we're on the point of introducing these
like orwellian microphones into
everybody's house and all that data is
going to be used to train uh neural
networks that will then become better
and better at listening to what we want
to do but if you think that the road to
AI goes through this pathway then you
really want to maximize amount of data
is collected you want to be working on
these big projects so it reinforces this
idea that we have to collect as much and
and do as much surveillance as possible
no AI doomers aren't proposing
collecting more data from people than
companies are already
doing ultimately I think
that AI risk is like string theory for
programmers you know it's very uh it's
fun to think about you kind of build
these towers of thought and then you
climb up into them and you pull the
ladder up behind you so you're
disconnected from anything and there's
no way to put to the test sort of
creating the thing which we have no idea
how to
do I don't think the Tower of thought is
that high when I say utility
maximization is in a tractor State
because agents that could maximize
utility when they get the idea to start
maximizing utility start getting more
hardcore it's a one-way Street where if
you don't feel like maximizing that much
utility maybe tomorrow you'll spawn an
agent that does but if you are an agent
that does you're not going to spawn an
agent that doesn't it's a one-way Street
in that sense and the definition of
intelligence
as creatively hitting small Targets in
exponentially large search spaces I mean
yeah this is it sounds complex the first
time you hear it but like there's not
much more to it these are simple
Concepts that have simple logical
implications I keep talking about
intelligence science intelligence
science isn't a particularly complex
field it's just a new field with a lot
of confusions it might prove to be
complex but the Doomer argument doesn't
require that complexity it doesn't
require building a big tower the big
tower is optional if we want to build
super intelligent Ai and survive if we
just want to pause AI it doesn't take a
big tower of arguments to see the
obvious reasons
why finally it it incentivize is crazy
uh one of the Hallmarks
of deep thinking about AI risk is like
the more outlandish your ideas the more
like credibility that gives you because
it shows that you're courageous enough
to follow these trains of thought all
the way to the last station yep that is
admittedly what we do here on Doom
debates we follow the Doom train all the
way to the last station and every time
people try to get off for reasons like
all the ones you described we have to
pull them back in saying no no no don't
get off on this stop keep riding the
train you didn't find a good stop to get
off at you had a really weak argument
why to get off on that stop so get back
on the train unless we finally face
facts and hit an actual break like
pausing AI so Ray Kur you know who
believes that he will never die has been
a Google employee for for several years
now and is presumably working on that
problem if Google hacks death I will be
so annoyed because like imagine you have
a 30,000 year browsing history and how
that's going to follow you around um I
think the most harmful effect I think is
what I want to call AI
cosplay so people who are really really
persuaded that AI is real and going to
happen uh start behaving like their
fantasy of what artificial intelligence
would do in his book Nick Bostrom
outlines these six things that an AI
would have to do in order to be able to
successfully take over and accomplish
its goals and the list is intelligence
amplification
strategizing social manipulation hacking
technology research and economic
productivity and if you look at what
people in Silicon Valley are doing is
they're trying to behave like like their
favorite AI Heroes uh it's it's a
sociopathic form of behavior but we see
it Sam Alman is my favorite example of
this a guy who owns why combinator
there's all sorts of attempts at like
Reinventing cities from scratch
maximizing personal productivity in time
doing behind the scenes stuff to
influence the US election it's very
skull and Dagger and it's going to
provoke a backlash by non-tech people
because you can't just like you can't
push on the levers of power indefinitely
before it's going to annoy your
Democratic Society that that you're a
part of okay this isn't much of an
argument I'm not going to respond to
that and I've U I even have note here
I've heard people in the so-called
rationalist Community refer to to people
as non-player characters people who
don't affect the world in a meaningful
way I've called people NPCs on a few
occasions I think it's a good insult
when somebody really should know better
about a way that they can take control
of something a way that they can
exercise some real leverage it just
requires going a little bit outside the
box breaking patterns a little bit so if
they're in a position where they really
should do that and they refuse then I
think lobbing insult at him like okay
fine being an NPC I think it's
appropriate it's not like I say it often
but it's a nice piece of vocabulary to
have in the toolkit that's horrible like
and so I'm in an industry where the
rationalist are the craziest ones of all
it's
it's it's getting me
down what I've come to think of these
like these AI cosplayers as is like
nine-year-olds who go in the backyard
and they play in the tent you know and
then they start they cast Shadows on the
walls of the tent and they start
freaking themselves out but it's really
just an image of themselves that that
that they're reacting to there's a
feedback loop between how you imagine
the ultimate intelligence to behave and
how you behave as someone who thinks
that they're smarter than the rest of
the world that's that's uh that's very
very harmful so what's the answer what's
the fix we need better sci-fi all right
this is Stanis LM the the great polish
SciFi author uh English language sci-fi
is terrible it's it's wrote it's boring
uh in the Eastern block we had the good
stuff and we need to make sure that it's
exported properly it's already been well
translated it just needs to be better
spread so Stanis LEM and the strugatsky
brothers are the ones who come to mind I
hope to hear other suggestions from you
what sets Eastern European sci-fi apart
is that these are people who grew up in
difficult circumstances experienced the
war and then lived in a totalitarian
society and had to express ideas
obliquely through writing so there's an
actual understanding of human experience
and the limits of utopian thinking that
is completely absent from the West not
to say that it's impossible I think
Stanley Kubrick was able to do it but
you know we just need to get it back
check out project Hail Mary that was
some great recent Sci-Fi finally I want
to put my um I want to put my cards on
the table like what do I think AI is and
the possibilities of it are since I've
been making fun of it it's only
fair I think artificial intelligence
right now is in the same position as
Alchemy was in the 17th century
Alchemist have a bad rep we think of
them as being Mystics as not doing a lot
of experimental science but research has
shown that they were actually extremely
diligent they used in some cases modern
scientific techniques and they had some
really really good ideas uh they were
convinced for example of the corpuscular
theory of matter right that everything
is made of little tiny bits and that
these can be recombined to create
different substances which is correct uh
they had um they were on the right track
their problem was they didn't have
precise enough equipment to make the
discoveries they needed to the big
Discovery you need to make as an
alchemist is mass balance that
everything that you start with weighs as
much as what you end with but some of
those things might be gases or liquids
and then they just didn't have the
Precision it had to wait till the 18th
century uh and they had some clues that
weren't helpful so Mercury is a metal
that is uh liquid at room temperature
you know woohoo it's no big deal to us
it's just a quirk of the periodic table
but to them that seemed to be a really
significant and deep clue
Mercury was at the heart of this
alchemical system and their search for
the philosopher stone which was going to
be a way to turn base Metals into gold
so I got to thinking about what if we
could send a modern chemistry textbook
back in time to Robert Richard starky or
Sir Isaac Newton who did a lot of
alchemical research what would their
reaction
be I think first they would just flip
through to see like did you find the
philosopher stone like the the key to
their Quest and our answer is kind of
like yeah yeah but you know uh it makes
radioactive gold so they' be like
radioactivity what's that like well you
know Invisible Magic rays that will kill
you if you stand in the same room as it
uh so a lot of the struggle would be to
just not make it sound mystical to us
it's scientific but to them it's very
very Voodoo and spooky and then further
on we'd say you know but we do use
transubstantiation your philosopher
stone in order to make this metal that
is very handy because if you take two
lumps of it and smack them together you
can blow up a city so that's kind of
cool uh and then further on we'd say and
like actually the philosopher stone you
were looking for it's up in the sky
every single star you see is you you is
the source of reactions that change
elements from one to another and every
particle in your body was actually once
in a star so think about that for a
while and how you feel uh and not just
that but you know we um we've discovered
that the forces that hold us together
are the same as the lightning in the sky
and the reason that I can see you is the
same reason that your cat gives me a
spark when I pet it and the force that
keeps me from vitting through the floor
uh is uh is the same thing as well and
that these forces are defined by simple
mathematical laws that fit on an index
card this part of mach's talk is great
he's just really good at talking about
science and just a smart guy overall um
I think I don't think it would be
possible to communicate this without
sounding like like like like a Mystic
and uh
invoking God and things that you know
Concepts that were that were at the
heart of their belief system so I think
we're in the same boat with artificial
intelligence I think that we have some
Clues we have some really big Clues
there's the mystery of Consciousness
that this box of meat on my head is
self-aware and that hopefully presumably
unless it's the simulation you guys also
experience this like uh through
awareness but we don't even know how to
ask questions about Consciousness
because it's so you know we're kind of
we're lost in the dark we have other
Clues like the fact that um
every smart animal seems to need to
sleep and it seems to dream we know how
brains develop in children we know how
important language seems to be to
cognition we have all these pieces and
we have pieces from computer science as
well we see we're having real success at
identifying things in uh in images and
sounds the way the brain seems to do so
there's a lot coming up but there's also
things that we are terribly mistaken
about and unfortunately we just don't
know what they are and there's things
that we massively underestimate the
complexity of just like The Alchemist
who held a rock in one hand and a piece
of wood in the other and thought they
were roughly the same substance not
understanding that the wood was orders
of magnitude more
complex we're the same way with with the
study of minds and that's exciting we're
going to learn a lot but it's going to
take some time and in the meantime
there's this quote I love if everybody
contemplates the infinite instead of
fixing the drains many of us will die of
chalera in the near future the kind of
AI and machine learning that we have to
face is much different and it has ethics
problems of its own it's like if those
Alamagordo scientists had decided to
completely focus on uh whether they were
going to blow up the atmosphere and
forgot that they were building nuclear
weapons that also had to be dealt with
wait but it's critically important that
we focus on the question of whether the
nuclear test is going to blow up the
atmosphere and then not do the nuclear
test if it was going to blow up the
atmosphere like stop everything while
you focus on that and resolve it so his
advice is applicable after we've
resolved the nominess of the nuclear
test this is an instance where if you
just follow his own analogy we really
need to focus hard on testing or somehow
knowing that the AI is not going to blow
up the world before we continue its
development and then deal with other
problems of continued development so
there's ethical questions in machine
learning and they're not about things
becoming self-aware and taking over the
world they're about how people can
exploit other people or through lapses
of thought introduce kind of
immoral uh immoral Behavior into
automated automated processes that are
become more and more important in our
daily lives and of course there's the
the question of how power relationships
are affected by Machine learning and AI
we've already seen that surveillance has
become a defacto part of our lives in an
unexpected way we never foresaw that it
would look quite like this but it is
here uh this is the NSA data center for
people who aren't familiar with it so we
have this world and at the top of this
world the people who are running the
show are obsessed with a crazy idea
so what I hope I've done today is is
shown you that you can learn something
from Stephen Hawkings cat no matter how
much they tell you to get in the carrier
Do Your Own Thing uh I hope that I've
made you just a little bit Dumber today
from my talk so that you don't get
caught on the super intelligence idea
and hopefully I can persuade you that in
the absence of good leadership from the
people at the top of our industry it's
up to us to try to make an effort to
contribute and to really think through
all of the ethical and difficult issues
that AI has created for us um mostly I
want to thank you for letting me blather
on about this weird Topic in front of
you and for your wrapped attention thank
you so
much okay so two things about mach's
talk are extreme impressive number one
the fact that in 2016 when nobody was
talking about AI Doom virtually except
Elon Musk and Alias owski and Nick
Bostrom a really small Niche during that
time he decided to give a talk on why we
shouldn't worry about doom and just get
into that fight which is such a popular
fight to have today such a popular
debate but he was having it in 2016 so
Props to him for going to an area of
focus that is now kind of almost
universally agreed to be very important
and very interesting and wasn't at the
time so that that's number one number
two is he's amazing at talking about
science and making these connections I'm
just a fan of him overall he's also a
successful entrepreneur that runs a
bookmarking site called pinboard so
that's why he mentioned being a web
developer um so those are like the good
highlights from the talk and then on the
bad side I just didn't find any of the
arguments remotely convincing and the
outside view arguments I you know I I
just engage with it for the hell of it I
don't even think they're really relevant
the inside arguments I think were pretty
meaty right it let me talk about things
like hey there's a speed limit to
Natural Selection and that's why we only
have one megabyte in the genome and
that's why we even have a childhood
phase and AI is not going to have a
childhood phase because it's not going
to have a one megabyte brain genome
right so that's when we got into the
weeds when we got into the substance of
the issues that tell us whether we're
doomed or not and I felt like all of his
issues were weak but for interesting
reasons overall I thought it was a great
talk because he lists it out so
interesting counterarguments against the
claim of AI Doom I do feel like I was
able to satisfactorily address all of
them one by one so basically like he
shot a bunch of bullets at me I feel
like I had a bulletproof vest I was able
to absorb them all it's up to you the
viewer to judge ma if you're watching
this video and you have thoughts I'd
love to hear them and better yet I'd
love to have you come on and just do a
live debate or a discussion about your
updated thoughts on Doom I mean I'm
responding to something that you said 8
years ago so it's totally fair to think
that you've updated your views and maybe
you disagree with some of the stuff
you've said and maybe you have other
thoughts maybe you have stronger
arguments why from the perspective of
2024 watching how things have played out
you have other reasons why you think
that AI is not going to kill us for
instance maybe you agree with Martin
cassado saying hey a these llm type AIS
are only going to be a little bit better
than stochastic parrots they're only
going to either simulate things using a
lot of computational resources or else
they're going to give you things that
are somehow close to something else and
some kind of probability distribution I
mean that would be the kind of example
of a non-d doom argument somebody would
come and make in 2024 that wouldn't have
been on the radar in 2016 so one way or
the other would love to hear more
updated thoughts about this from Mach I
encourage everybody to just follow Mach
on Twitter just because he's cool That
Just About Does it for today's episode
I've got more exciting debates and
takedown episodes coming up if you have
any requests just let me know in the
comments and just just to remind you
this is a mission that most of you
watching this channel are on together
with me we all would like to see Society
wake up to the Urgent threat of AI and
why pausing it is the only policy that's
not ridiculously Reckless so if you want
to do your part if you don't want to do
anything crazy if you just want to do
your part using a few clicks of the
mouse I make it easy for you you just
have to think of a couple friends or
some Forum that you're on where this
kind of content would be relevant and
then go paste a link to it just have to
paste a link or go to Apple podcasts and
write a good review of Doom debates to
get more people to watch it that way or
if you haven't subscribed to my YouTube
channel that's the easiest thing of all
you just go to
youtube.com/ Doom debates click
subscribe you've done your helpful thing
for the day thanks very much for your
support and I'll see you all next time
here on Doom debates