you have kids I think they're relatively
young right yeah 1 three and five okay
so I don't know what your plan is you
know to kind of protect them I don't
have a
[Music]
plan hey everybody Welcome to Doom
debates today it's an honor to have a
special guest Dr Keith Dugger from
machine learning Street Talk we're going
to debate the issues that I raised in my
previous episode
where Keith was talking about how theory
of computation relates to llms and
whether llms can truly reason he's been
generous enough to say that he wants to
push back with me live he's coming into
the lion's dead so before we get started
ke thanks for coming on uh tell us a
little about yourself and your
background uh well I have kind of an
Eclectic background I think um you know
when I was a kid I I loved everything
I've I've realized now that my passions
are really learning and problem solving
and so when I went off to school uh
basically I just I looked in a book of
um the the you know top starting
salaries for for fouryear degree
engineering majors and the top two at
the time were I think electrical
engineering and chemical engineering and
so I chose chemical engineering so I
ended up studying that both in uh
undergraduate and graduate school but
over that period of time there was just
more demand for software work so I ended
up minoring in computer science and my
research really gravitated more towards
applied math and and uh computation I
became disillusioned with with the state
of kind of let's say corruption and and
academic research so I became
essentially like a failed failed
academic I just couldn't ethically do it
so so I decided hey if um if I'm going
to have to deal with with corruption and
politics I'm going to go somewhere where
that's just openly and explicitly the
goal so I defected to the dark side went
to Wall Street you know I was one of the
kind of uh evil uh rocket scientists or
whatever for for8 years years or so uh
one day my kids asked me what I did for
work and I said uh well I I take other
people's money and I move it around and
every time I move it I take a little bit
and they thought that was an amazing job
description like you know best possible
job ever but it got me thinking about
this kind of how really empty what I was
doing you know was um so then I went to
Microsoft and did did a bunch of uh you
know technology strategy for
manufacturers people doing real things I
met Tim there so uh the co-host of
machine learning Street talk him and I
were both at Microsoft at the same time
he had this cool like paper review call
um that he that he had I showed up there
one day we hit it off um you know he
built this this YouTube channel that I
was really lucky to be a part of and um
that's how we ended up here today
awesome cool yeah thanks for the
background uh glad you found your way to
computer science for people who don't
know I studied computer science in
college and then I've been some form of
entrepreneur or Code Monkey my whole
life somewhat similar
backgrounds so Keith to kick things off
I think uh a lot of my viewers really
want to know what is your P
Doom yeah so when you ask me that um and
and by the way I I do not want to
pretend I'm a doom expert okay I mean
I've gotten involved in these
conversations lately but I had to look
up okay what does Doom really mean right
in that in that scenario and I see it
means sort of different things to to uh
different people and so if I if I take
it as kind of bostrom's you know
original definition which is essentially
we we lose out on our what do you call
it the cosmic endowment right you know
something really kind of catastrophic
happens to the human race uh I actually
think it's it's 25% or you know maybe
maybe closer to 30% but I don't think
it's going to be due to a super
intelligence I don't think it's going to
be directly due to a super intelligence
that decides to causes harm I think it's
much more likely that that human beings
are going to you know misuse um super
intelligence and kind of use it to
destroy ourselves if you will like we're
already harming ourselves um
significantly by by super intelligences
like in the at least s narrow super
intelligences you know so narrowly super
intelligent um ad machines and attention
you know Grabbers and and things like
that so MH yeah definitely something
that should factor into our our uh
politics and planning and and actually
one of the questions you had asked me
right is whether I would sign that that
uh statement and as soon as you you sent
it to me I'm like absolutely you know I
completely agree with it right yeah the
the famous uh statement on AI risk from
the center from AI safety uh you know
mitigating the risk of Extinction from
AI should be a global priority alongside
other societal scale risks such as
pandemics and nuclear war so we had this
conversation offline and I was like what
do you think of that statement Keith and
Keith literally went on the website and
signed it and showed me the email
confirmation yeah yeah I don't know if
they're going to accept my signature I
guess you know I don't know if I'm uh
important enough or big enough to uh add
it to their their their document but
absolutely I agree with that statement
and and signed it so thank you for
pointing me to that I got to say yeah uh
thank you for signing I got to say Keith
I was pretty shocked when we talked
offline and you told me what your P Doom
is because have you ever said that on
machine learning Street Talk No so again
this this doesn't come up so often and
and two things about Street talk so so
first of all it's a really contentious
or maybe contentious isn't the right
word but this ranker that we talked
about earlier right like if you say if
you get involved in this conversation um
you know you can definitely expect some
abuse right from from from any side and
so I think we we sometimes tend to steer
away from those you know particular um
uh areas likely where we'll get abuse um
as much as we can uh but also a lot of
times I'm there to play kind of a
devil's advocate right like to to at
least interrogate arguments or to expose
what I what I see or maybe the weak
points um weak points of arguments so I
just think there's never been an
opportunity for me really just to come
out and state my own you know personal
position on it until now you know
fascinating the other day I was talking
on my podcast about like a taxonomy of
doomers and it starts with the first
tier which is like you know elzra owski
Mei which I kind of count myself in that
tier but then we talked about people
like Dario amade who seem very cheerful
seem very normal they're working on AI
like everything's fine but you might
call them closet dobers right like you
never know that underneath the
underneath the surface they have like a
25% P Doom right what do you think what
do you think about that do you think
people should wear doom on their sleeve
a little more
explicitly uh well I mean I think I
think more attention is is obviously
being directed towards towards Doom I
mean it's almost you know phrasing it in
terms of of Doom is maybe part of the
problem honestly because because Doomer
ism is is nothing new right like there's
been all kinds of Doom you know the
Mayan calendar sort of spells doom and
you know some religious rap capture is
coming and it spells doom and so I think
it in a way it almost kind of tarnishes
the conversation because it gets
associated with with things like that
you know so apart from that it
absolutely needs to be a big part of our
conversation like maybe it should be
called you know AI harm because because
I think almost nobody could deny that
that AI is harming us you know now I
mean it's it's you know if you've
watched The Social dilemma right the
great documentary like you know it's
harming us now and and these tools will
continue to get more powerful they they
if they're misused the harm will
increase um so I think I don't know if
but people should absolutely be be
talking about the harm that AI is doing
now and how it can get much much
worse when you say AI is harming us now
would you still say that it's net
positive oh
boy that's a tough question that's a
tough question I think um that's a
really tough question um I think I would
say yes but that may just be my my bias
for for for working uh kind of closely
adjacent to it or in in Ai and I think
it may also be my failure of imagination
to imagine alternate worlds where there
was very less you know AI so I honestly
can't answer that question but you know
there's a great uncertainty there um I
just think it's inevitable so I I don't
think I don't think there's any way for
us to put the genie in the bottle and so
we have to we have to wrestle it well
this has gone in an unexpected Direction
because if you ask me whether I think AI
is net positive I'd be like yeah
definitely it's it's great right like I
use AI products all the time and like
okay sure yeah there's like some deep
fakes coming out and we got to deal with
them but like in my mind I think that it
is just increasingly positive until we
reach some threshold where it becomes
uncontrollable or too destructive and
then at that point the graph kind of
shifts from like up up up to like
permanently down that's how I see
things well you could be you could
entirely be right about the net net
positive you know net positive of AI I
think that's probably maybe the even the
consensus I'm just saying I haven't done
that analysis and I haven't even really
looked at an analysis of it so I don't I
just don't know um and and I do agree
again that there there will come a point
there will come a point where
um these these tools well let's just put
it this way they're going to become
increasingly increasingly powerful um
there will be an increasing chance for
harm and
misuse um I don't think there's going to
be this this F event of super AI which I
I think is maybe what you have in mind
there and then we can get into you know
later I think after we clear up some of
the turning things yeah why I think that
um but I I don't think it changes the
policy implications which is we have to
we have to take this seriously we have
to invest uh money into it you know we
have to tax the industry to have enough
money to do that we have to put in place
mitigation so I I have a lot of
agreement actually with yosua Benjo so
he I think he called himself um maybe a
doom agnostic or or something like that
but but because of the the probability
of it and the uncertainty of it he you
know he advocates for these mitigations
planning taking it seriously Etc I think
I that's probably
where I would put myself um though I
think I actually have a slightly I'm
less agnostic I think human beings are
absolutely capable of destroying
ourselves and we're already in that
situation and we should try to back out
of it and not get into it with super
super intelligence either awesome and by
the way yosua Benjo is on record
implying that he has a 20% P Doom so
that checks out that you're in the same
ballpark well we're slightly more
pessimistic CU I think
maybe 30 right you know like for example
there were some some estimates that the
Cuban Missile Crisis um basically
was I mean it varies between five and
50% but I think Kennedy was around a
third of a chance that that could have
gone wrong right yes absolutely it's not
good so Keith I think this is a major
point of consensus between our two
worldviews which is it sounds like you
don't think that Humanity has plot armor
right you you think that we're allowed
to die correct like the author is
allowed to kill us oh yeah 100% okay
right because I've definitely heard a
lot of yeah let me just say not only are
we allowed to die actually one of the
other sides of this debate is if we
don't
develop super intelligence that we can
that we can adequately have aligned and
controlled um we will die I mean we you
know we need Super intelligence to
escape all the various
Cosmic uh you know death timers that we
on so it's a funny thing we have to we
have to straddle this we have to walk
this tight rope you know it's a great
point that on a billion- year time scale
everybody's a Doomer I mean it's just an
obvious scientific theory that you know
the sun is not going to make the Earth
habitable in a billion years right so
the only question is it's the question
is when not if right yeah when when not
if you know I agree with Elon Musk we
have to become a space fearing space
fairing you know civilization we're need
Super intelligence for a bunch of tasks
and sure on a billion years you know the
sun's going to turn into a red giant and
engulf the Earth or whatever it's fate
but I think they're even shorter term
things you know there are right things
like blowing ourselves up with nuclear
weapons uh some some jerk getting enough
ability to to to you know print a
pathogen that that kills off a lot of
people uh billions of people maybe I
mean there's a lot of ways in which we
we can die so
okay all right so now let me try to find
a major disagreement because it sounds
like we've hit on so many points of
agreement like we both agree human human
existence is kind of fragile Doom is a
question of when and not if nukes are
super scary we might nuke ourselves you
know we might pandemic ourselves to
death like I think we agree on all these
background assumptions and we also agree
that it all Nets out to a pretty high PD
I mean you say 20 to 30 I say 50 I'm
happy to just say 20 to 30 I don't think
there's that big of a difference it's
not like I have a lot of digitive
precision when I say doesn't the policy
implications yeah exactly that's right
um so let me try to dive straight into
trying to get you to disagree I think
the heart of the issue is you kind of
make claims about a boundary of what
llms can and can't do and I don't really
see it as a hard boundary I don't have
that same confidence and crispness that
you seem to bring to the table so do you
want to just repeat the claim that you
made on your podcast about what's the
current boundary of what LMS can't do
sure and and we have to be uh we have to
be a bit precise here because llm now
has come to mean a variety of things
okay to to a variety of people and so
let me just be more specific about what
I mean by llm okay is you know so to me
an llm is a single uh neural network
that has a finite Conta text window
that's run either once or Auto
regressively okay until it generates
some some stop token itell until it
outputs sort of an end of end of story
you know uh token token if you will okay
so just let's accept for now that's my
definition of llm okay those things okay
are finite State automata and primarily
the reason why is because they have this
finite uh you know context window and a
finite sort of set of neurons so they
can only explore kind of a a finite set
of States um and therefore they belong
to a class and this is where I have to
be super careful and I kind of want to
slightly reframe what I talked about in
machine learning Street talk for this
conversation because you have the the
background to get it and I think it
actually be clearer for for other people
is that you know by historical accident
okay the field of computer science um
refers to machines you know all the time
right so there's finite State machines
or automat there's turning machines Etc
okay and and why is that well it's
because those were the machines and
these are these are um nomological you
know or nominal definitions of a machine
they're not physical I I know what
you're getting at right are you going to
like talk about like Lambda
calculus well no what I was going to say
is you know there are called machines
okay but what's actually more important
like the more important thing to focus
on is these classes of algorithms okay
so right you have algorithms which we
can just call finite State automata
algorithms and then outside of that you
have you know context free grammars push
down automata these are algorithm kind
of classes right Right computability
theory divides functions into these
classes based on whether they're
computable by certain models so I'm
following yeah yeah yeah so it it's not
like what I'm trying to say is llms are
limited because you know uh because
they've got finite memory and of course
you could just go get more memory and
add it what I'm saying is that the class
of algorithm that we end up training on
llms belongs to the class of finite
State automat and therefore you can't
just give it more memory so for example
if we if we suppose we have an llm and
we go try to do some you know some
problem that I claim it can't do like
give me the nth digit of pi all right so
somebody hey everybody I've got an llm
it gives you the N digit of pi cool
somebody comes in and ask for whatever
the you know one billionth uh digit of
piie and it runs for a while and and
maybe it gives the wrong answer or it
outputs token that says um hey I've run
out of memory you know give me more
memory right you can't give it more
memory because in order to expand its
context window what you have to do is
retrain it right that's just the way in
which we construct these lolms and the
way in which we train them so I'm not
saying that I'm not saying that a neural
network so a turning machine you know a
thing that can
run uh algorithms of a more powerful
class of algorithm um it's actually and
I really hate saying it this way like
it's just but it literally is Justice it
just doesn't do justice to Turing right
and the and the real you know genius
behind behind this Revelation but it's
just a finite state atata that has a
unbounded readr tape okay and I am not
trying to say and in fact I've said in
the show more than once that that finite
State automat that's sitting there as
the controller of a turing machine can
in fact be an inn it can be an neural
network it can be an llm it can be
whatever right I'm just pointing out
that it has to be able to utilize this
unbounded readr memory and every time
people have tried to train them to do
that they failed because we just don't
yet have
you know the the optimization methods
the training methods to to search the
space of turning complete algorithms
efficiently okay I want to give you some
credit where credit is due I'm going to
concede a sense in which I agree you're
right and furthermore I'm going to
concede a sense in which I was wrong I
feel like it's like so rare in debates
for somebody to call themselves out and
be like I was wrong that's like
something missing in debates so but yeah
so I will give that to you you you
deserve it so here's the technical point
that I messed up on last podcast I was
saying that if you look at something
like GPT 4 where it just starts printing
out output tokens the claim that I made
was if you just let it keep outputting
token after token and you don't put a
maximum limit on how many tokens it can
output I incorrectly said before that
you can model it as a touring machine
because you have this unbounded output
tape my mistake was when I thought of it
looking back on what it's already
written I forgot about the finite
context window if it only has a finite
context window then the output tape
isn't really infinite from its
perspective because it can't look back
infinitely far it can only look back
whatever a million characters back and
then it forgets what's before a million
characters back so it has this limited
window and the problem with a limited
window is when the output tape gets
really really long what you end up
seeing is you you get Cycles because
there's only like two to the power of a
million possible states that a million
size memory can be in right so you ultim
get a CLE so I absolutely ConEd that's
not a machine that's a finite State
automaton with a Prett generous finite
memory size but like I was wrong you
were right Fair okay yeah sure sure I
mean I I appreciate that so I mean look
I don't mind being wrong either you know
um in fact it can be kind of fun right
because because we learn new things and
and really like I said my passion is
learning so I really respect that that
you said that and I think more more of
us should you know more of us should
absolutely just be like it's not you are
not personally a bad person because you
disagreed with somebody and they happen
to be right you know I mean it happens
folks like it's better just to oh cool I
didn't know that and then just move on
right and let's let's make progress
let's Advance that's right losing points
in an arguments H happens guys okay but
now I want to come come back out
swinging because I I still feel like I'm
right in spirit okay now that I've
conceded the technical point in the
example I used with like a million bit
context window that's a large context
window that's a million bits of State I
mean two to the power of a million is
much larger than the size of the
universe right so like in practice when
you're doing anything useful like when
you as a human are thinking do you
really ever use more than a megabyte of
context well let's I mean maybe let's
shift away from humans and and just talk
about our our machines that we have now
because they're it's it's really kind of
crystal clear you know so for example
you know if you're a scientist at Los
Alamos and you're and you're trying to
do simulations of
um you know these nuclear weapons for
example that we've that we've built and
put all all over the the world that that
could kill us right uh you're going to
be having terabytes of of processing
right I mean I mean even even if you
want to play a decent video game okay
you're going to need 32 gigabytes of um
of processing so I think my push back
would just be that yeah so I think my
push back is just that look I get it
there there's tons and tons of very
awesome things you can do with with a
megabyte um I'm just saying it it just
as a matter of empirical fact many
scientific and business and economic
problems that we're trying to solve they
require more memory you know yes I mean
you're absolutely right but I mean you
use factoring as an example right if I
de a pencil and paper and I said Keith
Factor this uh six-digit number right so
like you'll get it eventually with a
pencil and paper in the entire process
of you factoring that number what would
you roughly estimate you know how much
of that paper are you going to use in
terms of bits of information seems like
a lot less than a megabyte well I don't
I don't know that I that I've used
factoring as an example because
factoring is actually you know factoring
a number is something that you can do
with you know reasonable amounts of
memory right because you can do it very
slowly you can just sort of go through
and try kind of all the prime factors
and as long as you can just perform the
factoring calculation on a 256 big
number or whatever I think examples I've
used are like arbitrary Precision you
know math and these these things come up
um so they don't come up in a lot of our
everyday experiences Tim Tim challenged
me this on you know on the on the
Discord he's like Okay llms can search
program space right because they can
output programs give me an example of a
program that you know that can't be
can't be output by GPT whatever its
current sort of maximal non-repeating
output is and I said okay okay anything
larger than it's than its window
stability like how about the Linux
operating system cuz the Linux operating
system is a massive many many megabytes
of um of characters right and okay fine
it can't it can't output an operating
system but it can still do a lot I'm
like but see these matter right like
these things are real programs that
matter um but of course you're right in
the sense that there are many many many
things that we care about many important
problems that don't require um those
those extreme amounts of memory but if
you were a super intelligence that
wanted to take over the world okay and
develop your own science and defeat the
entire combined you know power and
capability of humanity you're going to
need more than a megabyte of of memory a
lot more yeah okay fair enough but let's
let's keep it to simpler examples first
remember you had this really interesting
puzzle at the end of your episode a
couple weeks ago I don't want to repeat
now but it's it's the one with like the
rotating pillar and it was fascinating I
tried it myself and it made me feel dumb
for a little while because it was like
so perfectly set up as such a simple
puzzle and yet the answer took me like
an hour to finally get but even though I
used my whole human brain right and and
you showed how today's llms couldn't do
it right so you you that was a nice
separator right between Ai and humans
for today I don't think that I used a
megabyte of space during my thought
process no no no not at all not all so
yeah and and and there there I was
making um a different point
but okay
so what I was talking about my intuition
let's say the intuition that led me to
think that that particular problem and
by the way it was just the first the
first problem I tried okay so there's
there's something about my intuition you
know that's right here like I understand
that llms where they currently have
difficulty is they have difficulty in
um computations that need to unfold
precisely
over Windows okay like I can't you know
right now what right now let's say it
struggled like this this column problem
sort of you know it has five steps it
can be solved in five steps or at first
I tried to give it some leeway and let
it do it in like eight steps and that
just made it like more confused right so
so uh it can unfold over only five steps
right but these five steps have to
happen precisely and they're and they
have to happen in a dependent order in
which they
can in order to be a solution right and
I just thought hey maybe it'll have a
problem with this right because of that
that structure of the problem we tried
it out and it had a problem with it I am
not so there will be GP something that I
would bet could solve this problem
because it's a it's a sort of you know
very finite number of steps right like
it's just five steps I would hope that
that 03 or something like that will be
able to solve this problem do you think
that this particular example that
current llms are failing on does this
illustrate the profound Insight that
you're trying to share that current llms
are not Turing machines or is this
illustrating something
else um it it's kind of illustrating
it's illustrating
um both elements of turning machines but
on the other hand any finite problem you
know can can eventually be solved by
finite I'll tell you what it's
illustrating is so I get a lot of
pushback actually internally even in in
the mlst community about okay fine right
like these things are not turning
machines but it just doesn't matter in
practice it's just it's irrelevant in
practiti because all right so all the
problems I care about and this is what I
this is my argument when it comes to
that which is okay I I think it does
matter in practice and and I'll tell you
why which is
on the one hand you have a class of
algorithms say those outside of finite
state aatod that can sit there and
iterate on an unbounded you know amount
of memory the the critical thing there
is not so much that it's a potentially
infinite amount of memory it's that the
amount of memory is variable it doesn't
know ahead of time how much memory it's
going to need it gets a problem in maybe
it needs 500 kilobytes maybe it needs a
megabyte it doesn't know in advance
right so it has to have this capability
to kind of go back and forth and perform
these computations and if instead you
try to you try to decide ahead of time
right like how much memory do I need to
solve all the problems I care about okay
what you end up needing is an
exponentially large amount of memory
right so you can probably remember from
computer science that every time step
that you allow you you get an
exponentially larger number of states
right that you can explore because you
always are sort of like a a factor more
you know sort of per per time step right
um and so therefore uh
any circuit that that we try to build in
this way like is these finite State
machines is going to have this
exponential blowup problem which means
the depth at which they can kind of
think right the longer you the the more
you want them to think you need
exponentially more circuits per time
step you know if you will right right I
mean if yeah if you model it as as one
big circuit but I mean isn't there an
element of recurrence I mean we're
looking at the model like outputting
tokens yeah but it gets to read what it
outputed so
well that's exactly the whole Innovation
if you will behind behind 01 right is is
allowing it more time so they could it
so it could do more with its um and
maybe even off yeah go ahead I I'm just
confused because I think you just
invoked this mental image where uh a
single circuit to do a computation has
to grow exponentially larger with the
length of the computation but in the
case of an llm it's not a single circuit
it just feeds back the current state of
the tape into the original llm so you
don't have to keep growing the circuit
you just keep having the circuit process
the
take yeah yeah so it first of all it is
a single circuit like that was my
definition of llm right like it's finite
State at tomat that's run auto
regressively so it's a single circuit
that's run on this this tape with a
finite context one so I wasn't wasn't
quite done so I'm just saying I'm saying
to people like a if you try to take a an
algorithm that's a higher class um than
than finite State at tomat and projected
into finite State atom it grows
exponentially okay okay and and and open
AI knows this and that's why they're
like okay we're going to start doing
more inference time compute this way we
can do more with so any particular
Circuit of a given size if you can train
it to run iteratively for a period of
time you can do more right and so I
think I get what you're trying to say
and I think I I agree with your
technical point which is that if you
wanted to ask gbd4 a factoring question
right if you wanted to input a 10-digit
number and you wanted the zero shot
output right you wanted it to just give
you the answer on the first token then
you're correct that assuming factoring
is uh NP hard then you just need an
exponential size circuit right is that
basically the point you're
making well if you want to do it in one
shot you need an an exponentially large
circuit you if you allow it to iterate
for every kind of number of iterations
you do you cut that you know you cut
that down by a factor right so that but
then my my point which gets to the kind
of rotating pillar problem right and is
that we haven't yet mastered this
ability of training an llm that can
reliably operate over longer longer
context Windows it it sort of
approximately operates over over longer
context windows and so if you give it a
problem that requires these sort of
multiple precise steps those are the
types of or one class that's one class
of problem that it will will struggle
with was my really my only point right
you know in that in that teaser okay so
and actually by the way you owski makes
the same point so that you know he was
asked in an interview maybe a year ago
or something like that um are llms the
path to AGI right and the point he
brings up is well they're not reliable
enough to be right and it's kind of the
same point I'm making here which is if
you have to unfold your computation over
a bunch of steps and each of those steps
is unreliable the longer it is the more
likely you you fail right so I still
think that when you use computability
Theory to analyze what llms can't do
because of their finiteness compared to
a touring machine when that's the type
of analysis you're doing I think you
could just do that exact same analysis
about human brains you could just say
hey look human brains can't really think
because they're just finite state
automata right a human brain can only
ever get into a finite number of states
correct
well so let's remember and this is why I
I desperately want to reframe this in
terms of algorithms rather than the
machine you know it's it's sort of and
again it's really just if you're in the
know and computer science like this
never really causes a hiccup but when
you're trying to communicate externally
it does which is of course my brain is
you know finite mass and has a you know
more or less constant number of um
neurons Etc like what's up here in
machine sense like or in physical sense
is finite but it's running an algorithm
it's running an
algorithm from the class of Turing
complete algorithms and that it can use
external memory so I can go grab a stack
of paper I can write down some symbols I
can use a chalkboard I can use all this
kind of stuff in llm like the physical
brain there can could do could do the
same thing if we knew how to train it
could do okay let me also let me ask you
from this angle just in terms of like I
still want to pick out your you know
finite versus infinite um automaton
versus training machine distinction let
me ask it this way sure imagine the
smartest human alive like teren to the
mathematician let's say teren to can get
a lot done in one hour right like people
report they give him like their hard PhD
problem and he's just like oh here's the
answer right and often times it takes
him less than an
hour in that one hour does Terren TOA
really need more than a megabyte of
working memory
well to be honest I've forgotten what
the numbers are you know in in a human
mind I think it's a lot larger than that
I think it's more like pedabytes right
but uh you know I don't know I mean he
definitely doesn't need he doesn't need
um he doesn't need an unbounded amount
of memory it appears if he's able to do
it all in his head right I'm sure there
are things though that that he does that
require paper like I'm sure whatever
he's working on yeah okay me but what
I'm saying is like if you just Model A 1
hour teren toal system isn't that just a
finite
Staton as long as it's not using
external resources it's you know yeah
but the algorithm he's running is is a
turning complete algorithm not a finite
State automata algorithm does that make
sense I mean but that's that's what
computers are in real life right
computers are finite state automata that
Lara steering
machines right so this back to what I I
tried to point out earlier is um the
Focus right now that that you're on is
on the machine rather than on the
algorithm that it's running right and
I'm trying to say it's not so much the
machine that matters it's the algorithm
that's running on that machine and and
so you know if I just imagine the old
days right where I've got a a um a
machine whatever a turning let's call a
turning machine a giant box okay and
there's two class not two classes but
let's just only consider two classes of
algorithms that can run on there finite
State automata algorithms and turning
machine algorithms okay and it has a
tape deck I know that the finite State
algorithm will never ask me for another
tape because it just it's just an
algorithm that only needs like this
finite amount of memory whereas the
turning machine is going to say to me I
need another tape put it on there add
the next one add the next one go back to
the previous one go back to so it's not
the it's not the physics of the machine
it's the nature of the algorithm that's
running in it but if we talk about it
from a complexity Theory standpoint like
I'm going to ask Terence to a question
compx or computability which which yeah
I'm actually I'm going to shift to
complexity a little bit um okay so yeah
so if we talk to teren to he's not going
to spin off an algorithm that requires
an exponential search right he's he's
not going to do two to the 100
operations during the 1 hour office
hours that you have with him so if we
just know that whatever he's going to do
is a reasonable like polom time or just
like okay I gave him like a kilobyte of
input and he's going to give me like a
few kilobytes of output and he's not
going to spin off any super long
process that it's just like yes you can
say hey if we generalize the class of
functions that he's doing it needs
infinite memory but like realistically
you just need like an hour of teran's
time like you just need a few sheets of
paper you just need a few Google queries
right like you don't need anywhere near
infinite memory
I mean I feel like I feel like you're
just trying to say we can do amazing
things with a finite amount of memory
and and I don't I don't deny that well
what I'm saying is just like the
distinction that like when you try to
use computability Theory to say that
llms are only finite I'm really
struggling to understand how that exact
same claim doesn't apply equally well to
teren
stra because he's running an algorithm
that he has demonstrated you know
empirically in fact can go and actess as
much you know external memory as he
wants
to lolms cannot do that one more angle
to throw you yeah let me go to the next
angle okay you take the llm and you give
it access not to full python but just to
a single hash table an arbitrarily you
know infinitely growing hash table so as
it's writing it can just say hey
remember this remember this or even
simpler it doesn't even explicitely have
to ask for remember there's a special
token it can output where it just says
like revisit and then a token IND and
then next on the tape will be written
what was in that previous token Index
right so now it just because of that we
just smash through the context window
limitation because it's able it's now
able to effectively look back on the tip
if it wants to so with that simple
modification does that just completely
make your argument
irrelevant no it just makes it some some
different architecture where where in
which it is turning complete so if you
can do that so if you can modify an llm
to access you know just some some lookup
store uh you know right Azure Azure
table or whatever right uh and and take
advantage of it like actually take
advantage of it algorithmically um now
you have a turning machine I mean you're
calling it a different architecture but
like I could just go to the open open AI
API right now and just patch together
something where I can explain to GPT I
can be like hey by the way feel free to
write look up open parentheses 55 if you
want me to print the next token for you
if that's going to your so I can
basically explain the game to GPT and I
can also implement it and I can do that
all in like 15 minutes right so when you
talk about a new architecture it seems
like a slight
modification it's a slight modification
to the architecture getting it to do
something useful with that new
capability is the problem so I mean sure
you could go do that and then um and
then when we try to get it to for
example solve the pillar problem it's
going to fail right so imagine I make
that augmentation and then now I'm going
to ask it to factor number
and I'm going to tell you know don't use
Python you have to use pencil and paper
but if you need to use this special
lookup feature you can what is it going
to do it's probably not even going to
bother using the lookup feature
right in what sense it's going to do
what give you a Python program that it
ask you to go run on a touring machine
no no no imagine I say hey go Factor you
know
1,579 right factor that number and don't
use Python you have to do it using quote
unquote pencil and paper and if you ever
use too much paper and you need to go
revisit a previous token that you've
outputed that's behind your context
window you know in the output stream
it's more than a million tokens back or
whatever then here's a way to do it
right it's the one augmentation I'm
giving you if I give it that input I
would just expect it to take a stab at
the factoring problem using a small
amount of memory and not even bother to
use this new capability I'm giving
it I mean you could that could be your
expectation go try it I mean no I could
try right my expectation is you know my
expect a is um and again I don't focus
on factoring because factoring I I I
talk about things like let's say
arbitrary multiplication of two integers
so I mean today if you give it if you
give it's you make your modifications
okay to 01 or or whatever okay and it's
it's not allowed to go run a piece of
python code on a turing machine like of
course now we have a turing complete
system but it has this you know hash
table of yours right this little
database that it can write to and and
look up or whatever it's going to fail
to the multiplication it's not going to
fail because in principle it will always
fail it's going to fail because it
wasn't trained to be able to utilize
that that store right and that's going
to be where the challenge lies let's
talk about that in a second but like
this thing I'm trying to say now right
this this hypothetical I'm trying to set
up I mean isn't this supposed to be like
super useful I mean can you think of a
specific problem where this new
capability now that it has this revisit
function like which specific problem is
it now going to be able to do now that
I've solved your whole issue right your
whole issue was it didn't have access to
an infinite tape well now it does so
what can it do
differently are you asking what it what
01 specifically or what a hypothetical
thing that we train to utilize this this
new architecture could do okay so I
think the claim you're making is like if
if I'm just let me summarize you you're
basically saying okay if you give GPT uh
GPT 40 not even 01 let's just say 40 you
give that GPT this ility to revisit
tokens but it's probably not going to
get much value out of it because it
didn't have it at training time but if
it had it at training time and we change
the architecture of training suddenly
you're going to unlock more intelligence
or eventually somebody will unlock more
intelligence okay so that's basically
your claim AB one 100% yes okay okay I
mean one thing that comes to mind for me
is if you go to Terence H's office and
you're like hey Terren you know what I'm
going to give you um I'm going to give
you here's like an extra stack of paper
to work with Beyond like the five white
P of paper that are on your desk he's
Pro don't you think he's largely just
going to be like I can do it in five
pieces of paper for many
problems well again for many problems
absolutely so there there are many there
are many useful and and interesting
problems that can be solved with let's
just say small or whatever amounts of
you know Terren you you know Terren uh
what's his last name again teren to I
hope I'm saying that right teren tow
okay so whatever amount of memory he has
plus five sheets of paper there's tons
and tons of problems okay that that that
system can solve under those constraints
there are more problems it can solve if
it has 10 sheets of paper and there are
more problems it can solve if it had you
know 100 sheets of paper more problems
it could solve if it had a very large
database and here's the really cool
thing you don't have to retrain teren to
because teren is running an algorithm
that can already utilize unbounded
external memory I just I don't feel like
I've ever used more than a megabyte of
working memory to do anything in my life
what about
you
oh so working memory I don't know
probably not I mean again I'm not an
expert on Neuroscience external memory
100% because I've written you know so
for example I've written large large
bits of software that are larger than
that and I have to go back and revisit
this file and rewrite that one and
coordinate it with some other file and
so there's this large expanding you know
mass of spaghetti code right that's
larger than what I can hold in my head
okay and you agree that that's what you
did is analogous to giving an llm let's
say python right and I think you've
you've said okay yeah if you give an llm
python then I can't make a confident
claim on what kind of computations it
can do but because it wasn't really
trained with running python programs
it's probably just not going to be good
at it even if computability Theory says
that it could do it right yeah yeah
that's that's right so I mean if you
give if you give an llm uh if you give
llm python plus a machine machine that
can run it on on there you know you give
it a turning machine so you have an llm
and it has a turning machine and it can
run stuff on that turning machine then
that's a turning complete system um the
difficulty is in training it to got it
to take advantage of that
yeah okay um one interesting and I think
we will do that so that that will happen
you know I mean well I mean you know
people will get to the point where um
and and again this is why I like to
think about in terms of the algorithm
space right so we will get to a point
where we can efficiently search um over
a much larger space of algorithms
including algorithms you know outside of
FSA so okay so you can factor numbers
using pencil and paper you Keith what
was your training process like did you
really need a training process that was
like Turing complete and use unbounded
memory to train you how to factor
numbers well Evolution's already baked
in into me the recurs of mind so I mean
like this this is the the important
thing about Turing you know turning
machines is that they're another way to
think about them is that they're
recursive recursive recursively
innumerable you know computations right
and so that's already been baked Into Me
by Evolution so I just had to learn how
to adapt that inherent capability to
factor numbers or to multiply them as
the case may be okay it seems like when
we factor numbers as humans we just I
mean there's alith to do it but let's
say we just learn the alith of like try
to divide two try to divide try to
divide it's like okay we learn of the
number yeah right exactly so we learned
that algorithm and then we just do it so
like we didn't really need a feedback
loop where like some long process played
out and then like updated our weights
it's more like okay this is what
factoring is here's an algorithm we
could run let's run it but it it sounds
like you're trying to draw a distinction
that like the reason llms are stumbling
on factoring is because their training
process didn't include a feedback loop
after running turn in complete programs
right so it seems like there's a
disconnect
here there she why is there a disconnect
like it seems kind of obvious to me like
if you doll so again human architecture
already has baked into
it um this recursive you know comput the
recursive mind this recur recursive
capability to use external memory right
what I'm saying is lolms um they don't
they don't have that and we don't know
how to train them to have that
capability yet and so I don't see the
disconnect it's just like yeah they
can't okay if if I ask GPT 4 to factor a
six-digit number I ask Factor
30171 without writing code and this is
GPT 40 it said the number 30171 can be
factored into its prime factors by
manual steps check if it's divisible by
small Prime starting with two since it
is odd it's not divisible by two so
first of all it's following the first
step of the algorithm perfectly and it
kept going and going and um I think in
this case it actually got the answer
right uh which which is rare right it
gets it right like one out of 10 times a
lot of times it makes a mistake um but
my question to you
is is does this represent your claim
about how like in this example do you
think that it could do a better job if
something at its training time had a
touring complete feedback loop or like
explain what's needed to make this
example go better
yeah so it it definitely you know could
do better if if at a training time um it
had access to to so here it generates to
you kind of a program I mean you you ask
it not to write code like maybe
um
uh like what's the right way to say this
how about we just say like it can
generate code just to make it a bit more
you know because these steps or whatever
that it's outlining might as well just
be specified you know in Python so more
like like let's say you can generate um
limited code so code that's that's
clearer than English okay but it's
saying like basically like it's putting
out all the steps you know it can do
arithmetic and see like what if you as a
human get to punch in you have a four
function calculator so when you need to
do your trial division you can just use
your calculator there you go so some
small set of operations okay that it can
that it can do and so um if during
training time right we gave it a bunch
of uh problems you know here um Factor
these numbers here's the right answer
okay and we allowed it to generate code
go run the code on the turning machine
okay and then if it if it got the right
answer back it would remember or that
would be one of its samples in training
that this this is the kind of code that
gave the correct answer okay I 100%
believe that if it had that at training
time it would generate it would do
better on this task so it would generate
code that was more reliable more correct
for larger number
wait but if we're talking about writing
code that's a solve problem right I mean
the reason I said don't write code is
because if I let it write code it just
writes the correct code to factor a
number so don't you think we should
analyze the problem without writing
code sure but I mean we're writing
something like step that's why I said
like some limited steps like you can
divide you can do this you can't just
write a piece of python code that says
like call you know lib do you know yeah
okay nump pi. Factor right oh God it you
know this would be fascinating maybe I
can ask in a real here so what if I go
to GPT and I basically ask it like don't
write code except if you want to do
division how about that yeah just tell a
basic arithmetic operations the only
code that you can do is is basic
arithmetic operations yeah great and
then see if it can give you a you know
working
factoring uh piece of code and of course
these kind of problems right are talked
about in in elementary school so I'm
sure it will find lots of examples
online
right
yes here I just told it Factor 30171 but
don't write code except you can use
Simple bits of code to do basic
arithmetic yeah there you go all right
so first it's trying to do trial
division by two it hasn't even written
any code yet it's just starting to do
trial
division it claims it's divisible by
three I mean it's interest it has the
confidence to try to do math without
writing code so that that's interesting
you know a lot of times it's
overconfident about that kind of thing
with
see so it said three is a factor let me
see if I got that right uh it did so far
so without writing any code it just
happens to be right like maybe this
exact problem was somewhere in its data
set I'm going to tell it to please
continue because it just factored it
into 3 times 100, 567 all right so it's
continuing that's interesting so it
hasn't written any code yet um but like
I I guess you know I can't get it to
write the right amount of code but like
well I mean look I'm just trying to make
a very simple General point which is if
you if you want any any machine to be
better
at any task but let's say in this task
you know writing algorithmic steps to
factor numbers um you need to give it
that in its training data right and and
you need to give it that capability of
training time so I feel like GPT 40
understands factoring as well as I
personally understand factoring right if
you want me to factor something I'll try
writing python if I can't I'll take out
a piece of paper and I'll try doing my
own manual division the only difference
is that GPD 40 just can't actually do
division reliably without a calculator
that's the only difference isn't
it I I mean in a simple case right like
factoring um where it I mean there's
plenty of examples for it to have
learned from on the internet like how to
factor all kinds of factoring algorithms
Wikipedia's you know written about it
all this kind of thing um sure I think
what I'm saying is that if you wanted to
be able to write general purpose code
right or to be able to solve more
General classes of coding problems um
and in order to write uh turning
complete you know programs that needs to
be baked in at training time like we
have to actually give it that capability
and examples so we can learn to optimize
you know for that that skill yeah let's
try to find an example of a specific
problem where gp2 40 stumbles on it
today but if something in its training
was somehow a more Turin complete
feedback loop then we should think that
it'll do better because I think that's
an interesting claim you're making
doesn't make sense to me but I would
love to work through an
example I mean and none of the examples
I've given you know already are are
helpful like arbitrary digit
multiplication or yeah that's that's not
helpful so the input is like here are
two numbers just multiply digit to Pi
you know dig to Pi is that not is that
not helpful so if you want an AI or a
human to calculate the N digit of pi the
I think GPT will understand probably
better than I do what algorithm you
should run so isn't it already trained
correctly to get dance digit a pie if it
knows what algorithm to run to get
it it depends on on on what we're
talking about like cuz you know any um I
I feel like where we want to get to
eventually is is we're talking about
super intelligences right and and sort
of general intelligences and so any any
problems that are very well represented
in the training data set are kind of
sort of beside the point right because
CU because then you could accus of being
a stochastic parrot um yeah because it's
just giving you basically it grabbed the
code from Wikipedia and gave it to you
like the article on you know you
know how to how to calculate the in
digit of pi
right yes but you know just to frame the
reason why I am drilling down into this
you're Bas what we trying to get to yeah
yeah and and I do think I I do think
this is productive because the larger
point I'm trying to make is when I see
human brains think it really seems
similar to me from a theory of
computation perspective as when I see
llms think and the only difference I see
is just one of degree like oh the LM was
thinking great but then it messed up why
did it mess up I don't know but the type
of thinking it's doing seems very
similar but then you're saying no no no
it's so different because at training
time it didn't have the feedback of what
would happen when an algorithm with an
arbitrarily long tape runs and if you're
making that claim if you think that you
see a difference in the fundamental
nature of the thinking I would like to
just work through a good example to
illustrate your
point well so I mean you've said so
yourself and and everybody notices this
and open AI notices this and which is
that Beyond on a certain kind of point
they just start to break down so they
have this problem with long-term
stability right so in other words
they're they're applying a set of rules
in such a way that they break down you
know uh let's say more rapidly than than
we would like if you're trying to get to
to AGI right and that's the that's
really the point of algorithms right is
that they don't suffer from that like
algorithms that are turing machine
algorithms they have ability to compute
for an unbounded amount of time as
required by the input problem to get the
correct answer and they're able to carry
out this sort of precise exactly correct
Chain of Thought uh for as long as
necessary and I'm saying if you want to
get llms to that same capability we need
to be training them with uh turning
computations at training time training
them to run those computations at
training time I mean I I get your claim
on like a high level but I'm just
curious like what specific example it
maps to because I think it's interesting
that we both kind of agree that it's
it's an irrelevant claim on the example
of factoring right like it sounds like
llms pretty much get factoring except
they suck at the the long I I didn't
claim it was so uh first of all like I
haven't played around with you know
factoring right I mean I don't know why
we keep coming back to that the one I
usually use is arbitrary digit you know
multiplication
like that's a very
easy you why doesn't work and neither
does the N digit of pi like it fails to
do that all you have to do is just just
give me an llm that gives me the end
digit of pi and I say great now you have
a try and complete system I'm I'm trying
to get people to that point you know and
again with apologies because I
understand there are extreme risk to to
to getting us to to AGI so I don't I
don't want to say like I don't respect
that risk but just as the intellectual
exercise right I'm trying to get us to
move towards that and I'm suggesting
this is how we do it so when I as a
human do long multiplication the amount
of state that I'm holding in my head is
pretty di Minimus right it's like a few
digits worth of State yeah yeah cuz
you're using because you're using a
piece of paper me too thought the amount
of state I'm holding in my head is like
a couple digits because I'm writing it
down on a piece of paper that's
unbounded okay
so you got to have that unbounded
readwrite memory Okay so if we look at
how humans do long multiplication aren't
we just always finite automata who can
do multiplication up to like a million
digits before we forget or make a
mistake aren't we still find it
atom so again the controller of a
turning machine which is what you should
think of your brain as is as a turning
Machine controller not a turning machine
but a turning Machine controller is a
finite state
automata right but it's running an
algorithm that can utilize unbounded
readr external memory don't you think
that if you actually had a single human
try to multiply a million digit number
by a million digit number they would be
unlikely for them to get the correct
answer like there's some limit where the
human kind of TAPS out so it's not
really like
infinite no no I mean and I don't
disagree with that I mean sure humans
make mistakes you know we're not and by
the way um this is also a common I'm not
saying you have this misconception but
there is a common misconception that
Turing machines have to be exact and
perfect that's false because you can
always take a stochastic turing machine
and get it to whatever arbitrary degree
of you know Precision you require by
running it you know multiple times
running verification procedures blah
blah blah so could a human being
multiply a million digit number like I
could if you paid me enough like if you
wanted to pay to for me to prove that it
could be done you know would take me a
while right and that's kind of beside
the point because I'm talking about the
algorithm class here yeah yeah yeah
right so the where I'm going with this
is just that like when I see the average
human try to multiply two huge numbers
together and probably make a mistake but
still like make a good effort to me that
seems very similar to an llm trying to
multiply two four-digit numbers together
and similarly making a mistake like to
me there's just a difference of degree I
don't know how you solve it by training
the llms using touring complete outputs
and I don't think the humans who can
successfully multiply long numbers I
don't think that they were trained using
Turing complete outputs like I don't get
what fix you're suggesting
here I mean so first of all this is an
empirical question like I mean will my
I'm making a suggestion here I'm making
a suggestion and my suggestion is that
um and actually this and this is just a
a kind of hack solution I don't think
this is the optimal you know way to
train I don't think this is the optimal
path to searching the space of turning
complete algorithms I'm just saying that
one way you could do it um is imagine
you're doing the the 01 you know kind of
thing right is you could uh allow it to
run python programs um if they get back
the right answer then it's like it's
learning chains of thought right now
that kind of lead to Accurate examples
it would now instead be learning the
types of python programs that lead to
Optimal Solutions right and if you did
that at train
time doesn't already know python
well doesn't it already know the type of
python programs that help you
multiply well I'm I'm going Beyond like
just simple simple well documented well
documented problems like whatever
problems are in its training set it's
learning like the templates right like
these kind of right templates give me
good python programs and so then when
I'm asked for something novel and I try
to generalize to a problem that wasn't
in my training set um now I can generate
a new Python program completely new to
solve this new
problem you're going abstract again
right you're going abstract I was hoping
to find an example TR very practical
okay but if if you're getting I thought
we diverge I've already given you
specific examples like I'm you're saying
now you were attacking my suggestion on
how to build machines that that are
better at um generating you know turning
code let's say right well I I wanted to
find a specific example that contrasts
like okay humans really get this but
llms don't get it because it's too
infinite for them right like they're
using the wrong model of computation the
controller is too finite or whatever you
want to say like just illustrate your
point go go go to the go to the pillar
problem okay like the pillar it failed
to solve the pillar problem that's only
five steps like you know you did it I
can do it like everybody in our Discord
managed to do it the llm failed and like
it failed tragically like you know it
fails in hilarious hilarious ways and
and the and the reason why I was able to
so easily at the flick of like a tiny
amount of my personal computation throw
it in a loop is because I have this
intuition about the problems that llms
have and they have the problem of
carrying out precise computation that
unfolds over time now right now even
depth five confuses It 2 years from now
maybe I have to give it a problem that
has depth 10 what I'm trying to say is
there will always be a relatively small
dep that's going to confuse it until we
start training it in ways that allow it
to search the space of turning complete
algorithms okay so if I remember
correctly does that sense I think so but
let's do the example if I remember
correctly you asked the llm to do your
pillar puzzle and right off the bat it
started not really having a good plan
for how to solve it right so I feel like
in the first couple sentences you're
like uh oh it's kind of bsing like it
doesn't really get the problem and then
sure enough like it's screwed up so does
that really illustrate your point about
it not being turn complete it not being
able to utilize an infinite tape to me
that just seems like you know it went
off half coocked and then it wasn't
robust enough to fix itself right isn't
that kind of a different problem well
look I'm not running I'm not running a
research organization that kind of like
you know tries to do a very
careful um experimentation uh you know
to I mean I think we're kind of I don't
know I think maybe we we've said what we
can about this about this topic like I'm
just trying to say um if you want to
train if you want to train machines that
can generate tur complete Behavior
whether they're doing it internally
themselves or generating code that that
they're allowed to run on a attorney you
know attorney machine themselves um if
you want it to do well with that uh it
needs to be trained trained to do that
is really kind of the point I'm making
right I
mean is that really something this
example right see you gave an example
that's supposed to UST your point but
when we analyze the example I just don't
think that the analysis corresponds to
your your abstract Point here I think
the analysis is just that like it
started writing the wrong plan to solve
the problem and that's what screwed it
up yeah I mean I've given I gave a bunch
of examples and and it was it's sort of
like the no true Scotsman problem it
like you're like well arbitrary
multiplication sure it can't do that but
neither can people and it understands
and factoring numbers it understands as
well that doesn't count this one doesn't
count because like it clearly went wrong
at the beginning like okay I mean let's
I have't I I cannot give you an example
for now so we'll just okay I try to give
you get I'll give you
that yeah yeah likewise likewise cool
all right great zooming out a little bit
we're trying to talk about boundaries of
what llms can't do yet and my position
is like I don't think we know a boundary
besides noticing that they shoot
themselves in the foot at some point
like sometimes they get off to a good
start and then they mess up so that's
kind of my fuzzy boundary I just call it
robustness but I want to ask you if you
think you can see certain other
boundaries for instance is do you see a
statistical boundary where llms can only
do things that are statistically similar
to their training data what do you think
about that
claim yeah so I don't um
and do we really so are we focusing
again llm like is this uh is this my my
concept of an llm or is it maybe a more
generaliz kind of like let's say llm
system where the llm is the brain you
know of a system but it it can do more
things it can run some Pyon code it can
tweet things it can read the internet I
think it's most interesting to talk
about like the best systems today
including all the types of clever hacks
that people at AI Labs might have
thought of Okay cool so so more General
and let's just call those like I call
those llm systems or somebody recently
wanted to call them you know large
reasoning models um you know systems
that sort of thing so just whatever the
state-ofthe-art is um and this is this
is a point where um I've seen and by the
way um after having you know become
aware of Doom debates and you I watched
you know many of your episodes enjoyed
enjoyed a lot of them and a point that
you bring up often is the church touring
thesis right and this is something that
I absolutely agree with so um you know
inherently I am a machine I'm a bio
machine I'm running you know or at least
it's highly likely that I I am running
uh what we would call an algorithm right
um in the same sense of a turning
machine like this is highly likely there
there is a little loophole that you know
we can talk about maybe later or not um
let's just say we are so I'm a machine
I'm a bio machine um and if we're
comparing myself to some alternative
machine like I don't personally think
there is any limitation so I believe
that
machines um
constructed of non-biological material
you know running in silico as long as
they have a a realization they're
they're embodied right they have they
have like causal sufficient causal
structure um they can they can be
creative they can think they can reason
they can have agency they can they can
have Consciousness you know I'm going to
go out on the limb here and I hope you
know I hope I don't piss off Chalmers or
something but I think they I think they
they could be conscious
um I agree with for example Carl friston
who says that you know um Consciousness
is when you have a when you have a a
computation of sufficient temporal and
counterfactual depth right um You have
Consciousness right okay yeah fair
enough I don't have a strong opinion on
Consciousness I usually just talk about
the power of intelligence regardless of
how much Consciousness comes along with
it but let me ask about your 100-year
view because you're basically uh you're
saying things that I agree with right
which is just like hey look
fundamentally we're machines
Consciousness is physical right it
sounds like like basically you're a
physical materialist right I mean I
am okay yeah you know these these labels
sometimes like I don't know I think so I
think I'm a a phys definitely I believe
I'm a materialist yeah yeah okay yeah as
opposed to like a dualist who believes
inre ontologically fundamental mental
entities no none of that okay yeah yeah
all right me neither so in your 100-year
view if we just keep cranking on
technology development and nobody cares
too much about the risks and everybody
just tries their hardest to keep making
smarter and smarter and more powerful
AIS do you think within a 100 years
we'll get to the level where the AIS are
smarter than humans and more capable in
every way or do you see humans likely
holding on to some differentiating
power yeah yeah that's a good question
so um I think I I have to I do have to
at least take a bit of
time here to to at least delineate um
some some different concepts so I I
delineate between intelligence and
general intelligence um you know so to
me intelligence is is um you know a
skill at a particular problem and
general intelligence is the skill of
finding skilled
programs okay um okay and you know so we
can have we can have artificial
intelligence we can have artificial
super intelligence we can have
artificial general intelligence and we
can have AR or super you know artificial
general intelligence right we can have
all those those types of things
um I think uh if we keep going the way
that we're going I we already have super
int we already have narrow super
intelligences right we already have
narrow super intelligences those exist
today I think we will continue to see um
more General but still narrow
super intelligences you know within 100
years like this is not stopping it
whether or not we get to to super
artificial super general
intelligence within a 100 years I think
is more uncertain for me I don't know
can you think of an example of something
that you imagine regular old Flesh and
Blood humans being able to do in 100
years that AI can't
do well I think that I think that we
will be able to construct and train AIS
and AIS will not be able to construct
and train
AIS okay at least until we have super
artificial until we have you know
artificial general intelligence at a
super level or at least like human level
um that when you try to imagine a 100
years in the future you just don't think
that'll exist no I'm just saying it's
uncertain so I mean and this is you know
this there's always this problem of like
meta probability right because I don't
uh the probability of that happening in
my mind is highly uncertain it's like a
it's like a uniform distribution between
zero and one so I just don't know I just
don't know if this is going to happen
very rough ballpark like
5050 yeah yeah I mean I think there are
I think there are a few I think there
are several probably critical um
Innovations particularly on the training
side like I keep coming back to this
this issue of you know training them
right I think there are a few
Innovations we need to make um in order
to efficiently
search uh the spa the larger space of
algorithms um before we can before we
can get to artificial super you know
general intelligence so like Greg
Brockman and Andre karpathy are people
whose jobs if they can uh you know do
anti-aging and and be alive in the year
2124 you think their jobs will be
safe uh well I
mean like I said it's just High UNC so I
don't know I I I honestly like I don't
um okay and your intuition here is when
you think because you mentioned the
example of developing Ai and then you
generalize it to like well like deeply
understanding algorithms and being able
to search the space of creatively search
the space of algorithms could be like
the last hold out human skill in your
view oh no I don't think that's the last
I you know I don't know that that's
going to be the last hold out I think
there were probably the last hold out I
mean if we're talking about just sort of
replacing humans in terms of uh you know
professions jobs that that sort of thing
I think the last holdouts are more
likely to be um jobs like aesthetic type
jobs where human beings you know are
able to produce content that other human
beings really like like I think it may
be I think those May remain things that
are you know hard for for AI just
because they're different from us and so
it's almost like you know can things
that are very different from us produce
things that that uh really resonate with
us n you know I don't know really it's
interesting that you bring up that
example because that seems like the type
of example where you get a lot of input
output training data and you can really
just learn regression on that can't you
I mean aren't they already making very
beautiful
artworks well no not not really I mean
if you go look at the things that like
Dolly and and whatever generate um at
least for me at least for me personally
I get bored of them like quite quickly
it's kind of like after I've seen you
know five or six examples kind of like
ah whatever like it just doesn't really
yeah there's just some fire that kind of
Misses it's really apparent in writing
so when you read when you read material
that's generated by say GPT right now um
it's you know it's coherent it makes
sense it just it just
doesn't just doesn't have that kind of
like a panach and that that and you
think it's possible that a hundred years
from now it's still going to lack
pach I think it's it's plausible at
least not um let me put it this way uh I
think it'll be able to
generate um tons of content that that uh
has categories that really resonate with
people I think there will be some that
escape its capability and it'll have
more to do not so much with the with um
what it'll have to do with is that
whatever factors the human being is
aesthetically appreciating are just kind
of uh very difficult to deduce from the
training data there's just some there's
some stuff in our brain that we all
share that's not apparent in the
training data or easily deducible from
it okay I mean I'm tempted to move on
but like when you say not easily
deducible from it I mean don't doesn't
it have the full specification of the
output so whatever there is to be
deduced you H it's it supervised
learning like it has the entire output
it has the signal can't they just learn
it no because it's missing some stuff
that's in our head so we have this we
have this shared so you and I have this
shared you know genetic endowment right
I mean we have we have some stuff in our
brains that we have that we have in
common um you know like and and I think
that it's possible that that it's not
easily deduced from from the training
data if there's sufficient you know if
there's sufficient training data this is
kind of the whole argument of uh uh you
know the the the posy of data missing
missing data type type arguments right
which is if the space is high highly
dimensional right um You can just be in
a situation where it's really difficult
if not impossible to get enough data to
completely you know scope out the
manifold and that that Dimension right
yeah okay fair enough uh let's compare
our views on agency because I want to
ask if you think agency might be a
boundary between what AI can do versus
what humans can do maybe humans can do
more because humans have more agency
that seems like a pretty popular claim
that people make I mean it doesn't seem
like a good claim it seems like a pretty
easily dismissible claim but it's pretty
popular so my question to you is do you
think AIS are really far from having
agency is that a robust separator what
do you
think no I'm not I'm not I'm not
resonating not resonating with with that
one and I know um so my co-host talks a
lot about uh agency to you know to me
agency is is something and again okay
look disclaimer I am not an expert on
agency from a philosophical perspective
okay so I don't not even sure you know
what what I'm about to say is or is not
compatible or easily dismissed or or
whatever but my concept of agency is
just if I have some computational if I
have a machine okay that's in an
environment and it's sensing that
environment and performing a computation
and then it can perform actions in that
same environment that's an
agent and I don't see any reason why I
couldn't have a machine that that could
do that I mean I can build an Android
right that has a machine brain and and
participates in the environment so um so
I'm probably missing something deeper
you know to this this argument and and
maybe I think you are have an opinion
hick your right oh okay
okay test I just thought this could be
like if you were one of the people who
are like AIS can't have agency then I
could disagree with you but it sounds
like we agree well I want to give credit
to you know to philosophers I'm sure I
mean philosophy is a very sophisticated
field so I don't want to downplay this
as a potential issue
I'm just saying I don't see it you know
myself right
now all right let's talk about
intelligence versus optimization power
so yeah if I were to just Define
intelligence I always turn to the
optimization power definition because
even if I'm not going to win the
semantic slap fight about how a word
should be defined I can tell you that
optimization power is what I'm scared of
I think the thing that's going to end
the world is going to measure High
higher than humans in optimization power
I don't think we can Sur
building something with high
optimization power so when I talk about
AI becoming super intelligent the
conversation I want to have is the high
optimization power conversation and uh
for those of you who don't know the
definition basically optimization power
means that you can squeeze the future of
the universe or the future of a game
board in some game into a narrow Target
in state space so like a chess game I
can give you any chess position and if
you're really good at chess you can find
a sequence of actions that'll always
always map the chessboard toward a
winning state for you that's an example
of optimization you can do optimization
on the level of the universe if I say
find a way to attack this country and
make sure you win the war well let's say
that your resources you don't
necessarily get an advantage in
resources but you just have the best
clever plan and you optimize the
universe into the end state that you
want and you do so better than the enemy
can do it for you right so you win the
optimization battle um so what do you
think of optimization power as a mental
model is it a useful mental
model yes son I'm glad I'm glad you
explained that because um I think
optimization is one aspect of
intelligence I think there's there's
more to intelligence than just
optimization so and again I'm thinking
about these things kind of
algorithmically right so to me a sorting
algorithm is is an intelligent algorithm
doesn't have anything to do with
optimization you know I can I can frame
it into into like an objective function
or whatever but it's it's that that's
just the bolt on can I push back on that
a little bit I mean I so the input can
be a wide range of unsorted lists and
the output for for a given set of
numbers there's only one version of the
sorted list so you are actually seeing
an optimization Dynamic happening there
you what I'm saying you're not so for
example there's nothing more optimal
about sorting from A to Z than there is
z to a right this is just some and I and
I really this is only a small quibble
here because I don't so intelligence I
just think is a broad category than than
optimization however like optimization
is definitely definitely something that
intelligence
enables okay so I don't like I
don't is that is that okay if we yeah so
and and you know neither of us are
interested in just playing semantics so
let me tell you where I'm going with it
which is just I think optimization power
is the most interesting question when we
talk about ai's reasoning like instead
of looking at every Ai and being like H
I don't feel like that's reasoning I
feel like that's reasoning I think it's
useful to step back and look at the
input output and being like wait a
minute is it mapping a wide range of
inputs to a narrow range of outputs that
meet these optimization criteria if it's
somehow doing that then whatever is in
that black box I'm tempted to credit it
with being an intelligent Reasoner
because how did it achieve this input
output that is like hard for humans
right I tend to take that perspective of
things yeah and so I think I think
that's fine I think um I I typically
think um to me I I think more in terms
of like surch right like I've already
talked about earlier which is that you
know if I'm able to efficiently surge
the space of all possible algorithms um
uh you can call that an optimization and
and that's fine I just think about it a
little like slightly differently I just
think about more in terms of you know
search and and looking because a lot of
things um a lot of optimizations are
more like here's a structure and here's
a bunch of parameters right how can I
tweak these to make it you know perform
better in some certain way which I just
look at a little bit differently as let
me search a space of a whole bunch of
you know discret possibilities for
example and find one that find one that
performs Problem but it's the same thing
like it's we're talking about the same
thing and that's the scary creativity
and optimization and search I agree like
they're all getting at the same thing
which just like there's a lot of
possibilities but somehow you got to a
good one yeah exactly yeah yeah and and
I really like that I like you're framing
that because it's a you know it's it's a
powerful way I think of thinking about
the power uh of of intelligence right is
that in this very large space space of
things you get to you get to one that's
really good you know it works you know
maybe it's not the best of all but it's
like really good so I think um what I
want to ask you about though here
because I've seen you talk about this a
lot is I I get the feeling that maybe
you believe intelligence alone is sort
of sufficient it's kind of like if I had
a little soccer ball thing here that was
just super intelligent and as long as it
just had like one or two little sensors
like maybe just an ethernet port or
something and I plugged it in there that
were just all like immediately doomed
and and this is the part that I have a
problem accepting because I think that
intelligence is one thing but capability
power you know the ability to execute
whatever you're thinking in your mind
requires other things it requires you
know infrastructure physical things you
know uh manufacturing facilities you
know whatever else so what you know
where's where's the disconnect there
yeah I'm glad you brought that up
because that is a loadbearing Crux of
the dumer worldview the idea that like
yes intelligence is all you need because
intelligence can PR pretty quickly
finagle everything else that it needs so
imagine you just have the AI sitting in
the sphere in some data center even with
an air gap whatever pretty it can start
escaping right like if if it can just
convince some human in the data center
to escalate its privileges or find like
a zero day vulnerability are you
following the part of the story where it
hacks into a bunch of other computer
chips yeah so I think um I get you know
and I've Heard lots of kind of Tales
tales of this right and and
um you know I think these are all
imagined possibilities I think what they
don't account for is that there is a
super intelligence which is the
conglomeration of you know 8 billion or
whatever we're up to now people people
existing on the earth with all our
weapons all our you know all our might
right so if we have kind of mind and
might um you know it's starting at a
massive kind of disadvantage and I think
that that um it's always the Doomer
position doesn't matter how much might
people have it doesn't matter that on
many dimensions corporations are super
intelligences that there's a collective
intelligence of humanity blah blah blah
BL blah none of that matters we're just
going to imagine a little narrow path by
which it destroys everything and I think
that's kind of the weakest the weakest
point you know of of doomeris right it
sounds like that's the weakest point of
Thum of not not Doom in general yeah it
sounds like you made a previous point
that maybe you think my response was
adequate doe because I I think your
original point was doesn't it need more
actuators than just living inside a
computer chip I feel like I started
convincing you that was right about that
so then you moved on to another Claim
about like okay but how's it going to
take on human
society oh okay I I I didn't mean I
didn't
mean so I didn't mean the the actuator
thing right I'm I'm just talking about
in general might right so you have you
have human beings which are Super
intelligence like our Collective super
intelligence with tons and tons of Might
and then you have this new little ball
that shows up yes with an ethernet port
that has like zero might but you know
really gr intelligence okay I'm saying
like it's not clear to me that that like
any nerd in high school can tell you
that there's more to winning a fight
than than intelligence right yes so
first of all I think taking over a
billion computer chips is loow hanging
fruit what do you think about that
claim uh okay sure I'll Grant I'll grant
that okay so once you have a billion
computer chips and there's some
coordination among them right like it's
your botn net you're the super
intelligence from there it seems like a
pretty easy jump to having a bunch of
human employees because there's a lot of
way to turn genius and computation power
into humans doing what you say I mean if
you need money you know go make money by
predicting the stock market right like
arbitraging the stock market using your
intelligence okay so now you have money
so you can pay some people you can make
shell corporations you can pretend to be
people's girlfriend you can Ransom where
them so are you following the jump from
like being a billion Strong botn net to
having a bunch of people on your payroll
and who are loyal to you
yeah yeah so so no so this is and and I
and I've heard these I've heard these
stories before and I and I've heard like
the mini you know the many variations of
it you know Diamond manob Bots like all
kinds of things I just yeah like I just
wanted to know and I I didn't really
want to I don't want to try and dive
into those in great detail at least not
in this conversation but I just want to
understand like from your perspective
that's that's done sealed no problem
whatsoever like it's like in other words
you see that as a very strong High
probability argument that that if I just
have a ball that's super intelligent and
I plug it into an ethernet port that we
toast it's like
99.99% chance that that that we're toast
right um if it really is super
intelligent and it has some utility
function that it's trying to optimize
that's not the human utility function
yeah and I won't even say you have to
plug it into the ethernet port CEST okay
yeah yeah so I think maybe in some
future future Point it'd be fun to have
have a conversation about that I just
wanted to make sure that that that is
the you know the position right which is
um yeah because of course that that's
more a scenario where like a ball from
another planet shows up because it's
already developed it's super
intelligence it didn't have to go
through any right you know any child
stages or anything like that um but once
it's here like once there's a super
intelligent ball if it has Su sufficient
super intelligence and maybe not even an
ether netport but just you know a
radiator or something that it's kind of
yeah exact we're done for yeah yeah so
the problem is the way I like to phrase
it is physical reality has poor
virtualization so the idea of like okay
it's sitting in a ball that ball is
causally connected to the rest of the
universe like there's a lot of causal
links that humans don't understand well
or that they don't understand the
implications of right like you can send
a radio signal I mean we see these kind
of side Channel attacks all the time
right yeah yeah yeah and and I
understand I kind of just wanted to I
wanted to make sure for sort of my
future thought that that that is a
scenario that that you believe is you
know very like highly certain that it's
possible there's a a ball that's
sufficiently intelligent and maybe just
has some ability to observe and radiate
light you know or whatever it is and
yeah yeah I mean it's there's a couple
intuition PS right I mean imagine chimp
seeing like the first humans right these
hairless apes and being like you know
they're going to be able to wipe out
like a thousand mile circle with these
bombs that they're going to drop and the
TR is like um what you're telling me
that like you can just a hairless ape is
going to somehow go from being a
hairless ape swinging in the tree
branches to doing this magic right like
making these Rockets go to space like
that seems like a pretty tall order here
like it seems like you're going crazy
and it's like no no no like the the the
key was that the Universe just has these
paths like the universe is very hackable
and it has poor virtualization and like
you know it's low entropy like we just
live in a universe where you can pull
that off and the only limiting factor is
knowing how to do it right the smarts to
do it right okay yep so I mean even if
that's that's taken for granted then we
have to get to that point right so we
have to we have to build the AGI because
initially it's not going to be a ball
with a bunch of hackable you know
hackable hacked Universe capabilities
it's going to be a big data center right
um you know starting yeah okay yeah I
mean from my perspective as a Doomer
we're just getting closer and closer to
the game over condition which also like
to refer to as like we're about to Brick
the universe right sometimes you're
playing with your iPhone and the iPhone
gets bricked but the Ace in the Hole is
you can restart the iPhone but there's
just no equivalent thing that you can do
with the physical
Universe right right um now I mean in in
sort of your your
Doomer your perspective like your
worldview is it more likely that that
we're going to get to a super
intelligence that does that kind of
thing or that that we're just going to
destroy ourselves because we build you
know autonomous drones and uh and
somebody uses them to start a a nuclear
drone
War I think so I think nuclear
Extinction is very real it's like a 1% a
year threat and I'm a pretty big nuclear
Doomer I'm like uh there's a 1% a year
Extinction threat like that's very scary
but I think my P Doom from AI is like
50% by 2040 which is uh that ranks it
above nuclear Doom in my goom ordering
so I I just think we keep is that from
is that from AI because from an an AI
decides to do this or we misuse Ai and
and you know pretty much destroy
ourselves with AI I think the
distinction is smaller than people
realize like I I don't it's hard for me
to predict whether you can trace it back
to a trigger of like one particular
company that pushed one particular start
button I think it's just better to
analyze it as like these uncontrollable
agents keep existing keep getting closer
to this threshold where they're
uncontrollable permanently and then
there's some triggering event the
triggering that could just be some
random research lab researching it
running the next prototype and then
losing control or it could be somebody
intentionally you know having a secure
system where they have to like lift up
the glass and press the button and then
that's what kills us right like the
triggering mechanism isn't as
interesting to
me okay the triggering mechanism is not
as interesting all right I mean to me to
me it's interesting because it it goes a
lot towards prevention right like if
you're if you're trying to prevent the
scenarios and you're gaming out you know
triggering mechanisms matter a lot right
because they they especially matter if
you believe we're kind of on this tight
rope right that we need to we need to
develop super intelligence
to and I I say this roughly for Humanity
and air quotes to continue like for you
know a long time um then you would care
about what the triggering mechanisms are
because you want to try and press the
development of AI like for example
something I really believe in as perhaps
a viable path forward is let's just
suppose for the sake of argument that we
could enforce
worldwide um that we could only research
narrow intelligences and they had to be
you know very narrow and not even
getting close to General you know
General capabilities to AGI um like
would that be a viable path forward to
to avoid this type of triggering event
that you're talking
about in principle it could be but I'll
make an analogy to Nuke so I mean there
is kind of a natural analogy which is
like useful nuclear power generation
compared to a nuclear
expion and in that particular there's a
single scaler that can tell you which
side you're on right if if the reaction
is going super critical right where it's
each Neutron is creating two neutrons
and then you just spend your entire Fuel
and release a ton of energy then that's
bad but if each Neutron is only creating
exactly one more Neutron then you can
just keep generating power off it at a
good rate right and you can even damp it
down to zero when you're done so you
have a single scaler making the distin
when you're generating useful nuclear
power when you're generating useful AI
value unfortunately that separator right
the linear separator whatever it is um
it's like a very complex shape that
nobody knows right so we're just
exploring the space we're like hey look
I made it more powerful it's like um are
you about to hit uncontrollability like
I don't know it's just more powerful
it's more
useful no I mean it's so it's you know
it's an interesting question this is why
like for example in nuclear world right
it's it's about the Purity the levels at
which you're allowed to refine you know
nuclear material because weapons require
higher grade you know material than than
uh right than energy right and but I you
know it's it's interesting because I'm
not convinced I think there are ways
that we could Define what types of
intelligences we allow um potentially
I'm not saying for sure but I think it
it's possible we could Define that it's
kind of like maybe we could put in
barriers that say you know like and
again imagining you could enforce this
like that's part of the hard problem too
is getting everybody on the same page
and they're not being malicious actors
and Bad actors whatever else but just
suppose you could say okay look you know
you can never have a feedback loop
between like the output of a super
intelligence and its own code it has to
be kind of firewalled and you're not you
know and there's a whole bunch of other
restrictions on how the output is used
and you know maybe we can identify
certain types of computations that are
not allowed um to take place in there it
would be interesting to see if if kind
of the
intelligence science Community could
sort out um safeguards it would be
interesting to see if they could sort
out safeguards that the Doomer Community
looked at and said okay yeah if we could
enforce all these things then P Doom of
this kind would go down to some smaller
number yeah is there even any work being
done like yeah work being done I mean a
lot of people have that idea right of
like let's stick to narrow Ai and I
think maybe Alpha fold and Alpha proteo
those might be the biggest success cases
of like look it doesn't look like Alpha
fold is going to take over the world it
looks like it's useful narrow Ai and
that's great but the problem is if you
really want to get good at protein
folding it's always going to help if you
can bring to bear some general
intelligence right if you can step back
and you're like hm what other Factor
might influence this protein folding
right and you can start we have to
disallow that right like that that's
kind of if we you have toallow I guess
the question is yeah well the problem is
even if you want to disallow it if you
just train one of these narrow reasoners
to get better and better they might
stumble upon general intelligence which
is what happened to humanity like no
other animals ever stumbled on this but
when Evolution was just training us like
listen you're a narrow Optimizer you're
just trying to optimize genetic fitness
okay but then suddenly you know general
intelligence comes online and we can go
to the Moon if we want to yeah yeah but
we have the advantage of already knowing
about general intelligence and so and I
think we understand a lot about
computation and and even though the
these are difficult they're not
necessarily intelligible they are white
box in the sense that we can examine you
know what's happening we have total
control over the parameters and and you
know uh whatnot so I think we may have a
better chance than evolution of not
accidentally stumbling on on general
intelligence that's fair um real quick
as as we get to the wrapup here um what
are your thoughts on alignment do you
think that we know how to make AGI do
what we want as its intelligence
increases
so if we stay away from from general
intelligence um yes I think we do I
think we know um I think I think in fact
like this is probably like a bigger
topic we should probably talk again one
day but um sure you know I have a lot of
problems with like the orthogonality
thesis I don't think it's it's really
you know I think it relies on kind of
counting arguments that are flawed like
inherently with arguing okay yeah yeah
yeah so I I mean and I I don't want to
get into that rabbit hole but I think
I'll just say I think if we stick to
narrow intelligences I think we do have
um a good handle on how to keep things
aligned if we stray into general
intelligence um I think we have a
problem um you know especially if it's a
general intelligence that's allowed to
modify itself right like as soon as you
basically as soon as you have as soon as
you have a general intelligence that's
allowed to modify
itself um you set up Evolution you know
you set up an evolutionary system and
and right you know you you know no like
you can't you can't ultimately control
that does that mean that you believe in
instrumental
convergence so no I don't I don't
believe even because I don't accept the
premise that there is such a thing I I
don't accept the premise that a super
intelligence will have quote a final
goal okay and and I also don't think a
super intelligence will will um you know
allow itself to I don't think the super
intelligence will ever have a really
stupid goal I just don't or at least a
general super intelligence again narrow
super intelligence absolutely you can
have a narrow super intelligence but a
general superintelligence I think
they're inherently um topologies Within
the space of Minds that are totally
ignored or unknown to the Doomer
community and that the counting
arguments are flawed and you know
everything else and that a super
intelligence alone if it's not a general
intelligence cannot take over the world
okay how about let's go down the rabbit
hole a little bit of uh orthogonality
thesis and instrumental convergence and
then we wrap up how does that sound sure
sounds good awesome okay so on
instrumental convergence do you think
that if somebody went off and built a
general intelligence utility maximizer
with some utility function maybe a smart
utility function maybe a stupid utility
function but like if that exists then
would that agent converge to like
seeking power and you know internal
convergence what do you
think I think that I think that there
are goals and Minds that
would
um that would seek power that would do
those kinds of things I don't think it's
it's clear at all that that's the
majority of the landscape I think I
think there's no I've seen no valid
argument okay that that in this vast
space of you know Minds that it's almost
surely the case that you're going to
have an artificial super general
intelligence that is power seeking and
atomizing and sort of turns off like I
think the argument for that is just you
know not solid at all in fact like the
latest statement that I saw on um I
think visited uh Les wrong you know
sometime recently and and read the um
you know the the statement for the
orthogonality thesis I know you were
asking me about the the um convergence
You Know instrumental convergence I
think those two are actually like very
closely related because instrumental
convergence right or sorry um uh
instrumental convergence assumes like a
certain kind of distribution on on the
possible Minds you know which which
requires orthogonality in order for like
that that counting argument you know to
kind of exist and I mean the
orthogonality thesis was you know had
this sort of very strange phrasing in
there about like um you know there's no
difficulty in the existence of these
things and like mere existence alone is
insufficient for the claim that it's
likely right the possibility of
existence yeah let me try to summarize
what I'm hearing here about instrumental
convergence you're basically saying when
somebody builds a if and when somebody
builds an agent which is usefully
optimizing toward some utility function
right like powerfully optimizing toward
some utility function the simplified
model of utility maximization a model
that would have you predict that it will
instrumentally converge to power seeking
that model just won't realistically
apply to the actual AI that somebody
would ever
build I'm saying that I see no evidence
or argument that the that the chance
will apply is very
high okay so you basically think as we
navigate AI space right as researchers
keep coming up with new AI keep making
tweaks we're just never going to get
into that part of AI space where you
have something that's accurately modeled
as a utility maximizer or falls into an
attractor State becoming more and more
of a utility maximizer and then going to
instrumental convergence you just don't
think the slippery slope is going to
lead there you just think researchers
are going to only produce other types of
AIS
well I mean you keep trying to make this
a binary choice so I don't I think what
I'm saying is I don't think there's a
99.9% chance that we're going to end up
in the in the you know utility maximizer
that destroys the world I think I think
the space is a lot more constrained um
than than that and that and that there's
a very good chance we'll wind up with um
again if they're narrow intelligences
it's a very good chance we going to wind
up with well controlled ones okay if
they're General if they're General
intelligences artificial super General
intelligences I think there there's a
very good chance we won't control them
but I think what they will do will be
radically different than what you know
owski or you or anybody else has
imagined and so to to think that your
imagination that they're going to
atomize everything is just you're not
even remotely capable of predicting what
they're going to do at that point so you
do
we will control not to us at that point
so when you think about the mostus
scenarios where most of your probability
mass is going to you're basically
thinking of one scenario where the AIS
have gone generally intelligent they've
gone uncontrollable but you wouldn't say
that they're showing instrumental
convergence right that's basically the
scenario you're
imagining correct I think they're going
to show a lot greater variety than than
what we imagine in that scenario are
they refusing to be turned off just to
clar clarify like there's no off button
in that
scenario well the you know at that point
the only again I don't I I don't want to
imagine um what they're going to do
right some of them may just decide to
turn themselves off you know for reasons
that imagine trying to do something
right they've got a plan right they've
got like a plan that's going to last six
more months and then some random human
wants to come and like fight them or
turn them off in the scenario you're
imagining they're just going to resist
right yeah okay well if the human is a
state actor and sends quite a lot of
weapons at them are they going to invest
resources in their
defense I mean now we're getting into
sort of slipperier slipperier slope
right because it's entirely possible
that they may they may discover morality
right they may discover Concepts that
that would prevent them from from doing
that they may for example they could
decide you know what um there's these
other life fors here they're really
annoying um but hey you know just like
just like
the United States decided to end slavery
you know we're just going to go
somewhere else there's just no need to
stay here because these idiots like
can't even get off this planet and
there's a lot more convenient stuff
elsewhere so like Jupiter's actually a
lot more promising for whatever we're
trying to achieve and they go over
there okay um what about the scenario
where they have some goals that involve
doing something on Earth it sounds like
if you're if you're willing to
acknowledge that that's a plausible
scenario and and you're telling me that
they're going to be like hard to turn
off they're not going to be very open to
the idea of people turning them off at
any given time if that's not their plan
it just sounds like I might be able to
lead you down the slippery slope to be
like okay they're going to be quite
dangerous and quite seizing of power
resources no well I think I I think
where I would agree is is it will not be
up to
us okay so what so what I would agree
with is that um it's entirely possible
we could build artificially General
super intelligent you know entities at
which point we cannot control them and
it won't be up to us what they decide to
do so that's absolutely a realistic
scenario where I depart is that that
automatically implies they're going to
atomize the earth I think that's where I
think that's where the arguments are not
not strong enough um but even even the
scenario right of creating an asgi that
we that we uh don't have control over is
scary by itself right because it may
just decide hey these humans make really
great pets like um let's you know let's
make them pets I mean yep yep why not
right or like how about this like I know
um you're probably a fan of uh verer
venge Veni right or you I never got
around to reading his books but it's on
my short list of what I should read
goodness I know please just like at
least read the the preface of of fire
upon the Deep yeah yeah yeah that's
that's the one I recommended yeah you'll
love it so just read read the preface of
it um and in there for example there's
all kinds of um you know super
intelligent Minds that sort of decide to
do different things and this
particularly malicious one you know I
don't want to spoil too much but
basically it decides to use human beings
as a resource I mean not as a not to
atomize them but as a you know as a
robot and it has the capability of
essentially just reprogramming you know
the human mind to go and do its you know
the things that it wants right and act
as a you know as a as a self-replicating
you know automaton to manufacture things
and you know and also try to convince
the rest of the the Galaxy to join and
you know uh stuff like that
so great all right and finally so on
orthogonality I kind of know where
you're going like hey doesn't have to
have it doesn't have to be aoral maybe
it will discover morality right is that
like roughly how you think about the
orthogonality thesis and uh and and
maybe just remind people what is the
orthogonality
thesis yeah so the the orthogonality
thesis at least is you know bostrum
bostrum prevent presented it right is
that you can have any arbitrary degree
of
intelligence okay combined with any
arbitrary goal so it's just you know
goals and intelligence are orthogonal it
can have any yeah any combination of
that right which I agree
with yeah and and and I I don't so I
don't I don't agree at least especially
if you're talking about again general
intelligence so I think there's a again
this word intelligence kind of like has
a little bit of an ambiguity right are
we talking about general intelligence or
uh you know narrow intelligence so I
would agree you can have any degree of
narrow intelligence focused on any goal
right we're doing that today okay where
I disagree is that you can have any
degree of general
intelligence focused on a on a goal I
think there's a manifold there you know
there's a landscape um that so in other
words the design if you if you take the
design space of of uh super General
intelligent Minds okay um the the
manifold of mind of that kind cross goal
is a very small dimensional you know um
subset of that okay and so you can't
just make a counting argument and say
like therefore almost all generally
intelligent minds are are bad because
they've got these like stupid goals of
you know building paperclips or whatever
let me just clarify your claim so when
you quoted what the when you quoted what
the orthogonality thesis is I would say
you quoted a pretty weak version of it I
mean I don't even know what's the
official version what's weak versus WR
but I I consider it a weak version
because you just quoted it as saying
that you can have an arbitrarily
intelligent mind that can have an
arbitrary morality or an arbitrary set
of values so it seems to me weak to just
say can because can is just in the sense
of like in the space of possible
algorithms can it exist so are you
pushing back that it can't exist even in
the total space of theoretically
possible algorithms well let's I mean
I'll just so I I looked it up here let's
just read okay the strong form of the
orthogonality thesis from less wrong
which I'm assuming is our our Bible for
this purpose right like our one true
leader Okay so
uh it says there's no extra difficulty
or complication in the existence of an
intelligent
agent that pursues a goal above and
beyond the computational tractability of
that goal okay and first of all like I
can't it's it's hard to even make sense
of what extra difficulty or complication
in the existence try kind unintelligible
right let's just say exist no let's
let's stick let's stick with this this
thing here right like so because this
this really importantly summarizes my
objection okay which is there's no extra
difficulty okay in the existence what
this means to me right is that we have a
design space of of minds and again I'm
going to have to substitute in here
intelligent to General generally
intelligent okay so we had this design
space of generally intelligent minds I
think what no extra difficulty and
complication what that means is this is
uniform distributed right that they're
all kind of equally likely and that's
what I disagree with I don't think okay
the distribution of design space is
uniform I think it's highly structured
along a a manifold in which the vast
majority of General intelligences are
not are not mindlessly P pursuing some
stupid you know M paper clitch goal is
my point yes so I I understand that you
have objections to a somewhat strong
form which may be the default form of
the orthog cases I just wanted to
clarify what do you think about the weak
form where I say if I can navigate the
entire space of possible algorithms will
I find algorithms in the entire
cartisian product of intelligence levels
and
values general intelligence or or narrow
intelligence general intelligence
no no so so you disagree with the weak
form of the so for example like you
don't don't think that there is an
algorithm in the space of all possible
algorithms which loves death and murder
but it's also like extremely intelligent
so you think that's like an unlikely
algorithm that could ever exist so so
I'm sure that again General General
intelligent right so we're talking about
is there is there somewhere in the space
of all possible algorithms a generally
intelligent homicidal you know algorithm
yes that
exists there is not in the space a
generally intelligent algorithms a
paperclip
maximizer in the space of POS so what's
what's the bottleneck because that it
seems like there definitely is an
algorithm space no why is why is that
fundamentally impossible well I'm saying
there's a manifold in there I'm saying
there's a relationship between goals and
general intelligence that that for
example the hypothetical paper maximizer
is just almost by definition you know
not generally intelligent because a
generally intelligent super intelligence
that decides to make you know paper
clips is just
stupid like by definition and so this is
my point is that that I don't buy the
orthogonality thesis at all I think
there's a manifold structure in there
okay in which there's relationships
between goals and super general
intelligence let me just repeat back to
you tell me if I'm getting this right
the whole concept of
caring to make the universe into paper
CBS is not intelligent therefore you
can't have every possible goal with
every possible intelligence levels
because some goals are not
intelligent correct okay so that just
seems like you can't really entertain
the orthogonality thesis because we have
a disagreement about the definitions of
the words used in the statement of the
thesis
right well what I'm saying is what I'm
saying is that first of all at least the
ragon ality thesis that's currently as
of right now up on L wrong is
unintelligible okay he he's done a poor
job of of explaining in English whatever
it is he's trying to get across because
the phrase extra difficulty or
complication in existence is not
intelligible okay so that so A I'm
saying it's it's unintelligible B I'm
saying if we try to make it intelligible
what I'm saying is that in the
distribution of of Minds it's not un
formly distributed and so therefore
there's a manifold structure of some
kind in there which implies it's not
orthogonal there's a function okay and
you see why I'm asking about the weaker
form because we're not even agreeing on
the weaker form right so why jump to
theer form let's keep talking about the
weaker
form okay I mean so in the weaker form
well if we take if we take here's my my
hypothesis right is
that there's not orthogonality because
there's some manifold in there right the
possible Minds if we restrict it to the
subset that are generally intelligent um
not all
goals are on that you know I I would
even actually take issue with the
concept of a goal for a general
intelligence right like I think I think
a general intelligence has a
multi-dimensional goal but let's just
for the sake of argument say it could
have single-minded goals I'm saying that
that uh not all single-minded goals fall
on the manifold of General intelligences
if we redefine intelligence as
optimization power or if we just restate
the thesis and we say arbitrarily High
optimizing power agents can have
arbitrary goals if that's another weak
form of the orthogon pieces do you agree
with that weak form so so the way I
would phrase that if I'm understanding
correctly is we want to sub out general
intelligence for just intelligence right
for for General optimizing power because
basically you took issue with the
definition of intelligence like in my
definition of intelligence I defined a
paperclip maximizer as potentially
highly intelligent but you said no no no
by the definition of intelligence I
don't think it should count as being
intelligent if it's really really good
at maximizing paperclip so I'm like okay
well let's use a term that we both agree
on its definition so that's why I threw
out optimization
power yeah
so
um maybe maybe can I phrase it this way
which is you can have very optimal
paperclip maximizers you have very
optimal paperclip
maximizers um but there will always be
more
optimal more optimal machines that
refuse to be solely paperclip
maximizers okay so it sounds like you
are disagreeing with the most recent
weak form of the orthogonality thesis so
when I State arbitrarily High optimizing
power agents can have arbitrary values
you're pushing back and you're saying
nope the absolute highest Optimizer
power ones are never going to be paper
Clippers do I understand you correctly
that's correct yeah when I say never
going to be that's a statement about uh
searching the entire a perfect search
through the entire algorithm space is
never going to find a top level
intelligence faer cliip maximizer well
really well well really you know what
I'm trying to get out here is that even
if those
you know exist in uh uh in principle or
they're even they're even there in
reality what I'm trying to say is
they're a vanishingly small portion of
the space of accessible Minds right but
that's a very different right
vanishingly small portion that's a very
different claim from it doesn't exist in
the St of then take it then take it as
don't exist I don't care like I mean if
you really up on why I think it's very
interesting even the weak form of the
orthogonal thesis I think is an
important thesis like they claim that
somewhere in algorithm space there is an
algorithm that is as intelligent as you
want it to be but also expl explain to
me explain to me why so for example if I
say the probability you know is is zero
of that existing versus a probability is
1 time e minus 30 okay what difference
does it make to you for your argument
for your policy for what like what's the
Practical difference the difference is
that we can factor out how we disagree
about about the strong orthogonality
pieces because I think the probability
is a lot higher than you think it is but
before we have that argument let's at
least both agree that it's it's not zero
because from a from a computer science
perspective there's a lot of branches of
computer science that just talk about a
single best possible algorithm and don't
try to do probability of algorithms
existing right it's it's a separate
question of like does an algorithm exist
and then we can have a separate
discussion of like okay but will
researchers ever create the
algorithm okay so for the sake of AR
right I'm I'm going to continue to deny
that it's it's possible for now at least
like I don't think you presented an
argument that it's possible okay okay
you think sear through algorithm's face
is not going to find it the probability
is zero correct let's just that that
that's what I think right now you
haven't convinced me otherwise but let's
suppose for the sake of argument I it's
just it's there with some probability
you know choose whatever low low number
makes you happy I want to know where you
want to go with that for the strong or
ality thesis so let's suppose that you
know uh there's three Minds in the space
of all possible you know turning
machines that are that are paperclip
maximizers of the highest possible
optimization power now
what um so I get to assume I get to
assume that they already exist no no not
that they already exist like you can't
make that claim you can't possibly make
that claim like they're just in
principle possible we could we could
stumble upon one right so there's in the
space of all possible Turing machines
there's some some handful of Turing
machines um that are paperclip
maximizers of the highest possible you
know optimality but not generally
intelligent okay I see what you're
saying yeah so yeah you're making a
perfectly reasonable claim you're saying
look maybe all of these orthogonal
agents exist maybe really smart agents
that have really bad or really a moral
I'm not saying they exist they're
they're possible yeah yeah right they
exist in the sense of like being uh
elements of the set if possible
algorithms in a mathematical sense okay
yeah and then you're asking a very
reasonable question of like okay how
will the universe actually evolve right
what algorithms will physically exist
compared to the space of algorithms that
could exist in in that mathematical
sense very reasonable question and for
that we have to dive into the question
of like well how are the AI Labs doing
getting AI to do things that we value
right I feel like that's kind of an
empirical question and also you know try
to project extrapolate that
trajectory okay well I don't think we
have time to to get into that we should
one day but how does it relate to the
strong orthogonality thesis because you
told me that right if we resolve this so
we're now for the sake of argument we're
in agreement that in the space of all
possible algorithms there are algorithms
that are paperclip maximizers of the
highest pop possible optimality but not
generally intelligent how does that now
bear it all on the the strong form of
the orthogonality argument as posted on
L wrong.com the strong form of the
orthogonality on less wrong which which
I endorse is that I think there's a good
analogy you can make an accurate analogy
where uh if you want to guide an AI to
have a utility function that cares about
human stuff it's analogous to the
problem of guiding a rocket to go land
on Mars for like there is a lot of
guidance required and if you don't use
the appropriate guidance you're not
going to gently land on Mars right
you're not going to get an AI that cares
about you're going to get an A cares
about something
else so that's the counting argument
though right so it's not enough that
there's only three of them in this
entire space it's like there's so many
of them that guiding it is a very
treacherous course right I I mean I
think it's more complicated than the
counting argument it just it's an
argument about like the nature of
guidance right I mean is it a counting
argument to say it's hard to guide arak
dears I mean in some sense yeah but it's
over sure so I think like and I heard it
and I forget who said this once but I
thought it was a great analogy they said
like right now the way that we're kind
of uh chimping along with with our
development of um of of artificial
intelligence is we're we're trying to
land a rocket on the moon without
understanding orbital
mechanics right yeah it might even be
Source Tazer he's got a whole post about
that which I love by the way he makes a
lot of analogies con may may have said
that to me but somebody somebody said
that um is that that's kind of what what
you're talking about right yeah I'm
going to link this in the show notes
alazri kaski has a great post called the
Rocket alignment problem where he really
takes these analogies really far and I
personally love it okay yeah so I think
um is I mean again I'm not going to buy
I'm not going to buy the argument
however I think the policy that it that
it points to is that we need to be
spending a hell of a lot more time and
resources on learning AI mechanics right
or AI alignment mechanics like is that
that's where we would get to in terms of
action that we should take is that
correct or
yep okay yeah and I think I mean so I
agree with that I don't even need the
the orthogonality thesis to get to get
to that point I mean we
definitely you know I mean that's why I
said I've been in favor of things like
taxes and you know civil liability and
and know not sure what else
but great I think that's a good place to
RAB it what do you think perfect
enjoyed it yeah likewise so yeah I mean
to summarize we tried to hash out our
disagreement about uh touring machines
versus finite automata models of
computation um it feels like we didn't
get far in convincing each other but at
least we exposed enough of why we
disagree that at least the audience can
make up their mind
fair sure I I I guess I mean yeah I
think we definitely TSS enough that the
the audience can um yeah I don't feel
like there's much more we can say to
each other right it's in the audience's
hands now yeah yeah that's curious yeah
okay and then I thought it was really
fascinating to talk to you about you
know Doomer topics because it sounds
like you know you're not coming in with
a stuck position right you're willing to
engage in arguments you're willing to
update your view you have that oh yeah
larger background of like yeah life is
fragile like we're not we don't have
plot armor right so we're coming at it
from the same side of the table
here no absolutely I mean I I think and
interestingly enough I think even though
we have obviously like quite different
perspectives we actually want the same
thing in terms of in terms of of
policies which I think is is interesting
I mean you know I think yeah I think too
much time is wasted on on doomers and
not doomers or whatever whatever we're
called whatever labels arguing about the
differences in theory right um rather
than just focusing on let's let's what
are we agree with on policy I mean and
just get it implemented um sooner rather
than
later absolutely great and you know the
fact that you came into the lines then
and you were such a great Sport and you
offered to have this conversation uh I
just want to let everybody know if I've
ever done a reaction episode or named
you in Doom debates you're automatically
invited to do the thing the same thing
that Keith did come on over you got a
right to reply we'll have a a friendly
back and forth and you can make your
best case and you know I won't interrupt
you or uh you know get maliciously edit
or whatever you can make your full
unedited case to the Doom debates of
your Shi yeah thank you for having me on
yeah thanks so much for coming on really
enjoyed it I hope to keep the
conversation going I love the work that
you're doing with uh machine learning
Street Talk by the way so if you guys
like do debate I definitely recommend at
least checking out machine learning
Street talk because they're constantly
putting out good quality discussions and
some of them I agree with some of them I
disagree with but it's there's no
question they're doing great work yeah
thanks appreciate it thank
you once again I'm really grateful to
Keith for agreeing to do this I think
the back and four we had was super
friendly and productive it's not just
about hashing out our particular
disagreement it's also about proving to
the world that this is how discourse can
work that people with very different
beliefs about something like the limits
of llms when the stakes are so high that
whoever is Right might have a much
better mental model of how doomed we are
even in that situation we can just have
a friendly conversation anyway and we
can just honestly tell each other where
we agree and where we disagree it's not
that hard you just have to have the
social norm and expectation that people
do it instead of just retreating to
their own camps so luckily here on Doom
debates we're starting to see a Snowball
Effect where the show is getting more
and more popular more and more guests
are interested in coming on and that
makes the show even more popular and so
by this time next year I'm expecting a
lot of very high quality debates not
necessarily all of them involving me
sometimes I'll even bring on two other
guests to debate each other because as
you know the whole mission of Doom
debates is to raise mainstream awareness
and urgency of near-term AI Extinction
risk and to create the social
institution for highquality discourse
and debate people like Keith who are
coming on and being good sports even
without the social expectation that you
have to do that they're really helping
the cause and I appreciate it now as for
you the Doom debates listener you have
to do your part by not being a dick in
the comments okay normally I'm proud of
my audience you guys leave mostly great
supportive productive comments but I
have seen a few stray dickish comments
go by and I'm not going to tolerate that
so if you're being a dick to Keith in
the comments I'm going to ban you from
commenting okay I'm not a pro censorship
guy the last thing I want to do is Ban
but if you can't follow basic rules of
good discourse if you feel like you have
to be a dick instead of just arguing
object level then you can't comment in
my forums okay because it's too
hypocritical to be like yeah let's have
high quality discourse and then have
comments like keep can go to hell you
know no comments like that okay and it's
really not even the right Vibe because I
like Keith I like machine learning
Street talk I like keeping the dialogue
open and Keith and I have a lot of
common views and interests as you heard
so thanks for remaining civil and also
the other way you can do your part in
case you forgotten is you can smack that
subscribe button you can share this
episode you can do all kinds of things
to get us to that 2,000 or 100,000
YouTube subscriber Mark you know just
when I was feeling so good about myself
for crossing the 1,000 YouTube
subscriber mark one of my commenters was
like how come a thoughtful guy like you
only has a thousand subscribers and all
these channels that are talking BS about
how AI is great have like a million
subscribers and I was like you're right
I'm getting screwed here so let's help
rectify the situation by the way if you
went on my substack Doom debates.com
before this episode and you pledged a
premium subscription of $10 a month then
you got to see behind the curtain of how
I Was preparing to talk to Keith and
what topics I was putting into the
outline you got that kind of Insider
access and you even got to tell me your
thoughts about how I should approach
talking to him so if you want that kind
of access I don't just give that to
anybody you have to go to my substack do
debates.com and pledge that you're going
to pay me 10 bucks just because I think
it's good to maintain a separation of
like the Brain Trust compared to the
mainstream audience I think it works
well and of course 99% of the value will
always go out publicly for free to the
mainstream audience but it's a really
cool unique fun productive experience to
also have this Brain Trust of people who
are tossing their Hamiltons at me it's
fun to do both thanks for watching keep
your comments coming and I'll see you on
the next episode of Doom debates