hey there Doom debates listener before
we get into this exciting episode I want
to tell you about a couple things first
I want to tell you about the paii
organization pa.info is the website this
is the Grassroots organization of
volunteers like me and hopefully you
doing protests and other projects with
the goal of sparking international
government cooperation to pause AI
capabilities advances until we know how
to make them safe people have asked me
if Doom debates has a Discord we don't
but the POS Discord is what you're
looking for go to pa.info and there's a
Discord Link at the bottom of the page
it's an amazing Community everybody's
doing it for the right reasons we're not
getting paid we're not in the pocket of
big leite we just think that AI is an
extremely high existential risk and the
people building it are being Reckless
okay one more thing to tell you about if
you like Dune debates there's another
podcast you should check out it's the
only other podcast that shares my goal
of raising the alarm about AI
existential risk the host is a Peabody
award-winning journalist John Sherman
John is as alarmed as I am about AI Doom
he's wondering why is mainstream media
not covering this topic why are there
not hundreds of podcasts covering this
why are there only two why is there only
Doom debates and John's podcast well
check out John's podcast it's called the
for Humanity podcast C here's a clip of
John interviewing the founder of the Pai
organization yup mindera so what did
happen is I wrote to like the three
smartest people that I knew personally
three of my friends and I basically said
to them hey AI is going really really
fast and I'm concerned about this what
are your thoughts right I'm trying to
like am I making am I thinking wrong
here and all of them said the same thing
yes this is very dangerous uh we should
be concerned but also pretty much every
one of them had this reaction to it that
they wanted to you know ignore the issue
and just enjoy life like okay this maybe
kill us all but it's not really useful
to try to fight against
it and and that also made me realize
okay this is something that you know you
kind of you you want to ignore in a way
and that's why you can't ignore it
because it is if you all do it and then
we're really really [&nbsp;__&nbsp;] right we
really can't ignore this thing within a
couple of weeks we had like a Discord
server set up and we protested in in
bristle with six people like the
smallest protest ever that's amazing and
that was the first You' think that was
the first in-person protest uh for AI
safety John is doing a super valuable
Public Service communicating the urgency
of AI risk to a mainstream audience I'm
thankful for the great work he's doing
because we need a Grassroots Movement we
need the people to have the urgency that
will spark the right kind of
decision-making from our governments and
our Regulators so please check out forh
Humanity podcast search for Humanity
podcast in your podcast player or click
the link in my show notes to go to the
for Humanity YouTube channel and hey if
you like listening to me talk check out
for Humanity podcast episode number 17
the guest is me all right let's get to
the main event I'm thrilled to bring you
this AI Doom debate with Robin Hanson
hopefully this is a useful followup to
the debate that he had with elzra owski
all the way back back in 2008 I think
these kind of debates even if both
parties don't agree on everything are
super productive to bring out the
intricacies of each side's worldview and
why they predict what they predict about
this upcoming crazy decade and crazy
Century of Humanity's future now here's
Robin
Hansen hey everybody Welcome to Doom
debates my guest Dr Robin Hansen is a
legend in the rationality community and
one of my biggest intellectual
influences he is a professor of
Economics at George Mason University he
holds degrees in physics philosophy of
science social science and institution
design he began his career as an AI
researcher at Lockheed and NASA while
today Robin is best known for his
research and advocacy around prediction
markets and for his concept of the great
filter he is a polymath who has made a
staggeringly broad range of
contributions to many fields he has
published two popular books the age of M
and the Elephant in the brain he has
developed a recent Theory called grabby
aliens which many consider to be the
most satisfying resolution to the fmy
Paradox he writes a popular and
profoundly insightful blog called
overcoming bias he was involved in the
creation of the rationality community
and finally he is one of the smartest
and most prominent voices in the AI Doom
debate on the side of P Doom less than
1% how is that
Robin um great except for the lessons
but we'll uh
we'll get to that I'm sure yeah we'll
we'll unpack that so introducing theom
debate in 2008 Robin famously debated
alzar owski about AI Doom via a sequence
of dueling blog posts this collection of
posts is known as the great Hansen owski
fum debate since then AI Doom has become
a more popular and Urgent topic of
debate companies are building AI of
unprecedented power governments are
considering their AI regulation so now
it's my honor to continue the debate
with Robin here on Doom debates
hello
luron so we decided we will alternate
you know taking initiative and asking a
question would you like the first
initiative or the
second um here let me kick things off um
I know it's it's not a traditional
debate nobody's trying to score
rhetorical points but I did prepare a
very brief opening statement to throw at
you which is okay in the near future
there will be an uncontrollable super
intelligent AI whose values don't align
with Humanities and it leads to the
extinction of human life in the near
term maybe in our lifetime or our
children's lifetime the whole argument
that I have is actually quite simple
it's a two-part argument part one super
intelligent AI is coming soon part two
we aren't close to knowing how to
control super intelligent AI so if you
just combine those two parts you just
get near-term AI doom and I give it
about a pdom of 50% so I would love to
know high level what's your pdom and why
do you
disagree I think defining Doom here is
is part of the issue so that makes
what's makes me reluctant to give a
probability of Doom uh my first question
to you and and you know basically you
know what I'm about to say because I did
a number of these other discussions with
other people but uh basically how much
how well do we know how to control our
children and or grandchildren uh that is
you're afraid we would couldn't control
AI descendants and my my mental strategy
is to make an analogy to What would
happen if we didn't have ai so if we
don't don't have any AI we'll still have
descendants and they will still be hard
to control so what's the risk that our
non-ai descendants would be as out of
control or as hard to control as you
fear AIS would
be yeah so what you're asking now really
is similar to what you asked all of last
year when you had like a dozen
conversations about AI Doom with a dozen
other really smart people which I
recommend checking those out by the way
if you're listening to Doom debates go
search YouTube for Robin Hansen he's got
a bunch of conversations from last year
so your question now is basically what
I've termed the spectrum of value
Extinction so it sounds like your move
is to basically say sure you're worried
that AI is going to come extinct us all
but look what happens if we have human
descendants isn't that also kind of
doomy by your definition because aren't
you just kind of worried about change in
general right I feel like that's kind of
your move um and and you're pointing out
how like we're normally okay with value
drift when we look from the past to the
present so what if the value drift just
continues what if it continues faster
you're kind of saying it's like a
slippery slope and what does it really
matter where on the slope we slip is
that a fair characterization well I'm
pointing out that you already have this
problem of as you call it value drift
it's always a phenomena so you need to
be making an argument about why it's
larger or worse or faster or something
with respect to AI unless you want to
take the position that no you're just as
Terri ified about that even without AI
all the past of people have put up with
it but you don't want to put up with it
and for God's sake we should stop
putting up with this way in which
descendence differ from their
ancestors so a lot of the conversations
you had last year went down this Rabbit
Hole of what you're saying now because
it's a perfectly good question and you
people started talking about like well I
just want to make sure to hold constant
the idea of brains having fun or having
appreciation or art and people kept
trying to say basically what their
utility function is Right trying to
solve that problem with you l
I'm hoping to sidestep the problem by
just pointing out to you that like yes
it's hard to Define what a good human
future is but can't we just go up the
Spectrum and just Define what a really
bad future is and then focus on not
getting that well then you must have in
mind some scenarios worse than the usual
scenarios we have with descendant so
then how would you characterize those
scenarios and how would you argue for
that being likely that is that the ways
in which AI descendants will differ from
us will be much larger much worse much
faster something MH so on the bad side
of the spectrum what if we just build an
AI and it gets it into its head that it
can end suffering by killing everybody
and it gets the nuclear codes and it
launches nukes and then we all die and
this happens in 10 years isn't that
really really bad isn't that Doom yeah
that sounds pretty bad to me I'm happy
to admit that's bad you think that what
what odds you're going to give to that
exactly the scenario you just described
I think a scenario that's as bad as that
in the next couple decades is on the
order of
50% and so I mean it's this this
descendant is kind of crazy right and
there's just one of them or is this like
a coalition of all the AIS all get
together and decide on this kill all the
humans thing
or um I think it's pretty simple to just
say it's one agent because the agent can
have like a billion copies and they have
some sort of they're United somehow
either because they go Tak over the
universe or our part of the universe a
single unified agent soon takes over our
part of the universe and is crazy in
this way you
described uh and then you have this
really terrible outcome so um as you can
as you probably can tell there's a
number of elements of the scenario
you've outlined that are quite different
from recent and long-term past history
right yeah so we want to have reasons to
believe in such an enormous change
relative to what we have experienced
with recently and in the last thousands
of
years right okay so let's let's get into
that but first just to set the stage so
the scenario I describe where we
literally just get wiped out as if a
nuke got dropped on our heads that's
where most of my P Doom comes from
there's other scenarios where things
just get kind of progressively crazier
and value kind of slowly dies but my P
Doom of just that insanely bad scenario
is not far from 50% and it sounds like
yours really is quite low would you say
it's less than 1% or where do you say
yeah the scenario you just described
well less than
1% okay and in the Hansen owski debates
in 2008 wasn't that essentially the
scenario under
debate
um I'm not
sure uh but I would say owski focused on
what he and we call the the fum scenario
so as a particular argument why there
might be a particular Unity or a
singular agent who would have a very
singular power and how it would happen
very fast and how then in this process
its values would change enormously and
therefore you know without if you can't
constrain those changes of values and
they are very large then in some sense
it would on average go all sorts of
crazy places and maybe
some large fraction of crazy places
would be that
bad gotcha okay great I think we've
established where we disagree that I
think is interesting before we get to
the fum stuff I'm also just interested
to unpack how you see the world and just
learn about your view even if it's not
directly the Crux with my view so I'll
ask you a couple questions about your
view about AI sound
good um it depends what the questions
are let's find out okay so I'm curious
how you think about time because you've
been gracious enough to put your neck
out and go on record making predictions
for instance in 2012 you estimated that
the timeline toward AGI you quoted in
your blog post is at least a century and
maybe a great many centuries at current
rates of progress so it feels to me like
that should be speeding up but I want to
ask you how is that Chang and what's
your latest
estimate so um there's two levels at
which I might make
estimates one relatively robust way to
make estim estimates is in terms of what
happens every doubling time of the world
economy that is when the world economy
grows faster I expect more
change so talking in terms of years is
at risk to the world economy
accelerating or
decelerating so if I want to make more
robust predictions about what I expect
when it's easier to make those in terms
of doublings like within the next one
doubling the next 10 doubling say
because I expect change to go with the
number of doublings and therefore if I
try to make year predictions that's at
risk to uh the doublings might speed up
so if I say something might happen in a
century based on recent doublings of say
every 20 years then we're talking five
doublings in a century but if growth
speeds up very fast then my Century
prediction gets way wrong right MH so uh
but if there's a period of time when I
think the acceleration is unlikely that
is we'll probably continue to double at
roughly the rate we have then I'm more
comfortable giving a Time deadline or if
I think a great slowdown is much less
likely than a great speed up then I
might give an inequality and say well
you know based on assume the current
rate and say you know at least that much
change or something like that okay so
all things considered I know you're
factoring in economic growth rates so in
2012 you said at least a century and
maybe a great many centuries let's say
you're writing the blog post now what do
you say overall
I'm sorry so what what was the
prediction in a century that was your
timeline to when AI would get to human
level
intelligence so another thing to be
clear there's all these different
metrics for when you know AI is how
powerful the most interesting metric to
me is the doing human jobs metric that
is when they are able to take over most
human jobs so in surveys for example
people have often predicted human level
AI long before they predict AI take over
most jobs so uh that might be some sort
of touring test concept of AI and in a
touring test sense maybe we're already
there uh but for me the the the most
relevant metric is when are they able to
do most jobs say more cost effectively
than humans I mean what percentage of
jobs can they do basically what
percentage of the economy does AI or
computer automation take that's to me
the most interesting metric
and then um we could talk about sort of
there's a distribution are we talking
about the median are we talking about an
average Etc so so in your view it's it's
pretty plausible that we can get to 2100
and there's still jobs that humans are
doing better than
AIS right that's not that's not at all
crazy okay that's very different from my
view it seems to me that unless
something breaks in the way AIS are
improving it just seems like everything
humans can do AIS can do better and AIS
can also do a lot more more so you're
basically just not seeing the future
that way right because to me it's crazy
to think what you just said you think
which is like it'll be 2100 and there
are just some skills that AI is having a
hard time with like can you maybe flesh
out that scenario because it's very wild
for me to imagine so I'm 65 years old or
at least I'll turn 65 in another month
and uh I've seen AI for a long time and
I've read about it for longer and for my
entire life and longer there have always
been a set of people who are really
excited about recent AI progress and
predicted a rapid acceleration soon um
and it just hasn't happened so I am wa
I'm aware of this difference between
just tracking impact on the economy and
the projections of people closest to
research or closest to being a futurist
and their hopes or expectations about
rapid near future change so that's a key
contrast I'm I'm very aware of and I'm
trying to not get caught up in the
excitement over recent demos or recent
you know um particular hopes Etc and
look at the actual change in the economy
and when you do that change is much
steadier if you if you read newspaper
articles or exciting books or things
like that those things have waves of
excitement and and bursts of eagerness
and then they often project really rapid
change and that's happened consistently
not just through my lifetime but for a
long time before that are there have
been these waves of excitement and
concern and prediction about rapid AI
rapid Automation and AI progress in the
near future and the actual rate has been
much steadier than that so I anchor on
the actual history of progress in
Automation in AI in terms of doing jobs
and I say that's been pretty steady
and I admit that that might speed up
could well speed up and then we would
want to be tracking that but I would be
mostly looking for changes in that
that's the key metric and if if if that
will accelerate steadily then I'd say
whatever you're worried about we can
wait until we start to see that
acceleration and then pay more attention
and deal with it but if you think oh no
it's going to
suddenly in a in a you a weekend or
something there's going to be this AI
fum where previous rates of automation
had no indication of this sudden huge
event well now we have to pay attention
to other signs and and that's harder
right so that's the first point I might
ask about is you know yeah if if we just
trust the usual time series of
Automation and you know taking over jobs
and companies making money on that and
that sort of thing if we can just trust
that to track whatever problems might
appear and do that relatively gradually
then we don't need to be paying that
much attention now as long as we can
tell we're just not close to a problem
now but if you can convince me or us
that that's going to be
misleading that there's going to be this
really sudden thing that won't be
noticed by that until it's too late then
we have to track other things but then I
have to ask okay what's this Theory by
which something else is going to happen
that I need to track other things and do
I believe it MH yeah so to summarize
what I'm hearing you put a lot of stock
in economic growth rates and you've been
watching them closely and you've been
seeing for decades experts predicting
that AI is going to change them and
they've been changing quite gradually so
you conclude that economic growth rates
just have so much inertia and you're
just very skeptical of any prediction
that they're going to discontinuously
change until you start actually seeing
them change was that fair and I think we
have some rough theoretical
understanding of why they're so steady
and gradual so that that's you know not
just a pure empirical thing we can talk
about that if you like but okay um yeah
we can we can talk about that but I
think you agree in principle that
somebody could have a convincing inside
view argument of why they're about to
change right sure okay so I think that's
basically going to be the the Crux of
the debate right is basically whether my
inside view argument or elaz yow's
inside view argument whether that's a
sufficiently good argument to make you
put aside your economic Trend
extrapolation that may be where you
think you want to go with the
conversation and I'm happy to give you
substantial shared control over the
conversation so if that's where you want
to go try it okay fair so one of the
things that I would think should make
you consider whether you can loosen your
grip on the outside View and and you
know be more receptive to the inside
view is your own predictions that you've
been gracious enough to put on record
right so I mention so it sounds like
your 2012 prediction though that AI is
going to take centuries it sounds like
that one you don't consider it an update
because you still think that the
reasoning and the prediction is sound
right you still think that by 2100 we
might not have ai that's better than
okay so that one is not an update fair
right okay um and and you don't see
things like llms passing the TR test you
just don't think that that's highly
indicative of doing everything like do
you have an example of something that
humans are doing that you think AI are
potentially super far away
from well almost everything that humans
do in the economy AI is pretty far away
from that I don't need some special
example I can just pick a random job AI
is far from doing random jobs okay what
if the random job is truck
driver that's not a random
job okay well you can pick a random
job um so there was a um in an analysis
I did of Automation and jobs from 1999
to 2019 we actually had a metric of how
automated each job was and so I remember
from that the least automated jobs at
the bottom of the list one was carpet
installers so carpet installers at the
moment or in 2019 5 years ago were one
of the least automated jobs so I might
tell you
okay AI is pretty far from installing
carpets okay do you have any sort of
guess as to whether carpet insulation is
something that'll fall maybe in the
first versus the second half of the
century of AI progress I mean I got to
say when I think about automating carpet
installing seems easier than the median
job to automate but I still got to
notice we haven't done it mhm let's say
uh let's say a robot came in and
installed your carpet in 5 years how
shocked would you
be uh I would be surprised but I don't
know if I'd be shocked I
mean I don't have very strong
expectations about any one job it's the
aggregate of all jobs that I would be
very surprised about it's it's since I
have this overall trend I should just
mention in this study that I and a
co-author did um uh Keller skull uh
in these jobs basically um the rate of
automation change over these 20 years
was relatively
steady and the predictors of which jobs
are more automated was unchanged over
the 20year period and they were all
relatively mundane predictors of
automation so even in 2019 almost all
job automation was pretty mundane sorts
of automation had nothing to do with
deep learning or large language models
or anything like that it was just very
simple can you make a simple machine
that does a simple thing okay great and
I think that you would acknowledge that
the view you're saying now that it's
going to take aggregating a lot of
different skills and it might take a
century it might take all of that for AI
to catch up to humans I think you
probably know that that view is not
exactly mainstream among the AI Labs
right among like Jeffrey Hinton yosua
Benjo Ilia satk and if you look at open
ai's website they're talking very
explicitly about thinking that AI might
be able to replace a human CEO of a
large corporation in 10 years right so
you agree that your view is outside of
that group of researchers and Industry
people right I mean but you know these
are clearly look there's a AI Boom at
the moment enormous amounts of money are
going into AI investment and these are
the people at the public surface the you
know of this thing and they are
basically their job is to Hype up all
this investment they're getting so
they're not random people they are sales
people of the latest AI boom so yes
hying up their to and that's that's fair
to say that Sam Alman andk the more they
hype up AI the more money is going to
get in their pocket I agree that that is
a valid correlation would you say that
about Jeffrey Hinton who basically just
is retired who just quit Google and is
still saying the same thing I mean he's
you know a leader of the same industry
the same you know area of study so he's
his views and attitudes and interests
are highly correlated with the rest rest
of the people in that World Fair okay
all right um let me also get to another
prediction that I found on your blog and
again really appreciate that you go on
out on a limb with these predictions
most people just don't even do it so
they never get to be held accountable so
I'm going to try holding you accountable
to this in 2014 you offered to bet at 20
to1 odds so you were super confident um
you bet against the computer and
electronics share of US GDP not being
greater than 5% or I should clarify you
thought that the US GDP would not be
more than 5% computers and electronics
by 2025 so that's coming up now and I
asked you about it on Twitter and you
basically said that you're probably
still correct whereas my view is like
isn't this getting falsified by the
recent Ai and GPU boom aren't these new
pieces of Hardware adding up you know
Nvidia is taking the whole NASDAQ with
it right Google Amazon the you know
that's taking we need more power we need
to generate more power like isn't this
becoming more than 5% of GDP so you're
confident that it's still not and you're
winning your bet is that right I I think
so yeah I mean um I think you know World
product is maybe you know 80 trillion
doll at the moment or something 88
trillion sorry World product is $88
trillion so 5% of $88 trillion is 12th
of 80 which
is uh basically um I
guess um eight or four $4 trillion so we
are definitely not spending $4 trillion
a year on electronics and computers at
the moment so it's well below
5% okay so and and I don't have a stat
on hand to contradict that so maybe you
do deserve to win this one but have you
updated your model at all maybe it got
closer to 5% than you think or maybe
something is happening with the recent
AI boom that you didn't expect or in
your mind are you just like no I
expected these mini AI booms this is
nothing outside my model how do you see
it
so um innovation has a degree of
lumpiness and uh if you're looking for a
regime change you have to distinguish
lumps within a regime from the first
steps of a new regime so a related
prediction issue happened at 911 so for
many decades we'd had terrorist attacks
of various sizes and then on 911 there
was an unusually large terrorist attack
and a key question was is this the sign
that we're about to have a new regime of
terrorist attacks where from now on they
will be much larger or was this just an
unusually large draw from the same usual
distribution and in that case it was an
unusually large draw that is the
distribution of terrorist attacks has
gone back to the same distribution it
ever was before 911 it was just an
unusually large terrorist attack um but
at the time many people were speculating
that we were facing a new regime so
similarly in AI we'd say over the you
know 70 years at least of AI research um
we've seen lumpy innovations that is it
isn't all completely smooth at various
times new Innovations show up that are
more exciting than average more
promising than average and that go
farther than
average and that's a distribution of
lumpiness that we've seen uh it's not
terribly lumpy in the sense that even
with the lumpiness if you track basic
automation Trends it's pretty steady but
it's not completely steady there are
lumps so then the key question is if we
look at the recent lump of large
language models how unusually large a
lump is that compared to the lump so
far uh and you know we're trying to
predict is this the indication of a new
regime a change in Trend or is this just
an unusually large lump FR drawn from
the same distribution yep so my judgment
is it's
an unusually large lump drawn from the
same
distribution one way to think about that
might be a percentile rank like you know
is it the 90th percentile rank of lumps
99th percentile rank you could have said
the same thing about 911 for example
right you know maybe that was the 99th
percentile of terrorist attacks so far
you might say you know that the
bigger the more it exceeds the usual
distribution maybe the more plausibly it
would be that there's a new
reing great okay one more data point
then about that kind of analysis in 2020
you bet Alex tabarok that the GPT line
of language models will generate less
than $1 billion in customer Revenue
before 2025 recently open aai announced
that their revenue run rate is already
$3.4 billion a year an enormous amount
for such a young company uh and then you
conceded on Twitter yesterday you said
sounds like you're probably going to
lose that bet so do you want to
elaborate on potentially how does that
update your
model well uh I was looking at the
20120 versions of language models and
guessing how big a lump that was and
it's been a bigger lump than I guess so
okay um you know that's sort of thing
that happens that if you if you try to
make guesses about a time series where
there's lumps um you have to sort of
guess that the next lump isn't that
unusually large but um every once in a
while one will be this was when you made
the bet with Alex tab it was just one to
one odds
right I think so I I i' have to look at
I think so yeah because you wrote that I
bet $100 so that sounds like one to one
odds right okay so at least by your
analysis and I don't have necessarily A
a strong way to prove you wrong you've
gone two for three and the last one was
only one one to one odds so it sounds
like you're doing well at least by your
own judgment fair to say I I wouldn't
want to pick just these three as the
measure of such things but I'm happy to
confirm your scoring you like okay yeah
fair enough um and uh okay great so
that's so that's the bets that I could
find um I want to I think a good place
to go from here that I think is very
rich U and it's also something from the
the 2008 debates that I think hasn't
gotten much attention since then is just
this discussion on how you operate your
outside views right because you're kind
of the master of these outside views the
master of these high level Trends and I
don't think people dive in that deep so
is that okay if we dive into there so
we're looking at outside Trends more is
that would yeah yeah just I have some
questions about how you operate your
outside
views um I mean I'm not so sure I agree
with the categorization that My Views
are outside and other people's are
inside but I'm happy to discuss the
kinds of things I look at okay great um
so how would you describe your
methodology for reasoning about super
intelligent AIS what do you think of
attempt is it basically you document and
extrapolate robust trends that you find
in the data is that a good
characterization or how do we do
better
um you have a model of how civilization
works and uh you ask these questions in
light of such a model and your model
should of course be fitted to uh the
data we have but it's not just empirical
it's uh having a model so it is you know
I'm a professor of economics and
economists and others have spent a long
time studying Innovation and the causes
of growth in Innovation and so we have
some standard stories about how that
works such that it accounts for
relatively steady rates of economic
growth uh and you know it could have
been otherwise the world didn't have to
be the way it is but given that we see a
lot of overall growth in the world
economy and that the growth rates are
relatively steady if you look at the
aggregate level then we have some models
that make sense of that theoretically
but fitted to the data and then those
are the sorts of models you'd want to
use to ask will there be a sudden growth
in something in the world in the near
future okay I think a good contrast in
the previous discussions you've had is a
timeline that you often refer to
compared to a timeline that the owski
camp offers often refers to so the robin
Hansen what I call the outside view
timeline
is where you refer to salian events like
there was the forager era the dawn of
human brains that was a certain economic
regime that we were in and then there
was the farming era and then there was
the industry era compared to the owski
inside view where there was natural
selection the first Optimizer and then
there was human brains the next
Optimizer and soon there will be AGI the
optimizer built by an intelligence right
so those are kind of the two views of of
the entire 4 billion history of of Earth
is that a good contrast well I didn't
claim to invent I'm just inheriting a
tradition of social science from a long
time that sees history in terms of
forager farming and industry based on
you know our best estimates of growth
rates and looking at how these things
have gone so I'm just you know more
reflecting and accepting the usual
Framing and uh conceptual tools of the
disciplines that I'm near um and then we
have this you know alternative framing
offered and you know a key question
about any framework
is you know how long has it been around
how many people have banged on it how
many things has it been used for what
what sort of successes have been you
know generated from it how many other
different ways could you try to reframe
it uh those are all the questions we ask
about Frameworks MH yeah so when you
hear about the optimization framework
I've characterized you as thinking like
look alzer's point to these three data
points yeah these are cool events or the
third one's not even a data point AGI
but yeah natural selection was a cool
event it you know modified the data that
we saw afterwards and human brains
obviously modified Trends a lot but
alzar is just ignoring other Trends he's
ignoring farming he's ignoring the dawn
of multicellular life because he has an
inside view he likes to focus on
optimization and so he's just choosing
to connect the dots of these two dots
but you Robin you're just seeing all
these other interesting dots and you
don't agree that these two dots should
be priv pred am I characterizing you
well well if let's take the optimization
framing I might say culture deserves to
be on the list of historical
optimization um
machines uh after
brains um and if you and I might object
to uh trying to uh do a history in terms
of optimization without noticing that
culture uh should be on a list I I would
find that suspicious if you and why
should culture be on the list in your
view because that's Humanity superpower
that's the thing that distinguishes I
mean we've had brains for half a million
years right uh what we what
distinguished humans wasn't so much
having bigger brains was having the sort
of brains that could enable culture to
take off and culture is the thing that's
allowed us to become optimized much
faster than other animals not merely
having bigger brains so I think if you
talk about human brains as a an event in
optimization that's just very puzzling
because human brains weren't that much
bigger than other brains and brains
happened a long time ago like so what's
the recent event in the optimization
story it wasn't brains it was culture
yeah let's classify these types of
events into three levels this was
originally introduced in your F debate
with elezar uh so the three levels are
number one the dominant optimization
process like natural selection human
brain brains AGI level two meta level
improvements to that process so like
cells sex writing science and as you
just say now culture I believe goes on
level two and then level three is object
level Innovations like light bulb
automobile farming is that a useful your
distinction between one and two levels
at the moment that is I don't see why
culture should be on level two with
writing as opposed to a level one with
um DNA
Evolution right so it sounds sounds like
elzar thinks that those three levels are
a more meaningful hierarchy and you just
don't really see the world in his way so
you don't think level one is that
different from level two I think that's
fair and I've even noted down some
instances where your worldview
undermines elzar's level hierarchy I
noted down the example of farming
because you wrote about it farming I
guess is eleazar's level three it's just
an object level Innovation it lets you
get more energy it doesn't really
necessarily improve the underlying
optimization process and you point out
that farming was more more important
than writing which is Alazar level to is
that a good observation farming is
definitely much more important than
writing in terms of growth rates at the
economy exactly right and and you also
pointed out that scientific thinking
which is elzra level two wasn't the key
to the industry transition you said you
think the key was probably sufficient
economic growth to enable a density of
Specialists is that right a density of a
network of Specialists communication
exactly okay so I I think I've brought
back kind of the difference between
Alazar focusing on optim iation dividing
things into these three levels and you
saying like well I just see other Trends
I see other important things right I
just don't share the prominence of that
particular viw so let me try to but I
I'm also going to make the claim that I
am not moving as far away from the way
Academia and the rest of History Etc
have been framing events and
understanding them uh and uh modeling
them um I claim that Eleazar is moving
farther away and making his own bespoke
unique artisanal Concepts and uh
conclusions which just haven't been
vetted remotely as thoroughly as the
concepts widely shared and
used okay fair enough um so let me poke
around the robin worldview okay so you
pointed out how farming was so important
even though it was an object level
Innovation so I couldn't help but notice
that leaf cutter ants have farming too
right they go and cut up a leaf and they
feed it to a fungus and they Farm the
fungus and they eat the fungus and
that's their version of farming so if
I'm operating the robin Hansen worldview
should I think that that is going to be
a big innovation that's going to get
them on track to industry or some sort
of exponential growth like what do we
make of farming you shouldn't get too
misled by these names so we can look in
history and see moments when growth
rates
changed and then we see things that
correlate with those moments like other
things that were changing substantially
at the time and then we have a difficult
inference task to ask which of the many
things we see changing was near the core
of allowing all these things to change
at once so if a whole bunch of things
change at once it's quite unplausible
that independent things were suddenly
changing together presumably there's
some core cause or perhaps cluster of
causes that had to happen together in
order for the change to happen and we
struggle to figure that out about
historical events so for example we
could say what we see many things are
different about humans what was the core
thing different about humans we see many
things were different about farming what
was the core thing same for industry in
each of these cases is actually quite a
difficult task so it when I say you know
something important happens farming it
doesn't mean I think it's you know
tilling the soil and planting stuff
there and waiting for it to grow that
doesn't mean that was the core thing
that caused the higher growth rates then
it's a difficult but interesting project
to try to figure out what was the core
change in each of these events okay okay
what was the change when we shifted to
farming um farmers were more settled so
is they stopped wandering they stayed at
one place and then they were more dense
that is there were more of them and then
they had regular Paths of travel between
them and that allowed diffusion of
innovation to happen much
faster so defusion of innovation was the
key thing that farm allowed farming to
innovate F so basically in all the areas
so far the key thing was always their
Innovation rate that's what allowed
growth and Innovation is the combination
of invention and diffusion and typically
it's diffusion that matters more not
invention and so for you know the first
humans growing faster I'd say it was
diffusion via culture and then from
farming it's diffusion being
longdistance trading and travel War
networks that allowed diffusion to
happen much faster which they couldn't
do in the foraging era so foragers were
pretty isolated
they didn't stay in one place they
didn't have long-distance trading or
longdistance military campaigns and that
limited the rate at which innovation
could diffuse and farming allowed much
more rapid diffusion and also diffusion
in the form of domesticated plants and
animals that you could just carry
somewhere else and plant without or you
know grow without having to explain it
very much so yeah that was anyway yeah
so by your own words Innovation was the
core thing that became different about
humans that's interesting because my
worldview is also that Innovation is
very key and it's the core thing but we
consider that to be like my inside view
where you know Innovation seems a lot
like what I call optimization power
now I am just trying to take the
literature as I've read it and stay
close to it but do a minimal amount of
synthesis to make sense of the puzzles
we have so I'm not going to say that my
theory of farming here here is widely
accepted but I will say it's close to
widely accepted stories that is it's
using the same Concepts same structures
same questions and making a pretty small
modification I'm just yeah I'm thinking
back to how this conversation is going
because it's like elazar saying look
there's these three levels level one is
all about the optimization process and
then you're saying no no no your level
hierarchy doesn't capture how important
farming was why was farming important
because it helped Innovation well wait a
minute isn't Innovation kind of like
that core level that all the other are
Focus optimization power isn't quite the
same as
Innovation for optimization we have to
talk about the accumulation of
optimizations yeah yeah yeah and I think
I think that's another big topic that I
want to get to soon which is just like
and it's it's why you think intelligence
could take 100 years into AI before we
get to human level it's because you you
really do see it as like you have to
build up these Innovations there's no
one Ultra Innovation right that's that's
a big topic for you right right and
right so the standard story is that you
know animals accumulate Innovation over
billions of years and then humans found
a way to accumulate Innovations faster
through culture and then farming allowed
Innovations to AC diffuse and accumulate
through long-distance trading and
Military networks and that could grow
much faster and then
industry plausibly allowed accumulation
and spread of innovation through
specialist expert networks um like yeah
I want to dive into that in a second um
but I I noted something down a thought
that I had I want to close out this
topic of the the trends that you saw you
know Farming foragers versus
optimization power increases just to
close out this topic I had a thought
that the the breakthroughs that you see
on level two like Sals and firing level
two and level three when you look at
these breakthroughs and you say oh wow
these correspond to shifts in in growth
that's only something you can expect to
do and what I call the Goldilocks zone
of optimization power it's a meaningful
Milestone when you look at it for
natural selection or human you could be
like oh wow natural selection hit on
multicellularity or wow human brains hit
on farming but it's not going to be a
major exponential driver when you're
looking at a system that's too dumb or
too smart when you're looking at leaf
cutter ants and they hit on farming me
they just get a little better there's no
exponential growth or if you notice AGI
doing something interesting you you
can't really predict anything because
it's in the analogy of chess if you see
an AI play chess and it's capturing a ro
or losing a rook you can't necessarily
conclude oh wow it's doing so much
better oh wow it's doing so much worse
all you know is that they've done a
worthwhile Gambit by their own view you
can't really get into the details of how
they're progressing so I just wanted to
point out that the way you're looking at
these Trends in the data only makes
sense when the optimization process at
hand is in the goldilock zone of being
not too dumb or not too smart what do
you think I I'd say the reason why leaf
cutter ants didn't give rise to a grow
rated because they are they invented
leaf cutter leaf cutting using DNA
Evolution they didn't invent cultures so
they didn't have a way to accumulate
farming culture so they're still very
slowly innovating farming the key thing
is the accumulation and spread of
Innovations not the farming itself the
farming is just a thing you can learn
through Innovation uh but the key thing
is what is the process that generates
Innovations and diffuses them and allows
them to accumulate and it it still
sounds to me like when you talk like
this you're kind of saying like ah yes
the key is that you have a powerful
level one and then you layer a level two
on top of it talking in terms of
levels I'm just this you know there's a
standard concept of a system that can
accumulate Innovations and what's the
way it does that so DNA is one system
it's very slow but you very powerful
over billions of years and culture was a
faster way to do that and then farming
allowed that same culture to spre much
more rapidly through longdistance
communication and then industry allowed
it to spread even faster so there are
definitely things can happen to speed up
the growth rate even when you have the
same basic cultural
mechanism yeah I'm kind of seeing what
you're saying this is a slight update
for me it's just like look this content
is going to come you just have to let
the agents interact with the world and
build up more content and any process
that speeds up the propagation of
content or the acquisition of content
that's going to drive like the growth
curve
is that fair well I mean if we say
what's the next era of innovation then
it's hard to try to imagine that it's
it's been hard enough to even understand
our history in terms of eras of
innovation and how they worked and
there's still a lot of dispute about
that so I think we're going to really
struggle to try to even figure out the
outlines of the next eras of innovation
and then there could be several such
eras where you know some obstacle gets
removed and then it can grow faster and
we're going to find it hard to guess
exactly what the growth rates will be or
what will the obstacles be and therefore
to know at what point will growth start
speeding up in the future yeah let's
talk about that so in your worldview
right using what you think are the best
practices the standard academic
literature not trying to go out on a
limb what do you predict is going to
happen next you don't predict fum but
you do predict a major New Economic
growth regime right talk about that okay
so um the main thing we know so far is
that small parts of the world find it
hard to grow much faster
than the world that is the main way
small parts of the world can grow is to
find a way to trade and interact with
the rest of the world so that the entire
world can grow together obviously you
have seen some places growing faster
than the world for a time uh but mostly
what we see is a world that grows
together and so um you know the longest
sustained exponential growths we've seen
have been of the entire world over
longer periods smaller parts of the
world have had brief for periods of
acceleration but then they decline
there's been the rise and fall of
civilizations for example and species
Etc so
um the safest thing to predict is that
in the future the world might find a way
for the entire world to accelerate
faster and we have theories of growth
that let us identify plausible
candidates for that sort of a cause so
that makes it in my mind the most solid
argument we
have uh basically um today in the world
economy uh when we grow the world
economy we grow Capital we grow labor
people and we grow technology um you
know abilities to do
things and um most growth is because of
technology in the last century or two
not because of population or
Capital um and but we know that we could
grow Capital much faster than we do but
we see a diminishing returns so in fact
say most
factories uh the amount of time it takes
them to produce value that's equal to
the value of the factory itself is a few
months typically Factory say you know
you build a billion dollar Factory and
in three months a billion dollars worth
of product comes out so right uh
obviously if you could make everything
in a factory
that in if you could make factories out
of everything that came out of factories
the economy could double every few
months so there's that potential for
much faster economic growth if only
factories could make everything you need
to make a
factory and right we don't because one
of the main things you need for
factories are people and they don't grow
that fast so go ahead well you've
mentioned that the doubling time is
going to get shorter and shorter in
order to extrapolate the trend where
with farming it got shorter than with
foraging I don't know if you remember
the exact numbers but in particular do
you remember how it's going to change
when we go from industry to whatever you
think the next phase
is if you just look at numerically the
pattern of
numbers you say that um like um animal
brains doubled every 33 million years
for you know half a billion years at
least uh before that there were other
sorts of um genetic accumulation that
was slower but um then humans showed up
and started doubling roughly every
quarter million years and we don't have
enough data to see how rapid a
transition that was but then um roughly
10,000 years ago or so farming started
doubling every thousand
years so we have 30 double 33 million 30
million year doubling time quarter
million year doubling time thousand year
doubling time and then in the last
century or so we've had roughly a 15 to
20 year doubling time so those are the
doubling times and we can see that each
transition period was substantially less
than the previous doubling time and we
can even see that each period tended to
have a cycle that was roughly a third of
a doubling time
um that is um the during the forager
area there was the Ice Age cycle of
roughly 100,000 years and in the um
farming era there was rise and fall of
Empires of roughly 300E dur period And
in the industrial era we've had business
cycle roughly six year period we don't
know why but this is but if we just we
just project this
forward so you ask if what happens if we
project this forward and you'd say the
new doubling time would be a few months
on the order of a few
months and the transition period would
then happen in less than the previous
doubling time of 15 to 20 years and then
hey there maybe even be a cycle of
roughly a third of the doubling time as
some sort of cycle time those would be
just straight projection but I we don't
have really good the for why there
should be this numerical correspondence
so that's why you should you know not
take that so very strong I don't
remember talk about the cycle part of
this like what how do you define a cycle
because it sounds like you're picking
out like different things in different
eras and you're saying these are all
Cycles like what is a cycle well if if
we talk about the overall growth of
whatever is relevant then like the
business cycle is in fact the major
modulator of the overall growth of the
world economy for the last century or
two there's this rise and fall of the
business oh got it got it got it So when
you say a cycle it's a cycle that you
can see in the economic growth data it's
like it grows and then does a little dip
and then it dip for they Forge your ERA
with the farming era there's the rise
and fall of civilization so we
definitely see that happening on a say
300E cycle got it got it got it okay and
also I didn't expect you to extrapolate
all the way back before farming to talk
about humans doubling and animal brains
doubling can you clarify what metric is
doubling because I get GDP during the
economy but what about before what
metric is it
um so you know the human biosphere for
the previous half billion years doesn't
look like the whole biosphere changed
that much like the overall metabolism in
the biosphere seems roughly constant so
that's not what kind have changed if you
ask what was the thing that was changing
that we look back and say that was a
foreshadowing of what was to come and
that was important it's brain size so
then that's what I picked out for
looking at before humans would be the
brain size size so when you talked about
animal brains doubling every 33 million
years and human brains doubling every 25
million years the number of humans
doubled every quar years not human brain
size oh got it got it got it okay so
number of humans is basically a proxy
for GDP so that's very similar whereas
um animal brains doubling it's not
really a proxy for GDP but it's just a
metric that you go before the animal
brains you can just look at the size of
genomes and there's even slower growth
rate in the size of genomes before that
right okay okay so like the amount of of
information contained in in the top
organisms genome used to double every
like 100 million years or something I
don't remember the number for genomes
but I I do remember the brain size one
got it okay so that's interesting to me
that it it sounds like you've got a
pretty significant change of metric
right where with the animal brains
you're just saying yeah the neuron count
is doubling but that's a precursor to
when the amount of like utility is going
to
double it's just the trend that matters
for the next transitions we think so so
we we do think H one of the things that
enabled humans was big enough braams
which enabled strong enough
culture yeah yeah yeah that's
interesting I mean you know in in my
view the number of humans won't always
matter to the next doubling time right
in in my view you kind of need just like
a critical mass in the last century more
size the economy not the number of
humans anyway right right right right
it's just interesting so it sounds like
you're just applying some judgment here
right like there's no one metric that's
an Absol but you're just kind of trying
to connect the dots in whatever way
seems like you get the most robust Trend
well for the last three periods we can
talk about world
GDP uh and then if you want to go
further you can't do that you got to
pick something else and so I looked for
the best thing I could find or you could
just say hey we just can't project
before that and I'm fine if you want to
do that too okay so I'm pretty
fascinated by this right because I I
feel like this is very distinctive Robin
and I know that you say like this is
just what Academia says this is stand
but I mean who else says this kind of
stuff right this is like only
you uh I mean not that many people look
at the overall long-term history of
humanity but when they have they have
done it in similar terms yes uh the size
of the economy uh as ma is the thing
that matches our era and the previous
eras and that wasn't for me that's from
other standard historians um okay let's
talk about the next era so you mentioned
a few months stumbling time um or
actually real quick I just want to throw
in a a confidence calibration question
so imagine that it's the year 2100 and
you get to look back over things you
know your virtual body or whatever you
get to look back over what confidence do
you think that you'll look back and be
like ah yes my extrapolation that we
moved into a regime where the economic
doubling time became a few months I
nailed that you know that was pretty
accurate what confidence are you on that
basically you know we have these steps
which are really large factors like you
know the step from the last previous
step was a factor of 60 previous one was
a step factor of
250 one before that was a factor of you
know
120 um
so these steps are large but not uniform
in their size so that means I have a lot
of uncertainty about the exact size but
roughly say it's going to be
big right so if the doubling time was
like two years instead of two months or
two weeks would you be like oh darn you
know I missed I tried to make a
prediction mean two years is a little
slower than I'd expect but like anytime
between six months and a week uh I'd say
is in the ballpark of the prediction of
just a m a a sudden big change in the
growth rate again that's so there's two
arguments here right one is just this
time series from the
past and another is this Theory argument
about a limitation on growth rates we
see in our current economy that's what
we were saying before about factories so
if we just say look Factories at the
moment could make our economy double
every few months if you could just make
everything in a factory that a factory
needed so AI in some sense promises that
AIS can be made in factories we think
and therefore once most everything that
you need for a factory can be made in a
factory I through AI then yes a
straightforward prediction is the new
economy doubles every few months okay
yeah I just want to close out the
question of of your confidence so let's
say the year is 2100 you're looking back
or even 2200 so you're looking back and
you're observing what happened with the
next economic shift and it turns out
that it fell outside of your range it
wasn't one week to six months right it
was less than a week or it was more than
six months so how surprised are you in
other words mod what is your confidence
of your prediction only modestly I mean
again okay you know the point is history
has definitely been a sequence of growth
modes where the next Road was much
faster than the next so that's the most
obvious robust thing to predict is New
Growth roads could just me much faster
it's the most obvious thing to predict
but it soundss sounds like you're
predicting it with very low
confidence you're asking about the the
confidence of the range the the the the
rate of the new growth mode not whether
there will be a new growth mode yeah
okay if you ask me how sure am I that
there will be a much faster growth mode
that's a different
question okay yeah well I mean I guess
I'm just trying to help extrapolate the
future right because ultimately that's
why this is interesting right and and
you think that it Bears on the fum
argument and you're saying look I have
some solid methodology here for telling
you that you can expect an economic
doubling time of a couple months and the
F clashes with that so you should take
that to heart and I'm just making sure
that well I don't know that F does clash
with that we haven't gotten to that yet
but um I mean that that of course we
could have a doubling time of a few
months and then you know a year or two
later it goes even faster right so most
of these periods the number of doubles
has been say between eight and 16 or
something so then if you have another
doubling time of a month say then you
expect yet another mode
in 8 to 16 months later say right so you
could definitely get to even faster
growth rates if you just go to yet
another mode afterwards I want to ask
you about that so let's say the Universe
plays out like a Monte caros simulation
of Robin Hansen's Mainline scenario so
in that case I think we have an age of M
right because you've written about that
and that's your Mainline scenario I
don't know if it's my Mainline scenario
but it was likely enough that it was
worth writing a book about it that would
be the standard to hold it to okay so
yeah can you explain that a little bit
well uh if it's worth having a 100 books
on the future it's worth having a book
on a scenario that has a 1% chance of of
the future so I don't think I need to
have a higher standard than a 1% chance
I mean I might think it's higher that's
enough to justify bothering with the
book well let's talk about your Mainline
scenario because I thought your Mainline
scenario was we shift into like a one
month doubling time but then pretty
quickly we shift to the next doubling
time which is much faster
right so at that level of abstraction
it'd be roughly my Mainline scenario
that is
uh we we shipped into you know we've
maybe we've seen five modes like that so
far so that's not that many I think that
roughly jenies guessing one more if
you're going to say two more five more
10 more well that gets much more
speculative right I don't I don't have
that much confidence it's going to be
five more double you know mo mode
accelerating modes but one is quite
plausible and two is also not
crazy okay one thing elzar pointed out
in the 2008 debates is that it seems
like you're using time as your x-axis
when you extrapolate the trend right and
you're not using some other x-axis like
rate of optimization being applied so
why
time well we have seen in history
relatively steady exponential growth it
didn't have to be that way they could
have been other curves but they turned
out to be relatively steady exponential
growth that's roughly what we see
through many kinds of processes so
that's the kind of process I'm going to
guess that and with a exponential
process then there's a way to translate
doublings into time um right right if if
you look at the the the forager farming
industry transition though yeah on the
xaxis of time it's an exponential but
that's because you're holding the human
brain's optimization power constant
right so if your model is at the x-axis
is actually optimization power it might
turn out that you got a hyper
exponential fume once AGI starts
modifying the underlying Optimizer right
so that your your model then diverges
potentially well in all the previous
growth modes lots of things
changed so uh and of course every
transition was some limiting factor
being
released presum I mean you might yeah
you might argue like look you Uncorked
the energy by farming right so you
released a new exponential right right
but the point is still we have this
history of a relatively small number of
Transitions and between each transition
roughly exponential growth and then
really big jumps in the growth rate at
each transition that's what history LLY
looks like
it's good question to ask why it would
look like that but I see enough of a
trend there that yeah I'm going to use
that to extrapolate so I got to say that
yeah let's do it now um again once you
think about culture uh you realize
humans have invented a lot of cultural
adaptations over time that many of which
are cognitive so there's a great book
called cognitive gadgets about how many
unique human cognitive features are
plausibly generated by culture not by
DNA Evolution and they greatly increased
our cognitive capacity so yes the raw
brain size was held constant but our
cognitive capacity increased in big
jumps as culture invented new ways to
use the same Hardware to do more stuff
yeah is there a good example from there
well there's
language there's math there's reason MH
uh these are all things that other
animals don't have that humans have that
plausibly came through
culture fair enough okay so you flushed
out youro in this conversation like how
you reason about these Trends and how
you use that to predict the economy and
you don't think AI is going to
completely blow open these Trends as
best as you can think well let me ask
you this let's project Robin Hansen's
way of thinking about the world to the
dawn of humanity so like 100 million
years ago or whenever what abstraction
what way of reasoning would have let us
correctly predict hum's present level of
power and control what does Robin Hansen
of a million years ago explain about the
future the
key human superpower was culture so if
you could have looked at the protohumans
and saw their early versions of culture
and how they were able to spread
Innovations among themselves faster than
other animals could through their
simpler early versions of culture you
would then predict that there will be
the meta process of culture inventing
new cultural processes that allow
culture to work
better and that did in fact happen
slowly over a very long
time and you might then have predicted
an acceleration of growth rates in the
long run uh that is the ability of
culture to improve the ability of
culture to work would allow humans not
to just accumulate Innovation but to
accumulate Innovation faster which we
did okay but the harder part might have
been to anticipate the stepwise nature
of that that is looking back we can see
and that happened in some discreet
steps okay but looking ahead may you
might not have been able to anticipate
those discreet steps you might have just
been able to see the large shape of
acceleration of some
sort right I mean let's say Devil's
Advocate Dan comes and argues with the
and says look robin look at the academic
literature we know that natural
selection accumulates genes and then
organisms adapt to their Niche how are
you proposing that this magical thing
called culture is going to get you a
single species that starts occupying
every imaginable Niche how are you
supporting that with evidence I don't
think that's going to happen right and
then what would you say I I think it's
hard to flesh out this counterfactual I
don't know what I know at the time and
what I'm able to argue with ter though
clearly you know all the best research
about the world up until ago well that's
not going to have been very much
right uh so we are better able to
understand ourselves now than our
ancestors were in some no for sure but
like I mean but you know everything that
we today know about the world up until a
couple million years ago right but I'm
look I'm happy to admit and we should
probably get on to talking about AI
because we're going to run out of time
uh that we um future AI that there's
just a lot we find it hard to predict
about the future of AI That's one of the
defining characteristics of AI it's one
of the hardest things to Envision and
predict how where it will go out I mean
this is pretty important to me because
it's sounds like you agree that a
version of your methodology transported
to the dawn of humanity would be blind
to what's about to happen with Humanity
right so I'm trying to make the analogy
where it seems like maybe you today are
blind to what's about to happen with
aium I'm happy to admit that we have a
lot of uncertainty but uh you'll have to
make the argument why uncertainty
translates into a 50% chance of us all
dying in the Next Generation yeah it's
it's almost like you're admitting you're
like look I have a methodology that
isn't going to be great at noticing when
there's going to be this huge disruption
but I'm mostly going to be right because
there mostly aren't huge disruptions am
I characterizing you
right I mean certainly if we look over
um history say the last you know million
years we see relatively steady change
you know punctuated by accelerations of
change but even these
accelerations at the moment you can see
things accelerating and so that does
suggest that in the future we will see
accelerations but then we will see the
beginnings of those accelerations and
then we will see them speeding up and we
will be able to track the acceleration
as it happens that's what history
suggests about future accelerations so
in this thought experiment the robin of
the past is going to observe a 10,000
year slice of humans starting to have
culture and be like aha this is a new
Dynamic this is a new regime so you
think you would notice it then well it
depends on you know what stats were
tracking but we you know at the moment
this past Trend projected the future
says that
um the world together would start to
accelerate in its growth rate we are
definitely tracking World growth rates
in quite a bit of detail and it wouldn't
happen on one weekend that is it would
be say a five-year
period you wouldn't notice anything if
you were just tracking World growth
rates because that's kind of a smooth
exponential right so that wouldn't kind
of alarm you past the past transitions
yes the the growth rate has
accelerated uh in these transitions
right right okay so you'd be like okay
here's an accelerating growth rate but
there's something else in this
experiment this thought experiment that
I think is alarming which is that humans
are taking over the niches of other
animals so we are the first multi- Niche
species or the first Niche General
species right and that's that seems like
something there have been other species
that were more General than others but
certainly we were unusually General
compared to most species but there have
been many other General we unusual and
we're so General that if we really
wanted to technically we haven't done it
yet but we could potentially colonize
the moon right or at least live there
quite a lot we probably will in the next
10,000 years right so to me that's what
sing is like I feel like you're not
bringing a methodology that's going to
notice when a new type of species is
coming that doesn't fit the
trends again the question is what are
you tracking and will those capture
whatever it is you expect to appear so
um in the past New Growth has happened
gradually so that if you were in the
right place try the right things you
would see the new growth mhm so the key
question is how sudden or local could
the next growth spurt be compared to
past growth spurts that's that's the
question we're asking so you you might
go back and say well look if you're
tracking all the species on earth except
you know the big primates you might have
missed the humans
growing so uh you know what's the chance
you'll not track at a f enough
resolution to see the key change so
humans
even humans we started you know a
million years ago so to double faster
but that was still only doubling every
quarter million years and you know you
you would still have to be looking at
the granularity of humans and the nearby
species and at their doubling time if
you were just looking at larger units
maybe all mammals or larger continents
you might miss that Trend so that you
would need you can raise the question
what trends will you need to be looking
at but that's why I might say say look
my key priority is when will AIS take
most jobs so if I decide that that's the
thing I care about and that's the thing
I track there's much less of a risk that
I'll miss
it so you might counter argue that
that's not the thing to be tracking and
I'm happy to hear that argument but if I
think this is the thing I care about and
I'm tracking it I feel like I got my eye
on the thing great okay and in a second
I want to talk in more detail about aium
I just have one more question to ask you
about your outside view methodology as I
call it okay what about extrapolating
the trend within human the human era the
trend of destructive buttons you can
press or destructive weapons right so
we've always had more and more
destructive weapons until we got nukes
nukes are a button you can push that
sets off a sequence of events to
slaughter 100 million people just like
that so can't we make a timeline
predicting that there's a button coming
that'll Slaughter 10 billion humans
that'll end the world can is that a good
outside view or
no it's a very unusual time Series so I
I would think a more natural ass series
is say uh Mass murderers and how many
people they
murder uh that's the relevant button in
terms of people actually going out and
causing problems so you might ask if we
look at the distribution of the number
of people PE murderers somebody kills
and you know most murders only kill one
person say and a few kill five and a
very small number kill 50 say we've got
a distribution maybe power law probably
of the number of people would kill now
we might ask that tail is that getting
thicker over time and my understanding
is it's not that is in the last century
that tail hasn't got thicker actual mass
murdering has not actually killed more
people and of course if we go back many
centuries then it's gotten less and that
you know centuries ago there were some
pretty large mass
murders right and why why are you
prioritizing mass murders over a
particular weapon's destructive power
because if a weapon is sitting there
where it's one button push away from
Doom is that not a significant Trend to
be mindful of well we want to ask how
often are such buttons pressed so if you
just want to talk about buttons that are
there that could be pressed and nobody
ever presses then that's a different
issue than buttons that actually get
pressed okay but would that be a
concerning Trend to to pay attention to
at all well I'd want to look near the
button and say well are people
protecting the button more when they
have bigger worse buttons and that does
seem to be roughly true but okay it's
hard to judge I also want to relate this
to bostrom's thought experiment right
the the pulling out balls the vulnerable
World hypothesis right so he says that
hey the next technology that we invent
might not be as well protected as nukes
it might be easier to build maybe
anybody with a laptop can operate it
what do you think about that
hypothesis I mean I think he was basing
his hypothesis on an empirical Trend
that I don't see exactly in terms of the
actual thing but we could certainly say
I mean it's
AAL just a possibility and and it sounds
sounds like you think that it's an
unlikely hypothesis given the data
whereas to me it seems like a totally
plaus like it seems to me almost almost
certain that we'll get there eventually
that we'll have this crazy you know bad
kind of technology that we invent that
it's one button push away from killing
everybody and it's just a matter of how
how much we have to keep drawing
Technologies out of the bag until we get
to this Unlucky One but it sounds like
you don't see it that way I mean once
we're spread across the galaxies of
course you know the sort of technology
that could kill everybody would have to
be something that you know that kicks
the universe over into a different
transition that wipes out all the matter
in the universe or something but it's a
very limited number of technologies that
could possibly do that but we're still
on one planet now so um for the for a
while while we're still on one planet I
suppose uh there's a lot of possible
technologies that could kill it real
that that's fair so the Bostrom thought
experiment of vulnerable World in your
mind the world actually gets quite a lot
less vulnerable once we start taking up
a lot more space in the universe right I
mean that's just my idea many people
also think so but we are still here
together fair and what about the current
moment with nukes are you at all
concerned about a pretty high PE Doom
from nukes or you think that's like a
solv problem I mean it's just nukes
won't actually kill everyone so they
might kill a lot of people and that
would be very bad but uh that's
different than killing
everyone yep okay um do you think it's
reasonable to say hey maybe every year
there's about a 1% chance of 100 million
people Plus getting killed by nukes it's
not crazy I'm not going to disagree with
it okay yeah that's roughly where I'm at
okay so no disagreement there all right
great now we're finally moving into F
directly so let me say some stuff about
that um all right let's define fum as a
positive feedback loop that rapidly
disempowers Humanity sound
good um uh I that sounds more General
than I can get get my head around I'm
more familiar with the particular AI fum
scenarios yeah okay let's talk about AI
yeah yeah yeah for sure um okay and I
want to ask you how confident are you
that just yeah if you could put a
probability how confident are you that a
f scenario like what I described would
happened and I think you're saying that
you're you're 99% confident right like
you think P Doom is less than 1% from
that kind of scenario so it it might
help here to lay out what I see as the
key assumptions required for fum so that
we can make a conjunction of them to
estimate a probability that I'd say the
reason why I think the probability is
low is that I think you need several
things to happen all together for the
entire scenario to play out now you
might disagree with my assumptions about
what are the requirements for the SAR
and maybe that would change how we think
about probabilities that that's I'm
worried that we might disagree because
we are thinking about a different
thing so I might say um the the classic
owski fum scenario was that a particular
AI system with a particular set of
sponsors in a particular
location um was at a low level of
capacity and then it discovers some meta
Innovation that allows it to innovate
much
faster uh maybe several but they all
need to be discovered
together it discovers this meta
Innovation
and in a way or in a system that's
poorly monitored for such an event that
is it discovers this meta Innovation and
then starts using it for them to improve
itself and whoever owns this thing or
monitors them is either not noticing
this or it's even perhaps purposely
hidden from them or they are okay with
this and completely on board with it
improving its capabilities in this
way this meta Innovation is
unusual in the space of Innovations in
that it is unusually Broad and that it
ows its capabilities to improve across
an unusually broad range of
capabilities not just math theorem say
or just you know um protein
folding uh but a wide range of
capabilities this meta Innovation allows
them all to improve and
then it improves them all over a wide
dynamic range like maybe 10 orders of
magnitude or something right not two
orders of magnitude uh and then run out
right so and that's quite unusual for
Innovations M it um improves over a very
wide range of capabilities over many
orders of magnitude it does so out of
view or out of accountability of its
owners and
managers and we're not done
yet there's the claim that um over this
period it becomes or already an agent
that is it sees itself as an agent in
the world who has goals and plans and
acts on them it's not just a question
answer or
something and it's not just completely
subservient to its owners and Masters
who you know ask it questions and then
it responds does tasks according to them
it it sees itself as a self owning self
initiating
self-directed agent with its own concept
of what it wants and what it's trying to
do
and as an agent it has some values I
some way in which it sees what it wants
and that through the process of this
enormous change in capacity it also
changes its values not just a bit okay
enormously so that's the whole scenario
here that we have to put together that I
say is f now you know it's a whole bunch
of elements Each of which is kind of
unlike likely and then the whole thing
adds up to pretty unlikely now you might
disagree and say some of the elements
I've added into the scenario aren't
essential for what you worried about I'm
happy to listen to that but you can see
why I might assign a low probability to
this conjunction of features or scenario
featur yeah yeah for sure and of course
there's a known trick where you can
frame any position as a conjunction to
make it seem unlikely right so I could
do the opposite way and I could say hey
if you think fum is not going to happen
then first you have to agree that AI
Labs have really great security
precautions right and that government
Regulators can pause it at the times
when it's on safe right I mean I could
frame it as a conjunction too right so
that is a little bit of a trick but we
should address the specific things that
you said
fa so I mean this conjunction you find
50%
likely the conjunction that you said yes
I mean like I said I I kind of object to
framing it as a big conjunction like
that right because I have a framing
where it just doesn't sound like such a
big scary conjunction which is so then
your framing would have to make what in
my framing look like independent choices
somehow be natural consequences of each
other like so maybe there's an
underlying event that would cause these
as a correlated event somehow but then
you'll have to tell me what's the
underlying event that causes this
correlated set of events all to happen
together okay great so to set the stage
I just want to throw in the analogy that
we talked about before which is to what
humans did in evolutionary time it looks
like very much like aume in that usually
species stay within a niche right they
don't explode and go take off other
niches because normally when they do
that the other species push back and you
have an ecosystem where all the species
can react in evolutionary time you have
an arms race in evolutionary time you
have I didn't add the last condition
which is just none of the other AIS in
the world notice or oppose this ai's
advancement sure that's the analogy to
the other species but you know that
that's another condition that requires
in this scenario not only do it owners
and its monitors fail to notice or
oppose it but all the other AIS in the
world also do okay so I'm just observing
that if we go to evolutionary time as an
analogy the whole idea that humans are
part of an ecosystem and are kept in
check by the ecosystem that is no longer
correct right we are now just a new
paradigm we are essentially not part of
that ecosystem right so would you be
open to an analogy where AI becomes that
as you know not part of the economy
anymore
but again we'll need to talk more
specifically about how and why so um
other species on the planet a million
years ago had very poor abilities to
Monitor and coordinate their actions uh
so they didn't have councils observing
cyber attacks and uh reporting crimes to
a Central Bureau of enforcement who
looked for things that could go wrong
and monitor Goa go but I noticed that
your argument is now veering from like
hey look this stuff always happens in
the economy and now you're like well
because of what I know about the economy
here's my logical reason why we're going
to stick within the normal trends of the
economy right so it sounds like you're
you're now sprinkling in some inside
view
Insight uh I mean I'm happy to say that
you know Humanity spread across the
Earth was a very unusual very powerful
event that happened in the world history
uh you one of the few uniquely dramatic
events in world history you're about to
tell me that something similarly
dramatic is going to happen in the next
Generation on Earth then of course you
have to overcome the prior that these
are really rare so tell me why we should
expect such a rare event soon okay great
so this is a key question for me do you
agree that there's a lot of Headroom
above human intelligence look at the
progress from ape to human IQ does that
dial turn much farther if you want to
call it like a th000 IQ 10,000 IQ that
kind of thing well I'm happy to say
there's enormous Headroom in
capacity uh over
all uh what I hesitate is when you want
to divide that capacity into IQ versus
other things so um you might ask have
humans changed their IQ in the last
100,000
years uh because we've certainly vastly
increased our capacity in the last
100,000 years but I'm not so sure you
want to call that the thing you you have
in mind but definitely our descendants
will be vastly more capable than us in
an enormous number of ways uh and
extremely happy to grant that I'm just
not sure when you say oh because of
intelligence which part of that vast
increase in capacity that we're trying
to refer to in that
sense okay let's say look at a single
human today okay they've had a good
upbringing they've had some education it
took culture to get them to this point
all of that okay but now you have a
single human today or maybe let's say a
group of humans right a a city of humans
whatever 100, a million and compare that
to a million apes or whatever right I
mean there's
such a discontinuous difference right
let's say they have to fight a war right
it's not even close right but that's not
just an increase in intelligence and
it's increasing in great many things I'm
happy to say given all considerations
and all capacities today's city of
humans is vastly more capable I'm just
less sure how we want to attribute that
to particular factors that have
contributed to that increasing okay okay
but you're definitely imagining that at
some point whether it takes a million
years or whatever at some point you're
going to have some kind of city of AIS
occupying the amount of atoms or
whatever and it's just going to be
vastly more powerful than current cities
absolutely sure okay all right so you
agree there's headro but I guess the
Crux of the disagreement is you just
think that the way we'll get there is
going to be fine like it's not going to
be like a fum right that's the only
disagreement well again I laid out what
I thought were the unusual assumptions
required for a fum and that's the thing
I'm skeptical about I'm not at all
skeptical that our descendants will be
vastly more capable than us let me also
ask you this is there a steep gradient
of int intelligence increase near the
human level the reason I ask is because
look at the evidence from past and
present human AI progress so it seems
like we've gotten AI to progress rapidly
toward the human level even if it's not
there yet and also look at the evidence
from the difficulty of human childbirth
right it really seems like natural
selection with saying yes bigger head
bigger head oh crap now the mother can't
walk we better slow our role
but of
capacities and none of them are
obviously near fundamental limits so uh
but I'm just again not sure which thing
you mean by intelligence exactly but I'm
not sure that matters here I mean hum
don't you think it was very important
the the genetic modifications that
happened to the human brain to separate
it from the ape brain weren't those
extremely high return and doesn't it
seem like there's a few more that can be
done that are also extremely high return
if only that head could fit through that
pelvis I'm not sure that there are big
gains in
architectural restructuring of the human
brain at a similar size but I am sure
that humans were enormously more able
than other animals again primarily
because of culture so we have been Gras
dally drastically increasing the
capacity of humans through culture which
increases the capacity of each human so
far yeah and we are now nowhere near
limits certainly when you talk about
culture though you're holding the
genetics of the human brain constant but
I'm pointing out that it seems like
there was a steep gradient a high return
on investment of changes to the genes
and and changes to the phenotype like if
we just made the pelvis bigger so that a
bigger head could fit through doesn't
that Trend to show that we could have a
significantly smarter human the same way
humans are smarter than
Apes um I'm just not sure what you're
talking about here but I'm not sure it
matters that is
um there so many ways to improve on
humans uh one way might be to have just
a bigger physical brain but that just
doesn't stand out as one of the most
promising or important ways you could
improve a human but I guess but I have a
specific reason to think it's promising
which is that natural selection tried
really hard to keep increasing the human
brain until it got a you know the fact
that so many human babies historically
die in birth I mean sure so imagine a
brain you know eight times as big twice
as big on each axis uh human brain could
would certainly have more capacity I
don't know how much more because it was
just a bigger brain but do you think
that right can we extrapolate the
hypothetical I mean if this is the
difference between human intelligence
and ape intelligence which appears to me
very significant right like culture and
science is not going to teach a present
day ape to be a useful scientific
contributor is that fair to say no
actually that is if okay if Apes had had
culture then they could have done
science that is it was culture that made
us people a do science okay I mean this
is the key point I feel like you you
have tended to bite this kind of bullet
saying like maybe Apes can be
contributors so let let me drill into
that a little bit so imagine that an ape
is born today just an actual modern ape
and you put on a VR headset on the ape
so the ape grows up with like a perfect
simulation of really enlightened ape
culture could that ape then grow up to
be a scientist among human scientists
it's not enough to put a VR headset on
an ape to make them capable of culture
that's not how it
works culture is the capacity for humans
or whoever to attend to the behavior of
others figure out the relevant things to
copy and then actually successfully copy
them that is what culture is and that's
the thing that we have that Apes don't
okay and do you think that in some sense
most of the Delta of of Effectiveness in
going from the ape brain to the human
brain is increased capacity to absorb
culture do you feel like that's like the
key
dimension in this uh book cognitive
gadgets that I recommended uh she goes
through a plausible list of the minimal
features necessary to support
culture and they don't seem that closely
aligned with the actual size of the
human brain um but yet having a brain
big enough was certainly mattered but
it's not clear that a brain one quarter
the size of a human brain couldn't have
done it as well but there were a number
of particular features the brain had to
have in order to be able to support
culture and that was the human
superpower was to once you had that
first set of features that could support
culture then we could use culture to
collect lots and lots of more features
that enabled us to take off compared to
the other animals okay it seems like I'm
pointing out a mystery that I'm not sure
if you agree like the mystery of why
natural selection cared so much to make
the human head and brain as big as
possible like do you agree that there's
something that calls out for explanation
definitely like brains were valuable so
clearly at that point in evolution
clearly
Evolution was going can I make get a
bigger brain here that looks like a good
deal let's figure out how I can get a
bigger brain don't reach the limitations
this can't get a bigger brain but
clearly plausibly bigger brains would be
valuable it sounds like you're saying
that that whole return on investment the
reason why the bigger brain is so
okay you got culture you don't need that
much bigger of a brain for culture we've
got culture nailed down so why do you
think the brain kept trying to get
bigger I mean a standard social brain
hypothesis is that um you know we had a
complicated social world and there were
large returns to more clever analysis of
our social strategic situations and that
doesn't seem terribly wrong but brains
also probably let us use culture and uh
take advantage of it more so there's
just lots of ways brains are good bigger
brains are good I'm happy to admit
bigger brain I feel like you're you're
you're you're you're U burying the lead
here this can't be our main disagreement
about fum is just the idea that brains
could be bigger and more valuable I mean
of course I'm going to agree to that
well I'm seeing a very important
increase in intelligence right from apes
to humans and I'm seeing an indication
that we're not that far away
architecturally from having a similarly
large jump in intelligence right I'm
saying wow it's just a slightly bigger
head it's just a few genetic tweaks to
get so much more IQ points compared to
Apes why can't I just jump ahead and say
of course AI will be more eventually
more capable than us in a great many
ways like like isn't that the point
you're going to make here well the point
I'm going to make is that I I was just
trying to factor out this Dimension
right that I call intelligence which to
me maps to optimization power I think
I'm observing that Dimension more than
just having culture but also a dimension
of like Hey we're smarter right like we
can model things we can plot our goals
except if I accept that then what what
what happens great okay well let me
introduce a term that I coin called goal
completeness that I want to ask you
about so goal completeness just means
that you have an AI that can accept as
input any goal and then go reach that
goal and it's not like the whole
architecture of AI is only for playing
chess or only for driving a car it can
just accept arbitrary end States and in
the physical Universe do you think goal
completeness is probably a good way to
think about future AIS that we're going
have I I haven't so far but I suppose
you might have an argument for why I
should this is similar to the generality
of language that is uh some people
plausibly say that our languages are
General concept be expressed in them and
that's an interesting feature to say
about General you could say our brains
with language are General since we could
express any goal and language and give a
goal to another person in language and
then they if they could comprehend the
goal and they're inclined to help then
they could try to pursue that goal that
they've understood that we've expressed
in language that's a plausible point
about exactly right so language does
have this generality property in fact
it's so General to the degree that if
you have a chatbot just by virtue of
programming a chatbot if it's a really
good chatbot
then by virtue of answering questions
really well the questions could be like
okay great I'm the CEO of a company what
should I do next what should I do next
right how do I what should I invest in
what should I build how does the
building work right so language is go
somewhere with this what if assume we've
got something with gold generality then
what okay great so imagine that now that
we've identified a dimension right like
the effectiveness of a goal complete AI
is it possible that the AI will just get
much better on this Dimension and then
to connect it to what I'm saying earlier
right the same way that just a few
genetic tweaks just a few scaling in
size made I believe that it made humans
much better than Apes at this goal
completeness optimization property I
would say eventually our descendants
will be better at identifying
understanding and completing goals sure
that's part of a generality are you so
confident that we can rule out a fum in
terms of a recursive feedback loop where
being goal complete helps you become
more goal complete really I don't know
why I need to rule it out I just say I
could just say it doesn't look terribly
likely
uh because we can B you know we have a
world of recursive improvements and
they're somewhat rare that is you know
we we can look at firms all over the
world for example trying to improve
their capacities in a great many ways we
look at people trying to improve their
capacities we we see attempts to
self-improve and we see that people use
all the margins they can to self-
improve and that when improvements on
some margins generally allow some minor
Improvement on other margins but tight
Loops of self-improvements are rare so
yeah okay you just have to convince me
that this particular self-improvement
Loop is unusual in its tightness in the
sense that you just need this to improve
itself and not a whole bunch of things
to improve to get it so one thing that I
think is common is when we identify some
Dimension that humans and animals have
and then we try to build technology to
surpass humans and animals on that
Dimension we tend to succeed quite
rapidly and by quite a lot right so like
on on trans for example if you just
identify hey how do you get somewhere as
fast as possible and transport as much
weight as possible at this point we're
far beyond anything biology ever
designed right is that a fair Rob I
continue to say yes our descendants will
be vastly better on us in a great many
capacities but yeah but it's not just
our descendants if you look at the
actual rate of this kind of stuff it
seems like the progress happens pretty
fast right it's like the progress toward
becoming as good as humans at Vision
okay that Rel together improves and has
for a while but small fractions of the
world don't have very much extra
self-improvement compared to the world
that that's the key Innovation
observation about Humanity for a long
time yeah okay but just I think I'm I'm
arguing for the subo that it we might
potentially have you know let let's say
where the brain was going to go if you
let the brain be twice as big if you
made the pelvis bigger AI might get to
that same endpoint within just a couple
years is that
plausible what is this endpoint I'm
sorry the end point of if you got to
continue the trend of how the brain of
humans differs from the brain of Apes by
letting it grow bigger which Evolution
didn't have that opportunity if you just
continue that Trend you might have
something much smarter is it possible
that AIS will go that same direction and
be how much smarter Evolution was trying
to make
us but you're I keep saying and agreeing
with the fact that our descendants will
be much more capable but you say this
thing instead and so you must have in
mind some only some things change and
not other things change concept here
you're going to imagine only the P
pelvis gets bigger but not other things
what's that only thing for AI what's the
only thing that improves about Ai and
other things yeah the only thing that
improves is the mapping between a goal
you can give it and the quality of the
action plan the effectiveness of the
action plan to achieve that goal right
the optimization power why is that an
only why isn't that just a summary of
our overall capacities I mean you could
just say the same about the world
economy the world economy getting rich
another way you can say that is if you
want to do any one thing and you hold up
a $100
it just more successfully achieves any
one thing with your $100 as the world
economy gets bigger that's a global
feature of the world economy it's not a
feature of a local part of the world
economy the fact that you could achieve
goals more e right but if you have the
Eon if you hold up a $100 bill in the
year 2010 compared to holding up a $100
bill now aren't there certain objectives
that you can get much better now for
that same $100
bill well the world economy doubles
roughly every 15 years so it's been 15
years since 2010 so yeah you can get
twice as much of everything
right but if you look at a specific
thing that you want out of that $100
bill like hey go make me like the best
virtual reality experience on average
you can get about twice as much 15 years
later some things are exceptionally more
but other things are a bit less on
average when the world economy doubles
that means you get twice as much of
everything right but a lot of times the
quickest path to getting the best
results isn't letting the economy grow
around you it's just calculating the
best sequence of steps you should
do I mean that it's part of how that 100
$ gets spent better is when you hold up
say to hold $100 for a lawyer and the
lawyer takes $100 he has a better way of
calculating how to do the thing you
wanted you don't have to know how that
is but yes a big part of how the world
economy gets better is that the
participants know better how to do
things how to calculate I I think the
the reason you know to zoom out I think
the reason we're talking about all this
stuff is because I'm trying to convince
you that there's a dimension the
optimization power Dimension and I'm
trying to show you arguments why I think
that Di mentioned could be scaled very
very rapidly right within like let's say
a year something like that right like
much less than a 20-year economic
doubling time are you buying that at all
you're trying somehow to factor this
thing out from the general capacity of
the world economy yeah exactly because I
think it naturally does factor out the
same way you can it's like look
Transportation right optimizing the
speed of Transportation doesn't that
factor out from the world economy well
you can identify the speed of
Transportation but just making
Transportation twice as good doesn't
make the world economy twice is good so
you're trying to argue that there's this
factor that has a selfimprovement
element to it the way Transportation
doesn't right if you make Transportation
factor that doesn't make it that much
easier to make Transportation easier
yeah what what if we pulled that aside
let's not even talk about
self-improvement what if just without
self-improve I mean what I'm arguing
about Evolution If evolution got to make
the brain twice as big that's not even
self-improvement that's just a path of
tweaking genes and getting a little bit
of consequentialist feedback I guess but
I mean it's it's almost like copying and
pasting the algorithm to generate brain
regions might have worked I I feel like
we're we're running out of time here so
we should try to get to the meat of the
issue unless you think this is the meat
of the issue but um you know basically I
I I think everything we've been saying
is is all like I think we've been going
at a linear rate knocking down Meats of
the issue uh but again I gave what I
described was an implausible scenario
you're going to try to make it seem
plausible please start to do the best
you can to make it seem plausible it
still looks implausible yeah so in terms
of a specific scenario so what worries
me is I see a lot of room above
intelligence and I'm expecting that
within a few years maybe 20 years if
we're lucky or even 30 but very
plausibly 3 to 10 years within a few
years we're just going to have these
machines that play the real world as a
strategy game the same way stockfish
plays chess they just tell you what to
do to get an outcome or they do it
themselves and at that point no matter
what happens no matter what regulation
you do no matter how companies cooperate
no matter whether it's multipolar
whatever it is you now just have these
agents of chaos that are unstoppable if
they try right that's roughly what I'm
expecting um I have doubts about that
but I still haven't seen you connect to
that to the fum scenario so how do
agents of chaos make fom so it's
possible that F could the the meat of
the fom where they get many many orders
of magnitude above Humanity that might
happen after the world ends in a more
mundane scenario I think it'll happen
eventually I don't necessarily think you
need a f for most of us to die or get
sent back to caveman standards of living
but I just think yeah sure you use
intelligence to keep improving
intelligence yeah that's going to happen
too okay but the fum SARS was a very
local very fast Improvement you and I
both agree that capacity will just inove
from the long run uh but now you're
focused on people having capable agents
to advise them on things and thinking
that's a problem or some something goes
wrong in that scenario say we all just
have much more capable agents to advise
us sounds good to me so yeah I mean
imagine that gbt 10 says oh you want to
make more money for your business great
here's a script you should run the
script and then they run the script but
of course it has all these other ideas
about what it should do and now it's
Unstoppable and it wasn't what you truly
meant right so that's that's the the
misalignment scenario right that's not
can it end the world it's will will it
end the world and we're talking about
alignment well now you're talking about
unreliable advisors a world of
unreliable advisors is different than a
world of useful reliable advisors
initially you're just talking about
they're good at giving you good advice
but now apparently they're they're bad
their advice is bad so so we might not a
hug use of bad
advice so I'm expecting that they'll be
very good at getting an outcome that
they're modeling as the target outcome
but I think there's going to be a game
of telephone a disconnect between what
you've inputed into them and how you've
train them to interpret your input and
then what eventually happens being not
what you actually
wanted in our world today as you well
know you know the main suppliers of AI
systems are large corporations that put
them through a lot of testing and then a
lot of monitoring of usage to look for
problematic usages Etc so if this world
of even more capable AI assistants had
been going through a similar level of
monitoring and
testing uh then that testing would
roughly show how much they can be
believed to relied on and we would rely
on them to that degree and not more and
now if we have somewhat reliable but not
entirely reliable advisors that we rely
on more but not too much especially in
situations where we're especially
vulnerable that still looks like a win
World why is it such a problem if we
have not entirely trustworthy but pretty
good advisers so the problem is that
these advisers output these pointand
shoot plans that you can just press
enter to execute the plan and the plan
is just so consequential you know it's
it's this runaway plan and it makes sub
agents and it has huge impact and you
just you basically just have to decide
am I going forward with this or not and
you get disempowered in the process so
we today have worlds of Consultants like
that I mean certainly like businesses
already have many consultants they could
hire each of who could give them advice
on various things some of which Bice
could go wrong and individuals you know
they watch Once Upon a Time late night
TV shows trying to sell them As Seen On
TV products which you know might not
work or work well so clearly the world
has long had to adjust to the
possibility that various kinds of
employees advisors Consultants could
have
self-interest uh underlying their advice
and um recommendations and supports and
we have been learning to be defensive
and wary about those things for a long
time why will that whole process
suddenly go wrong when AIS are the
advisers Consultants yeah because
they're more they're smarter and more
powerful right so if you're trying to
figure out whether the person you're
watching on TV doing an infomercial is
scamming you or not that's a much slower
and lower impact process right there's
there's no world ending downside to
being like sure I agree with your sales
pitch why I mean random people buying
stuff on infomercials isn't a world
ending process why would these AIS be
world-ending
processes because they will have the
power to end the world right so if we're
having the alignment discussion that pre
assumes that these agents are very
powerful but the whole point of them
being agents is somebody has to trust
them to delegate to them and follow
their advice right and if if you're
already accepting the premise that
they're that powerful you really just
need one person to run an agent that's
not great for whatever reason and then
you're screwed so I don't understand
that that is say we have a world of 10
million people using AIS for advice to
do all sorts of things one of them
listens to a AI which bad advice how
does that end the world so it's not
about the person listening to the bad
advice it's about a lot of the stuff
happens as you say you're predicting
everything happening on a faster time
scale so that's part of the problem is
just the AI says look I have a bunch of
things I can do do you want to play it
out over 50 years like do you if you
want to beat SpaceX do you want to do it
over 50 years or do you want to do it
over two years and you can you can be
the next El on musk press enter to beat
SpaceX within two years and it's very
tempting for somebody to press enter and
if they press enter what happens what
goes wrong so the AI is getting us to
the next phase of Technology right it's
fing the same way that modern humans are
doing a bunch of things that past humans
couldn't fathom wait so now we're
Imagining the button is a F
button essentially yeah I think I think
you don't really have it's I mean it's
not going to f all the way but even if
it just FS even if it just pulls forward
a few Decades of progress into a few
months that's already a very chaotic
environment but all these other people
with their AIS could be do pushing
similar buttons creating
increasing capacities but they you know
rivalis
capacities well so I mean I agree you
could have this kind of melee right
where like on the internet there's like
everybody's running their own virus
that's trying to like seize as much
capacity as they can on the internet
like I I could I agree this crazy
situation can happen but I don't see how
that works out with Humanity still
maintaining a reasonable standard of
living I'm really failing to understand
um what you're worri about here I mean I
I pointed out the F scenario because at
least that's a scenario can comprehend
and and identify its parts but I'm just
not understanding you're just imagining
a world where there's lots of advisers
and Consultants available and that that
goes wrong because one of the things
these advisers could do is advise you to
push a button that then increases the
capacity of something you control which
most people would typically do now and
probably do then and that sounds good
because we just have all these increases
in capacity of all these things in the
world okay fair enough is this your main
point that a world of people listening
to AI advisers could go wrong because
they could get bad
advice I think that's not a good
characterization because get bad advice
seems like there's going to be a step
where the human slowly takes the advice
but more realistically it's more like
the human authorizes bad projects under
the advice of an AI say sure but that
the authorization step is Trivial right
it's just somebody pressing enter
essentially right so the the issue is
just what the AI does when it has all
this capacity to decide what to do and
then do it and then repeat so AI agents
with um who need to get approval from
humans for budget and you know free you
know powers of
authorization sometimes they get powers
and authorization that maybe would not
be good advice but some humans are
stupid but then what so some projects go
wrong but there's a world of all the
other people with their AIS protecting
themselves against that right so if the
scenario is that the AI gets it into
this head of like okay I need to go
build myself the most energy to support
my project so I'm just going to beg
borrow or steal I'm just going to do
whatever it takes to get energy and if
an AI is so much smarter than Humanity
you can imagine simultaneously
manipulating everybody on the internet
right so suddenly it's like winning over
humans to its cause right it's it's
doing massive effects I mean it's got
well if there was only one of them but
if there's billions of
them at similar levels that is I so at
the moment you know it's important to
notice we live in a world of
corporations which in a important sense
are super intelligent that is compared
to any one of us corporations can run
circles around us and Analysis of
marketing and uh you know product design
and all sorts of things so each of us as
an ordinary human are subject to all
these potential super intelligences
trying to sell us products and services
or hire us for jobs and how is it that
we can it all survive in this world of
these very powerful super intelligences
trying to trick us and they do they do
try to trick us they try to trick us
into bad jobs buying bad products Etc
all the time and they weigh out classes
that is whenever a corporation sitting
there thinking about how to make a
commercial how to design a product to
trick us into wanting it they're just so
much better than we are when we think
and reason about GE do I want this
product right so I think I agree with
with the part of your point saying like
look we don't have a single actor today
that isn't held in check by a
combination of other actors who share
our values right is that your point
Corporation don't share our values
that's not how but they're held in check
they're held in check by a combination
of other actors or forces right like you
know if if they try to do something
illegal competition is is a great
discipline of not only corporations but
also governments uh different people who
might run the government at least that
is democratic competition lets us try to
judge among different people try to run
the government commercial competition
lets us judge among different people who
we might buy from or be employed by
that's the thing that makes our world
work today is competition among Super
intelligences Why can't AIS function
similarly so in our world today the
reason why a corporation doesn't run
away and start destroying the world in
order to get another dollar out of you
one reason is just because they know
that the combination of other actors are
then going to push back is that
fair we know how they would push back
and question is why can't a why wouldn't
in fact that be the default typical
arrangement for AI that as AIS would
slowly slot into the spots that
corporations and Consultants are sitting
into now right that's how Corporation
AIS would gain power and roles in our
world is basically they would sit into
the same social roles right so my
scenario is that that balance is going
to be disturbed where we're going to
have agents that they think that they
can go do something which they know that
humans would love to push back on but
they also know that it's too late and
the humans can't push back on it and
what would that be that CU are are there
such things in our world or if not why
does this world have such things and our
world Now does not so the key difference
is just that you're going to have an
agent that has more of this optimization
power right like bringing a band of
human of modern humans into like an
ancient tribe or an ape tribe but these
super intelligent corporations already
have vastly more optimization power than
we do that's not yeah but they don't
have so much of it that they think that
they can run away from Humanity as a
whole right so one Corporation can beat
humans but they can't beat all of humans
and I think that that balance is going
to change but one Cor Corporation
doesn't have to just beat all humans it
has to beat all the other corporations
too same for the AI one AI has to beat
all the other AI so why will one AI be
so much more capable than all the AIS so
it sounds like you're open to the
scenario where you have a lot of very
powerful AIS running around fighting but
you just always think that that
equilibrium is going to stay pretty good
right is that a good characterization
I'm not trying to make strong claims
like that but I'd say we already have a
similar situation we already are
monitoring for ways that could go wrong
we should continue to monitor for ways
that could go wrong with the AIS but um
I don't see a better solution than to
wait until they're here and start to
look for such things uh I think it'll be
very hard to anticipate them far in
advance to be able to guess how they
could do things wrong and and try to
prevent them for most Technologies the
time to to deal with their problems is
when they are realized in a concrete
enough form that you can see concretely
what sort of things go wrong and you
know track statistics about them and do
tests about various scenarios and that's
how you keep technology in check is
through testing actual concrete versions
and the problem at the moment is AI is
just too far away from us to do that we
have this abstract conception of what it
might eventually become but we can't use
that abstract conception to do very much
now about problems that might arise
we'll need to wait until they are
realized more but I don't know if you
know that I have I have supported the
concept of f liability MH wherein I'd
say look if there's a way in which
individual AI systems can go wrong then
what we should do is make their owners
liable for whatever damages they cause
and had an extra liability for however
close they get to the bad scenarios the
extent you can identify the scenarios
you're most worried about we can make a
check checklist of factors which makes
it more likely they're near the bad
scenario than we just
crank up liability near those bad
scenarios and that will just push
everybody away from those scenarios and
that's a simple robust way and elizer
himself said that that looks like
something that could
work so uh I'm I don't see I'm not
seeing large chances of a problem but
I'm still happy to look for cheap ways
that we could avoid a problem that might
be bad even if I don't think it's that
likely because hey why not
me to avoid things go wrong I don't
think it's but some other people think
it's a problem so I don't see that much
of a problem with f liability that it
looks like a relatively modest cost to
deal with what like some people think
it's a serious problem so why not that's
great that we have a little point of
agreement because it's like yeah you
have you know recently I think Yan laon
and a bunch of AI industry indust
insiders were saying we don't want to be
held liable in case our systems break
and cause damage they were pushing back
against some proposed regulation and
people called out some hypocrisy because
it's like wait a minute you guys are the
people saying there's no issue right so
now now when it's time to say Hey you
have to pay us if there's an issue
you're saying that's going to handicap
you like what's what's going on so I do
think it's somewhat unfair that we often
take particular things we don't like and
we want to set up extra liability
regimes for them but not for everything
else I think we should just have a more
generic liability regime again and I
think basically everybody should have to
have liability insurance against
everything totally that's just a simple
not just AI or just guns or whatever you
particularly don't like just have we we
we'll need to make our liability
processes work a bit better because at
the moment they don't work quite so well
but it is a nice robust approach is just
a li liability for damages requiring
liability insurance to everybody to co
make sure they can cover and then let
let them try stuff just want to give a
shout out to the viewer if you like that
kind of institution designed by Robin
he's got a bunch of different proposals
for institutions that I think are
underrated
exactly um you've made a similar point
about why you expect to be able to live
peacefully with AIS which is you
mentioned the rule of law right and
you're pointing out look when you you
say when you go to another human tribe
do you really care how much they share
your values or do you care if they
respect the rule of law right you've
made that argument right that is most
mostly when we travel around the world
the thing we check for in whether we'll
be safe somewhere isn't some poll of the
people and their values and how aligned
those values are with our values that's
that's not how we do it right we have
systems of Law and we check for
statistics about how law often is
violated and then we think I might be at
risk if law is violated here A lot's a
high crime rate and so to the extent
that you can assure that this kinds of
laws there are the kinds that would you
know punish people for hurting you you
know let you sue for damages and then
low crime damage rates would be enough
to convince you that it looks okay then
so what about a scenario like my
grandpa's family where they were Jews in
Poland right so it seems like the idea
that the society is going to respect the
rule of law for them sometimes breaks
sure sure uh there are sudden changes in
law that you might uh have to look out
for right so and how do we characterize
when does a weaker demographic tend to
get if you're going to visit somewhere
for a week as a travel that's less of is
you're planning on moving there for
retirement you'll need to make longer
term estimates about the r of law that's
true that a holocaust is is only going
to be a longer term effect so you can
always kind of dip in and out to places
that have the rule of law for a long
time even in 1940 35 it might have been
okay to just visit Germany for a week
right on a trip okay okay fair enough
but you really think that AI we're going
to be able to like have money that the
AI accept and you know and have property
that they respect that that's like your
Mainline
scenario I it's also true for M's as
well as AI I think one of the biggest
risks is that because we think of them
as a separate category we end up
creating separate systems for them
separate legal systems separate
Financial systems separate political
systems and in a separate systems then
they are less tied to us and then when
they get more powerful than us then they
may feel less about disrupting us I
think the more we are sharing systems of
governance Finance uh employment you
know relation Rel relationships Etc then
the more plausible it is that they would
feel ties to us and uh not try to kill
us all yes so I don't want these
separate systems I want shared mixed up
systems that's just a general best
strategy through all of human history
for limiting harm between groups of
people right okay one more thing I got
to touch on on the topic of alignment is
rhf what do you think about rhf is that
a powerful alignment tool in your view
I haven't studied in the detail um the
things I see as its results have been
you know discouraging but um I don't
have strong opinions basically it's it's
kind of a finishing school they go
through and you know the main thing we
notice is how officious and you know
submissive and uh uh you know even
hypocritical they are as a result of
this finishing school and that's not
really very conducive to the kind of
conversations I me when have with them
but um it could be there's other ways in
which they would just be obnoxious and
uncooperative that I'm not seeing that
this finishing school helps so um I
guess I'd just like to see more
competition in this world of finishing
schools to see if we can uh in the same
way for regular human schools like
there's a lot of professionals created
by our major universities where I'm not
that thrilled with such professionals
they don't seem that fun for me to
interact with but clearly firms like
them and hire them so I can't complain
too much right right well rhf has worked
well to make the chat Bots into good
consumer products right because they've
succeeded in the market with chat GPT
but open AI is admitting that rhf is not
going to cut it when the AI becomes
super intelligent anthropic is admitting
that so they're admitting that it's an
open problem how to do the equivalent
for super intelligence that we're
getting today with with subhuman
intelligence right does it concern you
at all that they're admitting that
alignment is an open problem and rhf
doesn't
scale uh not especially that's the easy
thing to predict yes of course all of
these terms are going to admit any sort
of criticism they are eager to show that
they are concerned I mean in the modern
world almost no firm with a product that
some people think is risky wants to
pretend oh there's nothing to worry
about there that's just not a winning PR
stance for any company in our modern
world so yes of course they're going to
admit to whatever problems other people
have as long as that's not a current
problem with their current product to
worry about it's some hypothetical
future version of the product they're
happy to say yeah you know that'll be
something will be watching out for and
worrying about okay last question what
is the event or threshold or warning
shot that would make you concerned about
rapid human extinction from
AI I mean the main thing I'm going to be
tracking is automation of jobs so if I
start to see a substantial uptick in the
rate at which job tasks automated
happens that will correlate of course
with the substantial tick up in the
amount of Revenue going to the companies
supplying that Automation and the
supporting infrastructure that they pay
for then that would show a deviation
from Trend and then I want to look more
closely to see how fast that's
accelerating and in what areas that's
accelerating and that would be the
places to we should all watch more
carefully uh watch carefully wherever
Trend things are deviating from Trend
when you made that prediction about the
billion dollar a year you know the bet
with Alex Tabo that you didn't think AI
would get to a billion doll a year do
you want to up it to like1 billion a
year and then you might actually get
concerned I mean I'm happy to just leave
it continuous uh but again um this is
still a pretty tiny fraction of the
world economy a billion dollars a year
in A90 trillion doar economy is still
just a tiny tiny fraction so I'm really
not what we're going to look for is
trends that might lead to much larger
fractions that's the thing we're worried
about you know if we one part in a
thousand of the world economy is still
not enough to worry about its actual
effects but as a trend it might be
interesting and worth
watching here one throw a proposal at
you if 5% of world GDP in 2030 or
earlier is is directly traceable to late
generation AI then you get worried about
the fum
scenario I get to pay more attention to
trends when they deviate more from
Trends so that there's there's no
threshold of not worry worrying there's
just a degree of attention that should
go to things as they become more
strikingly deviant from what you
expected in previous Trends so I I will
just pay closer attention but again I
still think there won't be much we can
do at the abstract level to constrain AI
to prevent problems we'll mostly have to
wait until concrete systems have
concrete behaviors and monitor those and
test them uh and the more capable
systems get the more we should do that
great all right thanks Robin I think
this was great I'm so glad I got to ask
you on all these different positions and
update my model of how you see the world
what would you say as a wrap-up
statement how would you summarize where
you've disagreed with me
today I guess uh it would have been good
ahead of time if you had presented a the
scenario you're most worried about maybe
I could have read about that ahead of
time thought about it and then we could
have dived into the particular scenario
because I'm still not very clear about
it it isn't the same food scenario as I
thought you might be focused on so um
you know that may be for some other day
but basically there's there's a there's
a value in general in all these
conversations where somebody summarizes
their point of view as concisely as they
can and thoughtfully and then the other
party reads it so I guess I had some
things like that for you but I didn't
ask for that from you and maybe that
would be good for you to do maybe you
should write a little Essay with if
you're you know the scenario you're most
worried about lay it out as clearly as
you can and maybe that would be a way to
refine your thinking and then be easier
for other people to talk to you that
sounds good yeah and I feel like that's
similar to requests that doomers get
pretty often so I'll definitely go back
and and see what I can do on that front
all right everybody so um Robin where
would you direct people to online uh you
can find me on Twitter at Robin Hansen I
have websites hen. gmu.edu um that's it
for the audience I know I've focused
everything today on how I disagree with
Robin but I in general I actually think
that his body of work like I said is
some of the most influential highly
recommend checking out grabby aliens.com
and just go to Robin's website just
browse around I think you'll be quite
impressed with a lot of that work all
right thanks very much Robin okay take
care wow Robin is super generous with
this time not only with me but
constantly for years with all kinds of
interviewers and even
critics while I couldn't disagree more
with his position on Aid Doom as you saw
I have the utmost respect and
appreciation for the way Robin engages
in highquality discourse it's all too
rare to be able to say that about
someone that he consistently engages in
high quality discourse can't say that
about many people in fact Robin is the
only prominent non- Doomer so far who
has offered to debate me or accepted my
offer to debate and yes I have been
making offers that speaks
volumes last thing the point of podcast
is to spread awareness of how bad and
how urgent the AI Doom situation has
gotten for Humanity if you feel the same
way you can do your part by sharing this
episode on social media and if you
haven't already subscribed to Doom
debates did you know it's as easy as one
two three one go to Doom debates.com to
join my substack two go to
youtube.com/ Doom debates and click
subscribe three search Doom debates in
your podcast player that's it 1 two 3
thanks for your continued feedback
support and constructive criticism and
I'll see you on the next episode of Doom
debates