with Robin Hansen I come in as Kirby he
comes in as a puffer
fish hey everybody Welcome to the robin
Hansen post debate recap I've had a
chance to go over the 2hour debate sift
through it I've actually spent 5 hours
analyzing it I've read everybody's
feedback on YouTube and
Twitter and the consensus seems to be
that it was a good debate
there were a lot of topics brought up
that were kind of deep cuts into stuff
that Robin
says on the critical side people were
saying that it came off more like an
interview than a debate I asked Robin a
lot of questions about how he sees the
world and I didn't nail him and people
were saying I wasn't quite as tough and
forceful as I am on other guests so
that's good feedback I think it could
have been maybe a little bit less of an
interview maybe a little bit more
busting out my own position which is
also something that Robin point pointed
out at the end is he was a little
confused about what my position on AI
Doom was so good feedback I'll take that
to heart I always welcome your feedback
there is a reason why the robin Hansen
debate felt more like an interview let
me
explain most people I debate have to do
a lot of thinking on the spot because
their position just isn't grounded in
that many connected beliefs they have
like a few beliefs they haven't thought
that much about it when I raise a
question they have to think about the
answer for the first time and usually
their answer is weak so what often
happens my usual MO is I come in like
Kirby you know the Nintendo character
where I first have to suck up the other
person's position pass their ideological
Turing test and with a normal guest it
doesn't take me that long because their
position is pretty compact like there's
not that much substance to it I can kind
of make it up the same way that they can
comes in as a puffer fish so his
position is actually quite complex
connected to a lot of different
supporting beliefs and I ask him about
one thing and he's like ah well look at
this study that actually proves this to
me he's got like a whole reinforced
lattice of all these different claims
and beliefs so I just wanted to make
sure that I saw what it is that I'm
arguing against that's largely why I was
interviewing him the other reason why I
was interviewing him is just because
I've noted down a bunch of stuff that he
said over the years and I just want to
make sure that we put it all into this
context of whether it has any bearing on
Doom or how it's related or why Robin
says it I was aiming to make this the
authoritative followup to the 2008 F
debates that he had on overcoming bias
with Alazar owski I wanted to kind of
add another chapter to that potentially
a final chapter because I don't know how
many more of these debates he wants to
do I think elezar has thrown in the
towel on debating Robin again I think
he's already said what he wants to say
another thing I noticed going back over
the debate is that the arguments I gave
over the debate were like 60% of what I
could do if I could stop time and do it
all over again I wasn't at 100% And
that's simply because real time debates
are hard right you have to think of
exactly what you're going to say in real
time and you have to move the
conversation to the right place and you
have to hear what the other person is
saying and if there's a logical flaw you
have to narrow down that logical flaw in
like 5 Seconds right so it is kind of
hard mode to answer in real time I don't
mind it I'm not complaining I think real
time is still a good format I think
Robin himself didn't have a problem
answering me in real
time but I did notice that when I went
back over the debate and I actually
spent 5 hours on this I was able to
craft significantly better
counterarguments to the stuff that Robin
was saying mostly just because I had
time to understand it in a little bit
more detail so the quality of my
listening when I'm not inside the debate
when I'm just listening to it on my own
I'm listening like twice as well twice
as closely and I'm pausing and really
thinking like why is Robin saying this
is he referencing something is he
connecting it to another idea that he
said before so I'm just having more time
to process off
line so you're going to hear some
arguments now that are better than what
I said in the debate however I do think
that I did make a good amount of
arguments during the debate to the point
where I think we did expose the Crux of
our disagreement I think there is enough
back and forth in the debate where you
will be able to see just from the
footage in the debate that Robin sees
the world one way and I see it a
different way and you'll see exactly
where it clashes and exactly which
beliefs if one of us were to change our
mind could change the whole argument and
that's what rationalists call called the
Crux of disagreement the Crux is not
just some random belief you have it's a
particular belief you had that if you
switched it then you would switch your
conclusion and when I debate you may not
know this but all I do is just look for
the Crux I don't even try to quote
unquote win I don't try to convince the
other person I just try to get them to
agree what the Crux is and what they
would need to be convinced of and then
if they want to go dig further into that
Crux if they want to volunteer to change
their mind that's great but that's not
my goal because I don't think that's a
realistic goal right and I think that
just identifying the Crux is highly
productive regardless I think it brings
out good content the listeners like it
so that's what I do here at Doom debates
the other thing to note about the debate
is I came in with a big outline I'd done
a ton of research about Robin I'd read
pretty much everything he's ever written
about AI Doom I listened to interviews
so I came in with a big outline and as
he was talking I wasn't just trying to
respond to exactly what he was saying I
was also trying to guide the
conversation to hit on various Topics in
the outline and that's part of why I
didn't give the perfect directed
response to exactly what he was saying
but now I'm able to do it so you'll
maybe you'll enjoy these new type of
responses I think it's going to be
pretty rare for me to have such a big
outline for different guests largely
because different guest positions
haven't been as fleshed out and as
interconnected as Robin so I think this
is going to be a rare approach for me
but it is interesting to observe how
having an outline of topics changes the
kind of debate you get all right let's
go through the debate so I've clipped it
down to like the 30% that I think is the
most substantive the most relevant to
analyze and we're just going to go clip
by clip and I'll give you some new
thoughts and some new
counterarguments the first thing we talk
about is Robin's AI timelines he has an
unusually long prediction of AI
timelines he says yeah could take a
hundred years to get to artificial
general
intelligence and he bases out on a key
metric of human job replacement so he's
just trying to extrapolate the trends of
AI taking the job of humans creating the
economic value that humans are creating
that's his key metric I have a different
key metric because I think of things in
terms of optimization power my key
metric is the breadth and depth of
optimization power how many different
domains are we seeing AIS enter into and
how strong are they in all these
different domains so when I see
self-driving cars maybe I don't see them
displacing that many human employees yet
but I see wow we can handle more edge
cases than ever we can now Drive in the
San Francisco Bay Area to wh Mo and I
think last I checked something like 10
times safer than a human driver with the
data we have so that would be depth of
optimization right they can drive better
than a human at the entire San Francisco
Bay Area that's what I'm looking at and
that Trend seems to be going like a
freight train right it seems to be
accelerating it seems to be opening new
domains all the time I mean when you
talk about bread the fact that llms can
now handle arbitrary English queries and
they can connect together Topics in a
way that's never been done before across
different domains they can do a
primitive form of reasoning when they
give you the answer and they can do they
can essentially solve the Syle grounding
problem in these arbitrary domains so
I'm seeing all this smoke coming out in
terms of AI getting better at breadth
and depth of their optimization but
Robin has a totally different key metric
and that's where his estimate is coming
from for me the the the most relevant
metric is when are they able to do most
jobs say more cost effectively than
humans I mean what percentage of jobs
can they do basically what percentage of
the economy does AI or computer
automation take that's to me the most
interesting metric so so in your view
it's pretty plausible that we can get to
2100 and there's still jobs that humans
are doing better than
AIS right that's not at all crazy to get
Robin worried about AI Doom i' need to
convince him that there's a different
metric he needs to be tracking which is
on track to get dangerous I have to ask
okay what's this Theory by which
something else is going to happen that I
need to track other things and do I
believe it so here's where I explained
to Robin about my alternate metric which
is optimization power I tell him about
natural selection human brains AI Robin
doesn't even see human brains as the
only major milestone in the optimization
power story he talks a lot about culture
let's take the optimization framing I
might say culture deserves to be on the
list of historical
optimization um
machines after brains and I might object
to trying to do a history in terms of
optimization without noticing that
culture should be on the list I I would
find that suspicious if you and why
should culture be on the list in your
view because that's Humanity superpower
that's the thing that distinguishes I
mean we've had brains for half a million
years right uh what we what
distinguished humans wasn't so much
having bigger brains was having the sort
of brains that could enable culture to
take off and culture is the thing that's
allowed
us to become optimized much faster than
other animals not merely having bigger
brains so I think if you talk about
human brains as a an event in
optimization that's just very puzzling
because human brains weren't that much
bigger than other brains and brains
happened a long time ago so what's the
recent event in the optimization story
it wasn't brains it was culture here I
should have done a better job drilling
down into culture versus brains because
that's an interesting Crux of where I
disagree with Robin culture is basically
multiple brains passing notes the
ability to understand any individual
concept or innovation happens in one
brain culture doesn't give you that sure
culture collects Innovations for you to
understand quote unquote ape Culture by
itself without that brain support
doesn't make any economic progress but
on the other hand if you just give Apes
better brains I'm pretty confident
you'll get better ape culture and you'll
get exponential economic ape
growth Robin is saying look we humans
have had brains for half a million years
so culture must have been a key thing we
had to mix in before we got rapid human
progress right I agree that there's a
Cascade where human level brains don't
instantly F they build supports like
culture but I see a level distinction
let's classify these types of events
into three levels this was originally
introduced in your fum debate with
elazar the three levels are number one
the dominant optimization process like
natural selection human brains AGI level
two meta level improvements to that
process so like cells sex writing
science and as you just say now culture
I believe goes on level two and then
level three is object level Innovations
like light bulb automobile farming is
that a useful your distinction between
one and two levels at the moment that is
I don't see why culture should be on
level two with writing as opposed to a
level one with d Evolution so that's a
Crux Robin thinks quote unquote culture
is as fundamental of a force as brains
and natural selection I think it's
definitely not when we get a super
intelligent AI that disempowers Humanity
it very likely won't even have culture
because culture is only helpful to an
agent if that agent is dependent on
other agents now we get to Robin's
unified model of all the data points
that accelerated economic growth he says
it's about the rate of innovation and
the diffus Fusion of
innovation so diffusion of innovation
was the key thing that allowed farming
to innovate so basically in all the
areas so far the key thing was always
the Innovation rate that's what allowed
growth and Innovation is the combination
of invention and diffusion and typically
it's diffusion that matters more not
invention you catch that he said
diffusion matters more but when we have
a super intelligent AI a brain with more
computing power and better algorithms
than the sum of humanity diffusion will
be trivial diffusion is just one part of
this giant talking to another part of
this giant AI in under a millisecond
this is an argument for why to expect a
fum a singularity we are setting
defusion time to zero in Robin's model I
would argue that the Innovation part of
the equation will be vastly faster too
but the argument from instant diffusion
of Innovations seems pretty powerful
especially since Robin actually thinks
diffusion matters more typically it's
diffusion that matters more not
invention another Crux is the difference
between my notion of optimization power
and Robin's notion of accumulation of
optimizations optimization power isn't
quite the same as Innovation we have to
talk about the accumulation of
optimization I don't know Robin how
different are these Notions really
optimization power versus Innovation or
optimization versus accumulation of
optimizations if Albert Einstein invents
special relativity in 1905 and then
again Albert Einstein invents general
relativity in
1915 it seems like Einstein's brain is a
oneman optimization accumulator
Innovation accumulation seems like a
weird way to describe the cognitive work
being done in the field of physics the
work of mapping observations to
mathematical theories so I would frame
it differently I wouldn't say
theoretical physicists thanks to their
culture accumulate innovations that
improve their theories I would say that
Einstein had high optimization power in
the domain of theoretical physics he
used that power to map observations to
mathematical physics he was very
powerful as an Optimizer for a human
unfortunately he is now a corpse so his
brain no longer has optimization power
so we need other brains to step in and
continue the work that's very different
from saying hail Almighty culture it's
interesting to test Robin's model on the
example of leaf cutter ants who have
their own version of farming where they
feed leaves to a fungus and eat the
fungus the reason why leaf cutter ants
didn't give rise to a growth rate cuz
they invented leaf cutting using DNA
Evolution they didn't invent cultur so
they didn't have a way to accumulate
farming culture so they're still very
slowly innovating farming the key thing
is the accumulation and spread of
Innovations not the farming itself when
Robin says that leaf cutter ants are
quote very slowly innovating farming he
must mean that Evolution itself is doing
the innovating not one particular
species leaf cutter ants because that
one species is at a dead end of growth
the only species that's expanding
exponentially into the entire ecosystem
is humans when Robin says the key to
economic growth is to collect and spread
Innovations he's just talking about
optimization power but he's factoring
optimization power into these two
components that don't have to be
factored he's not seeing that the nature
of the work is a fundamentally mental
operation an algorithm its goal
optimization work imagine it's 1970 when
people didn't know if computers would
ever beat humans at chess Robin might
argue the key reason humans play chess
well is because we have culture humans
write books of moves and strategies that
other humans study humans play games
with other humans and they write down
lessons from those games in 1970 that
would seem like a plausible argument
after all you can't algorithmically
solve chess there's no special deep
insight for chess is there today we have
Alpha zero which immediately jumped to
human level play by starting from the
rules of Chess and running a general
purpose machine learning algorithm so
this decomposition that Robin likes
where instead of talking about
optimization power we talk about
accumulating and diffusing Innovation
it's not useful to predict what's
happening with
AI okay another Point Robin makes is
that quote small parts of the world find
it hard to grow faster than the world
small parts of the world find it hard to
grow much faster than the world
that is the main way small parts of the
world can grow is to find a way to trade
and interact with the rest of the world
so that the entire world can grow
together obviously you have seen some
places growing faster than the world for
a time but mostly what we see is a world
that grows together the longest
sustained exponential growths we've seen
have been of the entire world over
longer periods smaller parts of the
world have had briefer periods of
acceleration but then they decline
there's been the rise and fall of
civilizations for examp example and of
species
Etc so the safest thing to predict is
that in the future the world might find
a way for the entire world will
accelerate faster okay wait a minute
it's not the entire world that's been
accelerating it's humans Apes aren't
accelerating they are suffering habitat
loss and going extinct species that get
in our way are going extinct the human
world is growing because number one
humans have something of value to offer
other humans number two humans care
about the welfare of other humans that's
why we're a unified economy it's true
that the human world grows as a whole
because different parts of the human
world are useful inputs to one another I
agree that a system grows together with
its inputs it's just worth noticing that
the boundary of the system and its
inputs can vary we no longer use horses
as inputs for transportation so horses
aren't growing with the human economy
anymore they're not part of our world
moving on what kind of Trends does Robin
focus on exactly
he usually tries to focus on economic
growth Trends but he also extrapolates
farther back to look at data that was
quote a foreshadowing of what was to
come can you clarify what metric is
doubling because I get GDP during the
economy but what about before what
metric is it if you ask what was the
thing that was changing that we look
back and say that was a foreshadowing of
what was to come and that was important
it's brain size so then that's what I
picked out for to looking at before
humans would be the brain size and if
you can go before the animal brains you
can just look at the size of genomes and
there's an even slower growth rate in
the size of genomes it it sounds like
you've got a pretty significant change
of metric right where with the animal
brains you're just saying yeah the
neuron count is doubling but that's a
precursor to when the amount of utility
is going to
double it's just the trend that matters
for the next transitions we think so we
we do think H one of the things that
enabled humans was big enough frames
which enable
strong enough culture so Robin is
focusing on the trend that matters for
the next
Transitions okay it's nice that we can
look back and retroactively see which
Trends mattered which Trends
foreshadowed major transitions but our
current predicament as I see it is that
we're at the dawn of a huge new trend we
don't have the data for a super
intelligent AI fum today that data might
get logged over the course of a year a
month a day and then we're dead we need
to understand the mechanism of what's
going to spark a new trend so it sounds
like you're just applying some judgment
here right like there's no one metric
that's an absolute but you're just kind
of trying to connect the dots in
whatever way seems like you get the most
robust Trend well for the last three
periods we can talk about world GDP and
then if you want to go further you can't
do that you got to pick something else
and so I looked for the best thing I
could find or you could just say hey we
just can't project before that and I'm
fine if you want to do that too Robin is
willing to concede that maybe he should
narrow his extrapolation down to just
world GDP in the human era so that we
have a consistent metric but I actually
agree with Robin's hunch that brain siiz
Trend was a highly relevant precursor to
human economic growth I agree that
there's some deep common factor to all
this big change that's been happening in
the historical record I don't know why
robin can't get on the same page that
there's a type of work being done by
brains when they increase the fitness of
an organism and there's a type of work
being done by humans when they create
economic value that that what we've seen
is not the ideal version of this type of
work but a rough version and that now
for the first time in history we're
setting out to create the ideal version
space travel became possible when we
understood rocketry and orbital
mechanics everything animals do to
travel on Earth is a version of travel
that doesn't generalize to space until
human technologists set out to make it
generalize We Now understand that an
optimization algorithm one that human
brains manage to implement at a very
basic level of proficiency not like
humans are even that smart but an
optimization algorithm is both the
ultimate source of power in biological
ecosystems and the source of innovations
that one can quote unquote accumulate
and spread at a faster and faster time
scale accumulate and diffuse Innovation
is one way you can describe the
implementation of optimization it's
clearly not the most fundamental
description you want to analyze the
economy by talking about Innovations how
can we even Define what an innovation is
without saying it's something that helps
economic growth you know a circular
definition maybe Robin would say that an
innovation is knowledge of how to do
things better I think any definition of
innovation is getting at the more formal
definition which is something that grows
optimization power something that grows
your ability to hit narrow Targets in
future State space your ability to
successfully hit a Target outcome in the
future by choosing actions in the
present that lead there getting back to
to Robin's Mo Robin insists on staying
close to the data without trying to
impose too much of a deep model on it
but he's missing equally valid ways to
model the same data in particular
instead of modeling economic output
versus elapse time you could model
economic output versus optimization
input if you look at the the the forager
farming industry transition though yeah
on the xais of time it's an exponential
but that's because you're holding the
human brains optimization power constant
right so if your model is at the x-axis
is actually optimization power it might
turn out that you got a hyperexponential
f once AGI starts modifying the
underlying Optimizer right so that your
your model then diverges
potentially well in all the previous
growth modes lots of things
changed and of course every transition
was some a limiting factor being
released we have this history of a
relatively small number of Transitions
and between each transition roughly
exponential growth and then really big
jumps in the growth rate at each
transition that's what history roughly
looks like his point here is he's
already modeling that different eras had
a discontinuous change in the doubling
time so when we get higher intelligence
that can just be the next change that
bumps us up to a faster doubling time so
his choice of x-axis which is time can
still keep applying to the trend even if
suddenly there's another discontinuous
change in fact his Mainline scenario is
that something in the near future
discontinuously pushes the economic
doubling time from 15 years down to 2
months I would still argue it's pretty
likely that we'll get an AI fum that's
even faster than an exponential with a
two-month doubling time if you plot the
exponential with optimization input as
the x-axis then you might get a
hyperexponential fum when you map that
back to time an exponential on a log
scale but regardless even if it is just
a matter of AI doubling its intelligence
a few times it's still leaves my Doom
claim intact my Doom claim just rests on
AI being able to outmaneuver humanity to
out optimize Humanity to disempower
humanity and for that AI to stop
responding to human commands and for
that AI to not be optimizing for human
values it doesn't require the F to
happen at a certain rate just pretty
quickly even if it takes years or
decades that's fast enough unless humans
can align it and catch up their own
intelligence which doesn't look likely
okay this next clip in my opinion is the
most important exchange of the whole
debate it gets to the Crux of why I'm
not convinced by Robin's methodology
that makes him think P Doom is low it's
pretty long it's a six-minute excerpt
but I think it's worth listening to in
full because like I said this is really
the Crux of why I can't accept Robin's
methodology to lower my own P
Doom let's project Robin Hansen's way of
thinking about the world to the dawn of
humanity so like 100 million years ago
or whatever what abstraction what way of
reasoning would have let us correctly
predict hum's present level of power and
control what does Robin Hansen of a
million years ago explain about the
future the
key human superpower was culture so if
you could have looked at the protohumans
and saw their early versions of culture
and how they were able to spread
Innovations among themselves faster than
other animals could through their
simpler early versions of culture you
would then predict that they will be the
The Meta process of culture inventing
new cultural processes that allow
culture to work
better and that did in fact happen
slowly over a very long
time and you might then have predicted
an acceleration of growth rates in the
long run uh that is the ability of
culture to improve the ability of
culture to work would allow humans not
to just accumulate Innovation but to
accumulate Innovation faster which we
did okay but the harder part might have
been to anticipate the stepwise nature
of that that is looking back we can see
and that happened in some discret
steps okay but looking ahead may you
might not have been able to anticipate
those discreet steps you might have just
been able to see the large shape of
acceleration of some
sort right I mean let's say Devil's
Advocate Dan comes and argues with the
and says look robin look at the academic
literature right we know that natural
selection accumulates genes and then
organisms ad to their Niche how are you
proposing that this magical thing called
culture is going to get you a single
species that starts occupying every
imaginable Niche how are you supporting
that with evidence I don't think that's
going to happen right and then what
would you say I'm happy to admit that
there's just a lot we find it hard to
predict about the future of AI That's
one of the defining characteristics of
AI it's one of the hardest things to
Envision and predict how where it will
go out I mean this is pretty important
to me because it sounds like you agree
that a version of your methodology
transported to the dawn of humanity
would be blind to what's about to happen
with Humanity right so I'm trying to
make the analogy where it seems like
maybe you today are blind to what's
about to happen with
a I'm happy to admit that we have a lot
of uncertainty but uh you'll have to
make the argument why uncertainty
translates into a 50% chance of us all
dying in the Next Generation yeah it's
it's almost like you're admitting you're
like look I have a methodology that
isn't going to be great at noticing when
there's going to be this huge disruption
but I'm mostly going to be right cuz
there mostly aren't huge disruptions am
I characterizing you right I mean
certainly if we look over history say
the last you know million years we see
relatively steady change you know
punctuated by accelerations of change
but even these
accelerations at the moment you can see
things accelerating and so that does
suggest that in the future we will see
accelerations but then we will see the
beginnings of those accelerations and
then we will see them speeding up and we
will be able to track
the acceleration as it happens that's
what history suggests about future
accelerations so in this thought
experiment the robin of the past is
going to observe a 10,000-year slice of
humans starting to have culture and be
like aha this is a new Dynamic this is a
new regime so you think you would notice
it then well it depends on you know what
stats we're tracking at the moment this
past Trend projected the future says
that the world together would start to
accelerate in its growth rate we are
definitely tracking World growth rates
in quite a bit of detail
and it wouldn't happen on one weekend
that is it would be say a five-year
period but there's something else in
this thought experiment that I think is
alarming which is that humans are taking
over the niches of other animals so
we're the first multi- Niche species or
the first Niche General species right
and that seems like something have other
species that were more General than
others but certainly we were unusually
General compared to most species but
there have been many other General we're
unusual and we're so General if we
really wanted to technically we haven't
done it yet but we could potentially
colonize the moon right we probably will
in the next
10,000 right so to me that's what's sing
is like I feel like you're not bringing
a methodology that's going to notice
when a new type of species is coming
that doesn't fit the
trends again the question is what are
you tracking and will those capture
whatever it is you expect to appear so
um in the past New Growth has happened
gradually
so that if you were in the right place
tracking the right things you would see
the new growth mhm so the key question
is how sudden or local could the next
growth spurt be compared to past growth
spurts that's the question we're asking
so you might go back and say well look
if you're tracking all the species on
earth except you know the big primates
you might have missed the humans
growing so uh you know what's the chance
you'll not track at a fine enough
resolution to see the key change even
humans we started you know a million
years ago so to double faster but that
was still only doubling every quarter
million years and you know you you would
still have to be looking at the
granularity of humans and the nearby
species and at their doubling time if
you were just looking at larger units
maybe all mammals or larger continents
you might miss that Trend so that you
would need you can raise the question
what trends will you need to be looking
at but that's why I might say look my
key priority is when will AIS take most
jobs so if I decide that that's the
thing I care about and that's the thing
I track there's much less of a risk that
I'll miss
it so you might counter argue that
that's not the thing to be tracking and
I'm happy to hear that argument but if I
think this is the thing I care about and
I'm tracking it I feel like I got my eye
on the thing Robin is saying sure maybe
a fum will start but we'll have time to
adjust when we see the job displacement
data picking up steam right okay but if
you're a tiger species used to having
your own ecological niche and suddenly
it's the late 1700s and you see the
Industrial Revolution starting and you
see the doubling time of the human
economy growing what do you do then I'm
just using tigers as an example because
humans drove the Tasmanian Tiger to
Extinction in 1936 via hunting and
habitat
destruction this also gets to what I was
saying about Robin's claim that the
world grows together note the tiger part
of the world didn't grow when humans
invented factories that part of the
world is going to shrink that tiger part
of the world is going to go extinct
unless tigers can adapt fast enough to
maintain their level of tiger power if
you're a tiger species you get a few
decades to react to the faster doubling
time you see in the data during the
Industrial Revolution but your
adaptation process your Gene selection
process takes hundreds of thousands of
years to react to environmental changes
by selecting for new adaptations so your
reaction time is a THX too slow I just
want to throw in the analogy that we
talked about before which is to what
humans did in evolutionary time it looks
like very much like a fume in that
usually species stay within a niche
right they don't explode and go take off
other niches because normally when they
do that the other species push back and
you have an ecosystem where all the
species can react in evolutionary time
you have an arms race in evolutionary
time the whole idea that humans are part
of an ecosystem and are kept in check by
the ecosystem that is no longer correct
right so would you be open to an analogy
where AI becomes not part of the economy
anymore but again we'll need to talk
more specifically about how and why
other species on the planet a million
years ago had very poor abilities to
Monitor and coordinate their actions
they didn't have councils observing
cyber attacks and Reporting crimes to a
Central Bureau of enforcement who looked
for things that could go wrong and
monitor okay Robin so in the analogy of
a tiger species what would it look like
to successfully react to humanity Robin
would probably say that the key is for
the tiger species to notice an
existential threat emerging all the way
back before the Industrial Revolution
before the human farming revolution at
the dawn of human forager tribes that
had culture so the key is noticing early
enough that disruption is on the way we
just have to be early enough to stop an
exponential okay but we know that's
tricky business right like trying to
eliminate Co in the early stages by
having everyone stay home for 2 weeks
it's theoretically possible but it's
hard and it's tempting to react way too
late if we run with this analogy my
disagreement with Robin becomes about
how much smoke we're already seeing
about an upcoming fire I claim that
being a tiger species witnessing the
dawn of human culture is analogous to
being a human witnessing the dawn of
deep learning in the 2010s or even being
a human witnessing the dawn of
electronic computers in the 1950s
JJ good already noticed in the late
1960s that based on the progress of
these computers we might be on a path to
an intelligence explosion leading to
catastrophic risks for Humanity so the
only difference between me and Robin is
that Robin thinks we have the luxury of
waiting until we observe that AI starts
automating jobs at a faster rate while I
think the jobs data won't give us that
reaction time I think we're already the
Tigers watching the humans building the
factories we are already seeing the
early stages of an intelligence
explosion that's about to disempower us
we are about to find ourselves like the
Tigers kicking themselves saying darn it
we should have put a lid on those humans
when their primitive tribes were
starting to sit around a campfire and
tell stories that's when we should have
acted to save ourselves we should have
noticed that those campfire stories that
forages were telling around a fire they
were a call to action for us Tigers then
we would have had time to evolve our
tiger genes to stay competitive with
human
genes that's what Robin is saying in
this analogy and now we move on to
talking about AI fum Robin says he
arrives at a p Doom of less than 1%
because he multiplies out a conjunction
of independent assumptions so first
assumption is an initially weak system
rapidly improves itself to become very
strong improving many orders of
magnitude next assumption we fail to
Monitor and stop it next the system has
a broad range of improved capabilities
over many domains next it has goals and
plans and it acts on them next it it's
not subservient to its creators or its
human Masters next its values change
from the values of its creators next
none of the other AIS notice and oppose
this ai's power grab that's a
conjunction of seven assumptions if each
assumption is a bit hard to believe say
only 50% likely and each is independent
of the other assumptions then the
probability of the whole conjunction is
below 1% so that's basically what Robin
is saying in this part it's a whole
bunch of elements Each of which is kind
of unlikely and then the whole thing
adds up to pretty unlikely but this kind
of conjunction argument is a known trick
and I did call him out on that there's a
known trick where you can frame any
position as a conjunction to make it
seem unlikely right so I could do the
opposite way and I could say hey if you
think fum is not going to happen then
first you have to agree that AI Labs
have really great security precautions
right and that government Regulators can
pause it at the times when it's on sfe
right I mean I could frame it as a
conjunction too right so that is a
little bit of a trick I I kind of object
to framing it as a big conjunction like
that right because I have a framing
where it just doesn't sound like such a
big conjunction which is so then your
framing would have to make what in my
framing look like independent choices
somehow be natural consequences of each
other like so maybe there's an
underlying event that would cause these
as a correlated event somehow but then
you'll have to tell me what's the
underlying event that causes this
correlated set of events all to happen
together right for example to take one
of Robin's claims that we won't
effectively Monitor and shut down a
rogue AGI that might be a questionable
assumption when taken on its own but if
you accept a couple of the other
assumptions
like the assumption that a system
rapidly improves by orders of magnitude
and has goals that don't align with
human values well entertaining that
scenario gets you most of the way toward
accepting that monitoring would probably
have failed somewhere along the way so
it's not like these assumptions are
independent probabilities when I reason
about a future where super intelligent
AI exists I'm reasoning about likely
Doom scenarios in a way that
simultaneously raises the probability of
all those scary assumptions in Robin's
list now we get into to why my Mainline
future scenario is what it is a major
loadbearing piece of my position is how
easy I think it'll be for the right
algorithm to blow way past human level
intelligence I see the human brain as a
primitive implementation of a goal
Optimizer algorithm and I'm pretty sure
there's a much better goal Optimizer
algorithm it's possible to implement and
it's only a matter of time before it is
implemented in Robin's world view he
agrees there's plenty of quote unquote
capacity above the human level but he
skep Le that a quote unquote gold
Optimizer algorithm with quote unquote
higher intelligence is a key piece of
that capacity that's why I'm asking him
here about Headroom above human
intelligence do you agree that there's a
lot of Headroom above human intelligence
look at the progress from ape to human
IQ does that dial turn much farther if
you want to call it like a thousand IQ
10,000 IQ that kind of thing well I'm
happy to say there's enormous Headroom
in capacity where I hesitate is when you
want to divide that capacity into I Q
versus other things definitely our
descendants will be vastly more capable
than us in an enormous number of ways uh
I'm extremely happy to grant that I'm
just not sure when you say oh because of
intelligence which part of that vast
increase in capacity that we're trying
to refer to in that sense in my
worldview a single human brain is
powerful because it's the best
implementation of a goal Optimizer
algorithm in the known universe when I
look at how quickly human brains started
getting big bigger an evolutionary time
after branching from other apes and
reaching some critical threshold of
general intelligence that's hugely
meaningful to me there's something
called eniz quotient which measures how
unusually large an organism's brain is
relative to its body mass and humans are
the highest on that measure by far I see
this as a highly suggestive clue that
something about what makes humans
powerful can be traced to the phenotype
of a single human brain sure humans are
still dependent on their environment
including Human Society so much of their
brain fun adapts around that but the
human brain reached the point where it's
adapted to tackling any problem even
colonizing the Moon is possible using
our same human brain and body
furthermore the human brain doesn't look
like a finished product while the human
brain is off the charts big it seems
like the human brain would have grown
even bigger by now if there weren't
other massive biological constraints
like duration of gestation in the womb
duration of infancy and the size
constraint of the mother's pelvis which
simultaneously has to be small enough
for walking and big enough for child
birth I see the evolution of human brain
size as a big clue that there's a steep
gradient of intelligence near the human
level I.E once we get the first human
level AGI I expect we'll see vastly
superhuman AIS come soon after let's see
what Robin thinks is there a steep
gradient of intelligence increase near
the human level it really seems like
natural selection with saying yes bigger
head bigger head oh crap now the mother
can't walk we better slow our role but
of
capacities and none of them are
obviously near fundamental limits but
I'm just again not sure which thing you
mean by intelligence exactly but I'm not
sure that matters here I mean hum don't
you think it was very important the the
genetic modifications that happened to
the human brain to separate it from the
ape brain weren't those extremely high
return and doesn't it seem like there's
a few more that can be done that are
also extremely high return if only that
head could fit through that
pelvis I'm not sure that there are big
gains in
architectural restructuring of the human
brain at a similar size but I am sure
that humans were enormously more able
than other animals again primarily
because of culture so we have been Gras
dally drastically increasing the
capacity of humans through culture which
increases the capacity of each hum so
far yeah and we are now nowhere near
limits certainly when you talk about
culture though you're holding the
genetics of the human brain constant but
I'm pointing out that it seems like
there was a steep gradient a high return
on investment of changes to the genes
and and changes to the phenotype like if
we just made the pelvis bigger so that a
bigger head could fit through doesn't
that Trend show that we could have a
significantly smarter human the same way
humans are smarter than
Apes um I'm just not sure what you're
talking about here but I'm not sure it
matters are so many ways to improve on
humans uh one way might be to have just
a bigger physical brain but that just
doesn't stand out as one of the most
promising or or important ways you could
improve a human but I guess but I have a
specific reason to think it's promising
which is that natural selection tried
really hard to keep increasing the human
brain until it got a you know the fact
that so many human babies historically
die in I mean sure so imagine a brain
you know eight times as big twice as big
on each axis a human brain could would
certainly have more capacity I don't
know how much more because it was just a
bigger brain Robin doesn't the human
brain as having this special
intelligence power compared to other ape
brains he just thinks the human brain is
better at absorbing culture compared to
apes and maybe the human brain has
picked up other specific skills that
Apes don't have Robin doesn't see a
single axis where you can compare humans
versus Apes as being smarter versus
stupider the difference between human
intelligence and ape intelligence
appears to me very significant right
like culture and science is not going to
teach a present day ape to be a useful
science ific contributor is that fair to
say no actually that is if okay if Apes
had had culture then they could have
done science that is it was culture that
made us be able to do science okay I
mean this is that's the key point I feel
like you you have tended to bite this
kind of bullet saying like maybe Apes
can be contributors so let let me drill
into that a little bit so imagine that
an ape is born today just an actual
modern ape and you put on a VR headset
on the ape so the ape grows up with like
a perfect simulation of really
enlightened ape culture could that that
ape then grow up to be a scientist among
human scientists it's not enough to put
a VR headset on an ape to make them
capable of culture that's not how it
works culture is the capacity for humans
or whoever to attend to the behavior of
others figure out the relevant things to
copy and then actually successfully copy
them that is what culture is and that's
the thing that we have that Apes don't I
don't know Robin have you ever heard the
phrase monkey see monkey do it sounds
like that fits your definition of
monkeys having
culture culture is the capacity for
humans or whoever
to attend to the behavior of
the thing that we have that Apes don't
this is pretty surprising Robin thinks
Apes just need to be able to copy one
another better and then they'd get
exponential economic growth the way
humans have had I don't get how Apes
that are really good at copying each
other give you ape scientists if you
have an ape who can copy human
physicists really well can that ape
invent the theory of relativity I don't
get why robin is so hesitant to invoke
the concept of doing General cognitive
work having a certain degree of general
intelligence doing optimization work on
an arbitrary domain there's obviously
more to it than making Apes better at
coping I got to replay you what Robin
said about culture for a third time
before we move on culture is the
capacity for humans or whoever to attend
to the behavior of
copy and then actually successfully cop
that is what culture is and that's the
thing that we have that Apes don't okay
and do you think that in some sense most
of the Delta of of Effectiveness in
going from the ape brain to the human
brain is increased capacity to absorb
culture do you feel like that's like the
key
Dimension it's not clear that a brain
one quarter the size of a human brain
couldn't have done it as well but there
were a number of particular features the
brain had to have in order to be able to
support culture and that was the human
superpower once you had that first set
of features that could support culture
then we could use culture to collect
lots and lots of more features that
enabled us to take off compared to the
other animals okay it seems like I'm
pointing out a mystery that I'm not sure
if you agree like the mystery of why
natural selection cared so much to make
the human head and brain as big as
possible like do you agree that there's
something that calls out for explanation
there definitely like brains were
valuable so clearly at that point in
evolution clearly Evolution was going
can I make get a burger brain here that
looks like a good deal let's figure out
how I can get a bigger brain I don't
reach the limitations like just can't
get a bigger brain but clearly plausibly
bigger brains be valuable it sounds like
you're saying okay you got culture you
don't need that much bigger of a brain
for culture we've got culture nailed
down so why do you think the brain kept
trying to get
bigger I mean a standard social brain
hypothesis is that um you know we had a
complicated social world and there were
large returns to more clever analysis of
our social strategic situations that
doesn't seem terribly wrong but brains
also probably let us use culture and uh
take advantage of it more so there's
just lots of ways brains are good so
this incredibly valuable thing Robin
thinks big brains do is clever analysis
of our social strategic situations his
words clever analysis is there anything
else we might use clever analysis for
besides our social strategic situations
something that confers a fitness
advantage how about using clever
analysis to design a better tool instead
of just copying a tool you've already
seen this clever analysis power that you
think potentially 3/4 of the human brain
is for why isn't that the key
explanatory factor in human success why
do you only want to say that culture is
the key as in the capacity to copy
others well a major Crux of disagreement
for Robin is whether my concept of
general intelligence is a key concept
with lots of predictive power and
whether we can expect big rapid
consequences from dialing up the
intelligence level in our own lifetimes
it's interesting to see where Robin
objects to my explanation of why we
should expect rapid intelligence
increases going far beyond the human
level let me introduce a term that I
coined called goal completeness goal
completeness just means that you have an
AI that can accept as input any goal and
then go reach that goal and it's not
like the whole architecture of AI is
only for playing chess or only for
driving a car it can just accept
arbitrary end States in in the physical
Universe assume we've got something with
gold generality then one now that we've
identified a dimension right like the
effectiveness of a goal complete AI
is it possible that the AI will just get
much better on this Dimension the same
way that just a few genetic tweaks just
a few scaling in size I believe that it
made humans much better than Apes at
this goal completeness optimization
property I would say eventually our
descendants will be better at
identifying understanding and completing
goals sure when we identify some
Dimension that humans and animals have
and then we try to build technology to
surpass humans and animals on that
Dimension we tend to succeed quite
rapidly and by quite a
right so like on on transportation for
example if you just identify hey how do
you get somewhere as fast as possible
and transport as much weight as possible
at this point we're far beyond anything
biology ever designed right is that a
fair Rob I continue to say yes our
descendants will be vastly better on us
in a great many capacities but yeah but
it's not just our descendants if you
look at the actual rate of this kind of
stuff it seems like the progress happens
pretty fast if you got to continue the
trend of how the brain of humans differs
from the brain of Apes by letting it
grow bigger which Evolution didn't have
that opportunity if you just continue
that Trend you might have something much
smarter the only thing that improves is
the mapping between a goal you can give
it and the quality of the action plan
the effectiveness of the action plan to
achieve that goal right the optimization
power but why isn't that just a summary
of our overall capacities I'm trying to
convince you that there's a dimension
the optimization power Dimension and I'm
trying to show you arguments why I think
that Dimension could be scaled very very
rapidly right within like let's say a
year something like that right much L
than a 20e economic doubling time are
you buying that at all you're trying
somehow to factor this thing out from
the general capacity of the world
economy yeah exactly because I think it
naturally does factor out the same way
you can it's like look Transportation
right optimizing the speed of
Transportation doesn't that factor out
from the world economy well you can
identify the speed of Transportation but
just making Transportation twice as good
doesn't make the world economy twice as
good so you're trying to argue that
there's this factor that has a self
Improvement element to it the way
Transportation does it right if you make
Transportation factor that doesn't make
it that much easier to make
Transportation easier let's not even
talk about self-improvement if evolution
got to make the brain twice as big
that's not even self-improvement that's
just a path of tweaking genes and
getting a little bit of consequentialist
feedback I guess but I mean it's it's
almost like copying and pasting the
algorithm to generate brain regions
might have worked so what worries me is
I see a lot of Headroom above
intelligence and I'm expecting that
within a few years maybe 20 years if
we're lucky or even 30 but very posibly
3 to 10 years within a few years we're
just going to have these machines that
play the real world as a strategy game
the same way stockfish plays chess they
just tell you what to do to get an
outcome or they do it themselves and at
that point no matter what happens no
matter what regulation you do no matter
how companies cooperate no matter
whether it's multipolar whatever it is
you now just have these agents of chaos
that are unstoppable if they try right
that's roughly what I'm
expecting um I have doubts about that
but I still haven't seen you connected
that to the fum scenario so how do
agents of chaos make fum well I tried
but I couldn't convince Robin that we're
about to rapidly increase machine
intelligence beyond the human level he
didn't buy my argument from the recent
history of human brain Evolution or from
looking at how quickly human
technological progress surpasses nature
on various
Dimensions Robin knows it didn't take us
that long to get to the point where an
AI pilot can fly an F-16 fighter plane
and dog fight better than a human pilot
but he doesn't expect something like
general intelligence or optimization
power to get cranked up the way so many
specific skills have been getting
cranked up we moved on to talk about
what a fum scenario looks like and why I
think it happens locally instead of
pulling along the whole world economy I
have doubts about that but I still
haven't seen you connected that to the
fum scenario so how do agents of chaos
make F imagine that gbt 10 says oh you
want to make more money for your
business great here's a script you
should run the script and then they run
the script but of of course it has all
these other ideas about what it should
do and now it's Unstoppable and it
wasn't what you truly meant right so
that's that's the the misalignment
scenario right that's not can it end the
world it's will will it end the world
and we're talking about alignment when
we are having a discussion about
alignment which generally pre assumes
strong AI capabilities Robin didn't want
to run with the premise that you have a
single AI which is incredibly powerful
and can go Rogue and outmaneuver
humanity and that's the thing you have
to align he didn't want to run with that
premise so he kept trying to compare it
to humans giving advice other humans
which I don't even think is comparable
why is it such a problem if we have not
entirely trustworthy but pretty good
advisers so the problem is that these
advisers output these pointand shoot
plans that you can just press enter to
execute the plan and the plan is just so
consequential you know it's it's this
runaway plan and it makes sub agents and
it has huge impact and you just you
basically just have to decide am I going
forward with this or not and you get
disempowered in the process so
businesses already have many consultants
they could hire each of who could give
them advice on various things some of
which advice could go wrong why will
that whole process suddenly go wrong
when AIS are the advisers if if you're
already accepting the premise that
they're that powerful you really just
need one person to run an agent that's
not great for whatever reason and then
you're screwed even if it just pulls
forward a few Decades of progress into a
few months that's already a very chaotic
environment but all these other people
with their AIS could be do pushing
similar buttons creating their
increasing capacities but their you know
rivalis capacity so now we go to Robin's
argument that it'll probably be fine as
long as everyone is getting an
increasingly powerful AI at the same
time in my view there's going to be some
short period of time say a year when
suddenly the latest AIS are all vastly
smarter than humans we're going to see
that happen in our lifetimes and be
stuck in a world where our human brains
no longer have a vote in the future
unless the AI still want to give us a
vote in Robin view it's just going to be
teams of humans and AIS working together
to have increasingly complicated
strategic battles but somehow no
terrifying scenario where a rogue AI
permanently disempowers Humanity I agree
this crazy situation can happen but I
don't see how that works out with
Humanity still maintaining a reasonable
standard of living I'm really failing to
understand um what you're worried about
here you're just imagining a world where
there's lots of advisers and Consultants
available and that that goes wrong
because one of the things these advisers
could do is advise you to push button
that then increases the capacity of
something you control which most people
would typically do now and probably do
then and that sounds good because we
just have all these increases in
capacity of all these things in the
world is this your main point that a
world of people listening to AI advisers
could go wrong because they could get
bad
advice I think that's not a good
characterization because get bad advice
it seems like there's going to be a step
where the human slowly takes the advice
but more realistically it's more like
the human auth bad projects under the
advice of an AI say sure but the
authorization step is Trivial right it's
just somebody pressing enter essentially
right so the issue is just what the AI
does when it has all this capacity to
decide what to do and then do it and
then repeat AI agents who need to get
approval from humans for budget and
powers of
authorization sometimes they get powers
and authorization that maybe would not
be good advice but some humans are
stupid but then what so some projects go
wrong but there's a world of all the
other people with their AIS protecting
themselves against that right so if the
scenario is that the AI gets it into
this head of like okay I need to go
build myself the most energy to support
my project so I'm just going to beg
borrow or steal I'm just going to do
whatever it takes to get energy and if
an AI is so much smarter than Humanity
you can imagine simultaneously
manipulating everybody on the internet
right so suddenly it's like winning over
humans to its cause right it's it's
doing massive effects I mean it's got
well if there was only one of them but
if there's billions of
them at similar levels then Robin
Segways into a common claim of AI non-
doomers that today's corporations are
already super intelligences yet humans
manag to benefit from coexisting with
corporations so shouldn't that make us
optimistic about coexisting with super
intelligent AIS it's important to noce
we live in a world of Corporations which
in a important sense are super
intelligent that is compared to any one
of us corporations can run circles
around us and Analysis of marketing
product design and all sorts of things
so each of us as an ordinary human are
subject to all these potential super
intelligences trying to sell us products
and services or hire us for jobs and how
is it that we can at all survive in this
world of these very powerful super
intelligences trying to trick us and
they do they do try to trick us they try
to trick us into bad jobs buying bad
products Etc all the time and they weigh
out classes that is whenever a
corporation sitting there thinking about
how to make a commercial how to design a
product to trick us into wanting it
they're just so much better than we are
when we think and reason about GE do I
want this product right of course the
key difference is that corporations are
only mildly super intelligent if they
tried to overthrow the government they'd
still be bottlenecked by the number of
humans on their team and by the
optimization power of the brains of
their human employees still Robin argues
that competition can keep super
intelligent a in check competition is is
a great discipline of not only
corporations but also governments
Democratic competition lets us try to
judge among different people try to run
the government commercial competition
lets us judge among different people who
we might buy from or be employed by
there's thing that makes our world work
today is competition among Super
intelligences Why can't AIS function
similarly so in our world today the
reason why a corporation doesn't run
away and start destroying the world in
order to get another dollar out of you
one reason is just because they know
that the combination of other actors are
then going to push back my scenario is
that that balance is going to be
disturbed we're going to have agents
that they think that they can go do
something which they know that humans
would love to push back on but they also
know that it's too late and the humans
can't push back on it are there such
things in our world or if not why does
this world have such things and our
world Now does not the key difference is
just that you're going to have an agent
that has more of this optimization power
right like bringing a band of modern
humans into like an ancient tribe or an
ape tribe but these super intellig
corporations already have vastly more
optimization power than we do okay but
there's still a difference between
mildly super intelligent and very super
intelligent it's a big difference when I
talk about a super intelligent AI I'm
talking about something that can copy
itself a billion times and each copy is
better than the best human at everything
much better you think it's impressive
that Einstein came up with special
relativity and general relativity in the
same decade I'm expecting super
intelligent AI to spit out the grand
unified theory of everything the moment
it's turned on we're not dealing with
Walmart
here next we come back to the question
of how fast a single AI will increase
its capabilities and how humans can
monitor for that the same way Tigers
would have liked to monitor humans we
should continue to monitor for ways that
could go wrong with the AIS but um I
don't see a better solution than to wait
until they're here and start to look for
such things I think it'll be very hard
to anticipate them far in advance to be
able to guess how they could do things
wrong and and try to prevent them for
most Technologies the time to to deal
with their problems is when they are
realized in a concrete enough form that
you can see concretely what sort of
things go wrong and you know track
statistics about them and do tests about
various scenarios and that's how you
keep technology in check is through
testing actual concrete versions and the
problem at the moment is AI is just too
far away from us to do that we have this
abstract conception of what it might
eventually become but we can't use that
abstract conception to do very much now
about problems that might arise we'll
need to wait until they are realized
more again I don't know how this kind of
thinking lets tiger species survive the
human F in evolutionary time because by
the time they concretely observe the
Industrial Revolution it's way too late
for their genes to adapt Robin's
position is that if rapid intelligence
increases is a threat we shouldn't hope
to stop it in advance we should react to
it when we see it I think he's making a
very optimistic assumption about how
much time we'll have at that point he's
banking on the hope that there won't be
a rapid intelligence increase or that a
rapid intelligence increase is an
incoherent concept I don't know what to
tell you it seems perfectly coherent and
likely to
me next we're going to talk about
whether humans can hope to participate
in AI Society even if the AIS are much
smarter than we are I think the answer
is obviously no but let's see what Robin
thinks mostly when we travel around the
world the thing we check for in whether
we'll be safe somewhere isn't some poll
of the people and their values and how
aligned those values are with our values
that's that's not how we do it right we
have systems of Law and we check for
statistics about how law often is
violated and then we think I might be at
risk if law is violated here a lot
either's a high crime rate and so to the
extent that you can assure that the
kinds of laws there are the kinds that
would pun people for hurting you let you
sue for damages then low crime damage
rates would be enough to convince you
that it looks okay there so what about a
scenario like my grandpa's family where
they were Jews in Poland right so it
seems like the idea that the society is
going to respect the rule of law for
them sometimes breaks sure sure uh there
are sudden changes in law that you might
uh have to look out
for right so and how do we characterize
when does a weaker demographic tend to
get if you're going to visit somewhere
for a week as a travel that's less of
isue you're planning on moving there for
retirement you'll need to make longer
term estimates about the rule of law
that's true that a holocaust is is only
going to be a longer term effect so you
can always kind of dip in and out to
places that have the rule of law for
long time even in 1940 35 it might have
been okay to just visit Germany for a
week right on a trip okay okay fair
enough but you really think that AI
we're going to be able to like have
money that the AI accept and you know
and have property that they respect that
that's like your Mainline scenario it's
also true for M's as well as AI I think
one of the biggest risks is that because
we think of them as a separate category
we end up creating separate systems for
them separate legal systems separate
Financial systems separate political
systems and in a separate systems then
they are less tied to us and then when
they get more powerful than us then they
may feel less about disrupting us I
think the more we are sharing systems of
governance Finance employment
relationships Etc then the more
plausible it is that they would feel
ties to us and uh not try to kill us all
yes so I don't want these separate
systems I want shared mixed up systems
that's just a general robust strategy
through all of human history for
limiting harm between groups of people
Robin is generally great at thinking
about human society but he's just not
accepting the premise that if there's a
vastly higher intelligence than you it
doesn't need the concept of being in a
society with you if you're a Jew and
Poland in 1940 if you don't have
somewhere else to escape to you're not
going to be saved by the rule of law if
you're in a weak faction and there's a
stronger faction than you it's up to
them whether they want to cut you out of
their legal system their justice system
their economy and so on and thinking
back to Robin's comment that okay you
could be a Jew and travel to Poland in
1935 for a week okay but what's the
relev of that to AI Society I mean AI
Society is going to last a long time
right so I don't get it like humans are
clearly going to be cut out if the AI is
much more powerful than us and it's not
aligned so I just don't see how rule of
law gets around the alignment problem
unless you just assume away the premise
that there's a huge intelligence gap
which Robin does right but I guess
that's the only Crux so I don't even
know why we're having a discussion about
the rule of law when the only Crux is
whether there's a huge intelligence Gap
lastly we talk about the feasibility of
aligning super intelligent AI open AI is
admitting that rhf is not going to cut
it when the AI becomes super intelligent
anthropic is admitting that does it
concern you at all that they're
admitting that alignment is an open
problem and rhf doesn't
scale uh not especially that's the easy
thing to predict yes of course all of
these firms are going to admit any sort
of criticism they are eager to show that
they are concerned I mean in the modern
world almost no firm with a product that
some people think is risky wants to
pretend oh there's nothing to worry
about there that's just not a winning PR
stance for any company in our modern
world so yes of course they're going to
admit to whatever problems other people
have as long as that's not a current
problem with their current product to
worry about it's some hypothetical
future version of the product they're
happy to say yeah you know that'll be
something we'll be watching out for and
worrying about wow I really should have
pushed back on this sure we can dismiss
what the AI labs are saying but it's
pretty obvious why rlf really won't
scale to Super intelligence the feedback
loop of whether a chatbot has a good
answer doesn't scale when the topic at
hand is something the AI is much better
at than you'll ever be or when the AI
shows you a giant piece of code with a
huge manual explaining it and asks if
you want to give it a thumbs up or a
thumbs down that's not going to cut it
as a strong enough feedback loop for
super intelligent alignment if IID
pressed Robin to give me a more
substantive answer about rhf I think he
would have just said it doesn't matter
if it's not perfect we'll just augment
it or we'll find some sort of iterative
process to make each version of AI be
adequate for our needs that's what Robin
would probably claim even though the
safety teams at the AI labs are raising
the alarm that super alignment is an
important unsolved problem but I think
Robin would acknowledge that many of his
views are outside the current mainstream
like he doesn't mind predicting that AGI
might still be a century away when most
other experts are predicting 5 to 20
years so again it comes down to the Crux
where Robin just doesn't think there's
going to be a huge intelligence gap
between AIS and humans at any point in
time so he's just not on the same page
that super alignment is a huge open
problem last question what is the event
or threshold or warning shot that would
make you concerned about rapid human
extinction from
AI I mean the main thing I'm going to be
tracking is automation of jobs so if I
start to see a substantial uptick in the
rate of which job tasks automated
happens that will correlate of course
with a substantial tick up in the amount
of Revenue going to the companies
supplying that Automation and the
supporting infrastructure that they pay
for then that would show a Devi a from
Trend and then I want to look more
closely to see how fast that's
accelerating and in what areas that's
accelerating and that would be the
places we should all watch more
carefully uh watch carefully wherever
things are deviating from Trend but
again I still think there won't be much
we can do at the abstract level to
constrain AI to prevent problems we'll
mostly have to wait until concrete
systems have concrete behaviors and
monitor those and test them uh and the
more capable systems get the more we
should do that well I hope we get to see
10x more jobs being done by AI while
still leaving an extra decade or two of
time before AI is truly super
intelligent and there's no taking back
power from the AIS I think that's a
reckless approach to hope things play
out that way but Robin doesn't see it as
Reckless because he doesn't see
intelligence as a single trait that can
suddenly get vastly higher within a
single mind the good news is I feel like
I'm accurately telling you why Robin and
I disagree about P Doom which was my
goal in my opinion this debate
successfully identified the cruxes of
disagreement between our two views Crux
number one I think intelligence I.E
optimization power is a single Dimension
that can be rapidly increased far beyond
human level all within a single mind
Robin has a different model where
there's no single localizable engine of
optimization power but capabilities come
from a global culture of accumulating
and diffusing
Innovations Crux number two Robin thinks
we can look at data from Trends such as
job replacement to predict if and when
we should be worried about Doom I think
it'll be too late by the time we see
data unless you count the very early
data that we're seeing right now lastly
we reflect on how I presented my side of
the argument it would have been good
ahead of time if you had presented the
scenario you're most worried about maybe
I could have read about that ahead of
time thought about it and then we could
have dived into the particular scenario
because I'm still not very clear about
it it isn't the same food scenario as I
thought you might be focused on there's
a value in general in all these
conversations where somebody summarizes
their point of view as concisely as they
can and thoughtfully and then the other
party reads it maybe you should write a
little Essay with the scenario you're
most worried about lay it out as clearly
as you can and maybe that would be a way
to refine your thinking and then be
easier for other people to talk to you
Alazar has pointed out a few times that
from the Doomer point of view doomers
are just taking the simple default
position and all we can hope to do is
respond with counterarguments tailored
to a particular non- Doomer objections
or else write up a giant fractal of
counterarguments the giant fractal write
up has been done it's called AIS safety.
info check it out the simple default
position is what I said to Robin as my
opening statement we're close to
building superintelligent AI but we're
not close to understanding how to make
it aligned or controllable and that
doesn't bode well for our species
Robin's particular objection turned out
to be that intelligence isn't a thing
that can run out of control in his View
and that mainstream talk of a rapid path
to Super intelligence is wrong in his
view I think our debate did a solid job
hitting on those particular objections
I'm not sure if explaining my view
further would have helped but I'll keep
thinking about it and I'm still open to
everyone's feedback about how to improve
my approach to these Doom debates I love
reading your comments critique of my
debate style recommendations for how to
do better suggestions for who to invite
intros and any other engagement you have
to offer it's been great I look forward
to seeing you on the next episode of
Doom debates