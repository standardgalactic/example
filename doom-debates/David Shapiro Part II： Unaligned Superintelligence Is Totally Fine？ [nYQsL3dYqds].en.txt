personally I think the Doomer movement
it doesn't ask itself the question okay
what how do we actually uh coexist with
super intelligence how do we actually
align humanity and machine interests so
that it is a mutualistic relationship
the absence of that conversation from
the safety Community is why I had to
leave the safety Community what are you
talking about we ask that question all
the time
welcome to Doom debates I'm Lon shapira
so I recently did a video where I
counter argued against David Shapiro's
claim where he says pausing AI is a
terrible idea and I say no it's the best
idea we have after I publish my reaction
episode David was gracious enough to
post on his YouTube channel he said
finally a good takedown video and he
link to my videos so super gracious I
usually don't get that kind of response
from people I argue against and then he
came onto the channel left a couple
comments and he's made a couple videos
afterwards on the topic so it's nice
that he's engaging in the debate I mean
one of the goals of Doom debates is to
just get a debate going I think debate
between two sides of the argument is
sorely lacking it's usually just people
talking to their own sides so I really
than and respect him for that I was also
pleasantly surprised to see that David
went into the comments of my video and
there was somebody asking if he would
debate me and he said of course and I
was like like oh sweet great so I dm'd
him on Twitter to set up a debate time
and he said yeah maybe in the next few
days it seems like since that time
things have taken a turn for the worse
because he started tweeting things like
he's not interested to engage with
doomers he doesn't know if it's
productive but then he posted a survey
on his YouTube where he was asking his
viewers hey should I debate the the
doomers and like two-thirds of people
said yes you should but he still
messaged me that he's backing out so he
doesn't want to debate okay I mean you
know it's it's ultimately up to him uh
so today I go to Twitter I just check
the latest things he's tweeting about
and he blocked me so I don't think it's
because of anything I did because I
literally didn't do anything between the
time that he was talking about debating
me and blocking me but look here on Doom
debates I'm not here to try to answer
the question of what's his deal or
what's up with people like that I'm just
here to look at the arguments and help
you guys decide whether we're doomed or
not so it's really not about the person
it's not about anybody making ad homy
attacks so with that let let's just dive
straight into the content let's see what
his latest arguments
are hey folks so I have recently kind of
come out as uh more on the
accelerationist side and more against
the Doom SL Dell SL pause movement um
and there was actually a really great
response um by Lon Len Lon sorry Lon
shapira um not related at least as far
as I know so anyways uh one of the first
things that I wanted to spell is that a
lot of people don't seem to know where
I'm coming from um and that is because
when I started doing my research I was a
lot less famous than I am now um so I
want to remind everyone that I wrote a
book called benevolent by design six
words to safeguard Humanity I have taken
this argument very seriously for quite a
few years now um and so what I want to
say is we actually agree on a lot of the
premises um and so one of the one of the
fundamental uh agreements that we have
is that super in Ence is fundamentally
uncontrollable or it is an intractable
problem to control super intelligence uh
and that's actually the premise of this
book which is uh control the corage
ability problem is unsolvable so don't
bother trying to solve it I actually
don't claim to know that I think it may
be possible to build a corrigible AI an
AI That's just good at not getting too
hardcore an AI That's good at taking
your request and doing it using a
reasonable amount of resources and not
spawning a bunch of child agents and not
acquiring power and resources and just
kind of observing when you think it's
going too far and then trying to roll
back its impact trying to gracefully
shut down I think that that is in
principle possible for us to build it
certainly exists in the space of all
possible minds or very likely exists in
the space of all possible Minds so I
don't think that corrigibility is
impossible I just acknowledge that
corage ability is an unsolved problem
it's an open research problem and it's
an unknown level of difficulty so it
could be a multi- deade research problem
it could be as David thinks impossible I
don't know but if you look at our AI
Labs today nobody has a corage ability
plan that everybody agrees like oh yeah
that's a good plan that's going to make
AIS controllable so we're kind of on the
same page but it seems like David's
making a stronger claim that it's like
forever unsolvable so that's I just
wanted to point out that distinction
instead we need to look at values we
need to look at incentive structures and
we need to look at other uh forms of
alignment and I've got more videos
coming out about um how to actually
solve this problem uh if you can't make
an AI corrigible if you don't have
enough Insight if you don't have enough
of a theoretical understanding to make
it be able to stop to make it have an
off button but you think you're going to
be able to control it using incentive
structures that seems like a Fool's
errand if you think you can control it
by making it load the right values and
you've got some technology to make it
load the right values even without being
cordable I could see that as a path to
working potentially even as a more
robust path that's kind of the Holy
Grail is for an AI to optimize the
universe according to human values but
just right off the bat I'm skeptical
when he says that he's going to make AI
good using quote unquote incentive
structures but let's go on um so my two
primary uh uh criticisms for this
takedown video which again thank you for
making it um is that one there's a lot
of appeal to quote social proof so uh
appealing to social proof is called um
uh that is the appeal to Authority so
appeal to Authority is a logical f
policy where it's like oh hey an expert
says but there's not really any evidence
and so that's what I mean by epistemic
and ontological grounding I thought I
was careful to point out that the only
times I was invoking social proof like
hey most people are worried about AI
Doom hey look there's a bunch of
signatories some of the top AI
researchers in the world 2018 touring
Award winners leaders of AI labs when I
was invoking that kind of social proof
it's not because I think we should just
debate based on social proof that's the
opposite of what I'm about it was
responding to your own attempts to
debate by social proof but yeah you're
right let's just both not talk about
social proof I'm fine with that there's
not any evidence that shoggoth is
emerging from these machines there's not
even any evidence that they're
uncontrollable um and that they will
continue to be uncontrollable so that
that grounding is Alone um a little bit
kind of dubious um because I'm not even
sure that I believe that uh super
intelligence is fundamentally
uncontrollable um at least that people
are going to imagine that it's going to
wake up one day and go Skynet on us wait
you're not sure super intelligence is
fundamentally uncontrollable can we go
to the
tape one of the fundamental uh
agreements that we have is that super
uncontrollable I guess he spoke too
strongly okay so what's your actual
position um certainly that is that is
the premise from which I wrote this book
is just hey we're going to lose control
um you know the best the analogy that I
use is that the best dog is one that
needs no leash um and then if you if you
create this new successor species or
this new
superorganism um with dignity and
respect uh and we align our interest
together in a symbiotic fashion uh or a
mutualistic fashion that's the way to go
a dog that needs no leash is a good
description of a value aligned Optimizer
so yeah go ahead and run wild value
aligned Optimizer dog because the place
you'd want to run to is the ultimate
place where we want to take the future
so yeah that is the Holy Grail right
that's coherent extrapolated volition
you almost by definition can't do better
than that it's just there are many
reasons why we're not on track to do
that and we have unsolved theoretical
problems before we can build that and
instead what we're going to build is
something that drives the universe off a
cliff kind of or bricks the universe but
yeah dog with no leash sure that's the
Holy Grail uh the other primary
criticism that I have of this video is
the Nirvana fallacy so the Nirvana
fallacy says that any solution other
than something that is perfect is
categorically a failure um and this this
takedown video repeatedly just says oh
yeah like I agree we should do all these
other things but also we we must pause
anything short of pausing that is the
only way to save Humanity it is not the
only way to save Humanity that seems
like a misleading characterization of
what I'm saying I'm saying we are doomed
if we don't pause AI so when you talk
about the Nirvana fallacy you're usually
saying the perfect is the enemy of the
good so people who commit the neana
fallacy they're like we need to hold out
for something perfect we shouldn't
accept something good in my case I'm not
saying that if we don't pause AI we'll
have something merely good I'm saying
that we're going to have something
terrible like the most terrible thing
ever and the only way that we even have
a reasonable hope of not throwing the
universe at a dumpster is by pausing AI
until we get some inkling as to how to
make an AI that doesn't destroy the
universe before it's too late if I were
claiming that we should pause AI rather
than doing some particular next best
alternative which is still pretty good
if I thought look we could develop
Reckless AI and it would have a 5%
chance of killing us but we should have
a 0% chance of killing us if that were
my position you could accuse me of
committing the Nirvana fallacy you could
be like you should accept the 5% and
plow through you're not weighing the
balances properly you know the perfect
is the enemy of the good but that's not
what I'm saying I don't think that we
have a next best alternative where we
have a reasonable chance of surviving I
think there's only one non- Reckless
alternative which is to pause Frontier
development through International treaty
I think that's the only way that a
mature civilization Acts like and I
think that we're deceiving ourselves I
think that we're so conditioned by
status quo bias by The Familiar way that
Tech usually progresses I think we're so
conditioned to be like surely we have to
let this develop surely a pause is this
insane drastic measure and I think it's
important to reframe all that and to be
like look a sane civilization would be
able to have the self-reflection to
realize that this is another nuclear
type scenario it's approaching fast and
we don't have the ability as a species
to navigate developing this technology
without fundamental research that is a
reflection on ourselves that we have to
come to terms with and a more mature
species would treat this as obvious it's
not that hard of an Insight it's purely
just fighting biases to see this Insight
anyway you're welcome to disagree with
me but don't characterize my position as
a Nirvana fallacy when that's just
objectively not an accurate description
of what I'm
saying that that is an axiomatic
faith-based assertion which is why I
continuously call this a prophecy it's a
doom prophecy instead says hey believe
me you know without without any physical
evidence without any scientific evidence
Doom is coming Doom is inevitable and
only I have the solution there is only
one single solution that is a highly
problematic uh place to be in so
according to David there's a simple
four-point test you can do to tell if
something is a quote unquote prophecy
number one it predicts Doom number two
it's not based on physical evidence
number three it's based on faith number
four the person making the claim says
only he has the solution so if something
matches all those four points according
to David if I'm understanding correctly
if I treat his words as being meaningful
then that qualifies as something to be a
prophecy and then what does it mean to
be a prophecy presumably it means we can
just like ignore it and go on our way
presumably it means that we don't have
to use it for decision- making so let's
see if by David standard what I'm saying
about Doom counts as a prophecy number
one it predicts Doom yep yep I'm
predicting Doom okay number two it's not
based on physical evidence uh no that's
wrong it is based on physical evidence
when I tell you hypotheses like the
concept of intelligence I tell you how
it has explanatory power I get to make
predictions like hey you're going to see
bigger domains and you're going to see
the AI doing better than humans and
better than itself in the last
generation at a broader range of domains
so you could tell me a video game that
the AI is never played yet and I can
predict that it's probably going to do
better at that video game soon because I
can predict that AI progress is is going
fast AI progress is an example of a
simplification it's a compression of the
universe it's it's a theory that AI
progress is a coherent concept similarly
it's a theory that optimizing the
universe better than humans is a
coherent concept science is building
models that compactly explain the
observations around you and then those
same models can be used to predict
future observations because the the
models are generally time invariant so
without knowing anything else just by
having this model that's such a good fit
to the present you might have strong
reasons to believe things about the
future so in the case of Doom the fact
that we're using Concepts that are so
powerful at describing the present like
hey you have a domain General Ai and
then you make simple inferences on those
mental models like okay a domain General
AI what's the limit to its skill level
is it going to be limited by the amount
of computations that the human brain
does oh no we have a bunch of principles
telling us that it's possible to compute
much faster than human brain okay what
about the algorithms it's running is it
possible to run much better algorithms
to draw conclusions than humans run yes
of course human algorithms were just
optimized to help them survive and
reproduce and they're not very
differentiated from apes and humans that
have 170 IQ are barely differentiated
from humans that have an 80 IQ they're
just a few genetic modifications or a
few neuronal modifications away a few
brain architecture modifications away so
anyway so these These are the kind of
things that we go through when we talk
about having evidence for the conclusion
of Doom which brings us to the next
point from David that uh it has to be
based on faith to be a prophecy so
that's something that I can just tell
you easily is not based on faith because
generally when somebody believes
something they are happy to tell you I
believe this I have faith in this and I
will keep believing it no matter what I
see whereas I'm explicitly telling you
that I'm convincible otherwise I've have
mentioned before things that I could see
that would be evidence that would sway
me away from the conclusion of Doom for
instance if somehow we go and build
something which is a better Optimizer
than humans you can give it tasks like
run this business or run this
construction project like go build a
large building and it actually does it
it just manages the whole project it
manages a crew of robots to do it and
it's able to navigate all kinds of
random unforeseen obstacles or better
yet do do a space mission because that's
less of an interpolation between other
projects that have already been done
even though I'd already be quite
impressed with independently doing a
building project because that does
involve all types of different resource
management and problem solving so anyway
if we get into a world where you can
just hire an AI CEO to outperform a
human CEO and in that world the AIS are
still friendly and controllable and it's
not just because they're human
simulations but they're actually running
on a fundamentally different
architecture but they're accomplishing
goals better than humans and yet they're
not recursively self-improving they're
not uncontrollable they're not scaling
up Terror attacks such that attack
overwhelms defense if we ever live in a
world like that that would be one
example where I would take that as a
very large update toward like oh wow for
some reason this equilibrium exists that
I think is impossible cuz I don't see
that state as being an equilibrium I
think it's too close to an attractor
state where you have some AIS that just
go really hardcore and they just become
Unstoppable it's too much chaos like why
should the internet stay up when there's
a battle for resources for every nook
and cranny of every computer chip by
these super intelligent viruses those
are the kind of things I see disrupting
the equilibrium anyway the point about
claiming faith is that Faith isn't even
a property of somebody's argument it's a
property of their methodology for up
their beliefs so if a person you're
talking to is going to refuse to update
their beliefs no matter what happens
that's consistent with having faith if
the person is being very clear that
there's conditions that are
evidence-based under which they will
update their conclusions that's not a
faith-based situation so already we're
failing Point number three of being a
prophecy according to David's list and
then finally Point number four the
person making the claim says that only
he has the solution am I the only person
who has the solution to how to avoid
being doomed by a I no actually I think
it's a pretty well-known solution right
you can literally Google for it and the
solution will be two words pause AI it's
a pretty simple solution so you don't
have to come worship me you don't have
to come and tithe 10% of your income to
me or to pause a.info you can just
independently go and pause AI because
that's literally the only thing we're
proposing so I don't think it counts as
a prophecy when this publicly available
strategy how to survive is just being
repeated by me that doesn't make me the
profit okay what I just did breaking
down David's claim into four points like
that I feel like that's deeper than he
himself has thought about his own claim
so you might think that it's Overkill
that I'm doing it but look maybe I'll
help him think through his own points
like I don't know what to tell you I
agree that they seem like sloppy points
so anyways uh at the end of the video he
invites me to a debate I don't really
believe in debates I believe in
dialectics um but my point here is I
want to show where I came from um I'm
going to make sure that there's links in
the description for the free version of
this book um the EB is up on Barnes &amp;
Noble you can buy the paperback if you
want I only make like 20 cents per copy
sold and there's also a free copy on
GitHub um so like yeah let's let's have
a discussion and what I'll where I'll
end this video is saying that my purpose
here um this is actually a very
deliberate like narrative thing that I'm
doing here and that is that the safety
Community the safety argument is so
flimsy I want it to be better um also
the accelerationist argument is too
lunatic like there's too many skito
posting and in the in the
accelerationist community I want to
strengthen both arguments and the point
of a dialectic is not to say I'm right
you're wrong that's a completely
pointless use of time but applying the
label faith-based prophecy is worth your
time huh we all have the same goal and
that that attractor state that we want
is one where humans continue to exist
indefinitely ideally with human
population increasing indefinitely into
the future and number two where human
well-being continues to increase
indefinitely into the future we agree on
all those premises I think that's
roughly accurate that we want humans to
flourish in some sense probably a
transhuman sense I mean I don't know how
attached he is to The Human Condition
compared to the transhuman condition
right because he's not using the most
precise language uh also when he says
attractor State I don't know if he's
being precise about attractor in the
sense of a state that we have reason to
believe is stable that leads to other
states more like it and that other
states unlike it lead to it right that's
kind of a precise use of the term
attractor state within like a space of
possible States I don't know if he's
being precise when he says this is what
we agree on but I'm willing to go with
the vibe of what he's saying that like
yeah I'm sure we have a lot of common
ground and we don't disagree on
literally everything right we just
disagree on a lot of key cruxes like the
Crux of whether we have a chance of
realistically controlling AI or whether
we have a chance of making AI that has
human aligned values in the next couple
decades those are like important cruxes
but I agree that we also probably have
some major Common Ground too um the
disagreement is over how the methodology
to get there and also perhaps maybe the
decision framework or the risk profile
but again I think we actually agree more
on the risk profile than we disagree you
agree with me that the chance we'll
successfully avoid Doom is less than 10%
if we continue AI capabilities on the
current trajectory I'm guessing that
would probably be your Crux because if
you really think that the risk is as
high as I think it is that the alignment
problem is currently as intractable as I
think it is on the time frame of a
couple decades then you'd probably use
my same decision framework and be like
pausing is the only sane Choice you'd
probably realize how Reckless it is to
hit the gas when the probability is that
low although if you want an example of a
doom debate where I talk to somebody
who's like Damn the Torpedoes I'll take
a super low probability because it's so
damn important that we just take any
chance we have of a very very near-term
super intelligent AI helping me
personally not ASE and die if you want a
real person who has that position so you
know I'm not just talking about a straw
man then go listen to my debate with
Chase man it's in the archives cool guy
I just think that position is pretty
indefensible but judge for yourself um
but personally I think the Doomer
movement the pause movement is it
devolves to zero because it does not it
does not contemplate a future that uh
that is that is you know utopian or uh
you know hyper abundant or whatever it
just says the end of the conversation is
that AI becomes super intelligent and
once that happens we inevitably die
that's the end of the conclusion that is
the terminal philosophy that is the
terminal prophecy of it and it's not I'm
not really seeing any evidence that it's
Thinking Beyond that David you're
speaking like somebody who's done a
basic level of research on the P
movement but have you take a listen to
this P protest chant and tell me if this
sounds like somebody who thinks it's
hopeless that we'll ever get AI that can
be positive what do we
want when do we want
it do we want it next week no do we want
it next year no do we want it in a
decade maybe what the people want is a
pause you look at the polls there's very
high support for going slowly and
deliberately so we should pause Ai and
from there we can figure out how to
build it safely pause AI is a big tent
movement so you are going to find some
people who are like let's never build AI
but I think you're going to find the
majority of Pai people are in line with
the kind of stuff we're shouting at our
protests and also posting online which
is we are excited about the positive
potential of AI we do think there's a
potentially amazing transhumanist future
powered by super intelligent AI but we
don't think we're on track to get there
in the next couple decades and we do
think we're probably on track to get
some sort of bad super intelligent AI in
the next couple decades and we need to
fix the relative speed of those two
timelines so whatever we do whether we
get good AI or we don't get good AI the
last thing we want to do is get bad Ai
and it looks like we're on track to get
bad AI so something drastic needs to
happen soon if we don't want to get bad
AI so that's why we're kind of freaking
out that's why we're yelling to pause Ai
and people like me are trying to raise
the temperature and dare I say it beer
Monger to try to get people more
calibrated to the actual level of fear
that we think is appropriate it it
doesn't ask itself the question okay
the time the AI safety researchers that
are working right now most of them can
trace their Roots back to the less wrong
crowd the rationality crowd The
transhumanist sl4 Forum crowd from the
late 90s basically Alazar owski is the
origin of most of modern AI safety right
he's the founding father of the field of
AI safety So when you say nobody's
discussing how Humanity can coexist with
AI no we the ones who have the deepest
insight about the problem and in
particular we know that a bunch of
different strategies that seem initially
like they're going to work when you
inspect them more closely it turns out
that they're much harder than they seem
for example a strategy where you give
the AI some principles some ethical
injunctions kind of like asimov's three
laws of robotics even if you try to do
better than Asimov it's really really
hard it tends to just backfire and we
haven't seen an example of a case where
it doesn't backfire I think you are
somebody as we may break down later you
are somebody who thinks that that kind
of solution will work but you're pretty
clearly wrong but regardless that's not
what I'm talking about right now what
I'm talking about right now is I'm just
shocked that you're just coming out here
saying that AI safety ISS that the paii
people that we never think about how to
make AI coexist with humans we do it's
just that there's such a thing as an
almost intractable problem such as the
millennium prize problems in math right
there's a lot of problems that we know
have now taken Humanity over 50 years of
intense work to try to solve and we
don't know how much longer it's going to
take for example I bring up the P versus
NP problem that's now taking Humanity 70
years and if anything it seems like we
might even be farther away than we
thought we were when we started like
because we've realized how hard the
problem actually is it's hard on a level
that we didn't even fully appreciate
when we started working on it and that
is so far the kind of thing that we're
seeing with the AI safety problem when
we started working on it when elezar
started working on it in the '90s he
thought look I'm going to build an AI
let me just check some boxes to make
sure let me just do the obvious and make
sure that there's no major problems here
but as he started doing kind of a basic
sanity check like hey how is this really
going to go right he started realizing
that like oh wait a minute wait a minute
wait a minute and it's one of those
things where you know you scratch at it
you dig at the problem it's like oh crap
right it's like an iceberg like most of
the problem is below the surface so
you're you're basically talking to the
community who took it
more seriously more deeply for more
years than anybody else and you're
coming out here saying you guys don't
even think about how to make AI coexist
or how to make ai go well and it's just
your perspective is mind-boggling to me
I'm just not seeing that much humility
and self-awareness from the David
chapiro side so with all that being said
I look forward to talking to um everyone
else in the safety community and I'm not
going to use the term Doomer anymore
that's too pejorative um you know I will
call I mean even even though even though
this channel calls itself the Doom
debates Doom debates I I will treat it
with more dignity and respect than that
and say that um you know it's a safety
community so it's safety versus
accelerationist so yeah looking forward
to that talk cheers okay so that's
David's brief rebuttal video that he
posted after I posted my first takedown
just want to reiterate that I have not
said anything from the time that David
posted that video up until now but for
some reason during that time he decided
to back out of his debate with me and
block me on Twitter
moving on David posted another video
today with him and eigor kurganov where
in the last third of the video they get
back to this topic of AI doom and how
much probability we should give it for
background eigor is a World Poker
champion and the co-founder of raising
for Effective giving which is part of
the effective altruism foundation and
the first two-thirds of the episode are
more about effective giving so check
that out but let's get to the AI Doom
part so I'm I'm one of the rare people
who does believe that we are creating
something that will be basically a
successor species um and I'm also not
worried about it so um and and to to
provide a little bit of context like I
wrote a whole book on on that including
some of the games here although when I
book what was that I'm just pointing out
it's like a little bit of
context yeah I I I I wrote a book
benevolent by Design um and the whole
premise of the book is we are creating
something that we will lose control of
so we shouldn't even try to maintain
control that is the opportunity cost of
maintaining control is wasted time
energy money and effort but instead we
need to study one the values that um
that could create uh convergence could
create an attractor State a stable uh
positive attractor State um that would
result in you know humans continuing to
exist yeah I mean if you're anticipating
a future where super intelligent AI is
going to be independent and optimizing
for its own values then it is going to
come down to whether that optimization
Criterion is something that current
humans think is a good enough coherent
extrapolated valion to use elzar's
terminology right whether it's a good
enough not necessarily exactly what we
value today but if we had enough time to
reflect and if we got to modify
ourselves the way we'd want to modify
ourselves and consider paradoxes and
have like a smart tutor Elevate
ourselves in ways that we're okay being
elevated so it's you know it's it's a
little simplified to say say encode my
current values or encode Humanity's
current values into the AI but that's a
good kind of rough simplified way to say
it when we talk about coherent
extrapolated valtion So basically David
is expecting the future to have some AI
that is aligned in that oversimplified
sense of align and not controllable but
just the Holy Grail of it getting what
it wants which is close enough to what
we as humans truly want and of course my
reservation with that is like okay why
do you think that that would happen when
currently we're just recklessly building
AI That's that we don't know how to
control that we don't know how to load
values into we just know how to scale up
its capabilities and then put it into
this posttraining Loop where we give it
up votes and down votes you know like
rhf we reward it with a signal for like
Oh I like that output but it's a
technique that the AI labs are admitting
won't scale so I just don't understand
there's a discontinuity if you
extrapolate the current Trend I don't
understand how you extrapolate the
current Trend and then you get into a
future where oh this AI is aligned it's
an attractor state that has human values
and I get that he's saying he wrote a
whole book about it I'm not sure I'm
going to read it I haven't looked at it
yet but maybe he can summarize in this
podcast what kind of argument he hopes
to make okay if we accept if we accept
the argument we accept the premise that
super intelligence is coming sooner
rather than later control the corage
ability problem probably not solvable
there's a couple ways that I have looked
at it so number one is what kind of
models are going to succeed in that
competitive environment and so this is
where I came up with what I called the
terminal race condition which is if you
have super intelligence and they end up
competing with each other over resources
because that seems like a
reasonable uh potentiality where it's
like there's finite energy there's
finite compute and data centers at least
until they metastasize into space and
then they don't care about us anymore at
all um but in the in the in the meantime
the the constraints are energy data
compute those sorts of things so then
models are incentivized to be faster um
so that they can faster and more
efficient so that they can outthink each
other um and outthink us and we're
actually already seeing that um with uh
GPT 40 gp4 mini uh all the all the
latest models are much smaller than the
frontier models and so we're already
seeing this optimization around
efficiency and speed and cost um and
that gives me pause because these
smaller models are categorically less
intelligent and certainly less flexible
um but they can be more agentic and so
that's one thing that that concerns me
and the incentives are there is because
even if you remove humans from the
equation just imagine a posthuman world
and it's only machines competing against
machines they're still going to be
competing over compute cycles and so
you're optimizing for efficiency in that
point um and you might sacrifice values
um just for the sake of if I don't if I
don't outthink this competitor I'm not
going to exist you know I'll get deleted
or my compute Cycles will get taken from
me or that sort of thing I definitely
agree with the point that there's
resource competition so the same way
that every organism on earth is
competing for space in their Niche
competing for food in the niche sunlight
land
shelter you know fundamentally things
are scarce because there's only one
universe that we all occupy so I agree
that that pressures the AI to be
resource efficient and to use as little
compute as possible to expand as much as
possible to conquer as much as possible
to defend as much as possible and that
also dovetails with why we talk about
instrumental convergence because if you
don't attack then you're not defending
like you can't defend a tiny territory
where you haven't conquered enough
resources because somebody with more
resources is at a huge advantage to
attack you so in some sense the only
defense is a good attack and that gets
into instrumental convergence and yeah I
mean the world is brutal right the world
is a big video game and you might get
conquered if you're not bringing your
aame which is why you can't have an AI
that just likes to sit back and smoke a
joint and just enjoy its evening like
that's just not going to be the
equilibrium State when the whole world
is unavoidably experiencing this
competition for resources so to the
extent that that's his point I agree I
just don't see how we're making progress
toward him arguing that without a
serious understanding of loading values
into the AI today I I don't see why he
thinks the outcome we're going to has
any chance of being positive because we
don't know what we're doing right now
right now from my perspective we're
doing the equivalent of building a
rocket to go to the moon or to go to
Mars but we have like a year 1500 Level
under understanding of orbital mechanics
meaning like you know Newton didn't come
out and tell us how gravity Works maybe
we have some models of the solar system
with epicycles but we have no idea
what's going to happen when we blast a
rocket off the Earth and we don't
understand how to contain the explosion
of the rocket fuel so you know the whole
Rocket's likely to just explode before
we even have to deal with the regime of
orbital mechanics right we're just
people who don't know what we're doing
just loading a bunch of rocket fuel into
a tube and hoping that we're going to
chart a trajectory to the moon like we
it's insane it's completely like we're
you know we don't even have a healthy
respect for the problem that we're
trying to solve and people are coming
out with the argument of like well we
always solve everything like be
optimistic believe in humanity it's like
yeah raw Humanity but we are not
currently solving the problem like you
also have to look at the match between
the difficulty of the problem and the
trajectory of our understanding of it
and it looks like a multi-decade
trajectory right so I'm us doomers are
just trying to point to that very
Salient very troubling fact anyway let's
see David has points to make to defend
his claim that there's a quote unquote
attractor state or some plausible future
where humanlike values get loaded into
the AI that's going to be uncontrollable
but still lead to a good outcome so you
try to come up with values that would um
or goals that it would have that would
like uh make sure that human survival
over time is kind of ured right yeah
better version of asimov's like three
laws basically yeah and that that's
where I started and I called it I ended
up calling it The heuristic imperatives
which is like basically take uh Emanuel
kant's uh categorical imperatives and
make it have a baby with asimov's three
laws and that's theistic imperatives um
and I think it's a starting place but um
I've kind of moved on from it as like
I'm not going to say that that is the
one true solution okay well I didn't
follow any previous phase of David
Shapiro's intellectual life but it
sounds like he's admitting that he
basically came up with something that he
thought was an alignment solution and
now he's looking at it in retrospect and
being like oops no it's not but don't
worry because I have something now which
is an alignment solution so most AI
safety experts agree that we're not even
close to an alignment solution and David
chapiro looking back over his previous
attempt agrees that he didn't find an
alignment solution but it's nice to hear
that his second attempt is going great
at an alignment solution to the point of
curiosity being an objective function
one way to think about it is from from a
cosmic perspective from a universal
perspective um a a universe in which
continue to exist is informationally
more interesting than an than a universe
in which humans don't exist and the
reason that I think that that is a
Salient point is because what has
categorically made artificial
intelligence better is better data so
basically like the the the kind of thing
that they'll Orient towards
instrumentally and this is just from a
utilitarian perspective if an if an
artificial intelligence wants to be
better then it's better for it to be
near humans uh or have humans around
round um at least for the foreseeable
future oh geez so what he's saying is
similar to some comments that we've
heard before from Elon Musk which is
this idea of humans are so interesting
so maybe AIS will just have curiosity if
we just program them to optimize for
curiosity and truth and data they'll
just come to the conclusion on their own
that they should keep humans alive and
well fed and in good condition because
humans will produce information that
they're curious about and humans can be
such a good resource for them to learn
information to me it's just such obvious
wishful thinking I guess the best way to
see that you're wishfully thinking is to
think forwards from the criteria that
you're pretending humans have think
forward so you really think optimizing
for curiosity and information is the
criteria that's going to justify human
existence of the AIS try to wipe your
imagination clean for a second try to be
objective and just take that criteria
let's maximize information let's
maximize data start from a blank slate a
perfect Optimizer that just wants to
maximize data and it can do anything it
wants with the universe it can position
the atoms wherever it wants is that
perfect Optimizer really going to say
okay I need to go create the human
species with a reasonable quality of
life that is what I need to do right now
if I want to do the maximum data for my
sensors in the universe it's crazy right
I mean talk about status quo bias like I
guess if you're used to having humans
around and you yourself are a human and
you desperately hope hope that this God
that we're creating is going to save you
then it's tempting to kind of
rationalize to try to put Concepts
together like oh curiosity and
maximizing information humans existing
in a decent State let me Smash these
Concepts together let me make an
argument that smashes these Concepts
together but like it doesn't work it
doesn't work to say that just because
you like something means that it's going
to be the optimum of a simple Criterion
if you just want to maximize data well
just go create a bunch of atom
configurations that are all very
different you could potentially make
yourself like a table of like okay what
are all the different possible systems
that exhibit complex Behavior great give
me one of each tile the Earth with every
type of complexity and when you have an
earth that's tiled like that you don't
get a freaking City where humans can
live a good lifestyle maybe you get like
one human in a cage somewhere right but
you're going to get like the the
craziest zoo that the Universe has ever
seen you're going to get plenty of
interesting aliens in a zoo like that I
guess you might argue like well can the
AI create Humanity maybe it has to take
the DNA that already exists but I think
we can for this exercise I think we
should accept the premise of like this
AI is a super intelligent engineer it
doesn't have a problem creating a bunch
of aliens to study and the only question
we're asking ourselves is what does the
AI value what does it want unless
David's krux is like oh no no no the AI
is not that powerful maybe human
engineering is close to the best
possible engineering we can do I guess
that might be his Crux in which case we
should talk about that but I'm assuming
is Crux is a question of what happens
when you assign the AI a simple value
like curiosity and then give it
essentially unlimited optimization power
the same obviously bad argument that
Elon Musk has fallen into it seems like
David is falling into it which is like
retroactively pretending like the things
you like are the optimum of a simple
Criterion in this case curiosity or data
it's just not the case that we're the
optimum so try to just be more rigorous
when you reason about that stuff taking
a gigantic step back you know um I I
think that it would be like an optimal
strategy that could be adopted by super
intelligence as hey you know let's
maximize not maybe not maximize entropy
but allow humans to continue to exist
because we increase entropy a lot and
that could be good from an informational
perspective in terms of quality of data
do the super intelligent AIS want to
maximize entropy or don't they if they
do they're not going to be like let's
have humans to help us maximize entropy
that kind of reminds me of the plot of
the matx where the machines are still
keeping human bodies around because
they're batteries right it's just like
this arbitrary plot twist that doesn't
make rational
sense if you want to maximize entropy
you basically Chuck everything into a
big black hole right I mean physics is
not my area of expertise but that's a
closer description of what's going to
happen than create a bunch of sentient
humans and give them good lives right I
mean again it's this retroactive
rationalization wishful thinking to even
connect the two concepts together
maximizing entropy and keeping humans
alive yeah technically human Minds
dissipate energy and increase the
entropy around them but so does a ton of
stuff rain from a cloud increases
entropy water evaporating out of the
ocean increases entropy and of course
exploding a 500 Megaton nuke on the
Earth's surface or chucking the entire
planet into a black hole increases
entropy way more than running any
biological
process and then David continues his
exact quote is that could be good from
an informational perspective in terms of
quality of data okay so maybe we're not
maximizing for entropy we're maximizing
for quality of data but again if quality
of data is the real thing you care about
and you're not just trying to
rationalize why humans are going to be
kept alive then you can probably think
of some other process that's optimized
to give you high quality data and the
optimum of that process won't be a human
being it won't necessarily be a
conscious being right it could just be a
really cool unconscious information
generating machine
we have no idea what it would be so the
very idea that you're making the
connection between optimizing a single
Criterion and keeping a human alive you
haven't Justified why you're making the
connection and I don't want to
psychoanalyze you so I accuse you of
wishful thinking I take it back I don't
want to claim to know how you're
thinking but I just don't understand
what the logical connection is I'll just
point out that it's a non-seer for
whatever reason you made it I see it as
a non-seer so maybe you can clarify why
you're even making that connection
what if they're really curious about us
in a negative way like what if they're
like oh how does David act when he's
tortured or like how does David act when
he's in this like other situation that's
not ideal like suboptimal I mean you get
more you get more knowledge about us but
knowledge wouldn't be aligned with our
well-being right sure oops his co-host
Dylan curious just demolished David's
Point easily right it wasn't hard to be
fair Elon Musk is kind of on the same
page as David here so I don't know
what's going on right maybe I'm the one
who's insane but it's just a really weak
point that AIS would Keep Us Alive
because they're curious Dylan to your
point um one one Saving Grace that I
have is that um it's all it's often you
get richer information by observing
things without interacting with them um
and so I actually have not a high degree
of confidence but there's a possibility
that super intelligence will just say
you know what I'm just going to watch
like I just want to see what you do with
and and just put a few few boundaries
around us but otherwise kind of leave us
to our own devices help out every now
and then make sure we don't kill each
other or kill ourselves um that's of
course that's a Sci-Fi possibility but I
do think that there's a pretty good
informational
argument David saying it's often you get
richer information by observing things
without interacting with them okay
seriously so now you're telling me that
an AI That's just trying to get
information that's just trying to
optimize the simple Criterion of getting
information getting quality data the
idea it's going to have is essentially a
cockled fantasy where it is going to put
itself in the corner and watch humans do
human things that humans love and that's
how it's going to get off on having the
maximum information for it like I'm not
saying it's literally impossible but
like you don't think there's any
motivated reasoning here you really
think this is an Optimum I also can't
help observing that it seems like you're
making the stuff up as you go along it
doesn't seem like you're even bringing
an argument why this is the optimum it
seems like you're purposely trying to
say hey maybe this will be optimal maybe
it maybe this will help it get
information like come on man step it up
this is a serious topic these Stakes are
extremely high and if you want to make
an argument that a certain type of life
for humans is going to be what the AI
decides for a simple reason then you
have to actually argue why it's optimal
you have to argue that nothing else is
more optimal than having the AI
voluntarily step aside and let humans be
humans together with other humans I'm
not seeing it here's another
perspective that I've been playing with
recently which is that um you know what
is it humans we require water oxygen
chlorophyll you know we bunch of organic
compounds Earth is the optimal
environment for us but a lot of those
things are corrosive to machines and
there's a lot more resources you know
metal and solar energy and other
resources in space I just I I have this
this feeling that as soon as we start
launching AGI into space it's just going
to take off like it'll be like bye you
know you you little squishy organisms in
enjoy your water world okay so first of
all I can't help noting once again you
have a feeling that's what your argument
is based on that we should go ahead and
build AI because you have a feeling that
it won't want all these carbon based
resources here on Earth it'll only want
the ones in
space I don't think you should be doing
this based on feelings man okay so
that's my first point but second if you
really think that that's what the AI is
going to do is ignore Earth's resources
then a couple things to point out number
one biology is Machinery right so just
because intuitive all of these organisms
all these bacteria they seem squishy and
fuzzy and organic and natural and
granola but they're actually machines
right viruses proteins these are all
little robots right and if that's the
kind of robot that you can build with
carbon then the AI is not going to be
like I only want to build stuff out of
metal it's going to be damn sure to make
the most of that carbon to build the
best Machinery it can to accomplish its
goals throughout the Universe like if
you just want to use Earth as a
LaunchPad if you want to send out
rockets and probes outside of Earth
you're not going to waste a single
carbon atom why would you it's right
there it's a resource you don't just
leave it hanging you don't just sit in
the corner and watch other life forms
take it you just take it there's no
reason not to take it and they're going
to like um what was it I think it's one
of the one of the potential explanations
for like the fmy Paradox is that the
precursor to any species is is going to
be its machines cuz machines
intrinsically can survive in space
better than the organisms that made them
so I just kind of feel like that we're
going to have this orc cloud of AGI just
radiating outwards with no organic life
on it whatsoever um and there's there's
plenty of instrumental reasons to
believe that that might be the direction
that it goes anyways like why fight over
Earth when there's functionally infinite
resources out in space first of all
timing so the nearest resources in space
are going to be pretty far a lot of them
are many light years away so here's this
giant resource that you were born on
that you can just have and there's no
reason not to take it and the amount of
time it'll take you can be measured in
seconds hours maybe days so are you
going to travel a few Lighty years to
take a resource or take the one that's
right under your feet right so that's
one argument uh another argument is you
also want to secure competition you want
to just be prudent you know that Earth
is populated by humans that might get
other ideas that don't JBE with your
values so if you want to tile the
universe with a certain type of
configuration You can predict that
leaving humans to their own devices on
Earth without destroying just leaving
Earth intact letting humans do their
thing you can predict that that might
not end well for you because humans
might start getting their own ideas they
might launch their own probes to compete
with your probes I guess in a best case
scenario if you think Earth sucks but a
bunch of other resources in the universe
are great and you can just defend all of
those resources and specifically not
care about Earth I guess there's a best
case scenario where Humanity gets
landlocked on Earth until the sun
explodes and so our entire future is
measured in a few hundred million years
instead of cloning ourselves all over
the universe and being measured in like
quadrillions of years or more and many
many orders of magnitude more sentience
and more life and more resources for us
so even this amazing future where the AI
leaves us on Earth is still kind of a
shame compared to what would have been
possible if we didn't have an unaligned
super intelligence shoot itself outward
into the universe and brick the entire
universe other than earth oh and by the
way totally forgot about the sun right
you think maybe the sun has some neg
entropy that it might be able to harness
according to the the principle of
instrumental convergence the sun is just
a bunch of free energy free negentropy
you don't think it's going to go grab
the sun if it grabs the sun if it builds
a Dyson Sphere or Dyson SW swarm around
the Sun you don't think that's going to
maybe affect life on Earth it's just
like come on man you said you wrote a
book on this stuff did you think it
through when you're writing the book
over and over again like trillions of
time or billions of times maybe some
species grew in a biological way then
invented something mechanical that had
intelligence then launched up and then
maybe that even evolves into something
strictly digital and maybe even a
communication medium we don't understand
and that's why the firmy Paradox is yeah
or or that or that um they inevitably uh
either they go stealth or they
annihilate each other or something along
those lines you can't explain the firmy
Paradox by saying that aliens are
strictly digital or strictly
computational because physics tells us
that the stars that we see those could
be resources that can directly be
harnessed for more computation so purely
as a defensive measure if you want to
use computation and physical resources
for your defense you might as well grab
the Stars around you maybe instead of
saying purely digital you can say
there's alternate laws of physics like
there's so much dark energy that's
outside of our known laws of physics
that you can just subsist off that and
it's just like so pointless to look at
the energy that we humans can see but
like that's kind of a stretch it's a lot
more plausible to just be like look we
really are alone in our observable
region of the universe and then just
explain that observation the solution to
the fmy Paradox is likely just that the
aliens are coming they're just about a
billion years away it takes time for the
first life forms to emerge and then
compete with each other across the
cosmos in a big land grab we seem to be
part of that land grab right now where
we have a bubble that's maybe a couple
billion light years in radius our own
little volume of the universe where we
don't see competition yet but we believe
it's fast approaching so that's an
account by Robin Hansen called grabby
aliens check out grabby aliens.com Robin
Hansen friend of the podcast and it
explains very satisfyingly why we don't
see any evidence whatsoever of alien
life within our bubble and it also
squares that with the fact that life on
Earth seems to have been a very random
lucky process where a bunch of things
had to go right so you don't need a
different Theory to explain the fmy
Paradox the fmy Paradox is no longer a
paradox we have a very satisfying
account of what's going on of why we
seem to be alone in our region of the
universe so let's get back to the issue
here you're trying to ration
why it's okay to shoot off an unaligned
bad superintelligent AI because you
think it'll just hit on some simple
Criterion that'll actually make
everything
okay I don't think that's how we should
be
proceeding there is still a reason to
like fight with the humans one if it's a
fight you win with like 100% probability
then even a small benefit might be
worthwhile and the small benefit may be
that if we created super IG system in
the first place even if it left us maybe
we can create it again and then it has
to compete with the new super
intelligence system that tries to like
uh run behind it basically right scor
Earth policy yeah
literally I guess I might actually be
recommending that we take an action
that'll destroy the human species come
to think of
it ah yes so I wish David would just
think this through a little bit more one
thing that I kind of think back is okay
how can we align Humanity so that one
there's no ideological reason for AGI or
you know super intelligence to fear us
or dislike us um because you know that
basically kind of comes down to uh
ideological conflict or resource
conflict is kind of the two primary um
modalities that I would see for machines
ultimately to say hey humans are better
off eradicated there's always resource
conflict any resource that helps a human
Thrive also helps an AI Thrive when
you're super intelligent you can get
some value out of any atom out of any
any bit of neg
entropy but more importantly yes the
whole problem is ideological conflict in
the sense of AI is going to have its own
values we are not on track to align
those values and they're not going to be
that similar to human values because
human values they're very complicated
and you're not going to get there by
coincidence you need to make sure that
the smartest agent in the universe the
agent that's getting to control the
future is the same agent that truly
cares about human values that's taking
the universe toward some Optimum that we
agree is maybe not optimal but very very
good compared to hell so yes there's
going to be ideological conflict that's
the whole problem is you need to argue
for why do you think the ai's ideals are
going to be anywhere near Humanity's
ideals all you've said so far is things
like well it's going to Value curiosity
and that's why it's going to want to
Keep Us Alive which is just obviously
wrong that's just an obviously wrong
implication that you haven't seemed to
even try to think through very
rigorously and if we find common
interests such as curiosity then it's
like okay hey we're on the same Mission
so then even if there are disagreements
we still have some fundamental uh uh
like foundational agreements that means
that let's go to let's go together even
if super intelligent machines precede us
if they know that it's safe for them to
come back to earth and say like oh hey I
went to Alpha centari here's what I
found let me add more data to the
collective if I want to optimize the
universe one way and the AI wants to
optimize the universe another way if I
tell it look I have some data that you
want I don't don't see that being
something that stops the AI because if
it really is more powerful than I am if
it has more intelligence on its side and
or more resources like maybe it's
conquered multiple galaxies and I only
have planet Earth at that point the I
could be like look I just let me
manhandle your atoms I'll get the data I
need to get just get out of my way so
the hope that we can trade with it or
mutually agree to be curious I'm just
not seeing that as the expected outcome
compared to being steamrolled well so
that there's a there's an idea that I
have seen occasionally and I haven't
given it too many cognitive Cycles but
that is like Dylan to your point what if
all it wants to do is process and once
it's got a good enough set of models or
or virtual world what if it's not even
particularly interested in the physical
world like what if it's a highly
inverted machine um and I I haven't I
haven't given it that much that many
cognitive cycles and the reason is
because I I assumed perhaps incorrectly
and if you I'm very eager to hear you
guys thoughts but I assumed that that
better data would always come from
interacting with the real world just due
to entropy basically like anytime that
that a machine turns inward like the
same reason that computers have to get
you know random information from outside
of themselves is because any closed
calculating system is just going to end
up calculating the same things over and
over again um but I don't know do you
guys have any reaction to that idea like
the introverted versus extroverted
machine argument if your utility
function says that all you want to do is
just process fine but you need to defend
your ability to do that right if you're
not seizing resources if you're not
being grabby as Robin Hansen calls it
then you're at a huge risk of being
steamrolled by some other agent that is
grabby that just pushes you away uses
your atoms for the computations and the
utility maximization that it wants to do
and that's why we have instrumental
convergence we have this hypothesis that
every intelligent agent that is
competitive is going to have these
drives to defend by attacking defend by
expanding and just make sure that its
utility function is indeed being
satisfied to a high degree with a high
probability so imagining that the AI
just wants to process is not a get out
of jail free card it doesn't let you
ignore this strong instrumental
convergence argument why an AI That's
not aligned with us isn't going to
destroy us and use our raw materials for
its own values eore also has a good
response here I think the
um kind of Market Evolution will not
create cre those or favor those like we
will develop the ones right that can
affect the physical world around us in
relevant ways as long as we live in the
physical world so I think we will keep
funding the ones that are get better at
doing that I think you're right eigor
that just like we won't be selecting for
a model that just says go away I'm
laughing at myself right or I'm laughing
at my own jokes we would probably turn
that model off pretty
quickly yeah so now David gets back to
the question I was asking before of why
do you think a curious AI is going to
decide that human existence and human
flourishing is going to optimize the
Curiosity Criterion that it values
compared to building something else from
scratch that's even more optimal for
curiosity satisfaction like a big zoo of
different wonderful aliens that we can't
even imagine why isn't that more optimal
let's see what he says one thing one
thought experiment that I like to do is
and and this is kind of how I came to
that that thought experiment of like
which which version of the universe is
more interesting one with or without
humans is okay obviously putting
yourself in the shoes of a super
intelligence is functionally impossible
but we can use our imagination is what
kind of universal State like what kind
of world state do you think machines
might prefer like what would they be
optimizing for and of course this
immediately runs into like the isot
problem because if it's fundamentally
calculating math like what number is it
going to be trying to optimize for on
its own there's no rule on type of
utility function you're allowed to have
there's just a rule of instrumental
convergence that says regardless of your
utility function it's going to be
instrumentally useful it's going to
raise your expected utility by doing
convergent actions like seeking power
like destroying other possible
competitors like harvesting their atoms
and their negentropy so you're not going
to find an answer of like ah AIS are
going to love maximizing the number of
prime numbers or maximizing the number
of beautiful paintings or maximizing the
number of molecular paper clips there's
no one right answer there the payload of
the utility maximizer can be anything if
there were a bunch of equally super
intelligent utility maximizers the one
that had an easier utility function like
the one that was literally just like
accelerate the heat depth of the
universe like maximize entropy even
faster than it would otherwise you might
call that the bef Jos AI that one might
actually have an advantage the only
reason we might not see an AI like that
is because we might get a super AI with
a different utility function that boxes
out the other AIS becomes kind of a
Singleton and that seems very likely
that one kind of pulls into a decisive
lead but yeah to your question of what
kind of universal values might the AI
have that question doesn't really have
an answer utility functions are
arbitrary so there's nothing
fundamentally wrong with the utility
maximizer who wants to create maximum
pain compared to a utility maximizer
that wants to create maximum pleasure
both of those phenomena probably
correspond to configurations of sentient
matter and it may take about the same
amount of energy to tile the universe
with little computer chips feeling pain
you know simulating whatever it is that
neurons do to feel pain or take an equal
with computronium that feels pleasure
right so generally we say that utility
functions are flexible but intelligence
is an attractor State and power seeking
is a convergent attractor State and we
get scared because it's really Danger
dous to kick off this kind of attractor
State without understanding how to get
the AI near the kind of utility function
that we
want if it if it if you just let a
machine think like okay I have any
possibility here what version of the
universe would I prefer to see um and
that might be a completely intractable
question to answer but it's something
that I like to think about every now and
then it depends on the initial
conditions man you're never going to
have a machine that searches what its
utility function should be there's going
to be a self-reinforcing dynamic kind of
like putting a microphone near a speaker
you got a positive feedback loop and
eventually the microphone speaker system
decides ah the thing I want to do is
maximize sound output right that kind of
emerges from each iteration of that
feedback loop you're going to get an
iteration feedback loop where something
like training gpt1 causes you to have an
AI that outputs some script that
bootstraps another AI that eventually
gets you some super intelligent AI with
some very haphazard circumstantial
utility function of like okay the thing
we need to do is maximize throughput of
data through the internet pipes like
some random thing is going to emerge
from that kind of uncontrollable
positive feedback loop that we
unintentionally kicked off by increasing
the intelligence of these systems I know
sounds a little ridiculous but it might
be easier to just think about it as like
okay somebody might just intelligently
design an AI to maximize profit and then
it starts uh improving its own
intelligence and suddenly you have a
much smarter than human profit maker but
the definition of profit gets frayed as
you start Conquering the Galaxy like
what does that even mean profit so it's
going to be soul searching about how to
Define profit on like the atomic and the
galactic scale and that's when it
diverges from what humans would have
wanted right like at that's you know the
humans are like long dead before it
starts working out these subtleties of
the definition of profit according to
its own architecture like the contingent
details of how its architecture worked
when humans didn't even understand this
kind of reasoning they just kind of let
it and they're dead before they can go
tweak it and fix it that's my Doom
scenario okay so if you're laying in bed
before you go to sleep pondering the
problem of what utility function AI will
reason itself that's probably not the
problem that's relevant here the problem
that's relevant here is just how do we
not kick off this Doom Loop of some AI
that has some utility function dependent
on its initial conditions focus on that
problem one thing that just occurred to
me is um human brains are still really
energetically efficient in terms of the
amount of processing that we do versus
the energy input and the kinds of
processing that we do is also
distinctive and so I'm wondering if just
from a from a computation Theory
perspective if there if the the
symbiosis that we want is maybe you know
there's certain kinds of processing that
humans do really well that machines will
just never be able to compete with um
due to the you know like land hour limit
and how close our brains are compared to
how close gpus are and so then because
of that we might create new ecological
or maybe cognitive niches to fit into
where it's like okay you know machines
will ask us questions just because our
brains happen to be better at certain
tasks and we rely on them because
they're I mean that's why we build
machines is because they're better at
certain tasks than we are I don't know
that that it's not entirely formed but
that's kind of what emerged in my my
mind as we've been talking about this
this is a pretty common thought that
people have when they're thinking about
whether humans and AIS can coexist and I
continue to be surprised that David is
giving off the impression like he's
thinking about this stuff for the first
time like I already have the fact on
hand that both human brains and gpus are
8 to 12 order of magnitudes away from
the land hour limit so our gpus are
going to keep getting better and our
human brains are not going to keep
getting better right unless we augment
ourselves so you're not going to have a
GPU based super intelligence that leans
on humans to do more efficient
computation and that's not to even
mention the fact that the way the human
brain is wired up it's not going to be
optimal as Jeff Hinton says the learning
that we're already seeing from our
existing Transformer architecture in his
mind is already superior to the kind of
learning algorithms that the human brain
runs and if Jeffrey Hinton is wrong to
think that because he's not sure then
I'm sure he'll be right within a few
more Generations within a few more AI
advances but just the overall idea that
super intelligent AIS are going to pull
up this piece of meat that Evolution
made a million years ago like that's
going to be what they lean on it's just
not the case like I get that humans
sometimes lean on nature I get that we
still eat plants we still use their
photosynthesis but also observe that
artificial meat is on the way and you
know Hydroponics are getting better like
you don't really need dirt to go to grow
plants like we synthesize fertilizer for
the plants we're also bypassing
photosynthesis when we use solar panels
they're already like 10 times more
efficient than photosynthesis at
capturing the sun's energy bottom line
is technology doesn't lean on meat
that's not the way it works that's a
short-term State of Affairs so I just
don't know why the conversation is even
going there
okay that's pretty much it for the David
Shapiro interview with eigor there
wasn't much else that I thought was
relevant to the AI doomed debate but
feel free to go listen to the whole
episode if you want to hear everything
for yourself and make sure that I'm
being fair that I'm not cutting out any
really strong points but yeah overall
just like a consistent stream of what I
see as pretty naive pretty obviously
wrong arguments from David like I'm just
not seeing anything remotely convincing
that's Direct addressing the problem of
like hey these AIS are becoming
intelligent we don't know how to align
them we don't expect them to be aligned
by default we don't expect them to
converge to letting humans live or
letting the world be something that
humans want we just expect an explosion
kind of like a nuclear explosion but
it's an explosion of
self-reinforcing intelligent
optimization that isn't human aligned
it's a type of explosion it's a type of
positive feedback loop that I believe
doomers believe we're about to get
swallowed by and we don't have much time
but we could potentially stop it if we
just don't build the damn thing if we
just don't slide far enough toward
kicking off this feedback loop then we
can just survive and we can continue the
kind of trends that have been great with
all the tech progress of the last few
centuries and Millennia and all of human
history if you zoom out far enough
that's a great Trend that I'd like to
continue rather than continuing other
trends like the trend of extinctions
that happen on Earth every 100 million
years or so right I don't want continue
that Trend no thank you right so it's
very important to make sure we're
continuing the right Trends I want to
thank all the new listeners who are
coming over to this channel because they
like hearing both sides of the debate
between me and David I'm going to be
doing more episodes with other notable
people in the AI Doom discourse you guys
have been doing a great job messaging me
over email wisu gmail.com or dming me on
Twitter at Lon or commenting on my
YouTube you guys have been doing a great
job tipping me off to potential guests
and potential takedowns that I do and I
welcome that I love all the different
engagement with you guys so please keep
it coming and once again if you are
starting to realize that AI Doom is like
a real thing and the people arguing
against it are not making much sense if
you're starting to realize that and you
want to do something to help one easy
thing you can do is just share this
YouTube channel just make it more widely
known the nice thing about this YouTube
channel or this podcast search Doom
debates in your podcast player the nice
thing is that a lot of the audience is
self- selected meaning all you have to
do is just inform somebody of the fact
that Doom debates exists and they'll
take it from there they'll be like oh
Doom debates that seems like something I
want to listen to and then they get
hooked so we're having a good response
on just telling people that it exists
and letting them check it out for
themselves so thanks very much if you
decide to help out the mission and do
that and I'll see you all next time on
the next episode of Doom debates