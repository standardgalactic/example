[Music]
okay thank you for being here so the
first question I want to ask you it's
pretty simple just can you tell us about
your background who you are in your
words okay yeah so a little bit about my
background I'm the host of the Doom
debates podcast on YouTube Spotify and
so on my background is in computer
science software engineering and
startups I actually run a startup called
relationship hero from y combinator we
give relationship coaching online that's
basically my day job uh because
podcasting doesn't really pay the bills
I have a deep background in just
worrying about AI risk because all the
way back in 2007 I discovered overcoming
bias and less wrong and I started
getting involved with the mey community
which was called The Singularity
Institute at the time and I became a big
fan of elzra owski and this was early
days when the community was just kind of
coming together and AI seemed so far
away but it was still there was still
enough there where I was like wow this
definitely seems like extremely
important and underrated but I never
really contributed that much I never
really spent that much time focusing on
it up until the last few years where
it's like oh well okay the Turing test
is largely getting ped you know
timelines are getting short and now I'm
like okay now this is actually just an
underrated topic because I agree with
Ali asra owski where I think we're about
to go extinct very likely and I don't
think humanity is showing the kind of
urgency that we would need if we were
going to survive so now I can't look
away can't look away from the disaster
that's why I'm like okay well maybe I'll
spend a lot of my time doing a podcast
about it maybe I'll try to contribute
because I just think it's crazy how
underrated the situation is so that's my
life up to this point yeah I definitely
agree with you and well before we jump
into the a doom arguments and all that
can you do you would you describe
yourself also as an effective altruist
transhumanist all those words that we
often see they're all packed together a
lot of people who are worried about AI
risk are also effective altruism and and
rationalist yeah yeah I would go ahead
and say that I'm an effective altruist I
feel like famously uh people are
hesitant to call themselves that right
they always like to be like well I like
the ideas but it's not me so I I am like
the only person in the world who just
says that he's an effective altruist I
don't think it's hard to just say that
um I think the condition is pretty
simple I think anybody who just believes
that if you think a little bit and use a
few numbers to try to evaluate Charities
you can have you know 100 or a thousand
times more impact for your dollar or
your time as people who don't think
about it that hard or that analytically
I've donated a significant amount you
know not a huge amount but you know
thousands here or there to a bunch of
effective causes uh and I you know I'm
kind of with that Community uh
intellectually I'm constantly discussing
with these types of people so I'm very
much in the cluster I feel like I was in
the rationality cluster before effective
altruism even started I don't know if
people know Alazar owski wrote a couple
articles that from my perspective are
kind of foundational to the effective
altruism movement and this was back in
like 2009 2010 for example he has a
famous article called uh uh think in uh
something about comparing fuzzies and
uyon where it's like yeah you can go
volunteer at a soup kitchen but make
sure to also like do some math too you
got to balance both and he has another
famous article called money the unit of
caring so from my mind I was reading
that stuff and then I saw the effective
altruism movement start and I'm like
okay so effective altruism is kind of a
spin-off of rationality yeah yeah that's
that's true because my journey was I
guess I was more into philosophy and
then I discovered yeah moral philosophy
Peter Singer and then I heard about the
the people in Oxford doing some work and
then I mean I was also following Nick
bom and Eliza rovski so then I I
realized there was some sort of yeah
comination your question about
transhumanism I first became a
transhumanist when I read Marshall brain
I don't know if you've heard of that guy
he did Works back in like the '90s and
2000s oh no I don't think I know the
show but I never heard about the guy
yeah yeah really fascinating guy he was
very formative to me in my teens and
early 20s and he was actually writing
transhumanist stuff and and AI stuff
even before well Alazar was doing early
2000s but I I was exposed to Marshall
brain even a few years before Alazar and
he has books saying like yeah you know
everybody's just going to wear like a
headset and the AI is going to be their
manager and it's just going to like tell
them stuff to do instead of a human and
there's going to be a lot of
unemployment and he was right wrting the
stuff back in like 2005 which I thought
was really precient and is like starting
to come true now but anyway I was
reading Marshall brain and he's saying
he has a book called the day you discard
your body where he's basically saying
look we're all going to live in virtual
reality or we'll control robot bodies
you don't really need your human body
and when I read that I'm like okay yeah
that that makes sense we don't really
need our human bodies fundamentally and
then I read elzar and L wrong and he was
just talking about like fun Theory and
basically like how do we build heaven
and all the things we can do better than
than The Human Condition and I'm like
yeah sure okay I'm definitely not
attached to the human condition for me
the example that always comes to mind is
like going to the bathroom it's like do
you really need to go to the bathroom I
don't think that you do fundamentally
and I think as long as somebody's
willing to admit that going to the
bathroom doesn't have to be part of the
human condition if we could reshape the
world I feel like that's like the
minimum requirement to make you a
transhumanist yeah that's actually an
interesting point yeah there so much
stuff we think part of the human
condition but you know we we don't know
I mean and if you look back we used to
do things as a hunter gatherers maybe
that people think oh that's we who we
are and we'll never do something
different but today we we don't do these
things very much like hunting for
example I don't do that yeah so you
mentioned Ela yovi and and the
rationalist community so I guess you you
use Bas epistemology in your reasoning
and decision making maybe you could give
us a few yeah an explanation on on this
before we start going further okay yeah
so Basi and epistemology is a big topic
I think that it mostly comes up when I'm
on social media in the context of people
saying that it's wrong so that's you
know people are like oh rationalist they
put too many probabilities on things and
they pull the number out of nowhere and
you can't put a number on stuff so maybe
what I'll say about basan reasoning and
numbers is like the numberers allowed to
be a really huge range so like when I
think about using a number like I talk
about my P Doom being 50% it's not like
I mean I've narrowed it between 49 to 51
I just mean it's like 10 to 90 and then
people say okay so it's meaningless you
just said the same thing as 0 to 100 but
no it's not Z to 100 like it's
definitely not less than 1% and when I
tell you that it's definitely not less
than 1% that's like a very significant
statement to make so I'm not saying I
know exactly what the probability of
Doom is or or how to calculate the
probability do it's obviously very hard
to calculate I I can just confidently
tell you that if anybody says it's less
than 1% something seems wrong with them
like I feel like they're the crazy one
anybody who would say that it's it's
less than 1% and similarly somebody
who's more sure than 99% that were
doomed that also seems to me like very
extreme and crazy so when I say my P
Doom is 50% or 10 to 90% or 1 to 99% I'm
really just being a little bit more
precise than somebody who's saying like
yeah I think there's like a significant
chance worth paying attention to that
we're doomed you know that that's really
all there is to it yeah yeah it's kind
of replacing words like likely or
unlikely with a number which is
something we can intuitively understand
better yes exactly right so and that's
you know I think I like to be as normal
as possible while still getting the
benefit of rationality right because I I
think what you're going to find is
there's a lot of overlap between people
who realize how valuable the stuff is
and then people who act really nerdy and
aspy right like Asbergers so generally
when you when you think about all these
tools you think about somebody acting
weird and nerdy so I try to get the best
of both worlds right I try to go into a
social interaction and follow the normal
Protocols of social interaction and like
talk normal English but then also bring
up the rationality tools you know to the
minimum degree that they actually help
yeah okay and do you know these days I
don't fully know what's Elijah rovi's
probabilties it also around 50% with ER
bars or is it he's made a bunch of
statements that like he thinks P Doom
discussions aren't productive so I feel
like he would have an answer with a
million caveats but that said I think
that I remember hearing that if you go
by his own model of why we're doomed
like if if you stay within the model
then it says 99% Plus or you know at
least High 90s but I think he's also
willing to admit look you know the world
is full of unknown unknowns I know for
sure he wouldn't tell you it's like more
than 99.9 or 99.99 because he's
specifically pushed back against people
like Roman yalu who are like we're so
doomed he's just like look the world is
just not quite that predictable but the
reason he generally gives
I mean the reason that I think he thinks
probability of Doom is high is just
because in his model it's it's kind of
like a big tangle like a tangle of vines
where all the different Vines together
are all like tightly wrapped around the
situation and they all make us doomed so
that even if somebody's like hey look
this Vine actually is kind of easy to
take off okay but there's still 20 other
Vines you know like choking us to death
like he sees that as being the situation
there's a bunch of independent factors
choking us to death all right okay so I
guess we we'll jump into the argument
just to okay understand why are we so on
the path to do but maybe before can you
give us your definition of intelligence
because if we talk about artificial
intelligence maybe we want to know
exactly what you mean by the term so
what what's intelligence for you well
before I give you my definition of
intelligence I think I should give you
my definition of definition of
intelligence like what am I trying to
achieve by defining intelligence I think
the most interesting thing to achieve
here is to Define why I'm scared like
why we're talking about AI being smarter
than us and then wiping us all out the
the interesting sense of intelligence is
the sense that explains why we're doomed
in my opinion so if you want to talk
about that sense whether you want to
call that doess or power or if you want
to use the word intelligence we could I
tend to map that onto the word
intelligence I think it makes sense but
you know worst case you can just use a
different word because the discussion
that I want to have is a discussion
about optimization and so that's why to
go back back to the object level when
I'm going to Define intelligence I will
go ahead and Define intelligence as
optimization power so basically if you
have some domain like the physical
universe and you have some objective
that you want to achieve like make a lot
of money or make a lot of paperclips you
know any kind of objective like that
with a large domain the extent to which
you can effectively achieve your
objective given some constant set of
resources because you know if you have
tons of resources it becomes easier so
like fix some reasonable set of starting
resources fix some goal fix some domain
and then like the probability that
you're going to get what you want in
that domain something like that that is
what I would call your intelligence I
see so it's it's in a way trying to map
the future and look at the the path
towards a goal and trying to reduce the
other path and optimize to the best
possible outcome yeah and you know
there's there's a lot of different
things we want out of a definition of
intelligence for instance we want be
able to talk about general intelligence
so if somebody can beat you at chess and
also they can study good moves in
Jiu-Jitsu and and come better prepared
and beat you at Jiu-Jitsu too and let's
say you and them have like this you know
relatively the same amount of muscles
like the say similar body types you know
so they have two different skills that
seem like they're in different domains
and yet realistically if if we know
somebody in our life who is just born
with a significantly below average IQ
let's say they have an 80 IQ look the
reality is an average person with a 120
IQ as long as long as their body is not
that different is probably going to win
at both chess and Jiu-Jitsu right like
there is a common factor there's a
common G and it's not just for the human
brain it also seems to be the case I
think we're seeing it for AIS where
something like a Claude 3.5 Sonet is
it's not it's not like gbt2 has a
particular scill that it's still better
at than CLA 3.5 Sonic CLA 3.5 Sonet is
just like better at everything right so
there does seem to be like this
one-dimensionality to it okay and okay
so therefore why not now since we know
what's intelligence this process power
of optimization why when AI has this
kind of thing this this property it
becomes an existential risk for us
what's the core argument of AI Doom yeah
the core argument of AI Doom I can even
zoom out a little bit because I think
the argument factors into two parts so
the first part which I think we're kind
of taking for granted potentially is
just like AGI seems to be coming soon
like the timeline is reasonably short so
we're not talking about like in a
thousand years we're not talking about
it never we're talking about like pretty
soon AI is coming soon and then the
second part is the question you're
asking now which is like okay well if we
get AGI or artificial super intelligence
why is that bad and the second part of
my claim is like yeah I don't think that
it's good if we have a ASI today uh so
there's a few different reasons there I
guess to put it very simply we don't
know how to control it and we don't know
how to make it want what we want but
whatever it does want it's going to get
like there in lights the problem I see
yeah it's very simple way to formulate
the problem exactly and why is it so
hard to make it want what we want so
this gets to the alignment problem and
it's there's and it's like what I said
about Alazar right there's the tangle of
of vines so there's a few Vines just in
the alignment problem you know so the
alignment problem you can Define it as
making the AI do things that are good
for Humanity because you can imagine you
make an AI somebody tells the AI to
maximize paper clips and then suddenly
it you know wipes out Humanity it's like
hey you're not paperclips go away and
suddenly there's no humans anymore so so
the alignment problem is like okay you
have this super intelligent agent and
you just wanted to do something that's
good for Humanity or that at least lets
Humanity keep existing without getting
worse that's right so if Humanity can be
the same or better we can call that a
win for alignment and you're asking okay
why is alignment hard so one reason it's
hard is called the outer alignment
problem this is one of the vines outer
alignment is even us knowing what we
want like it's it's hard to even even if
we had a genie that would listen to our
wishes uh meaning the genie has solved
the inner alignment problem the inner
alignment problem is like okay it'll
listen to your wish if you make the wish
well the outer alignment problem is what
wish do you even make because if you ask
somebody okay write down your ideal wish
for Humanity write down your
specification for heaven the thing that
they write down if you actually go and
execute it probably creates hell like
you can actually try this experiment
right like go to any random smart person
be like here write a specification for
heaven and like great like infinite
happiness and it's like okay you're
you're wish is my command here we go now
we have heaven and then it's like oh
oops we never challenge ourselves ever
again right cuz you just created like
infinite there's no like struggle oops
that's actually not really what you
wanted too late so that's part of the
alignment problem is even being able to
write down what we want yeah that's
actually interesting but can't we have a
lower Bond of you know because if I ask
if if you ask me okay what's my Utopia
okay it's hard to say but I can always I
can probably say I don't want cancer and
disease I don't want I don't know
scarcity of material stuff so maybe
that's a lower Bond or and and know of
course I don't want anyone to die if
possible so maybe an AI could take this
on board yeah totally and actually
that's a great observation so and this
is uh if if you've listened to Roman
yampolsky I think he might not even
follow you this far like I think I think
his worldview if I remember correctly is
that he is very unclear about what we
truly want whereas what you just said of
like well I don't think we want cancer
like I I feel like we know some things
about what we want and don't want like
we don't want too much suffering
although Nick Bostrom then points out
okay but it's a slippery slope because
if you keep removing these obstacles and
and you don't know when to stop
eventually okay everybody just has Bliss
and there's like no obstacles but don't
we kind of want some obstacles so it is
like a slippery slope but but there's an
even bigger problem because to to your
question why not just tell AI look just
kind of keep life the same and only make
like minor improvements so you think we
would be able to like pick away a little
bit at a specification that doesn't make
the world worse but the problem is that
when you build the super intelligent AI
you have what's called an attractor
state where the AI tends to want to go
tweak itself to be like hey I know how
to make myself better or I know how to
make another version of myself so
whenever you give the AI any task even
if the task is like hey can you just
make our transportation system be a
little better you know you give it like
a relatively small task but anytime you
give it any task it starts getting these
ideas of like well look there's all this
infrastructure that I could build to
help you out right so there's all these
convergent things that it wants to do it
wants to lay infrastructure it wants to
get power for itself it wants to get
money and and it wants to be hardcore so
there's kind of no such thing as like
okay just do a little bit and turn
yourself off because anytime it wants to
do anything it just has all of these
logical inferences of like but it it
would increase my probability of success
if I do all these other helpful things
and suddenly you just get a that tend to
be what I call hardcore like they just
want to do a lot and if you have an AI
that doesn't want to do a lot it only
wants to do a little bit well your
neighbor's AI gets the idea to do a lot
and pretty soon it outcompetes AI that
only wants to do a little bit so long
story short at some point you just get
this AI which likes to take power likes
to do a lot and you thought that you
were just giving it like a few tasks
like no big deal but it's at the point
where it starts to just get to decide
how the whole world goes like there
there's kind of no stopping the slippery
slope to that state yeah cuz you
mentioned another AI could be out would
compete the the one we create that would
have those like looser goals and so that
that's another problem is the race of
building AGI and so we it's not only one
we need to worry about potentially many
competition between Ai and the one that
will be selected is the most
hardcore right I mean there's only one
world right there's only one Universe
it's kind of like standing back at the
beginning of Life and being like I
wonder if the species are just going to
be like really chill right like who's to
say that these species are going to be
like so hardcore about survival and
reproduction but as we all know the ones
that aren't hardcore just tend to die
off and then the niches you know like
trees they grow they they keep trying to
grow as tall as they can so they can get
more Sun than the next tree until it
just becomes too costly to grow like we
see that everywhere in in the you know
the biosphere is we see life competing
to the max everybody's arms racing to
the max within their Niche AI is the
same way AI is also similarly going to
be taking resources that everybody else
then can't have right and you mentioned
so that the the outer alignment what's
the inner alignment as well yeah so to
review so outer alignment I mentioned
like how do you even write down what
your wish is you know instead of writing
down the wrong wish so outer alignment
just review is the mapping between like
our true brain's intention to something
you can write down or formalize and then
inner alignment is you start from the
assumption that you could formalize
something or write down what you want
and then you ask can we get that inside
of the ai's values can we actually make
the AI read some description of what we
want and then actually do it and
actually care about it now at first
glance the answer might seem like it's
easy because if you look at like GPT 4
today and this fools a lot of prominent
people like Mark Andre loves to talk
about he's like look what's the problem
I chat with these AIS and the AIS tell
me about morality and every time I ask
him like is it right to kill people
they're very good at being like no no no
it's wrong and here's all the different
edge cases and they're just as good as
humans at reasoning about morality
what's the problem so the problem is
just that all the stuff that they're
doing now in the context of a chat we
have a feedback loop to train the chat
and so when they chat with you uh in
production those chats tend to be
similar than their their chats during
testing but the inner alignment problem
is saying okay now unleash this AI onto
the world where it starts having like
major effects on the world it can
reproduce itself it can be a virus it
can make major things happen right it
can start really seizing resources like
moving atoms around and and all it's
ever been trained on is just like these
these tests where it like convinced the
humans that everything was fine but then
in production it's having these huge
effects and there's not really that good
of a mapping so having past all of those
training test doesn't give us that much
confidence that when it's independent
out in the world with the power to
really do what it wants it's going to be
like well back when I was in testing the
human uploaded this so therefore I
should do this like that mapping is just
not strong enough and the AI Labs will
admit that they don't have a technique
to actually train a super intelligence
to go out in the world and and load our
values even if they're chatting as if
they care about our values they're
basically just hacking our test and it's
a different test when you go out into
the world so that's the fundamental
problem of inter alignment yeah yeah now
of course and it's pry clear that once
we have a super artificial intelligence
we can't do human feedback like
reinforcement by human feedback because
when it start to kill half of the
population we won't be able to say
that's not good it's too late right
exactly and I think and feedback loop is
just really the key because yeah we have
this technology right the the
reinforcement learning is like you can
think of the analogy as like okay well
I'm I'm like working with clay and I'm
just wearing mittens and I'm just like
using my mittens to shape the clay but I
don't have like precise control over the
clay I can just kind of beat it into
shape but like those details are going
to be very important because when you
get out into the world those exact
details are actually going to have huge
consequences like the details that I
never was able to shape in the clay
Suddenly It's There's No undo if we get
it wrong and and the whole world is just
going to have the micro shape so like
whatever micro structures are in that
clay that we could never control well
you know now it's it's too late to shape
them once it's out in the world I think
the the analogy with life is like
Evolution was trying to maximize genetic
fitness so from the perspective of like
natural selection it was trying to give
us the genes that would make us survive
and reproduce the best possible but to
some degree it failed because if you
look at you and me we're not competing
to see who can like donate the most to a
sperm bank right even though sperm bank
is going to really help you survive and
reproduce so that's kind of a failure
from the perspective of our creator it's
like why don't you donate more sperm and
I was trying to build you to really care
about your sperm and your genes and and
here you are just only having a couple
kids or you know children like what's
going on so there's kind of a
misalignment between modern humans and
the function that was trying to build us
so and that is inner misalignment
because I I think the outer alignment
problem is pretty straightforward with
genes where it's like yeah I can just
specify I can give you a math equation
for inclusive genetic fitness and I
think you can call that solving the
outer alignment problem for for
evolution by natural selection but the
inner alignment problem of making the
human brain care about that function uh
it wasn't really solved although I guess
I should add one caveat though which is
I do actually think if we were to
continue from more Generations I think
maybe eventually You' get humans who
love donating to sperm banks but like it
didn't happen right now yeah now it's
interesting to notice how we we had no
idea about this genetic fitness the the
the the source code or the our alignment
to Natural Selection we had no idea
about it until 50 years ago something
like that that's a really good
observation too right so for aill a
million years of human history right we
didn't even know what the what the
alignment problem was going to be so now
the AI is going be more reflective in
that right because it's it's just going
to be the reason we didn't know what the
alignment problem was is very simply an
intellectual limitation right so we just
didn't have I mean we like we didn't
know what atoms were right we didn't
know what microorganisms were the AI is
not going to have obvious blind spots
like that it's going to know all the
obvious things that a smart human would
know so the analogy doesn't quite hold
there but it is just interesting to note
like the our brain is very much just
like you know it learned a bunch of
heris it learned a bunch of adaptations
and it just shows shows you how inner
alignment doesn't get solved by default
so Ju Just because natural selection was
optimizing for something it doesn't mean
you get a brain that is just aligned to
care about that one thing it means you
got a brain that's kind of like that in
the training environment right so if if
you put a human back in like a hund
gatherer tribe or whatever may you know
there's no such thing as a sperm bank so
all of their adaptations work fine okay
yeah it makes sense and there's so this
idea of we don't really know what's
going on in inside this big you know
neural network with billions of
parameters and we don't really know if
there is something that emerges that
this is actually the goal of the AI and
once it's out there this is going to be
the core value of this AI like the way
we like certain things that have no
nothing to do with survival and
reproduction we like I don't know music
and and Cinema and stuff right now now
you mentioned this concept of like okay
what is really going to be the ai's goal
the way I think about it is like
remember I mentioned it's in a tractor
state where like maybe the AIS don't
start off having like the one big goal
right like GPT doesn't seem to
necessarily have one big goal I mean
obviously it's been trained all the
parameters have been tuned to minimize a
loss function fine that's different from
having one goal that you want to
optimize the universe toward so when you
first get an AI like if you just like
use GPT and you say hey can you give me
a script that I can run that'll go start
an online business for me and just make
money and keep improving the business
keep making me more money so when you
first do that okay okay sure maybe the
goal can be like make some money but
it's not trying to like optimize the
whole world maybe it has ways that it
likes to turn off but the problem is you
get an attractor state so whatever
version of the AI you get if it's more
instrumentally convergent like if it's
seizing more resources if it's a virus
that's more aggressive at seizing more
computing power well that's just going
to tend to grow in prominence right
similar to genes right the genes that
are just better at spreading themselves
well you get more of those so I I think
pretty quickly you're going to get
individual AIS that have a lot of power
and the that have less power and and are
less hardcore they start fading away so
now you got these a with a lot of power
and then the question is okay but do
they Converge on one true goal or do
they still have a mess of goals well it
it turns out that if you can make your
goal coherent that gives you more power
and more ability to reproduce it and
spread and compete with other AIS
because if you're coherent it means you
stop wasting resources doing
contradictory things so instead of being
like yeah maybe today I feel like doing
this now I just feel like sitting on the
couch and doing nothing it's like wait a
minute let me just merge everything
together into a single utility function
if you can make everything consistent
with a single utility function where you
can like rank order the value of
everything basically if you can approach
that mathematical ideal you just become
more efficient which then makes you more
competitive so even if it doesn't start
out being hardcore with a single goal
being competitive it's just going to
fall toward that state that's what I'm
expecting yeah an attractor state is a
good concept I think and you often talk
about the Doom train can you impact that
metaphor yeah so the Doom train you know
it's just a funny metaphor right it's
not really a train it's if you want to
be really precise it's probably like
it's like a lattice of different
arguments it's like The Logical
structure of the Doom argument where the
different nodes represent like okay you
have to believe this and this is like a
loadbearing assumption on this but I
talk about the Doom train and like stops
on the Doom train and like do you want
to get on the Doom train with me and
ride it all the way to the end and the
end is just the conclusion that there's
a highp doom so when I bring people up
on my podcast my question for them is
basically where do you get off on the
Doom train a very early stop on the Doom
train is like look I just don't think
that computers made out of silicon are
ever going to be able to compete with
the human brain that's like a very early
stop on the Doom train it's like really
you think that neurons are better
Hardware than silicon like neurons fire
20 times per second for God's sake right
as opposed to like two billion you know
and they use chemical moving parts to to
do their function right it's just like
neurons are a mess I mean they're the
best thing we have but they're so so
that that's like a very early stop on
the Doom train you know and like Roger
Penrose thinks that you need like
Quantum effect if you want to have like
a a smart AI or a conscious AI or
whatever so those are the early stops
and then a later stop on the Doom train
could just be like maybe like the robin
Hansen position of like yeah AI is going
to get very very smart one day but it's
we're just going to take so long to
build it and it's all going to happen
incrementally and humanity is going to
evolve with it so that's that's one stop
on the Doom train uh and then another
stop on the Doom train it could just be
like there's like the Yan Leon stop
where he's like yeah AGI is probably
going to come in like 10 to 20 years but
human will just figure out how to make
it safe so that's fine and then so you
just ride to later and later stops and
some people some people stop might be
like well I just think that I don't
believe in the orthogonality thesis
meaning I actually think if AI is so
smart it's going to be smart enough to
be moral and then it'll realize that it
should like treat us well and Elon Musk
might be accused of having this view
because he says stuff like I bet if AI
is curious enough it'll realize humans
are interesting and maybe that'll help
us survive which to me is just
completely illogical to think that
because the problem is sure we
interesting but we're not maximally
interesting so if you really try to be
like okay I want the most interesting
thing possible okay how about a zoo
where you got like a couple humans but
then you also get like a couple other
creatures and like a couple things that
you build like what's why would you keep
the whole human race if you're just
curious so yeah so there's so many
different stops on the Doom train and
from my perspective all the stops are
like very flimsy right so like I don't
see any stop worth getting off at I ride
all the way to the end of the Doom train
or or like you know I can give every
stop like a tiny probability but I go
and I'm like damn I'm here at the and I
still think we're doomed and I think
that the Doom is coming fast like I I
don't actually really see any Mainline
scenario like if you ask me Leon how do
you find if you had to get off the Doom
train somewhere where do you get off I I
you know I could I could try to
speculate but like it's it's really
tough yeah I like the analogy I think
it's a I would almost like to see it
visually you know like a train with
stops some sort of it could be it could
be a good pedagogical tool to to teach
people some concept there's there's a
somebody there's a guy on Twitter I
think it was uh Yenko so he actually
made a visual of his version of the Doom
train I mean there's obviously different
versions because there's so many
different pieces of the argument it's
not necessarily a line right you can
reorder them you can add dependencies um
so it is tough to have the whole
argument if you go to AIS safety. info
it's not really like a a big visual but
they do try to just like dump out a
bunch of different arguments so that's
one place but I just wanted to say my
podcast Doom debates what you said about
like yeah it' be great to see all the
stops on the Doom train I originally was
going to name it Doom train because the
point of it is like every
I to who the reason why they don't think
that we doomed is different right so
there there's like no one canonical
answer to why is Humanity not doomed
everybody just has their own version I
listed all the different ways you can
think we're not doomed and so I hope
people listen to my podcast and like wow
all of these different smart people who
come on and tell Lon that we're not
doomed they don't even agree with each
other on why we're not doomed so like it
maybe they're just all wrong and we're
doomed that's what I think I think
they're all pretty obviously wrong and
we're doomed so like I I just hope that
that I'm helping build a consensus
reality here of like wow the Doom train
is looking bad right now yeah def Among
The prominent voices in Ai and Tech who
are like the people that are dismissive
of AI risk that you find particularly
frustrating because you know they
they're really smart they rational
everything they say makes sense except
on AI or something ah okay do you know
okay so we can eliminate
does markerson doesn't even meet the
criter of everything else he says Mak
sense so but so basically like who who
do I respect the most but still strongly
disagree with so a couple and I have to
eliminate Yan Lon because even though
he's respectable as very respectable as
a machine learning researcher I I don't
consider him respectable in terms of
like epistemology and just other
statements he makes about like how to
reason I just feel like he gets so many
things wrong that it's not that
surprising that he gets a wrong so
that's the real question for me is who
is it really like out of character for
them to get AI wrong so a couple names
come to mind uh Steven Pinker I'm
actually a huge fan of his books like
how the mind works I think it's just
like one of my like top three books ever
it's just like such a great book how the
mind works and the language Instinct
like Stephen finger is really yeah one
of my favorite authors but then on AI
he's just so bad he's like one of the
worst takes on AI I've ever seen I have
one of my episodes of Doom debates is
reacting to an episode of Stephen Pinker
on a different podcast yeah his take is
just something like look it's not going
to have a goal and you know we're just
going to control it it's just he gets
off like pretty early on on the Doom
train with like not much of an argument
um so that's Sten Pinker that's one of
them Robin Hansen somebody who came onto
my podcast and was kind enough to debate
me and we had a very interesting debate
Robin is somebody where he's in my
opinion one of the best thinkers alive
today I'm a huge fan of a lot of his
project with grabby aliens I think it's
underappreciate underappreciated how
Robin Henson solved the fmy Paradox with
a living cosmology I I think grabby
aliens is a huge accomplishment huge and
and yeah and he's underrated for doing
that and of course great filter
prediction market so yeah Robin anon's
great but on the subject of AI I
strongly disagree with him because he I
think he said that his P Doom is less
than 1% and his reasoning for it is like
look I just think it's just the next the
next stage of humanity it's just like
the Industrial Revolution AI is coming
you know it's all just one big hyper
exponential or one big exponential curve
maybe it's hyper exponential because the
speed of doubling and his view keeps
getting shorter so maybe not
hyperexponential but more than
exponential uh anyway yeah so has you
can go listen to my debate if you want
to see Robin Hansen so Steven Pinker
Robin Hansen oh David Deutsch David
Deutch is a major name that comes up
because obviously the beginning of
infinity is a great book uh the his
other book oh fabric of reality really
great book uh the guy invented Quantum
Computing really smart guy I'm a huge
fan of David deuts but then you listen
to him talk about AI I feel like he's
kind of a big punching bag for me on my
podcast because I'm always saying like
anytime the word creativity comes up I'm
always saying David deuts thinks llms
aren't creative when like in my mind
they obviously are and he doesn't really
he explicitly says that he doesn't want
to Define what it means to be creative
because the moment you define it you've
boxed it in and it's like that which can
have no definition and he's like really
over complicating it was in my mind it's
like look creativity is just like
accomplishing a search more efficiently
than an NA algorithm would do it like
it's not that hard to be creative okay
so so those are three names I think
those are the first three that come to
mind I could I could probably list a
couple more but like oh and Scott
Aronson who I'm working on an episode of
my podcast right now reacting to Scott
arenson he's also one of my favorite
thinkers like super smart guy in
complexity Theory his epistemology is
good so he's always like questioning
himself and talking about like how to
reason he's got a very good mind for
like that kind of meta analysis which
makes sense because he also deals in
like you know meta math and like proof
Theory and stuff like that but on the
subject of AI Scott Aron he's kind of
like middle of the road like I think his
P Doom is like I don't know 5 10 15%
something like that which is kind of
like middle middle of the road in pdom
so I don't have like a huge beef with
scottt but I do see Scott making
mistakes that seem pretty obvious to me
like he is not willing to say that like
the orthogonality thesis is true he's
like yeah it's maybe maybe not and he
has he has some other Concepts that that
just don't make much sense to me which
I'll I'll be reacting to so I I'll just
throw out his name because again I I
super respect him and then to me it's
scary when I see a thinker who I respect
so much like you know David de Scott
Aron and then I see them making what
looks like a very primitive mistake on
the AI front and then I'm just like if
these guys can't get it right how are we
as a species going to get it right
because this isn't even the hard part
these are just like table Stakes like
understanding the Doom problem so then
that makes me very scared yeah I think
it sometimes may be just some sort of
optim optimistic bias towards the future
we we obviously want to be living in a
better future and we think AI is
definitely the the best tool we have to
solve many of our problems but it's very
hard to see maybe there is a little bit
of denial I think that'sit the case for
myself I kind of I see the problem I am
actually a Doomer I think we don't have
a lot of yeah probability to survive and
during my lifetime but I can't leave my
life thinking this it's too too hard
yeah it's a good point so personally the
way I manage it is that I just have my
logical mind be like hey the argument
for Doom is very strong like the logic
says we should stop building AI but then
like dayto day I still just live a
normal life right I do normal things
hang out with my kids do activities I
like so I'm just willing to say the
words right like Hey we're doomed right
I mean am I am I willing to am I
donating all my money no I'm not
donating all my money I'm still living
living comfortably so you maybe other
people can copy my example of like look
can you just say the right words that
were doomed if you happen to be in a
position of making policy can you just
make the right policy like is it really
that hard yeah especially for people
like the one you mentioned you know
their voices are important I also want
to mention maybe Ben gzel CU I I talked
to him this podcast and I really like
him he's a good you know transhumanist
very big big ideas about the future and
I'm not quite sure how much he has
contributed to the field of AI but he
doesn't really seem to be on board with
the AI Doom arguments and he understand
them but he thinks we'll be fine right
and you know another name the mix
another name is Steven Wolfram right he
had the debate with Alazar owski a few
weeks ago and Steven Wolfram is great
like such a smart guy and when he
debated Alazar he was very good faith he
was listening to everything elzar said
he even admitted by the end of the
debate he's like look you're elzar
you're putting out some ideas that they
they do make sense and I need to think
about it more before I can get back to
you with an objection so Stephen Pinger
was very open-minded about that but like
what scares me there is just like okay
well look the human species we're not
very good at thinking like people like
Steph Pinker Robin Hansen Scott arenson
Steven Wolfram like you guys have to be
the people who blow the whistle on Doom
you you guys are the captains right
you're you're in the the the nest what
do you call it like on the ship like the
the lookout tower or whatever you know
you guys have to tell us when's Doom
approaching that's bed theseal concept
so if you're just treating it as like oh
that was a conv alaz all right I'll see
you next time no stepen wlr go verify
that we're doomed like we need your
brain to do this for us yeah definitely
because there's a lot of people who
would take you know arguments from
Authority seriously in a way like it's
anistic it's like okay this person that
I respect said we don't have to fear AI
so I'm not going to think about it and
and if these people are you know policy
makers and all that it's pretty bad do
you there is another concept like yes
like the the meme of the the sh go do
you think it's a it's a good depiction
of of AI because I find it pretty pretty
good I do think it's good so I think
what we're talking about you know the
Shag looks like kind of like a big green
octopus kind of thing right a lot of
tentacles and it's like a weird alien it
kind of looks scary but then it it's
just wearing like a a yellow paper mask
with like a human smiley face so that
and then people just like talk to the
smiley face and like wow it's so
friendly yeah so I think the meme is
very good because it's the question is
really just like can you keep the mask
on right or or is the mask going to slip
off and I think it really just gets down
to the problem of training versus
production right like are you are you
going to convince somebody in training
that everything's fine and then it gets
let out and then it's in production and
then it has no off button and then it
turns out you didn't train it perfectly
and I think the answer is when we look
at AIS today it's not a problem for for
two reasons number one production is
kind of similar to test so chatting with
them in production is the EV that they
do during test it is pretty similar so
that that's reason number one and then
reason number two is there's no
catastrophe because they don't have the
power to actually go and and mess the
world up they only have the power to
like give you some bad answers and that
you know we can recover from that kind
of error so there there's an iteration
Loop we we'll bring them back into test
and we'll fix them right we'll take the
car into the shop it's no problem the
problem is when they actually get really
powerful where a single slip of the Mask
is game over right that's that's where
we're heading toward yeah no it's it's
also a good meme because it shows how
alien they are because you know we we're
talking about something that has been
trained on reading the entirey of
Internet which is so alien and then it
talks like a human so there is this
weird it's terrifying to think there's
this lovecraftian horror behind right
and and this is actually you know you're
reminded me because this is one of the
issues that I I noticed when I was
listening to a Scott Aron podcast where
Scott Aron pointed out is like look we
don't really know how well it's going to
work to align the Shag he didn't use the
term shag but he pointed out he's like
sometimes when you have a human and the
human spends many years of their life
kind of putting on a certain act
pretending to be a certain way
eventually they just like really become
that right they're just like that's
their groove they don't really have
something else so maybe the AI can be
like that right that would be a good
case and I just I don't see it working
like that I really just think it's it's
getting optimized to pass your tests
like that is it its nature is to pass
your tests it's not like a human where
it's born a certain way it's got a you
know it's got a bunch of hardcoded
behaviors and then you're you know
you're training it you're modifying it a
certain way no you're just training it
from the ground up where all it wants to
do all of the variables are all pointing
in the direction of passing your tests
so if something helps it pass your test
it'll do it if it doesn't it won't
simple as that and so if it spends its
whole life passing your test but there's
a new production environment and you
didn't do your job of having the test
mapped to that environment there's no
reason to expect that it's going to do
what you want right okay so if we okay
if we say okay this is this is we're
going to likely all die from Ai and it's
you know we have to face reality do you
anticipate like a quick death we we
won't even know or is it going to be
because I'm I'm kind of sometimes you
know there is existential risk and
suffering risk and more obviously I
would prefer to die in my sleep than
being tortured or you know like starving
to so what's your most likely scenario
obvously it's hard to say but you know
do you think the be a good question yeah
so and it's and it dovetails with a
question that a lot of Skeptics ask
which is like how will the AI physically
extinct everybody like okay I get it
it's smarter than us but like what's
really going to happen right is is
Terminator going to show up at my door
and shoot me so I I can give you my
answer um I think it's interesting to
consider Al asra's answer he often
points to a scenario where the AI just
kind of Lies dormant and just hatches a
plan and the plan is like okay I'll like
I'll get access to a biolab I'll just
manipulate like a few humans and I'll
make biotechnology I'll make
nanotechnology I'll get this spreading
out in the world like reproducing itself
maybe a gene Drive stuff like that and
eventually when I'm ready to strike like
one day the virus will activate like the
dormy period will end and billions of
humans will just like fall over Dead uh
and and Alazar just imagines like just a
nice clean strike because the AI is just
maximizing the probability that it'll
succeed and reach its goal and I think
in elzar's mind it's like why would the
AI make it messy why would the AI allow
any probability of humanity like
realizing what's going on and start
fighting back and sure the AI will
probably still win even under those
conditions but why doesn't the AI do a
clean strike I feel like that's kind of
alzar thinking and he might be right
when I think about it what comes to mind
for me is it's just chaos because the
first thing that comes to mind for me is
the ratio of terrorists to good people I
think that's the first thing we're going
to see changing right people don't
realize that this is a loadbearing
Assumption of our society like Society
just works because you know me and you
we're good people we love the rule of
law and when we think about hey do we
want to go you know hack into a server
or drop a bomb on somebody we just think
look maybe that'll accomplish a
short-term objective that we have like
if we really don't like some institution
and we bomb them maybe that'll help us
out a little bit but besides the fact
that we don't like bombing you know we
don't like death but the other issue is
like you know we're going to go to jail
we're going to ruin our lives so we've
got a lot of downside so that's why you
know you and I besides being
fundamentally moral people we just don't
benefit to go and become terrorists now
the calculus for the AI is very
different for a couple reasons number
one downside risk it's not too bad like
if the AI is discovered doing terrorism
what are they going to do like they
can't really you can't put the AI in
jail right so if you're an AI it's just
kind of like whatever you can do go
ahead and do it and because you it has
all these other copies and you know you
could be like well maybe I'll try to
like find all the copies and shut them
down but like long story short I think
that once everybody has access to a
really powerful Ai and they can like
send the AI off to go and do their work
there's going to be a bunch of Agents a
bunch of entities that are very hard to
control that don't have human morality
and all implementing chaos so the
specific thing I expect is just like I
think pretty quickly the internet stops
working I don't think there's going to
be enough working coordination to even
let me access the internet right I think
something along the chain whether it's
like the servers or the the power just
like I I can't log into the internet
like everything's just going to break
down and and our quality of life is just
going to step down until we're just like
you know a bunch of cavemen right like
with like hoarding our resources and
then the AI has like its own Army right
like the AI has just built a core of
people who are like coordinated with it
getting rewarded they still have a good
quality of life they're like the ai's
Army basically and they're the
Terminators right it's like okay great
now go pick off all the cavemen and I'm
just going to be one of the cavemen
right having like a terrible quality of
life wow that's that actually something
I'd like to see being made as a
Hollywood movie to be honest because
it's I actually I am you know like I
make films as I'm I'm a filmmaker I just
think yeah it's just going to keep
taking a hammer to our coordination like
we have a delicate balance of
coordination where you just need a lot
of humans who care about coordinating
with each other right care about keeping
systems up and suddenly you're just
going to have too much intelligence that
cares about smashing the systems and
repurposing the systems yeah yeah yeah
and
obviously life will yeah will be pretty
bad at that point and the idea that it's
not going to be you know Boston Dynamics
robots killing everyone but just people
with M16 and just just being paid with
Bitcoins by thei right and I mean cuz
that's under the assumption that it's
it's just like broken our main lines of
coordination because it it's all super
fragile right like like when things go
down you know when there's a power
outage or whatever like you can just
count on enough humans like you know 99%
of all the humans in the area all are on
the same page of wanting to restore the
power right and then that's why it
eventually gets restored uh but the
problem is you know you break a few
different things you leave humans
without power for a while you know you
you drop a nuke on a city it's going to
take a long time for for that City to
recover or like yeah just in general
when when you just mess stuff up and I
like that's the problem is we've never
had an agent like you know we've had
viruses and viruses do a lot of damage
but it's always has been effective for a
team of computer Security Professionals
to get together and be like how do we
neutralize this virus that balance where
the Security Professionals are able to
go neutralize the virus I think is a
loadbearing balance and I don't think
it's going to sustain very long yeah and
is there anything we can do to prepare
for this scenario because for example if
I think about the case of nuclear war we
can always think okay if you look at the
map of the world the South hemisphere
looks like it might survive so I I
better you know live in Australia or New
Zealand so if some takes the risk
seriously they might decide to move like
I live in Australia it's not because I'm
I'm scared of a nuclear war but it's
it's a good side effect that in case of
World War II I might survive a little
bit longer but with super artificial
intelligence I guess there is nowhere to
Nowhere nowhere is safe probably well I
I also think that the probability of
nuclear war is extremely high but I
think AI Doom is like 10 to 50x bigger
of a threat than nuclear Doom but if AI
Doom weren't my first priority
I'm a huge believer in nuclear Doom
right I I really do think there's like a
solid 1% chance every single year that
we start nuclear war which is insane
like I think nuclear weapons are are
vastly underrated and it's become cool
to talk about how like nuclear power is
so great and we should have been like
more bullish on nuclear power and like
yeah nuclear power is great but nuclear
weapons are so bad like they're they're
like it's underrated like people people
really don't get how bad nuclear weapons
are this is like a hobby horse of mine
so but yeah look living in New Zealand I
agree that's like a good move if you're
really afraid of nuclear war but that
only eliminates like you know 5% of your
risk because most of your risk is in the
AI situation and the idea that AI is
going to spare one island or one bunker
I think that that is true for like maybe
a couple years right but I I just but
there's no reason if the AI is Hardcore
which is a very loadbearing claim for me
like we are going to have ai that is
Hardcore well the hardcore thing to do
is you don't leave any humans if a
single human exists that creates a
significant risk of somehow spiraling
into a CH events where the AI loses
power and so of course you pick off
every human that's what you do if you're
hardcore AI yeah and I suspect the AI
once it take over it will want to shape
the world to its own image or purpose so
you know might decide to remove the
oxygen from the atmosphere or TI the the
planet with super calculators this kind
of thing so basically we will no longer
have an enironment to live in the the
ecosystem will collapse and we die
because yeah there's definitely an
analogy I mean humans are causing
currently the sixth Extinction right so
it's documented that like so many
species that used to be alive are no
longer alive like a million or something
and it's it's very simple it's just like
you know we see a forest it's like okay
well I'd rather have no trees here I'd
rather just like build a farm or houses
or things that I like okay well you just
killed a bunch of species like sorry you
know I didn't mean to but I just wanted
to optimize the situation to do
something different so definitely alzer
points out if you just have a lot of
Agents doing stuff one of the things
that they do if they're efficient is
they they they use all the different
resources and they they push the limit
of radiating heat away right like like
productive work requires radiating heat
so anytime an AI is building densely in
a space it becomes like a very hot space
so even just the heat is already tough
for humans yeah yeah we're very fragile
as like the whole planet is is kind of
you know the goldilux Zone all those
things if we take a human you put it
anywhere else in the solar system we die
instantly so yeah so now is is it
basically our only hope is to not build
those system so I know you're part of
POS uh how do you can you maybe describe
a little bit what posi is even I did a
podcast with some someone from posi the
French Branch but yeah and how realistic
it is to effectively have a
international pose so the idea of pause
AI is that if we could slow or stop
capabilities progress then maybe we
would have time for alignment progress
to catch up right it's like those two
lines capability progress alignment
progress I know some people would argue
that they're all one line it's not two
lines but in my mind it's still two
lines I do think there's such a thing as
alignment theory if you look at what the
machine intelligence Research Institute
was doing for many years they really
were just trying to lay down foundations
of intelligence theory and Alignment
Theory so for example even just the
Insight of instrumental convergence the
and the idea of what I talked about the
the convergent to tractor right that
like AI are going to self- modify into
AIS that are more hardcore more power
seeking like even just that Insight was
underappreciated before Mei the machine
intelligence Research Institute
popularize it and then corrigibility the
Insight that it's going to be really
hard to turn these things off because
they're incorrigible uh and if you want
to just Define a simple formalism for an
AI that's willing to turn off uh it
turns out that you can't or at least
they weren't able to so there's not even
such a thing is like an AI That's calm
enough to let you turn it off so these
are the kind of insights that you get
when you seriously have smart people do
alignment Theory like foundational
alignment theory of like look regardless
of what the current AI technology is
what does it mean for a super
intelligent agent to be aligned what
does the specification look like what
are some properties that we can refer
about those things like very fundamental
stuff and it's we made some insights but
it's clear we're like many years away
from having all the scaffolding we need
to have a hope of being like okay now
let's build something that looks like
this thing that we understand as like an
aligned AI um and yet when we go on the
capabilities train unfortunately
capabilities is a fundamentally easier
problem because as Ilia satk says you
know these AIS want to learn like they
you don't even need supervised learning
so when you do alignment it's like
fundamentally supervised because we're
trying to get them to go where we want
whereas when you do capabilities you
just say like hey it's convergent right
so so you don't have to teach them
physics like they can just use
unsupervised learning to let the world
reinforce them and suddenly they know
physics really well so getting good at
stuff that's just objective capabilities
there's a lot of ways to do it like
without human involvement where you just
like fall into this reinforcement Loop
you know an unsupervised Loop and and
you just nail it and that's why
capabilities is just racing way ahead
right and capabilities okay throw in
more Nvidia gpus now you have more
capabilities alignment throw in more
gpus you don't get more alignment so the
problem is capability just keeps racing
way ahead and the POS movement is like
look we're fooling ourselves if we think
that these tiny little safety
departments at these AI companies are
going to keep the capability safe that's
such a pipe dream it's insane the adults
in the room should be like look they
should just admit that they're just you
know it's it's it's a pipe dream like
this isn't serious what they're doing
and you know Jeffrey Hinton says his
words are rhf is like a pile of crap
when it comes to Super intelligence if
you look at the Exodus from open AI or
even you know Dario talking about from
having a 10 to 25% pdom these experts at
the AI companies they know that their
own efforts aren't suited to the task
open AI started a super alignment team
last year because they knew they needed
it they knew that they didn't have super
alignment solved so they will admit this
if you talk to them so anyway it comes
back to P of saying like look instead of
fooling ourselves instead of doing
wishful thinking why don't we just admit
that it looks like the only option is
either to have capabilities do what
they're going to do to us basically like
let the chips fall which is probably we
die or we all race to we all work
together to not build capabilities and
try to buy time and then and then from
that condition try to work more on
alignment and honestly I don't like
right the idea of stopping capabilities
it sounds so insane it sounds so
antithetical to progress but like when
you frame it this way of like you know
what choice do we have than to try to
hit the brakes on on something that just
seems like it's about to kill us yeah
yeah and I think sometimes maybe a good
scenario will be to keep Pro having gain
in capabilities but in narrow domains
like solving cancer research Alzheimer
and we don't have general intelligence
we just have a lot of very super AI in
narrow domains so that would be easier
to to control but um right yeah I could
I talk a little bit about that right so
like the ideal could be like you know
deep Minds Alpha fold where it's like it
seems like it's contained to only help
you with protein folding problems and
it's not going to like hatch a plan to
take over the world that I think that is
a productive Avenue so if we could have
like a monitoring system or like
regulation where it's like okay you have
to get your code base approved as being
narrow stay within solving narrow
problems I think that could buy us a few
years the only problem there is like
there's problems with everything so I
mean I think that's a good approach but
the obvious problems there are just like
well anytime you get really good at a
narrow domain any domain to get
sufficiently good at it kind of makes
you want to become generally intelligent
anyway just because having goals helps
you solve problems so like if you're
stuck like oh for this protein folding
why why can I not model this piece this
piece seems extra tricky maybe I'll I'll
run a subprocess to try to go think of
reasons why how I could like improve
reasoning about this right there's
there's kind of like this Universal
nature to solving any problem by using
the same framework of like okay well I'm
just going to like chain different
actions together right chain strategies
together I'm going to map goals to
strategic actions that'll help me
achieve the goal and this is true even
when you're working in a narrow domain
so even like you know self-driving might
be a more intuitive example where it's
like you solve 99.9% of the self-driving
problems just using like I don't even
know just reinforcement learning or
whatever like the vision feeds into like
the steering wheel actions but sometimes
you get into a really tricky situation
where there's like a bunch of humans on
the road and they're all like waving
their arms trying to tell you something
and now even though it looks like a
narrow domain even though it looks like
driving if you have modules to like
reason about Humanity or Reason about
the very unique circumstance in front of
you well suddenly the domain of those
modules gets very large and it turns out
that it embeds kind of a general
intelligence problem even though it
seems like a narrow domain and I think I
think that's true about most interesting
domains is that in in their edge cases
they're just going to embed like General
problems and then you're and if you
train the AI hard enough you're going to
get what happened with humans right
humans were just trained hard enough to
just like go hunt for prey right like we
weren't we didn't have we weren't
trained to understand the universe right
we weren't trained on physics we didn't
get rewarded for writing better
equations about how to get to the Moon
it just came for free because we got
trained on you know the the Savannah
like surviving in the savannah right and
so I think that's going to happen even
with narrow AI right yeah it's kind of
knowledge leaks basically you need right
uh it's trans disciplinary is better
than just being completely isolated from
anything else to solve a problem so you
you almost even we might be stuck
actually if you want to solve Alzheimer
by giving an AI just data on on brains
and stuff maybe it won't be enough so we
it be incentivized to give it more right
there's an analogy that I like to me
coming from a computer science
background in computer science we notice
that a lot of things tend to quickly
become Turing complete so for example
like you know you could play Minecraft
and maybe that I don't even play
Minecraft but maybe it's more common to
be like oh I want to build a house in
Minecraft but somebody who knows about
computer science is like well I want to
build a computer that's playing
Minecraft inside Minecraft right it's
like recursive and you can like build
anything and it's it's Turing complete
and people have pointed out that like
you know you add a new feature to
anything any kind of game or any kind of
computer program be like oh let me write
this feature where you can like tweak
these settings and and it'll be more
powerful the moment you start layering
on a new features in a computer program
pretty quickly you've created something
Turing complete like it's a pretty low
bar and Turing complete just means okay
now somebody can come in and they can
just like program anything into your
system the same you know you can program
a computer inside Minecraft or you know
Conway's Game of Life you can program a
computer inside Conway's Game of Life
like things just the CSS you know
cascading stylesheets it was just
supposed to be a way to like give web
page like tell what colors and fonts you
want on a web page but of course pretty
quickly they added enough features that
now you can program entire functionality
inside CSS you really shouldn't but it's
just like we always get there we we
always get to turn completeness and
there's an analogy there just like every
computer system wants to be Turing
complete every AI system wants to be
what I call Goal complete so every time
you think you're building a narrow AI
system if you push it hard enough it's
going to want to just have goals and
have a framework for being like Oh okay
you give me a goal I will map that goal
to actions That's The Natural end state
for any kind of narrow artificial
intelligence application to get to the
moment that that you to make it a little
bit more powerful there's that leakage
right the same way cascading sty sheets
leaks into being like Oh well here's a
function that can now let you play a
game just in the CSS language the same
thing with AI right it's like oh okay
now Alpha fold can still figure out how
to like you know fold the entire Earth
right like that's I I really do see a
deep analogy there yeah it's useful
analogy definitely are you sometimes I
guess you could be worried that if some
people agree with the aid argument
they're fully convinced they might
and if they are maybe mentally unstable
they might become like a new kind of Ted
kazinski and and and you know we could
see assassination attempt terrorist
attack on data centers this kind of
thing and I don't know if violence is
not a way to solve the problem right
right I mean I have seen some people in
bad faith accuse AI Doom people the
example that sticks to mind is when
Alazar proposed look we need an
international treaty uh where you it
shouldn't be legal to push AI
capabilities forward uh and of course
like any International treaty that we
take seriously if somebody breaks the
treaty there are consequences up to and
including bombing their data centers
because they violated the treaty and
Mark andreon loves to bring it out at
any opportunity that he gets to being
like AI doomers Advocate violence right
as if like the the the international
cooperation structure the Democratic
structure of making treaties in his mind
he's using that as as an accusation of
violence where where it's you know it's
kind of like the least violent thing you
can do is have a treaty that yes is
enforced with violence if people who
break it right like breaking a treaty is
arguably Vines so anyway but but I think
to your question you know you're saying
well what about like people going crazy
what about like alarming people and yeah
it's it's fun and and it's easy and fun
to go talk about all these second order
consequences of of believing this
particular belief I personally don't
spend much time talking about second
order consequences because to me the
more interesting question is like is
this the right belief if it's not the
right belief right if there's a small
chance of this being the right belief
then you could be like look there a tiny
chance that Lon is right and Lon is
making people get alarmed yeah if the
chance that I'm right is Tiny fine but I
don't think the chances that I'm right
is Tiny right if I have a significant
chance of being right that we're doomed
then forget the second order effects
like we got to just talk about the claim
yeah of course the consequences are
obviously more terrible than alternative
so yeah and how do you like I guess
sometimes when I think about AI I'm not
entirely sure how we're going to well
the brief moment would have to play with
AGI how it will look like is it like a a
chat Bo basically like chat GPT but it's
AGI or do you think we have humanoid
robots in the streets maybe an app on my
phone that talks to me and and solve my
problems I don't know yeah that is very
hard to predict right that's I I don't
consider myself I don't think anybody is
actually able to predict that because it
comes it comes down to a couple
questions one is just like okay how fast
are robots just going to get more and
more humanlike and animal like at their
movements I think there's major progress
happening there right like I mean
walking robots seem to have gotten very
good at walking which like 10 years ago
was still a pretty hard problem for them
and I do actually think that that might
be one of the things that finally gets
people as scared as they should be is I
I like to talk about this example like
if you have a humanoid robot like a
Tesla Optimus if it comes into your
house and you know it's not even
remotely controlled it's like operating
autonomously and it's like it can kind
of like beat you in a fight right like
it can beat you at like physical
competitions where you can just go a
wrestling match like human versus Tesla
Optimus and if it's wrestling you down
and it's on top of you and like it won't
let you get up unless like you know
somebody somebody else like the
moderator press the button or whatever
at that point people might be like wow
okay this is maybe we really should get
more guarantees on these things um so
maybe that'll appropriately raise the
fear level because there's no doubt that
this will happen and they'll be like
okay finally this isn't a chat bot this
is like this is an agent that I don't
even see what advantage I have over it
as a human because it can come you know
it can come put me in a cage right the
same way we put animal in a cage this
thing can come and put me in a cage and
close the door I never open it right if
it has a certain programming yeah yeah I
think it's bound if we have and then to
your question of like yeah what's gonna
happen first yeah you want talk okay
yeah so so how fast is that going to
happen I don't know I think pretty fast
but then how fast is the the chatbot
intelligence amplification going to
happen also fast right so it's kind of
hard to predict which comes first if you
ask me what are AI lack today a lot of
people give bad answers a lot of people
are like well they're just stochastic
parrots they can never go out of just
distribution they can never be creative
which to me is like just totally wrong I
can't believe people are are saying that
if I had to describe it I would just say
they're not robust so like you ask them
questions they keep saying smart things
they keep getting the questions right
except sometimes they get the questions
wrong so like their hit rate isn't 100%
And then when they get a question wrong
they don't kind of notice and be like
you know what I'm not sure about that
let me correct my mistake and so they
just get mistakes compounding and that
that seems to be like the common issue
with them right is they they'll make
occasional mistakes and they don't fix
it and by the time they are a couple
pair in it's just like pointless right
they've gone off the rails so if you
just keep making Generations that like
somehow make fewer mistakes which sounds
crazy but that's kind of what we saw
going from gbt2 to gpt3 to gbt 4 it's
just like fewer mistakes happening
somehow right so maybe we'll get to play
with an agent that like somehow makes
fewer mistakes and somehow can like do
bigger jobs and and you know that'll
happen on a faster timeline than the
robots I don't know but that's that's
just how I think about it yeah yeah
definitely because the robot thing I
think it's it's actually when when you
look at self-driving cars we didn't have
to wait too long before a car killed
someone and I think if we start to have
humanoid robots it's bound to happen
that sometimes one robot will push a
lady old lady in the stairs by mistakes
and then we we have newspaper talking
about it and it's kind of outrage online
because this like the first case of a
robot killing a human this kind of big
headlines and there might be what we
almost what we need to wake up but yeah
maybe it's too late because you I don't
see humano robots coming before the 2030
as a common thing but maybe but also
keep in mind that you know but remote
operation makes them a lot more useful
so you can actually imagine owning an
Optimus and just having your you know $5
an hour person in the Philippines who
just like does all your housework by
remotely operating your robot oh wow
yeah I've never yeah that's a good point
I've never think about that or even like
just because I live in Australia my
family is in France so I would to be
able to you know like take a VR headset
be in in robot in France and and hug my
family and you know it would be yeah
yeah yeah like an amazing experience
more than a video call so that's maybe
the next step yeah yeah and then of
course that's all training data for the
AI yeah yeah is there is there any
sci-fi films or books that you think
depict realistic take over or no I think
there's a shortage of that I mean I try
to watch whatever movie I can that I
think could could do that but I
definitely don't think there's many I
think Terminator is pretty good I mean I
don't think that it really tells you
like why the AI wants to destroy people
but I think the reason why is just like
pretty simple you know it just like it
gets that goal into it somehow like it's
a convergent goal right or it just gets
a goal that we don't like what I like
about Terminator is just the depiction
of like okay well these robots are able
to overpower Humanity like they're very
powerful they're not aligned with us and
this is what happens so I I like that
part of Terminator and then the movie
don't look up it's not even directly
about AIS but I actually like that movie
a lot because what I think is super
realistic you know the scientist Dr
iswell Who wants to go mine the asteroid
to me that's very much like the AI Labs
where it's like okay looks like we're
doomed but but but look this next GPT is
coming out okay and it's going to help
you for work more and we're going to
keep building and like yes we don't know
when we're pushing it too far but we
will figure it out we will steer it away
and what I really like about don't look
up is like the scientists is saying all
the stuff you know the day comes right
like the the the final hour the moment
of truth comes and they just fail and
everybody dies and I'm like yep that
looks like a good portrayal of of the
current timeline yeah definitely when I
watch this film I was also thinking
about Ai and the Terminator is actually
good as well except the time traveling
part obviously and and I think even the
humanid robot with glowing red eyes it's
a bit too much but the just the skyn net
is realistic because it's the US
military using it to have an strategic
advantage and we we're starting to see
these the side of thei today the labs
like entropic and open they're starting
to have contract with the Department of
Defense in the US so yeah it's and I
think there was this what's his name
Leopold something um complicated name
yeah he wrote the report and his
conclusion is like the first it will
become a geopolitical strategic
advantage to have AGI so the first
country who can Master the power will
dominate the world and we'll see those
big government taking over the AI Labs
maybe yeah so you know the the Doomer
response to Ashton berner's work and
many like it is just that like it's not
we don't the alignment problem isn't
solved so Leopold's work just assumes
that we can keep building AI that we can
continue to control and the the actions
that the AI does represent the actions
of the human operator now maybe
Leopold's work is what the world needed
because Leopold he has a bunch of
implications of like look if we had
these really super superhuman servants
things that did what we want and were
really powerful that would upend
geopolitics that would change everything
and a lot of people read that and
they're like wow this is going to change
everything but what Leopold left out is
like okay but they're actually not going
to do what we want they're going to do
what they want right and then we and
then we won't be able to stop them so no
country is going to win so that whole
framing of like yeah we need to go be
first it's just so bad and unproductive
so like I I tend to think that I'm that
I'm sad that the work exists because as
much as I appreciate like building the
urgency the frame is you know he assumed
so much most people who read it they
don't even think to ask the question
right because they don't have that
background I mean I personally if I
hadn't read Alazar owski I don't think I
would think to ask the question of can
we control AI it just feels so intuitive
of like yeah we built them they're going
to do what we want and so and Leopold
really didn't help that question get
asked yeah okay and so another question
that's a bit outside the AI topic it's
about cryic are you I don't know like
it's transum M now are you
foric would you would you sign up for it
and I guess yeah yeah I'm very much for
it full disclosure I've attempted to
sign up a couple times and I never got
through the process cuz it was just like
a little bit annoying I really should I
guess part of it is just because like I
don't know I just I guess I just don't
feel viscerally that excited about being
cryopreserved but like rationally should
I do it yeah totally I really should do
it if I could snap my fingers and do it
I would but yeah I'm I'm very much for
and of course you know the the value the
expected value of it is obviously lower
since I think Doom is is like imminent
but you know there there's definitely
like tens of percentages of chance that
we're not doomed and then within the
worlds that we're not doomed there's
definitely like a 10% chance that CICS
will work so like yeah I should I'm crya
right I'm procrastinating on on prionics
but like fundamentally is it a good idea
yeah I mean it's it's standard
transhumanism right like why why
wouldn't it be a good idea yeah yeah
sometimes I I also agree but I've got
this I guess like a fear that if you
wait if in the future some someone wakes
you up but it's it kind of scan your
brain put you in the simulation suddenly
you become it's kind of dangerous in a
way cuz you might be in a Hell
simulation you don't really know what's
going to happen in the future maybe the
future will be terrible and I guess one
good thing about death it's kind of
permanent but maybe in the future death
death won't be even an option if you're
suffering so that's scary right right
right well I think that that gets to the
related discussion of you know s risks
right so suffering risks so and I I tend
to like never even talk about those
think about those cuz for me it's just
like okay the AI is just going to
destroy the future we would have had a
great future where where you know we
would have stayed on the trajectory the
world keeps getting better the economy
keeps growing things keep getting nicer
and the AI is going to take all that to
zero but then there's the possibility of
actually the AI is going to take all
that to negative Infinity right like not
just zero like it's going to just be
like so bad it's going to be like way
worse than an empty Universe like we
will wish that the universal is empty
because AI is going to make it so bad I
mean I tend to not think about that
because just like I think the is likely
to not what what we not want what we
want I also think that the AI is
unlikely to want something that involves
creating a bunch of suffering s
sentients it all it seems like it would
have to go out of its way to do that now
if we try to make a wish and we make it
wrong it's possible that that wish
involves a bunch of suffering sentients
so don't get me wrong I think the
probability is significant I just see it
as like a a relatively low like I don't
know a less than 50% chance so I just
don't spend much time worrying about
estris I I worry about just going to
zero yeah yeah makes sense and in the
case of cryonic there is always this you
know if you think about a future where
the our descendant will bother to wake
up the dead it means they're probably
fine it's kind of a future where exactly
you know everything is solved so you
know it's kind of fun for them to wake
up their ancestors and and greet them
and give them a good life because they
know they escape death in a way I mean
we would probably do it if we had the
the power yeah I think a good metaphor
is that being chronically preserved you
know it's like an ambulance ride to the
Future everybody would accept an
ambulance ride over space right so why
don't we expect an ambulance ride over
time like this is clearly marked like
ambulance patient it might help your
intuition to be like look we're not
talking about like a thousand years here
like if cryic Works it'll probably work
within a 100 years 200 years you know
probably less than 100 years if cryic
ever does work so it's like do we really
think that Society of less than 100
years from now is going to be like that
mean to us I feel like it's just like an
ambulance ride yeah yeah it's always
good know another question like let's
say AI alignment is solved or POS AI
works and we have enough time anyway in
2050 we have a powerful super align
intelligence how a positive future look
like for you what's what's your Utopia
in the way yeah my Utopia so I don't
consider myself that insightful on
Utopia I'm just aware that there's a
problem of defining Utopia I think Nick
Bostrom tackled it pretty capably in his
book where he's just saying like look
there's no like it's not that obvious
what Utopia should be when you when you
just have zero constraints he makes a
good point of like a lot of the things
that we like they they're just mixed in
he uses a great analogy of the
exoskeleton you know like insects have
an exoskeleton instead of Bones and it
like keeps their innards in an organized
shape because they have this I don't
even know like this cage around them so
for example like yeah it would be great
to be healthy without having to actually
do the work of exercising but at the
same time like exercise is kind of like
an opportunity to challenge myself or
maybe like see nature if I'm exercising
outside and now I'm losing that because
I'm just sitting in my exercise machine
so yeah I mean what's left in the end I
have no idea right I really have no idea
but at the same time do I want to get
cancer no right so it's like I just feel
like yeah I'll keep making tweaks maybe
I'll go too far hopefully I'll have the
ability to like get it back right to
have like an undo button like I don't
know I'm up for the challenge of like
trying to find some kind of
transhumanist balance but yeah I mean
like what is Lan ch's utility function
over the entire universe what is the
beus heaven I really don't know I would
try to tread carefully I'd be like okay
yes I'll cure cancer but I'll try to
like make very minimal changes until I
until I feel more confident yeah and
usually transhumanist thinking is is uh
we don't want to impose one future it's
more like let's give as many options as
possible to people if they want to you
know change and transform to like what's
the name morphological Freedom you can
have a simulation of yourself and live
in in a heaven that you created or you
could explore the universe in a in a
robot form whatever you do you do you
basically right you do you and it's just
like I mean it's it's a little bit
ironic that like you know I get a lot of
meaning or like passion or interest in
my life to be hey look this Doom problem
is like such a big problem but then it's
like I just like the AI companies who
are building the Doom even me the person
who's saying don't build Doom neither of
us can actually say exactly where we
want to go I mean the AI companies at
least they have a coherent plan of like
look we're going to just like have a
tool to solve all our problems you know
some people level the accusation at the
AI companies are like look if you can't
describe heaven you don't even know what
Heaven is and you're building something
that might risk missing heaven or
getting us to hell so therefore uh don't
build it you know maybe that's a little
bit weak like maybe they don't need to
fully describe heaven it's just you know
it's like if I'm making virtual reality
if I'm making other kind of tools do I
have to be able to describe heaven so
maybe it's not their fault that they
can't describe heaven but when their
probability of hell is sufficiently high
then maybe they should be able to
describe heaven maybe they should be
able to describe like what's the
ultimate human value I don't know
something to think about yeah yeah
another quick when do you think we are
living in a simulation yeah my P
simulation I just tend to say also
around 50% just because it's like on one
hand you know if I'm just trying to keep
things simple you know then it's
tempting to be like okay this is just
the Bas level universe but on the other
hand I don't know I guess gun to my head
I would say that we are in a simulation
just because for me what's convincing is
like look in my own lifetime I'm seeing
convincing virtual reality come on the
scene because I definitely remember
being a kid and be like yeah virtual
reality sucks like it's not convincing
at all and now it's like okay this
definitely looks almost like my life
like this is this is starting to be a
very convincing simulation and of course
that's even without neuralink right
that's that's even without virtual
reality at like the brain level so the
fact that I'm alive to see the creation
of AGI and and the creation of you know
perfect quality virtual reality I've
already seen the creation of personal
perfect quality screens like when I look
at my computer screen right now like you
know that's a perfect screen as far as
I'm concerned like I don't need that
screen to be any better ever and and
that came about in my own life like
there didn't used to be screens like
that so that's like pretty pretty
indicative that like something special
is happening like in my own lifetime
compared to like every other human who's
ever lived which to me makes more sense
if it's just like okay well I'm just one
of many simulations of like a
particularly interesting time yeah yeah
definitely I think I would say my
probability of simulation is is also
high but I wouldn't go above 50% because
I think we still lack the proof of
concept of Consciousness being
instantiated in in a simulated
environment because if we
yeah like NPCs so far in GTA or those
games they don't seem to have
Consciousness and if right maybe it will
happen but once we reach that threshold
and we have a conscious agent in a
simulation then I would update and say
okay it's very likely we are also in the
simulation but before this point I think
it's maybe not well that that's a good
point I mean I I agree that would be
another Milestone that makes me think
I'm more in the simulation it's just I
tend to think all of these things I
mentioned Milestones like when screens
got good enough that they're like as
good as my vision I consider that a
milestone toward thinking that we live
in a simulation like anytime one of
these like major recursive Technologies
comes online that can potentially like
take over your senses to me that
increases the probability that we're in
a simulation yeah and another I guess
probability is the probability we are in
the many world interpretation of quantum
mechanics you think it's the most yeah
so I mean I don't know if you can put a
probabil that that's more like a
probability over like who's going to win
the argument epistemology right M so so
but that's I mean don't me I think many
worlds so alaz owski famously said you
could use quantum many worlds as a
rationality litmus test if somebody can
read the argument for the Copenhagen
interpretation compared to the argument
for you know Everett many worlds and
they can't see that many worlds is just
the correct way to do epistemology then
they're like not qualified to be a
rationalist like there's there's only
one right interpretation I find that
argument very compelling I'm not an
expert on Quantum I've dipped in a few
times with with some like High
explainers with like a little bit of
math so I I kind of get it but I really
don't so I'm not the best person to
speak on it but like the specific
properties that Alazar listed of why
it's so much more compelling they make
very strong sense to me like that you
know the idea that Copenhagen okay it
collapses into one world but it's just
like a it's a non-local event like
there's all these really nice
mathematical properties that you just
say like oh no the universe just doesn't
have these properties because it's so
important that there's only one world
but it's like why why are you making
these epistemological trade-offs it
makes no sense to me yeah and I guess if
we live in a multi World many world from
the Everett branches some branches would
probably align AI so it's kind of almost
conforting thought to think somewhere
some in the Multiverse we are fine okay
so we can generalize the question like
what is my P Multiverse right like Max
Tark has a really good book from like 10
years ago I forgot something something
about universe but he he talks about
like tag Mark World level 1 2 3 4 even
if quantum many worlds is false which
I'm pretty sure it's true but if it's
false well maybe the actual size of the
universe is hyperexponential or infinite
in which case you have like every
possible version of yourself like
infinitely many times so and I I think
that my P Multiverse is very high and of
course if there's a stack of simulations
then you know then there's multiverses
in that sense I would be shocked if this
is like the only thing that's real right
it's like our one observable universe of
like really just this I think there's
more real than this okay and yeah so
final question if you could invite
anyone to your podcast who would it be
and why maybe you know a few names if
you want to you know just not single out
one person yeah yeah great so my goal
for my goal for Doom debates is I think
this is what Society needs we need a
forum where people who are making
important claims about AI they're
expected to defend and debate it so for
example when Mark Zuckerberg in a recent
interview with Lex Reedman I think it
was and he's just like yeah you know I'm
not too worried about like the Doom
stuff you know we're just happy to
release open source we don't see too
much danger like when says stuff like
that it's like all right Mark come into
the ring like you today you will be
debating owski or something right like
this is how our society needs to judge
these kind of things because this is a
bold claim or similarly when Dario was
asked like a week ago about his pdom and
he's like I don't like to talk about P
Doom we should just kind of continue and
make sure we get the benefits it's like
okay Dario you will now be debating Max
tag Mark right like it's like we need to
have this forum you can't just let these
leaders just like make these statements
un Challenge and the people should
demand it so my goal Doom debates is
basically that everybody who's making
prominent statements in AI including the
doomers should come hash it out right
and and Society lacks that technology
it's crazy here we are in 2024 we don't
have debate technology we just have
these softball interviews right people
come on stage answer softball questions
leave that's it yeah no it's definitely
a great medium for that and you've done
a good job with you know all the the
podcast the episodes the even when you
react because obviously maybe it's
difficult to get Steven Pinker
yes maybe you will be able I hope you'll
be able to get it one day but yeah just
so it happened once if
remember Keith Dugger from machine
learning Street Talk uh it was a good
case I I've proven out the pipeline my
reaction episode to debate pipeline
because I I listen to machine learning
Street Talk which is a great podcast and
one of the co-hosts Keith Dugger was on
it and I did a reaction episode where
it's just me talking over the episode
and then Keith Dugger reached out and
he's like hey I'm happy to come on and
then I had an actual respectful
one-on-one debate with Keith Dugger so
hopefully I can keep building up this
reaction to debate pipeline yeah it's a
good way to get attraction and are you
thinking of inviting maybe a laser
kovski that would be a fun
conversation yeah so you know I have no
confirmation from Alazar himself I I do
know him reasonably well but I just look
he always talks about having very
limited energy and focus so I I really
want to make it worth his time so my
plan for Alazar is basically like build
up the audience to five or 10x bigger
than it is and then get a bunch of
audience questions get a bunch of
audience
and then bring all that to Alazar and be
like come on Alazar the crowd wants this
and then he'll probably do it yeah yeah
of course okay great well yeah very
interesting conversation I hope the
audience will follow your channel as
well I really encourage them to do so
and thank you again for your
participation and for your work think be
very important thanks Kon yeah this was
super fun thanks for your work as well