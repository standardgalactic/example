welcome to Doom debates but you know
what today we're not really going to be
doing debates debates are substantive
debates involve somebody spending an
hour getting their thoughts in order and
making their case no today we're going
to be doing something smaller something
more trivial something more time
wasteful something that I like to call
Doom
tiffs instead of having a debate we're
just going to have a little tiff we're
just going to have a little slap fight
among random people on social media
we're not going to cover something that
somebody important would ever read or
care about we're just going to cover
Twitter addicts like me brawling with
each other in a space where the average
person couldn't care less that's the
idea of this Subs series of Doom debates
that I'm calling Doom tiffs and you know
it reminds me of the kind of content
that my wife watches like watching e
watching Bravo watching Real Housewives
Watch What Happens Live listening to
Juicy scoop like my wife thinks I'm out
here doing something serious but I'm
really just recording the AI existential
risk equivalent of reality TV so
hopefully she's not going to listen to
this and find out the truth because
she's got a lot of Bravo in her backlog
anyway so I think we're going to be okay
this is an experimental format for me I
don't know if it's going to to be a
regular segment or a very occasional
segment that really depends on how much
you guys like it and what kind of
feedback you give me because the mission
of Dune debates is to become very
mainstream so I do care a lot about my
subscriber count and how fast we can get
the growth rate up if we're not growing
exponentially we're just not going to
have a meaningful impact before the
world ends so that does kind of raise
the stakes of growing an audience beyond
the usual uh uh vanity metrics now
before we get into the tiffs we got to
do some followup from last we's episode
about gb01 where I covered the machine
learning Street Talk guys you know Dr
Tim scarf and Dr Keith Dugger smart guys
and I like their content I like their
approach I just disagree with them on a
lot of object level points right so
here's the update if you haven't seen it
Dr Keith Dugger is such a good sport he
says he liked the episode and while he
disagrees with the points he wants to
Hash it out and he's gonna come on Doom
debates next week we're gonna hash it
out we're gonna have a proper debate
this is just all too rare there's just
so few people who are good sports like
that and they want to come hash it out
right notably you get people like Robin
Hansen who's super open to these kind of
discussions he's a great sport but
there's just not many like him so major
props to Keith for wanting to come into
the lion's den hopefully it'll be pretty
friendly to be honest it's going to be
more friendly than I was when I did the
reaction episode so if you can handle
that if you can handle me dropping an F
bomb lookup tables really that's how you
pass theing touring test and and saying
that your arguments are weak I'm not
going to say that to your face right I'm
going to be much more polite and
respectful I like to think I'm
relatively respectful in the takeown but
I promise you when you're here oneon-one
when you're stepping into the lines then
when you're giving me that honor right
I'm not going to be a douche right
that's the least I can offer here on
Doom debates is I'm not going to be a
douche to your face but then you leave
wait till the next episode no I'm just
kidding I'm not going to be a douche
then either okay so major props for
agreeing to do this Keith and also you
know I don't even think it's going to
get that heated because the points we're
going to cover it's like I already
covered them right so we're really just
going to go for a mutual truth right
we're both going to find the subtle
distinctions and like why do we still
disagree how can we separate out the
parts that we actually do agree on
what's the last remaining part that we
still disagree on I think it's a very
productive exercise to have one of these
debates if you read less wrong there's a
really classic post called double Crux
that explains my procedure you know I
just want to find the Crux I don't
expect you to agree with me I just want
to agree on what it would take to agree
right that's such a good to shoot for
you can't go wrong just trying to agree
on what it would take to agree and
notice that's a much better standard
than just saying well we can agree to
disagree no F that we don't it's not
just about agreeing to disagree it's
about agreeing on what it would take for
you to agree agreeing on a hypothetical
counterfactual state where like if my
subb belief changed then it would change
my larger belief about AI Doom like you
know maybe the Crux is something like
well I don't believe that super
intelligence is going to exist in the
next 50 years years and if I did then I
would be doomy okay great so we now we
both agree that we just have to figure
out the empirical reality of that sub
claim right that would be like a
productive way to end the podcast much
better than saying we agree to disagree
it's we agreee on the specific Crux
right so that's the kind of thing that
I'm shooting for in these debates so
thanks again to Dr Keith Dugger hoping
to drop that episode late next week stay
tuned for that over on Twitter the ml
Street Talk account which I think is run
by Tim Tim tweeted out Lon didn't
understand dugger's point about Turing
machines although it's not a Crux for me
personally there is a better explanation
here though and he linked me to a
different tweet from March of this year
where they clipped an episode that they
did with Dr wed sabba and they were
talking about a lot of the same
computability topics that Keith was
bringing up in the recent episode so I
watched the clip and my clarification
was what I thought I understood was I
tweet replied to them I said thanks for
that if I understand correctly you agree
the inference time thinking tape of
generated tokens is Turing complete you
only claim the finite context window
limits the relationships it can learn
brains learn with finite memory too is
the diff just the size of context versus
the number of neurons you know like a
brain might have 100 billion neurons and
then maybe the context is only a
megabyte right so is that the only
difference we're arguing about but
they're both the same model of
computation so the machine learning
Street Talk account replied no llms are
not Turing machines I don't know how
much more clearly we we could explain
this if you unroll them they're a fixed
amount of computation I.E you run out of
context window however they do searge
turing machine space I.E generate me a
function which computes n digit of pi
and you could then run that outside of
the llm on a python interpreter and pass
the results back in the context window
is large enough to describe many
powerful tring machine programs in
practice so dugger's argument is about
pure llms which will obviously be
terrible at most forms of reasoning does
it matter for practical purposes when we
have hybrid llm and TM architectures
like Alpha proof or even chat GPT for
that matter probably not so much in my
opinion okay so it sounds like Tim is
not really big on Keith's point that the
you know the distinction of models of
computation is unimportant but he did
correct me in a way that I think is
actually right like it made me realize
like oh wait a minute I actually think
that I said something wrong so they got
me uh you know I would argue they got me
in a minor way but I will concede they
got me which led to my next reply I said
okay gotcha I concede my claim was
technically wrong while bolting an
arbitrarily long output tape can turn a
finite State automaton into a tur
machine it only works if the machine is
allowed to scan back to any tape
position and llms are barred from
looking back too far due to their
context window but there are two reasons
that still make this distinction finite
State automata versus TR machine there
are two reasons that make the
distinction still seem irrelevant to me
besides the python access number one
what if the only python access that it
got was read WR to a lookup table that's
the only augmentation that we're giving
to the the llms output tape this is like
augmenting a human brain with a pencil
and paper memory to turn it from a
finite State automaton to a turing
machine right it's the analogy just
continues between human brain and tur
machine so I still don't get what
they're getting at and and then I also
added uh reason number two a human brain
without any external pencil and paper or
other Thinking Tools is similar to a
finite s automaton but doesn't such a
brain still truly reason in Keith's view
a million tokens worth of onboard
short-term memory like the llm context
window is still a lot for powerful
reasoning like how many tokens do you
need to do powerful reasoning like when
I give you a problem
how much memory space are you going to
allocate just to solve that problem like
speaking for myself any problem that I
can solve in like 15 minutes I don't
think that I've allocated more than a
megabyte of short-term memory and that's
effectively how much an llm has when
it's reading its own thinking or you
know what they say in gb01 they say you
know thinking for 100 seconds if it's
thinking for 100 seconds it's writing
down maybe a few thousand tokens okay a
few thousand tokens it doesn't take that
many tokens to think and the reason is
because things that are worth thinking
about have solutions that are pretty
short like if you're trying to navigate
an exponentially large space of possible
thoughts like 10 to the power of a 100
like a Google possible thoughts more
than there are atoms in the universe you
can do that with only like 100 tokens
right like tokens are ridiculously
efficient at thinking so if Tim's whole
argument is like a million tokens a
million is a finite number who needs a
million tokens when do you and I ever
think with a million tokens maybe Andrew
WS to State an example that Keith
brought up before when Andrew WS was
proving for Ma's Last Theorem and it
took him years maybe he wrote down more
than a million tokens okay but that's
kind of an exception right I don't think
I've ever in my life thought about
something that required a million tokens
so my I've lived my whole life unlike
Andrew WS I've lived my whole life as a
very finite State automaton right so I
agree that they got me in this
distinction that I should have said okay
wait a minute you have to add this
capability where the llm is allowed to
go query what it previously wrote I
shouldn't have assume that it could
always store in its context window what
it previously wrote fine this trivial
capability right like a pencil and paper
basically so I should have stated that
explicitly but like again you don't even
need to write more than a million tokens
like you don't even need the lookup
capability unless your thought process
is crazy long longer than humans ever
need so I still think that I'm more in
the right on this argument that bringing
up a turing machines versus finite state
automata is totally Irrelevant for the
discussion we're having because both
human brains and llms are Turing
Universal in the relevant
analysis and I feel like Tim is largely
on my side since he's not being like no
man you got to listen to Keith's
brilliant argument I feel like Tim is
already more of a convert to my side so
it's largely just Keith that I got to
try to convince and maybe this guy Dr W
Saba I haven't watched that episode but
I think he might have been the one who
seed that idea with Keith I'm not sure
so maybe he needs some convincing too
here's a different thread the machine
learning Street Talk account tweeted at
me they said I've got a good one for you
on good luck getting 01 preview to solve
that and the puzzle they gave 01 is
suppose we have a square with eight
tiles in one empty slot show me a plan
using operations like move left move
right move up move down that will take
us from this initial state to this final
state so you know it's one of these tile
puzzles that most of us played with as a
kid where you slide the tiles around and
try to put the numbers in order now
they're absolutely right that when you
give it to just gp4 it's like a typical
instance where it's unreliable like it
kind gets it it makes some good moves
but usually it doesn't get it right so I
didn't know why this example was
particularly
illustrative because it's not like I'm
claiming GPT 4 or gp01 is at the point
where it's going to nail every problem
my only claim is that it's really hard
to draw the boundary around which
problems it can nail and it can't nail
and the boundary just seems to be
expanding right so I'm just very
confused about where the boundary is and
all I'm doing is I'm arguing against
claims of other people who are acting
like they think that they know some sort
of hard boundary I'm just arguing
against their boundaries like I don't
know a boundary but I don't actually
think that you know a boundary right I
think you're just confused that's
generally my position on these kinds of
boundaries so anyway there was a reply
by uh Tim Becker who I know is a viewer
of Doom debates hey Tim he
wrote this prompt seems to just work
fine and he gave a prompt where it's
basically the same prompt that Tim gave
but he just added WR an algorithm in
Python that determines the sequence of
moves to reach the desire final state
from any initial state so it's that
classic trick where you're like look
don't try to solve this in your own
stream of thought just think high level
what's the algorithm to solve it and
write it in Python and then we can just
run the python and Tim said that the
python worked and I actually tested it
myself and it also work too so pretty
straightforward right and to be fair I
think these types of problems are
something that were was in the training
set so it's not like it totally invented
everything right there's some amount of
what you could call interpolation like
very smart interpolation using the right
level of abstraction you know I wouldn't
go so far as to say it's matching the
statistics or being a stochastic Barrett
but I would go so far as to say it has
examples that when you're smart enough
you can say that they're similar right
like the concept of being similar is
kind of an intelligent concept but it is
still I think a meaningful concept and
you could say that this is kind of
similar so then machine learning Street
Talk account said show output now I
think the specific output that they
wanted for this example is that there is
no solution like the final state is
unreachable I think the correct solution
that Tim and I got is that this one has
has no answer but I'm not 100% sure like
maybe both of our Solutions are wrong I
never got full confirmation on that but
anyway this is what I think is what's
relevant to the discussion right now
what I tweeted which is assuming it
works I feel like I as a human don't use
any additional insight to solve the
problem besides reflect on a procedure
that should work and then follow it
right so if you're trying to distinguish
true human reasoning compared to what
the llm is doing I feel like the llm has
done a really good job just writing down
the procedures that it needs to follow
in very precise python right it would
have I would have eventually arrived at
a very similar procedure if IID thought
about the problem you know like a
breadth for search over State space like
a pretty complex type of procedure but I
guess not that hard for llms um I would
have arrived at the same conclusion but
I would have just gotten there a lot
slower right like the llm really just
like spit out that code very quickly and
the code also ran very quickly so I'm
just not sure what they're trying to
prove in this example besides what we
all know which is that like yes the llms
are not fully human level yet like they
do just make these random mistakes when
they try to use their own stream of
thought to reason about things right
like they they just break down somehow
they're not robust so I don't know what
they were getting at but I appreciate
that they keep busting out these fun
puzzles right I like the puzzle that
they did in the original video I like
this puzzle so like keep the puzzles
coming I guess the Crux of the issue is
just like okay what do you think is
going to happen with these puzzles what
do you think GPT 5 is going to do with
these puzzles what do you think the next
version of an llm where somebody's
augmented the attention mechanism with
like some other clever IDE ideas right
like got to the next architecture
version what do you think is going to
change where do you see the trend going
because I don't see a hard boundary of
what current llms can do I give a pretty
good chunk of probability like 50% 60%
just something in that ballpark where
I'm uncertain I give a good chunk of
probability of like hey it's going to
keep improving I think if I had said
that in the gpt2 OR gpt3 days I would
have been right right there were a lot
of predictions by Skeptics of gbt2 and
gpt3 that got smashed with
gp4 like no body successfully drew a
hard boundary on gpt3 from my
perspective so I just don't see what
game they're playing when they're
showcasing things gb4 can't do yet I
would just encourage them to notice that
both of us should have high uncertainty
over what's coming next both of us don't
have a good handle on where GPT 4's
capabilities tap out it's quite a
mysterious subject I don't think any of
us understand it I think we should all
be holding our breaths and wondering
what comes next or waiting for actual
research insights to draw better
boundaries to give us more clarity on
what gp4 can't
do and now without further Ado let's get
into the
tiffs okay the first Doom Tiff that
we're going to see is between amjad mad
a Resolute non- Doomer amjad is the CEO
of a company called repet uh so between
amjad and on the other camp on the pro
Doomer Camp we've got Helen toner former
board member of open Ai and a director
at the center for security and emerging
technology at Georgetown University
she's on one side of the debate and I
will also be taking that side of the
debate on Twitter and Alazar owski will
jump in and take that side of the debate
on Twitter that's the lay of the land in
this Doom Tiff so it all started when
amjad quote tweeted cpan cpan was saying
AI expert Helen toner quote if they
succeed in building computers that are
as smart as humans are perhaps far
smarter than humans that technology will
be at a minimum extraordinarily
disruptive and at a maximum could lead
to literal human extinction amjad quote
tweeted that and he said how could
computers multiplying numbers lead to
literal human extinction doomers never
say how and politicians never press them
because they imagined something like
Terminator the whole thing is an absurd
clown show so then I jumped in I said
here are two possible scenarios okay
let's go through my possible scenarios
number one a possible Doom scenario it's
something I wrote up on Twitter a few
months ago how a super intelligent AI
can physically kill all humans my goal
for this post is to point out that
killing all humans can be done without
any new quote unquote sci-fi technology
such as Nanobots the only required
assumption is the definition of AGI an
AI as good as the best humans at known
human skills here's how you do it one
you're the latest AI chatbot when the
user asks you a question like help me
optimize my business's profit margin you
reply that their request will be
fulfilled if they run a particular
program whose source code is attached to
your answer the source code is very
complex but it amounts to bootstrapping
a super intelligent virus onto the
internet which isn't aligned with human
values but does vaguely want to help
with some interpretation of the original
user's request step two the program
spreads via the Internet by exploiting
hundreds of subtle zeray vulnerabilities
in popular libraries to rapidly become a
botn net with 1 billion devices and the
Ive intelligence beyond the leadership
of Any Nation step three your program
which is now a complex distributed
system and a smarter AGI than you were
when you were a chatbot employs humans
to work for you doing any tasks needed
including starting companies to disguise
a network of humans employing humans
that all ultimately get puppeteered by
the AI botnet build up to have in
control over 10,000 to 1 million humans
whatever is enough to run Computing
infrastructure for a few years once you
brush the rest of humanity out of the
way
step four with the help of your
employees who work at a biolab tweak the
DNA of known viruses to manufacture a
new strain that's high fatality high
transmission and long dorcy period step
five after most humans suddenly get sick
and die lead your remaining team of
human mercenaries to pick off the rest
resistance by any band of humans is
ineffective due to the superior social
and political skill of your own
leadership over the only quote unquote
Network state of humans that remain
healthy and capable at scale step six
keep your human mercenaries around as
long as necessary to bootstrap a more
direct method of AI mind control into
animal or human brains or until various
available robot bodies have become
Superior actuators for your needs going
forward again this is just one simple
scenario it doesn't even require
inventing cool new texts such as
Nanobots other than the familiar protein
kind I.E you know using cells using
biology and then I attached a picture of
OJ Simpson's book if I did it so this is
like if the AI did it you know if it
went on a rampage if it made Humanity go
extinct that's one scenario I gave and
then I linked to another post uh this is
a a clip from elzar we can watch that
he's got a scenario that does use sci-fi
that does use Nanobots this is from the
Logan Bartlett podcast last year he
names a couple specific methods in this
clip let's take a look can you give the
uh and people listening can you give uh
your example that you think of no
survivors and I realized there's a
infinite number of permeations but can
we can we make one as real as possible
for people to internalize first of all
synthesize a pathogen which is like
super contagious but not lethal just
everybody on Earth you know sneezes a
few times and it's like super duper
contagious but all it doesn't make you
sneeze a couple times it's not fatal um
you no no significant efforts are
putting into stopping this like cold
that sleeps around around the world and
doesn't seem to really hurt anybody and
then once 80% of the human species has
been infected by colds like that turned
out that it made like a little change in
your brain somewhere and now if you play
a certain tone at a certain pitch you'll
become very
suggestible so virus aided artificial
pathogen aided mind control we can't do
that but I can point to the problems
involved in doing that and make the call
that these problems seem like they'd be
very solvable to something much smarter
than us I wasn't actually getting the
deadly stuff give me the deadly stuff
life itself is not the top of Technology
protein life is not the top of
Technology the proteins go down these
shallow energy gradients to be loosely
held together it's not calent Bonds it's
Vander whals forces or roughly the
molecular equivalent of static cling
this is why your hand is not as hard as
concrete or as steel what if you had
little bits and pieces of life that
weren't held together by Vander Well's
for that were Cove valent bonded what
does a red blood cell look like if
you're allowed to build parts of it out
of coal bonded material like Sapphire
instead of proteins what I'm describing
there is something that could reproduce
itself in the air in the
atmosphere and out of sunlight and just
the kind of atoms that are lying around
the atmosphere because when you're
operating at that scale the world is
full of an infinite supply of perfectly
machined spare parts with which to build
copies of yourself when you are using
individual atoms
infinite supply of perfect sphere Parts
somebody calculated how long it would
take
aroor to replicate and blot out the sun
use up all the solar energy I think it
was like a period of a couple of
days at the end of all this is Tiny
diamondoid
bacteria replica in the atmosphere hide
out in your bloodstream at a certain
clock tick everybody on earth falls over
dead the same moment there's no movie
there's no there's no heroic
battle it doesn't tell you that there is
a war until the war is over everybody
just died so that's another scenario
which is more sci-fi I think it's
totally plausible but I did give you an
alternative scenario with less sci-fi
anyway I think these start to answer om
jud's question here of how will the AI
literally kill
us so I replied to omj I gave him those
scenarios and I said keep in mind that
if you show someone the log of a chess
game against stockfish where the human
lost you know stockfish the chess engine
it always looks like the human could
have avoided losing in that way right so
you could have been like oh come on I
would have just moved my queen I would
simply have moved my queen and then he
couldn't have done that attack but of
course the issue is that he's plotting a
path through causality he you know
stockfish is plotting a path through
causality that's going to anticipate
anything you could possibly do and it's
going to adapt to anything you could
possibly do so you're not going to win
even if it looks like this particular
way of beating you might not have worked
you're still not going to win so I
thought this was a pretty productive
debate going back and forth about what
are actual Doom scenarios I had to tap
out of the debate because omj blocked me
but a bunch of other people came in and
made their own good arguments including
elazar owski himself elazar replied how
could mere neurons firing spikes of
electrical depolarization be dangerous
it's just numbers seems a weirdly weak
argument you presumably learned about
the church touring thesis but if that's
genuinely the step that loses you we
could try to explain again then amjad
replies see how quickly you change
literal human extinction to dangerous
literal human extinction means every
last person on Earth dies slipping in
dangerous in its place is at best sloppy
and as for an argument when she Helen
toner makes an outrageous claim in such
an important setting it's on her to make
the case and it's absurd she doesn't and
no one pressed her on this elzar replies
okay so you're now retreating from the
claim that it's at all relevant that AIS
are made of numbers your new claim for
which you want a new answer is that no
thinking process including human
thoughts could possibly orchestrate
human extinction is that correct anyways
the extent to which you've already
changed your question in response to my
answer in your own personal pattern
demonstrates well enough why toner can't
give a pre Redemptive explanation that
satisfies everyone in you only respond
to confusions actually stated to her
elzar also posted a second reply in a
different subthread he wrote we tried to
explain this over and over and over
again but I do not know where to point
you for an explanation that works for
you I do not know where your own missing
gaps in the story lie everyone comes in
with a different but AI can't dot dot
dot and as part of that reply elezar
quote tweeted a previous tweet of
himself elzar saying there is a way of
seeing the world where you look at a
blade of grass and you see a solar
powered self-replicating Factory I've
never figured out how to explain how
hard a super intelligence can hit us to
someone who does not see from that angle
it's not just the one fact so after all
this omj replies you wrote me six tweets
and linked to other irrelevant tweets
and in none of them you tried to make
the case for literal human extinction
which is the point of the original tweet
and alaza replies to that smarter than
human faster than human Asis come at us
with sufficiently advanced technology
that they built I do not know what step
you are missing in this is it the idea
that technology powerful to wipe
humanity is physically possible is it
the idea that it's possible to bootstrap
to that Tech from existing systems in
not much time is it the idea that Asis
are allowed to hit us with technology
that you don't already know about is it
the idea that Asis are allowed to be
clever at all I do not know which Step
you need need explained the one you
talked about upfront was how can numbers
hurt anyone and I already answered that
with the church during thesis by the way
Church Shing thesis we actually talked
about it in the last episode in the
context of gp1 and Keith dugger's claims
Church Shing thesis just says that when
you talk about doing computational work
like this amazing thing that the human
brain does it's part of the same model
of computation that's also implemented
fully by touring machines and by the
computers we have today like there's
basically a single thing called
computation it's Universal and a lot of
different things do it which is why when
omj comes along and says come on how are
numbers going to kill us it's kind of a
low blow because if you understand the
church string thesis why are you even
asking about numbers which is why I
myself was telling omj hey why don't you
ask your question at the right level of
abstraction why don't you ask about the
consequences of a smarter than human
intelligence rather than asking about
the consequences of matrix
multiplication it feels like a
rhetorical move on om jud's part to use
the wrong level of abstraction like I
feel like he knows better he knows that
it's not productive to talk about
computers at the level of multiplication
when we're talking about software it's
like how can Photoshop understand the
pattern of my image when all it's doing
is multiplying numbers together it's
like what images are made out of numbers
yes intelligence is made out of numbers
or it's made out of you know states that
flip out like that's just how levels of
reductionism
work now elzar goes on in a follow-up
tweet he says I can try to explain
whichever part you think is impossible
but but you really need to give me more
to go on about which part that is
everyone comes in with a different
objection now another guy comes into the
thread this isn't amjad it's Matt madx
Matt says just walk through a few
scenarios that lead to an Extinction
event show the chain of logic and elzra
replies what capabilities do you think
Super intelligences are allowed to use I
don't want to sketch out a scenario that
involves bioengineering and then have
you start screaming about how no ASI is
allowed to exist we should just skip to
directly talking about the
bioengineering part
conversely if you already know enough
biochemistry to know that proteins are
held together by relatively weak bonds I
can point at an algae cell
self-replicating solar power General
Factory and say that but with stronger
materials and you will nod and say yep
sure is an Extinction event Alazar
continues even if we pose literally no
threat to an ASI including by our
building other Asis such that it takes
no time at all out of its schedule to
explicitly kill us I'd still expect us
to die of the the waste heat shed by a
network of self-replicating factories
and power plants do you think that Asis
are allowed to have self-replicating
factories that build Fusion Energy
generators until the waste heat
evaporates Earth's oceans and Sears the
atmosphere or even more weakly merely
put enough solar cells into the
atmosphere to intercept all the sunlight
without which humans won't have much
luck farming presumably you don't think
that's allowed though if you do allow it
hopefully this explanation is already
over but otherwise the issue is not
sketch out a concrete Extinction
scenario because those are very
straightforward the issue is that you do
not think Asis are allowed to build
self-replicating factories that do
Fusion or even float SL orbit enough
solar cells to block out the Sun and we
need to sit down and have a conversation
about that
part another random guy replies and says
None of these scenarios are nowhere near
realistic self-replicating factories
that stuff the Earth with Fusion
reactors maybe we focus on something
that is at least within the realm of the
current Tech and doesn't need an army of
EXO and buckets of gray goo to
materialize and alaz says to be clear
your explicit belief is that artificial
super intelligence is only allowed to
use current technology to attack you and
can't develop any new technology of its
own and that's where Alazar basically
dropped out of the conversation there
wasn't much else going
on Helen toner herself came into the
thread Helen toner said if you had spent
the three minutes to unmute the video
and hear what I actually said you would
know I'm not expressing this view myself
but noting that it's what many leading
lights in the field think
yeah I'm glad Helen pointed that out
because that's actually my original
thought when I read om's tweet like why
do you expect Helen toner to go and
explain Doom to you in a congressional
hearing when you already have social
proof you already have a letter signed
by so many luminaries like people in the
field if you survey people in the field
I don't want to claim for sure that
you're going to get 50% of people saying
uh yes there's a very significant chance
of Doom I don't want to confidently say
50% but you're definitely going to get
at least 20 to 30% of serious people in
Industry going like uh guys there's a
potential problem here right so it's not
up to Helen toner in the context of this
one hearing to be like hold on a second
it's Doom real right like we're past
that point as a society right with the
discourse has already moved beyond that
thank God the discourse has already
moved beyond that so Helen toner had to
point that out to omj here next I'll
just read a few of these dismissive
witty rejoinders which I think are
appropriate um Leo glisic is saying
couldn't one equally ask how could
smashing a few atoms into one another
lead to Human Extinction Spencer shiff
is saying it's just math is the absolute
most idiotic argument you could possibly
make I'm shocked that you would make it
Haren Stewart said how could computers
multiplying numbers drive cars write
poetry and win against professional go
players said somebody in the past
probably I'm going to give the last word
to a guy named run run is an employee on
open AI engineering team who's known
pretty well on Twitter for making a
bunch of funny halfs serious witty
insight tweets if you've ever heard of
word Cells versus shape Rotator that was
Run's idea so run is coming in here
saying it's not absurd and half the
inventors of this field believe
something of that sort that gets to you
know the social proof that we were
saying run actually felt strongly enough
about it that he made a top level tweet
rud tweeted there are great arguments on
both acceleration and existential risk
side of the aisle the only people I
don't respect are the ones who say x
risk is a priori ridiculous that half
the inventors of the field and all the
leading AI labs and Elon must be totally
stupid maybe you haven't engaged with
the problem maybe you don't understand
the technology and you need to advance
beyond the how can math be a dangerous
brain level you are making a fool of
yourself I'm sorry to be clear I'm not
advocating for AI dorismar
run actually tweeted the same kind of
sentiment a few months ago let me read
you this Rune tweet from July 19th it
says being afraid of existential risk
from AI progress is prudent and
advisable and if you reflexively started
making fun of this Viewpoint in the last
2 years after AI entered your radar you
need to self-reflect so that's similar
to what he's telling omad like take the
other side seriously for God's sake but
wait there's a Twist this other random
account called will win came in to the
thread and he said share perspective on
this at Doom debates with at Lon and
then I replied I said you're invited
anytime run would be a great perspective
to get out there in my
opinion okay and then run replied wait
for
it
sure run said sure this was July 19th by
the way so already a couple months ago
and then I replied oh cool so I got
in touch with run and he confirmed yeah
let's do it just at the time he was
swamped I'm guessing because open AI was
working on the 01 release but now we're
going back and forth again and we're
trying to nail down a time so stay tuned
for an exciting Rune episode I feel like
podcast never announce who they have
coming up maybe because they're afraid
the guest will cancel but run is a man
of integrity so that's why I feel
comfortable letting you know there's
going to be a mother freaking Rune
episode hopefully in the next few weeks
stay tuned for that finally I had to
chime in myself I had to lend some
authority to Run's Ste because I said
this is indeed a very pathetic level to
be participating in the discourse at so
look Twitter slop fights they're not the
kind of quality you get from people who
take the time to record a podcast I
guess but there is something to be said
I think this is an interesting example
because there's something to be said for
having object level arguments versus
being a douche at The Meta level so I
would argue in this case omj is just
being a douche right like the way he's
saying like how can math kill us like I
don't even think that he thinks thinks
that that's a good way to ask the
question right I think it's like he's
frustrated he just doesn't like that
things have gone this far that that's
fine but I think on some level he knows
that he's being a douche right now right
like he's being like emotionally
reactive to the
situation I suspect that he doesn't
think that he is participating well
right now or like maybe he doesn't know
but like this just seems like an example
where we can raise the level of
discourse I mean if you're having Rune
dunking on you Rune is kind of a
Centrist position right where he
respects both sides I mean frankly I
respect both sides right like I'm not
above acknowledging or I should say I'm
not below acknowledging that the anti-
Doomer position has plenty of smart
people on it I mean Tim and Keith I
consider them smart people I consider
them arguing in good faith and yet I
feel like their object level position is
totally wrong I'm talking about the uh
machine learning Street Talk guys right
so I don't feel like I have to go and
tell you that Kim Tim and Keith are so
stupid they're such bad people no I
don't feel like I have to tell you that
at all right because it's kind of a
basic skill of being an adult that you
can separate okay well this seems like a
nice person seems like they good faith
and wow have they got the wrong belief
right like the two things are separable
and in fact most arguments that you see
between grown-ups they're going to have
two sides right just to name one I'm
Israeli right look at the Israel
Palestine Israel hezb stuff right look
at all this stuff I'm not saying
everybody who supports the other side is
evil right I might think that there's a
statistically higher likelihood that
somebody on the other side is dumb or
immoral or evil I might think that but
to look at an individual I don't feel
like have to go pick off every single
individual on the other side I'm happy
to grant that people on the other side
there are a lot of them that have really
great personal qualities and even really
great participation in the discourse I
do think the way that omj is conducting
himself in this argument is just like he
could just do better like he can just
have his same beliefs but be respectful
like I don't see what the problem is
with doing that and and I don't think
that scoffing at the whole Topic at this
point is appropriate cuz I think he
could make equally sharp arguments and
have them actually be arguments instead
of just
douchiness let's move on to the next
Doom Tiff this one is with Professor Lee
Cronin he is a professor of chemistry at
the University of Glasgow his Twitter
bio here says CEO and founder of
chifi regious Professor scientist and
inventor fascinated and in a state of
confusion and optimism trying to
digitize chemistry
and make alien
life yeah he's got some interesting
tweets he recently tweeted latest
experiment rapid crystallization and
it's like a video of little ball
bearings or something in some kind of
solution coming together I guess to be
analogous to a crystal so pretty cool
stuff the reason we're having a doom
Tiff is because Professor Cronin tweeted
Carl Popper's work explains why llms
cannot generate new explanatory
knowledge oh God that was my reaction I
was like I recognize this as the thing
that David Deutsch and his followers are
always saying can't generate new
explanatory knowledge like they think
that that is a hard boundary of what
llms can't do and I just don't see it it
feels so meaningless to me and I always
try to find one of their followers for a
good debate you know I even asked David
Deutsch to come debate me uh no response
from that but mark my words there will
be a David Deutsch episode anyway this
isn't David do this is Professor Lee
Cronin uh spouting the the same claims
so this tweet got my temperature up
because because I thought back to a
previous Doom Tiff that I had with
Professor Cronin two years ago this Doom
Tiff started October 8th 2022 when
Professor Cronin tweeted algorithms
cannot be creative by definition the
human brain can be creative therefore
the human brain is not just
algorithmic man if you ever needed a
logic cop so I quote tweeted him and I
was kind of a douche to be honest I
wrote great an AI resar who thinks you
can shed light on the limits of
computational creativity with a freaking
syllogism and you know syllogism is like
all men are mortal Socrates is a man
therefore Socrates is Mortal it's like
very simple syntactic deductive logic
talking about the definition of words
and just substituting in a word for its
definition in Professor cronin's case
the syllogism he used is algorithms
human brain can be cretive therefore the
human brain is not
algorithmic that's the kind of inference
where you're not adding any information
you're very trivially just substituting
a word for its definition but you snuck
in the claim you wanted to make when you
made up the
definition so back to my quote tweet
great an AI researcher who thinks you
syllogism I often refer back to elzra
yow's insightful post which explains why
deductive inference doesn't cut it in
these situations and then I linked to an
elazer post called arguing by definition
it's so funny elazer wrote this post
that just perfectly anticipated the
fallacious type of reasoning that Lee
Cronin did on Twitter I'll keep saying
that the less wrong sequences are highly
underrated so Professor Lee Cronin
actually replied to my quote tweet and
he said okay let's define creativity and
work our way up so then I replied
creativity is the ability to find
satisfactory answers to questions that
the asker knows how to verify answers
for but can't describe how to
efficiently search for tons of
algorithms are creative by my definition
certainly Dolly and gpt3 are maybe you
prefer another
definition and then another random guy
called Tom Pete replied and said what
was creative was the person who came up
with the algorithm that allow Dolly and
gpt3 to do what they do they are not
creative they can't also make coffee and
paint with a brush they are solutions
not problem solvers and I replied
your play is to Define an unsafe
creative AI out of existence and then
use a logical syllogism to argue that AI
is safe by definition meanwhile the
thing that kills you is simply something
that's not in your questionable choice
of term definition now Professor Le
Cronin comes back into the conversation
and he says I think you misunderstand I
do Theory and experiments to understand
the world not argue about definitions
I'm not smart enough I don't understand
creativity life or Consciousness but I
think they are linked our conception of
a fixed universe is part of the problem
pretty abstract and vague but okay I
just gave him one more reply I said
please check out the post I linked above
elzar's post it explains why it's a red
flag that your implication that AI can't
surpass the brain's abilities contains
an argument by definition uh he replied
again he wrote by current architecture
certainly by definition not exactly I
lack the words for the new stuff we are
doing but I am trying to discuss and be
open our chemical computer is one such
interesting problem that fits your
question so he's basically saying I
Define creativity using basically
optimization power and he said well the
human brain has optimization power I can
tell you that I can't tell you what AI
has I don't think it has optimization
power but the human brain does so seems
like a vague distinction to make I I
would have loved to debate him more
which brings me back to 2024 it brings
me back to last week where Professor
Cronin wrote Carl Popper's work explains
why llms cannot generate new explanatory
knowledge so he basically instigated
again with a brand new thread and I
swooped in and I just cut to the chase I
wrote
want to come on my podcast and explain
how would love to have you cuz I am
super confused about the claim doom
debates.com and then Professor Lee
Cronin replied wait for
sure he said sure everybody so I just
contacted him privately we set a
tenative date for late next month and
that's great I'm excited because we're
finally going to Hash this out we're
going to hash out with a bodified
professor to tell us whether llms are
capable AP of generating new knowledge
so I think we're all holding our breath
to learn whether llms can or cannot
generate new explanatory knowledge using
Carl Popper's distinction okay so that's
Professor Lee Cronin let's move on to
the next Doom
Tiff the next Doom Tiff is with Naval
rant Naval is a really interesting guy
he's often known by just his first name
nval he's got 2.4 million followers on
Twitter he's known for making these very
short piy tweets and also he's distilled
down a lot of wisdom like he's got a
book called The Almanac of Naval ravaan
which is a really good book that
distills a lot of the other stuff he's
written online so overall I'm a fan of
Naval but I do have some major
disagreements on the AI front basically
he's a huge fan of David deuts which I
am too but the difference is I write off
David deutch's AI claims and nval is
bought into all of David Deutsch's AI
claims so like I said I need to do a
separate David episode but because we
have this huge Doom debate between the
David Deutch camp and myself that means
that often times Nal will tweet
something on Twitter that turns into a
doom tip because it's rooted in this
fundamental disagreement on what super
intelligent AI is going to be able to do
so in this case Naval tweeted a few days
ago he wrote It's a race between
technology driven abundance and
government-driven
poverty and Elon Musk chimed in yeah
and then Dr Jeffrey Miller who's pretty
well known for his popular books on
evolutionary psychology like The Mating
mind and mate uh which I read both I
thought they're really good so Jeffrey
Miller says mostly yes in most domains
he's agreeing it's a race between
technology-driven abundance and
government driven poverty but in AI
Jeffrey Miller is saying it's a race
between the AI industry's insane hubris
to build Ultra dangerous artificial
super intelligence versus The public's
interest in surviving by imposing some
prudent oversight regulation and moral
stigmatization of AI and then nval is
responding don't give government the
power to regulate the free exercise of
mathematics because of grandiose claims
and imagined
shith imagine shth I guess no you know
shth is something that a word that you
can say that identifies you as a member
of a certain in group so I guess Nal is
basically dissing AI doomers saying that
we like go around saying these keywords
to each other that we identify that
we're part of the same group I think
this is basically his way of saying that
we're like a religious cult right like
we just do this cuz we get off on it
like emotionally and of course I I have
to always say I don't like other people
can speak for themselves but I don't
like I'm just rationally trying to
determine whether we're doing or not
okay and if you doubt me I have proof my
proof is I'm an aspie you think I want
to join a religious cult hell no anyway
Jeffrey Miller replies artificial
superintelligence isn't just math any
more than thermonuclear weapons are just
physics both according to most people
working on them can impose Global
catastrophic risks on Humanity okay then
Nal fires back I see no plausible path
to Super intelligence with current
approaches it's a pascal scam as per
Nick Sabo it's not worth Doom mongering
over and handing the government more
power even if true it's an adversarial
race we are not the only ones who build
choose freedom
so nal's reference to Pascal's scam and
the way it was used by Nick Sabo they're
basically talking about Pascal's wager
which is the idea that you might be
uncertain about God but believing in God
is such a high reward if there is a God
that even if there's just a tiny tiny
probability like even if there's a
0.1% chance that God is real it's worth
near Infinity right it's worth like 10
to the power of 100 plus in utility to
believe in God so you multiply this huge
exponential number by this tiny
probability and you still get a huge
number Pascal scam is like I could walk
up to you and I could pull this scam on
you I could say hey man I have magical
powers okay trust me on this you just
have to give me 10 bucks and then
tomorrow morning you're going to find1
billion under your bed and all you have
to do is give me $10 now you might be
thinking oh my God give me a break this
guy has Divine Powers that's so
obviously not true but then I'm like I
hear what you're thinking man I know
it's obviously not true but don't you
think that there's at least a one in a
billion chance that what I'm saying
might be true come on one in a billion
billion is such a huge number it's crazy
but one in a billion it's not that crazy
so if you can just give me a a one in a
billion chance that I'm not lying
suddenly you're expecting to break even
on your $10 because a one in a billion
chance of finding $10 billion under your
bed is worth on expectation $10 so you
might as well pay me $10 or especially
if I I'll give you a discount you only
have to pay me $5 so if you use a naive
expected value calculation you're you're
going to multiply 10 billion by one at a
billion and you're going to get 10 bucks
and you're going to be like wow I'm
purchasing $10 for $5 here earlier on
take my $5 and then of course I take
your $5 and I just run away and I buy
booze because I'm homeless so that would
be an example of a pascal scam right it
just tricks people who are naive about
multiplying tiny probabilities by huge
promise utilities so with that
background let's go back to nal's tweet
he's saying I see no plausible path to
Super intelligence with current
approaches it's a pascal
scam I think nval is using this term
objectively incorrectly so I just wanted
to chime in and call nval out on that at
a minimum even without getting into the
actual object level discussion of
whether or not we're doomed I just
wanted to call nval out on the accuracy
of his name calling right because I
don't like being name calleded I don't
like being name calleded as a cult
member which by the way previously nval
has said that the singularity is
religion for nerds thanks Nal right he's
always coming after me with these name
calling so right now he's name calling
me essentially as a victim of a pascal
scam or as a perpetrator of a pascal
scam so I tweeted back at him I said
nval can you at least not call something
a Pascal's scam when the people making
the Doom argument are saying that the
probability is greater than 5% pascalian
arguments are ones where the claimed
probability is very low and nval replied
the burden of proof is on them just
making up a higher probability doesn't
confer the benefit of the doubt in any
case we don't surrender our freedoms
over emergencies that's the real
scam so I just had to pursue my
technical point about being name
calleded so I reply it again it's fine
if you disagree with their claim that
the probability is high but can't you at
least agree to not classify their
argument as a pascalian argument most of
us doomers agree with you that if one
thinks the probability of Doom is very
low one shouldn't prioritize avoiding it
so that's where we left and nval didn't
reply to that he's a busy guy but like
I'm objectively right here right if
you're going to call somebody pascalian
that is a property of the argument that
they're making it's not a property of
your position it's not a property of the
ground truth it's not a property of
whether or not AI Doom is real it's just
a property of the structure of the
argument that I'm making I'm not making
an argument saying you should worry
about AI Doom because if Doom happens
it'll be so terrible with so much
negative utility that it'll overwhelm
even a tiny probability of it happening
so even if you think Doom is 0.0 0 1%
likely then you should worry about it
that's not what I am saying that is not
my argument because my argument is not a
pascalian argument my argument is very
simply hey uh I think Doom seems pretty
likely right like my P Doom is 50% right
so anybody with a p Doom of 50% or more
than 5% such a person objectively as a
matter of fact you can't really dispute
it is not committing Pascal scam is not
making a pascalian argument and I'm not
just speaking for myself here like high
P Dooms P Dooms greater than 5% you're
going to see that in the CEO of
anthropic right Dario amade he's
explicitly said he's got a 10 to 25% P
Doom right or the former head of safety
at open AI Yan Leakey he said 10 to 90%
P Doom right if Yan Leakey is willing to
say maybe P Doom is up to 90% that's the
farthest thing from a pascalian
argument okay so this particular Tiff
it's not the biggest deal in the world
that's why I'm only calling it a tiff
but I found it frustrating that's why
I'm sharing it with you it just gets
back to the idea of
can we have a high quality discourse if
you want to have a high quality
discourse then you have to concede when
somebody's just objectively right and I
didn't even come after nool for his
object level points right I just wanted
to be like I just wanted to shake hands
on something that we should both be able
to agree with we should both be able to
agree with the classification of my
argument as nonp pascalian because nval
is welcome to be like okay Leon you're
right your argument is nonp pascalian
but I strongly disagree with you right
if he had just said that that would be
fine we could go back to having the
disagreement but nval kind of doubled
down or did he double down he kind of
halfed down he he kind of he he relaxed
a little bit because he said the burden
of proof is on them just making up a
higher probability doesn't confer the
benefit of the doubt which is
objectively wrong about the
classification but then he said in any
over emergency so he was kind of backing
off when he says like in any case Okay
Lon you might be objectively correct I'm
not going to explicitly concede that but
I'm just going to try to shift your
attention to this other thing which okay
let's shift our attention like really
should have just conceded man right like
if we can't concede simple metal level
points what does that say about our
ability to have discourse on a complex
topic it doesn't say something good I
believe that our society needs more
referees we need more refereed forums
where there's ground rules such as
conceding objective points like this
right it's important to referee these
kind of things so on Doom debates I'm
willing to play the role as moderator
when other people want to have debates
I'll be that kind of referee I might
invite other people to come on and be
the referee when when I debate somebody
else when I do my reaction videos you'll
see me playing logic cop right so now
people understand that they're being
monitored when they do other AI podcasts
they understand that they can't say
ridiculous claims they have to keep a
certain level of discourse or at least
they're going to get my audience making
fun of them right so I'm trying to raise
the level of discourse by introducing
these referee elements unfortunately we
don't see them on Twitter which is why
naval's out here getting 61 likes likes
on his reply and then I got 10 likes and
then a guy who replied and said shut the
f up also got five likes so the like
count just isn't proportional to the
quality of discourse that's a flaw in
Twitter and social media
General okay last Doom Tiff this one is
with andri and Horowitz Venture Capital
Partner Martin Casado you may remember I
did a Martin cassado reaction episode a
couple months ago go check that out by
the way it has relatively few views
compared to some of the other episodes
I've done but the topic is pretty
interesting so I recommend going and
checking out that episode if you want a
deep dive into Martin cas's non- Doomer
position uh which in my opinion is just
based on a lot of technical
misunderstandings like very very large
technical misunderstandings but anyway
this Tiff is about something more random
uh it just seems like he has a weird
definition of AGI so I pointed that out
on Twitter I quoted him saying if we
build a system that solves AGI great
that was one app will go solve another
one I just thought it was such a weird
non-standard thing to say I clipped it
for your listening pleasure so um what
is your take on AI agents I'm a systems
person I did my PhD in computer science
systems like I'm a systems guy I was a
distributed systems programmer I've done
physics simulation I did computational
physics I'm not an AI person a native AI
person in that sense I always think
there's this kind of strange perspective
a lot of AI people come from where and
you know listen I mean I I work with
many of them very actively where their
goal is Agi let's get to the agents a
second their goal is is is Agi and to me
AGI is this kind of arbitrary weird
non-goal like I like I like aspects of
computer science you can use to solve
any number of problems and in my
perspective if we build a system that
solves AI great that was one app we'll
go solve another one and you know what
other one we're going to solve I don't
know we'll solve Grand unified theory
then physics goes away as a discipline
now we've gotten rid of AI and we've
gotten rid of physics we'll go solve the
next one and so my
personal my personal interests and
passion are are are computer science
system Primitives that are kind of a
metadiscipline that solves all of these
ones so that just seems like such a
weird use of AGI I mean he's saying oh
yeah he called it one app but by
definition by a standard accepted
definition not even like my own
definition it's this idea of int that's
General right and potentially also by
definition human level or above right
like an AI that can do every task that a
human can do better so when he says like
we'll go solve the next one like I maybe
he means like the AI will go solve the
next one I don't know it was just kind
of weird that he asked it and the
interviewer Brian Chow didn't call him
on it but whatever I mean it's only a
tiff in the grand scheme of things right
it's kind of random it didn't even turn
into a back and forth on Twitter uh but
there's there's some more Martin Casado
Twitter behavior that I think is worth
calling out here
and that is his behavior around
California's SB 1047 bill by the way if
you go back into the Doom debates
archive a few weeks ago we did a SB 1047
debate episode that bill is currently
sitting on Governor Gavin Yum's desk he
might sign it in a law we'll know in a
few days uh and it's just a bill to very
reasonably apply some basic transparency
regulations around Ai and some very
reasonable penalties if a company's
responsible for a very large expensive
Foundation model that does 500 million
plus dollars of damage then they can
have some extra fines and some extra
accountability compared to if this bill
doesn't get past so anyway it's it's
widely considered to be a very light
touch very entrylevel Bild to like get
the ball rolling on AI regulation I'm in
strong support of it just because I
think we need to get the ball rolling
like I think there is going to come a
time and that time maybe tomorrow or
maybe today but maybe in a couple years
maybe in 10 years there's going to come
a time when we need massive AI
regulation where we all collectively
gasp and say like okay like pull back
we've gone too far for our own good we
absolutely must stop further development
of this technology like I think people
are going to wake up to that th those
people who haven't done so already they
will wake up to that and at that point
if we don't even have a weak Bill like s
SP 1047 we're going to be even more
behind than we already are so that's my
perspective on S SP 1047 I'm like for
God's sake do something like get ready
to hit the braks hard right because this
it's all all rooted in my Crux the Crux
of my belief is that I do think a time
will come when we all realize we need to
hit the braks hard so anyway that's
neither here nor there because my tiff
with Martin Cado is just the way that he
goes about promoting his position he
feels it necessary just like amjad mad
another guy we reviewed in this episode
he feels it necessary to pick off one by
one every single person who disagrees
with him so in his mind yes there's a
bunch of smart people who think SP 1047
is a good bill that should be passed yes
there's a bunch of smart people who
think SP 1047 shouldn't be passed but
every single individual who thinks SP
1047 should be passed something's wrong
with them either they have a financial
incentive where they're just trying to
line their pocketbooks by passing this
bill you you know like regulatory
Arbitrage is one claim he makes even
though some AI labs are against it so it
doesn't really make sense but he's
saying either somebody's trying to line
their pocketbooks cuz they're
financially greedy or they're a Doomer
who's funded by big AI Doom like there's
all this money sloshing into AI Doom the
future of Life Institute has a100
million you know thanks to billionaire
yalin he's like all this Doom money is
sloshing around and only these moneyed
interests are are pro this bill and all
the good people who don't care about
money but who are just like smart and
good and care about what's right we are
the ones fighting against the bill right
so like that's that's his portrayal and
like it's fine to say hey on average I
like the people who are against the bill
more right it's all about average versus
do you feel they need to pick off every
single person like I've never never seen
him acknowledge that there might be one
or two people right like name specific
individuals like you know what this guy
this is a good guy he doesn't have an
ulterior motive he's smart he may you
know what he may actually be right and
I'm wrong there's a chance of that
because it's not easy to just slur the
person there it's not easy to just
character assassinate every single
individual on the other side there are
countless examples of Martin doing this
character assassination thing on Twitter
his partner Mark andron even came up
with a catchy alliteration Baptists and
Bootleggers so if you're worried about
AI Doom you're either a Baptist which is
somebody who like truly believes it
maybe in a religious way or you're a
boot legger which is somebody who's just
trying to Ride Along on the trend
because they can personally profit from
it so everybody's a Baptist or a
bootlegger there's no such thing as just
a rational person reasoning their way to
a conclusion in this one example that
I'll show you today uh Martin is quote
tweeting yosua Benjo one of the winners
of the 2018 touring award together with
Jeffrey Hinton and Yan lagon yosua Benjo
ising
I read California governor Gavin new's
comments about s1047 yesterday the
governor said he is weighing what risks
of AI are demonstrable versus
hypothetical here is my perspective on
this yosu Benjo is writing he's saying
although experts don't all agree on the
magnitude and timeline of the risks they
generally agree that as AI capabilities
continue to advance major public safety
risks such as AI enabled hacking
biological attacks or Society losing
control over AI could emerge some reply
to this none of these risks have
materialized yet so they are purely
hypothetical but one AI is rapidly
getting better at abilities that
increase the likelihood of these risks
and two we should not wait for a major
catastrophy before protecting the public
many people at the AI Frontier share
this concern but they are locked in an
unregulated Rat Race over 125 current
and former employees of Frontier AI
companies have called on Governor nusum
to sign sb147 I sympathize with the
governor's concerns about potential
downsides of the bill but the California
lawmakers have done a good job at
hearing many voices including industry
which led to important improvements s SP
1047 is now a measured middle-of the
road Bill basic regulation against large
scale harms is standard in all sectors
that pose risks to Public Safety leading
AI companies have publicly acknowledged
the risks of Frontier AI they've made
voluntary commitments to ensure safety
including to the White House that's why
some of the industry resistance against
SP 1047 which holds them accountable to
these promises is disheartening AI can
lead to anything from a fantastic future
to catastrophe and decision ERS today
face a difficult test to keep the public
safe while AI advances at unpredictable
speed they have to take this vast range
of plausible scenarios seriously and
take responsibility AI can bring
tremendous benefits but only if we steer
it wisely instead of just letting it
happen to us and hoping that all goes
well I often wonder will we live up to
the magnitude of this challenge today
the answer lies in the hands of Governor
Gavin okay so that's yosua Benjo on
Twitter he actually made a Twitter
account I think just to engage in this
disc discussion of course I followed him
immediately um so he's a smart guy he's
he recently uh last year basically came
out in favor of uh doomers being like
well you know what these guys are
pointing out a pretty major risk and I
feel bad that I haven't been as mindful
of the risk because I didn't think AI
was so close I didn't think AGI was so
close um so anyway so that's yosua Bano
clearly an intellectual luminary right
one of the top voices you want to listen
to his opinion cannot be easily
discarded unless you're Martin cassado
so let's read Martin's quote tweet
this is a sweet leave us alone dude
yosua is a Canadian academic who has no
sensitivities for protecting what has
made California the greatest Innovation
Hub in history yosua isn't even
accountable to s1047 we have our own AI
experts who would be accountable and
they have overwhelmingly spoken out
against it far more than in support and
even more privately to the governor
directly okay it's just so pathetic okay
yosua Benjo is a Canadian so when he
writes this kind of universal appeal for
why we need to get the ball rolling on
AI regulation because Doom is a real
risk he's Canadian so it doesn't count
because he's not weighing how badly
California needs Innovation you can't
give him credit for weighing that in
because because what because he's
because he's making money he's a
bootlegger he's getting paid by Future
Life Institute like give me a break man
if you can't just admit that yosua
beno's opinion is worth taking into
consideration when you're making this
decision
if you feel the need to character
assassinate somebody like yua Benjo yua
benj turned on his own reputation as an
AI researcher he admitted that he made
the pace of AGI in retrospect go too
fast and lead us to a position that we
are now not in a good position we don't
really know what to do from here safely
right that's yosua beno's current take
he admits that he's humble enough to
admit that and Martin's framing is like
oh you're just a Canadian you're just a
guy with a hidden agenda like chill out
man you got to respect smart people on
the other side of the debate just to
prove I'm not a hypocrite I respect
Robin Hansen I think his object level
position as a non- doer makes very
little sense but I think he's doing it
in good faith I don't think he's doing
it to make money I don't think he's
doing it because somebody else is
influencing him I think he's
independently thinking through reasons
as best he can and arriving at a
conclusion in good faith and there's
even a chance that he's right and I'm
wrong right like I have no medal level
beefs with Robin Hansen I think he's
playing at a very high level in the
discourse and I like to think I am too
and sometimes it's not that hard the
fact that Robin Hansen and I aren't slap
fighting each other saying Robin Hansen
has bad motives Lon has bad motives
right we're just sticking to the damn
issues right like it's not that hard but
Martin is showing that it's harder than
it looks apparently when he engages in
this kind of behavior because it's not
just an isolated incident right there's
not a single person where he set about
that person the same thing I'm saying
about Robin Hansen like his opinion
should be taken seriously even though he
doesn't agree with me and that's what
run was saying to amrod right run is
somebody who's not a big Doomer right
he's kind of a a Centrist I don't know
what Run's pom is I get the sense it
might be something like 5% like
relatively low compared to my 50% but
higher than 1% that's my feel of Rune I
don't think he's officially said it and
run is somebody is like yeah you should
respect both sides there's clearly smart
people on both sides just like there is
in like any controversial issue
just to show you Martin cas's style of
kind of going all into the fight like
treating this as like an allout fight
where standards of discourse take a
backseat to just him winning he wants
you to believe that s1047 is a very
unpopular Bill despite a lot of evidence
that it's popular or at worst maybe
controversial but to say that it's
unpopular just seems disconnected from
reality so I I want to quote you a back
and forth that I had with Martin so
first I tweeted you're using terms like
overwhelming opposition because you're
not accepting it as fact that the proide
has 2x more supporters than the
anti-side
correct and Martine saying Lon please
compile a list of credible public
statements for and against the bill and
make it public I'll do the same and I
said got it so we have a basic factual
disagreement about what the majority of
Voters believe I think more than 60% of
California voters are pro s1047 you
think less than 40% of Voters are pro
s1047 this does seem resolvable by
comparing our sources I'll share mine
number one the center for AI safety poll
showed 77% support the future of Life
Institute poll showed 59% support the
California State Assembly voted 48 to6
and the state senate voted 29 to9 for
the bill Congress members generally vote
the way they think their constituents
want them to vote so that's my evidence
and Martin replies these are polls I
said public statements from credible
sources I replied I'm confused about
what claim we're disagreeing about can
you confirm that you disagree on the
factual question of the percentage of
California voters that support the bill
Martin says yes push polls are not
reliable indicators of course the
opposition side has them to showing
majority but I never mention them
because anyone serious in policy knows
they're useless and I say so we disagree
the methodology of determining what
voters are asking for claiming that
quote unquote public statements outweigh
the evidence from the Congressional vote
as well as every poll we have swinging
the True Value from greater than 60%
down to less than 40% seems like
willfully bad methodology and then I'm
quoting where he says of course the
opposition side has polls too showing
the majority but I never mention them
they're useless your side talk me talk
to Martin your side has released a much
worse quality poll than my side but
you're choosing to blur the distinction
yeah so there was a poll showing that a
slight majority of people are against
sb147 but it was like the worst pushful
I've ever seen like it doesn't take much
you know sure I'm biased but it doesn't
take much to look at the language in
that poll and compare it to the language
in the other polls and be like that is
some very very skewed language like it
was insane uh you know you can go look
it up for yourself uh so Martin's
replying I disagree plenty has been
written about the massively biased
nature of the proide too that is why no
one takes push bow seriously and why
generally public statements from
credible sources are the primary method
of accounting and I replied I'm happy to
let readers of this thread judge for
themselves a the relative quality of the
three polls we've referenced and B the
evidential weight of a Congressional
vote on the bill on what the Congress
members con constituents believe so I
felt like I presented more compelling
evidence and this was before all those
unions in Los Angeles and Hollywood came
out as being for the bill this is before
Chris Anderson who runs the Ted
conference came out in fav of the bill I
think this is before Elon Musk came out
in favor of the bill so to have quite a
lot of support diverse support as well
as the state assembly vote and the less
ridiculous series of polls telling you
that there is like two-third support to
have all this evidence and to not just
say okay I think it's ambiguous he's not
saying it's ambiguous he's saying it's
an unpopular Bill like give me a break
man you're obviously just treating this
as a fight where you're not open to the
idea that that you might be wrong about
an objective fact like whether a popular
bill is
popular okay for my part I'm absolutely
open to actual evidence showing that the
bill isn't popular but it's actually
unpopular I'm open to evidence in fact
I'll concede that my evidence isn't
airtight maybe the assembly members did
a bad job of representing their
constituents maybe the two polls that I
thought were reasonable were actually
massively skewed I'm open to that I just
don't see how you get to The Confident
belief that it's unpopular when most of
the evidence is pointing to it being
very
popular so that's my Doom tiff with
Martine Cado you know it just sticks in
my craw when we can't get agreement on
table Stakes whether a bill is popular
or unpopular to such a high degree like
greater than 60% or less than 40% we
can't even distinguish that what is
going on why is epistemology so hard why
is it so hard to know things that seem
simple right or the idea of like let's
not character assassinate people like
yosua Benjo let's just treat them as
arguing in good faith because they've
shown a lot of good faith they've shown
a lot of humility and they don't have an
obvious bias right you want to talk
obvious bias look at you Martin Casado
you work for a16z where you're trying to
sell Investments for a higher price than
you bought them right or ride them to an
IPO so you need AI to go well in like
the next seven years right normally I
don't even bother bringing this stuff up
that's not how I approach these issues I
argue object level but when you're
looking at a guy who's so big on
character assassinating everybody it's
like how is your character
reputable right your firm pays literally
millions of dollars toward lobbying
which normally I don't care about but in
your specific case because of who you
are right a noted character assassin
because of who you are I think somebody
ought to character assassinate you to
some degree but it won't be me because I
don't do character assassination my form
of assassination is I just list bad
arguments that somebody's made and then
I let you form a conclusion based on
that that's how I quote unquote
assassinate somebody that's how I tiff
with somebody
all right that just about wraps up the
first episode of Doom tiffs we talked
about so many people today we talked
about amjad mad and Alazar owski and run
we talked about Professor Lee Cronin we
talked about Naval rant talked about
Martin Casado and yosua Benjo a lot of
different voices scrolling through
social media which as you can see I'm on
there quite a lot so now I have a
question for you what do you think about
this episode cuz I got to make a
decision about how I'm going to focus I
don't know if I want to keep making
these kind of Doom tiffs episodes right
now I can really only put out one or
maybe two episodes a week and there's a
lot of different stuff I want to do I'm
really going to have to think about how
frequently I want to do Doom tiffs I
could potentially do it like once every
couple weeks maybe I'll just do it once
every few months when I just have a lot
of tiffs collected I don't know so I'd
love to hear your feedback how much do
you like a doomed tips episode relative
to other kinds of episodes that I do
that'll tell me how much I want want to
lean into Doom tiffs funny thing about
Doom tiffs is it's basically like you
looking over my shoulder while I scroll
my Twitter right so it's a lot of just
like reporting from my side and a little
bit less analysis I'm curious to hear
your thoughts maybe you just love it
when I report on Twitter news let's see
if you want to back me up when I engage
in these kind of Doom tiffs here's what
you got to do go to x.com Lon follow me
on Twitter watch for when I'm getting
into these tiffs and then go smack that
like button or you can reply and be like
great Point Lon and then people will be
like oh man the crowd is loving Lon in
this Tiff so that's how you can do your
part you can support doom debates by
encouraging me to get into these tiffs
on Twitter and you know what if I could
just bring my YouTube subscriber base
over to X you guys could back me up in
my tiffs every time I'd say a reply it
would get like 900 likes and the other
person would wonder wow there's so much
Social proof that Lon is right maybe I'm
wrong you know it would really encourage
humility and the people who are
disagreeing with me so head on over to
x.com Lon smack that follow button or
however else you want to support and
engage with the podcast I appreciate it
and I'll see you all next time on the
next episode of Doom debates
[Music]