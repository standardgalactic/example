I think it's really simple you have the
base model which is just trained to
match the statistical distribution of
text this is basically a autocomplete um
and then and they've just gotten so
massive now they have these massive
tables of of um lookups that they can do
lookup tables really that's how you pass
the touring test with a lookup table
welcome to Doom
debates so
gpt1 just came out and today we're going
to be looking at a podcast by two pretty
famous guys in the AI Community Dr Tim
scarf and Dr Keith Dugger who co-host a
podcast called machine learning Street
Talk the number one AI podcast on
Spotify definitely worth a look they
have a Viewpoint that's pretty different
from mine pretty easy to disagree with
they are non doomers who don't think
that current AI is that close to True
reasoning so because of that I thought
it would be fun to dive into their
recent episode that came in the wake of
gp01 see what kind of stuff they're
talking about how they're analyzing it
and that'll be great for me to kind of
push on and give you a different
perspective on how I analyze gp01 and
you'll get the benefit of hearing both
perspectives at once and kind of getting
a 360Â° view of gpt1 H what do you think
good idea for an episode well having
just recorded the episode I do these
intros at the end I can tell you I do
think it is a pretty good episode I was
pleasantly surprised how meaty and
substantive their episode was and I was
interested to push back and explain the
Crux of our disagreement Dr Tim scarf
has done stints at Microsoft as a
principal engineer Chief data scientist
at BP and founded several other Tech
startups and has a PHD in machine
learning and a first class degree in
computer science Dr Keith Dugger
obtained his PhD from MIT and has done
stints at IBM research Wall Street and
Microsoft and like I said they got a
popular podcast that has a bunch of
great guests and high quality exchange
of ideas all right let's dive
in the first topic they get into is
computability theory and how that
relates to AIS many of the things in
theory of computation in my educated
opinion really sit on the cognitive
Horizon of the vast majority of
smart people okay like like I and many
other smart people have said theory of
computation is a class that blew their
mind and you know Turing was a a genius
you know beyond beyond Geniuses because
he invented like this field really I'm
talking about turing machine versus push
down automata versus finite State atoma
Etc these are not trivial in the
slightest they are are they are not
technicalities they are extremely
powerful and vital and
important fundamental concepts of
computation what is computability theory
it's something I studied in college as
part of my computer science degree it
goes back to
1936 with a famous paper by Alan Turing
on computable numbers that kind of
kicked off modern theory of computation
the core principle of computability
theory is that when you define certain
simple model mod of computation like
most famously a turing machine but
there's a bunch of equivalent models of
computation you can Define these are
just mathematically formal models
they're mathematical structures where
you can say look imagine an infinitely
long tape divided into a bunch of cells
each cell can contain a zero or a one or
maybe a larger number it doesn't really
matter and then you've got a read right
head that can scan left and right across
this arbitrarily large tape it can scan
left and right it can read the value on
the tape it can write values on the tape
it can do conditional logic like oh if
there's a one here then move left if
there's a zero here move right so it can
just do a few basic operations I've
pretty much told you everything that it
can do uh it can also maintain an
internal State like it can be in one of
a finite number of different states so
that's pretty much everything that a
turing machine can do it's like this
simple mathematical model the
fundamental principle of computability
theory is these Turing machines have
ridiculously high limits on the possible
set of things that they can compute for
you like you can take any arbitrarily
complex function that you want to
compute and there's some corresponding
configuration that you can give to a
touring machine there's some
corresponding touring machine that will
actually compute this arbitrarily
complex function so when I say any
function for instance like I can give
you a huge number and if you want to
factor the number you want to find the
prime factors of this 100 digigit number
these huge prime factors yeah it'll take
you a crazy amount of time but you just
need a turning machine to do it you
don't need to fundamentally invent a new
model of computation to do it because
you already have a turing machine and a
turing machine turns out to be a
universal model of computation turns out
to be able to simulate any other type of
device that you can imagine building to
do your computation work for you a
turing machine can simulate it it turns
out and this is kind of at the core of
the theory of computability it's called
Turing universality it's this idea of
like hey I write my programs in Python
you write your programs in C++ but you
and I turns out we can both write the
exact same set of programs we can both
emulate or simulate each other's
programs using our programming language
so there's no such thing fundamentally
as a more powerful programming language
not from the perspective of which
functions it can compute now of course
you can analyze differences like some
programming languages might make it easy
to write programs that are more
efficient or some programming languages
might make you write fewer lines of code
but the observation that both
programming language or any programming
language can ultimately compute or not
compute the same set of functions is
extremely profound and computability
theory was founded on this idea of like
let's find the limits of these models of
computation and let's let's figure out
what kind of different models of
computation there are and then it turned
out as Turing already observed like wow
this one pretty simple model is already
kind of reaching the ceiling like
there's not really anything that's worth
doing that you can't do on a tring
machine it's just a question of like how
do you do it with fewer resources how do
you do it in a way that's convenient to
build that doesn't take many lines of
code there's this thing called The
Church touring thesis which is just a
really strong conjecture a really strong
belief a really strong guess frankly
that we're never going to think of a
better model of computation that's more
powerful than Turing machines and other
equivalent models like we're already at
the peak of what it means to compute
something definitionally that's what the
church touring thesis says it says hey
let's stop looking for definitions of
models of computation like we got it
guys we know what computation is and so
far it's lasted almost a century with
nobody coming in and saying credibly
like no I've got a better definition for
computation is like we feel like we know
what computation is so that's the church
string thesis now Within These Sky High
limits of computability theory defining
what computation is under that you've
got this whole study called complexity
Theory which is a very rigorous study of
the amount of resources that it takes
both in terms of time computation steps
and in terms of memory which is also
called space and there's actually a
bunch of other resources that you can
measure to but that whole study of how
much resource does it take to compute
something and how does the resource
requirement scale as a function of the
size of my input so famously like if you
want to factor a number and you only
have a classical computer we think it
takes exponential time to factor the
number so if I give you like a 100
digigit integer it's probably going to
take you something like 2 to the power
of 100 computation steps to factor that
number and now you're getting into like
an amount of steps that the entire
lifetime of the universe can't
necessarily contain so that's why we
think factoring numbers is hard that's
an example of something that complexity
Theory tells us is very very hard while
computability Theory just tells us like
oh yeah factoring numbers is computable
no problem computers can factor numbers
so that's the difference between
computability Theory and complexity
Theory those are both the two main
branches of theory of computation so
that's my little overview of theory of
computation it's not directly relevant
to most of the AI discussions we're
having just because even complexity
Theory doesn't give you that many bounds
to what AIS can do like intelligence
kind of hacks around complexity Theory
it takes so many shortcuts that it kind
of rips apart this whole idea of like
what is the complexity of a problem like
it doesn't matter because you're taking
so many smart shortcuts that you're
somehow finding your way to the answer
even if complexity Theory tells you is
supposed to be like super complex like
we're just surprised at all these
shortcuts that these AIS are finding so
I generally don't talk that much about
complexity Theory and I certainly don't
talk much about computability Theory
because computability theory is just
such a sky high limit of what's
computable versus not you won't hear me
talk about these theories that that much
on this podcast but I just wanted to
give you a brief overview so now you're
ready to go hear what Tim and Keith have
to say about it as it relates to AI Tim
and Keith are going to use computability
Theory to say hey we've got these
Skyhigh limits on what computers can do
and the AIS that we're building aren't
approaching those limits let's listen I
can build anything out of you know nand
Gates and so therefore I can build
anything out of um out of uh um neurons
including like the computer that said
these people should know better like
they honestly should know better because
they should remember they should
remember the days I I'm even young
enough or old enough to remember let's
say um uh the days when computers
literally had
tapes okay where you would build a
computer and you would program a
computer and that computer is a finite
thing okay it sits there in a room it's
got like a fixed number of vacuum tubes
or transistors or whatever the hell okay
it's it's a finite volume finite masting
sitting in a room okay it's clearly a
finite entity as like all computers are
but these computers had a tape deck okay
and that tape deck was like the tape
from the Turning machines and it was a
readwrite working memory so the computer
could sit there and turn the wheels of
the tape right backwards and forwards
and go along and write stuff and read
stuff and guess what but it could run
out of memory and a little light would
turn on Okay or a little counter which
would
say insert another tape and you take off
the tape and you set it in a box and you
go over to your box of uh empty tapes or
tapes you don't care if they get
overwritten and you put it on there and
the compute continues you don't need to
reprogram the machine you don't need to
rebuild it you don't have to retrain it
like you would have to do with a neur
Network that program is constructed such
that it will just continue the
computation and this can go on forever
like the machine can keep telling you
add next tape add previous tape add next
tape add previous tape add previous tape
add previous tape insert previous tape
right and you just go back and forth
forever like for some unknown amount of
time and if you run out of tapes in the
room well you drive over to the
warehouse and you grab another box and
if you run out of tapes in the warehouse
you go to the Factory and you build some
more the point is all during this time
you never had to reprogram the machine
so that's a correct description of how
the physical computers that we've built
that we call computers the fact that
they can request more tape so that their
memory can be arbitrarily large that is
in fact a defining property of a
computer of course we rarely have our
computers today requesting more memory
cuz we already have plenty of memory we
already have plenty of storage it's
usually not a bottleneck but yes we have
computers and they act like computers
now let's see what his point is about
AI neural networks are a like when we
train neural networks when we train them
and it's critical it's not just the
neural network it's neural network plus
the procedures we use to train it okay
they learn algorithms that don't know
how to do that they don't know how to do
that they can't tell
you I've run out of memory during
inference time I need you to add more
and then you add more and does something
it says I've run out give me another
tape we don't know how to train
algorithms that know how to use an
expandable an expandable a potentially
infinite amount of memory this is the
critical difference and those guys
should know better they should know
better okay if you look at an llm or a
traditional neural network he's right
that it doesn't have that arbitrarily
large memory thing that computer
programs General have because when you
look at just an Alamar and neural
network you know how many layers there
are so you know ahead of time okay all
the different numbers are just going to
propagate through each layer and get to
the end and there's let's say 96 layers
in
gpt3 and that means there's going to be
96 of these iterations and the amount of
memory that it's going to need is just
proportional to that and proportional to
how many neurons it's using so it's not
like getting into a loop and the loop is
unpredictably asking you to use up more
memory these kind of controls go
structures like Loops branches uh kind
of fundamental unpredictability like you
don't know exactly what's going to do
you kind of get surprised these are
distinguishing features of the the
better models of computation so he's
right to point out like oh my God a
neural network is like a kind of a
simpler or model of computation it's not
using the full power of a touring
machine okay that's fine but for a while
now we've been in this regime where
neural networks just have this stream of
thought and yes the stream of thought
has a finite size engine outputting it
but like the tokens keep coming and you
can ask for an answer of an arbitrarily
long length and this is exactly what we
see with a human brain I mean every time
I as a person every time the next word
comes out of my mouth it's pretty
reliable that the next word is going to
come unless I spend a crazy amount of
time thinking between words it's pretty
reliable you can count on me to only
spend like a few seconds or even less
than a second thinking of each word but
if I'm sitting here thinking to myself
talking to myself eventually all the
words are going to add up to something
that really does harness the full power
of my own human computation right it's
just a matter of time like Let The
Tokens build up Let The Tokens influence
other tokens let the tokens suggest
going back and rewriting other tokens I
mean this stuff all happens after each
individual token is outputed so if
you're using the word inference time to
just describe this forward pass of how
you get each one token then okay yeah
it's not a full model of computation but
if you just step back a little bit and
you're like okay the whole process of
what it's doing outputting tokens but
what do the tokens do and what happens
when you think for 10 seconds or think
for 10 minutes or or some larger amount
of time this larger process that we call
thinking not just outputting one thought
token at a time but thinking it's a
richer process and it has branching
structure right you can go back and edit
yourself you can decide what to do based
on what your thought was that's where a
lot of the interesting computational
complexity is going to come from of
course right so why are you dising
neural Nets based on the short process
that outputs one token at a time I don't
get it especially now that this is
supposed to be an episode about
gpt1 where the whole thing about 01 the
whole specialty is it gets to think and
think and think before giving you the
answer so the part where it's thinking
and thinking and thinking and deciding
what to do and reflecting that part has
a branching structure right so it sounds
like this is like the worst time for him
to be making the point about the simple
model of computation that is a feed
forward neural network right on the heel
of gp01 which is like transcending that
Paradigm and doing a lot of things past
the token generation part of the
thinking okay but if we look at the
class of useful programs that we might
want to do here on Earth they could
still be represented with a very large
finite amount of computation in in a
neuron Network so the argument is
basically for a large class of useful
problems we can use neural networks what
is the main argument why we need to have
this different mode
computation well I would I would first
say you know we've already the world's
already run into plenty of problems that
um um that require a turning machine and
require memory like this thing about
insert tape and whatever you know it's
it's a it's a real it's a real
phenomenon right like adding more nodes
adding more storage and you don't have
to reprogram reprogram the machine and
we have many you know many theoretical
results about problem we care about that
require Turing machines they require
Turing machines in the sense that you
know when some input comes in um and it
can be some finite input by the way some
finite input is given to the machine um
the amount of time and storage that it
needs to compute the answer to the
problem that we care about is a variable
and it's a variable that that depends on
the input and in fact it's a variable
that we may not even know ahead of time
right
so there's a large class of of things
that do this so no matter how big you
make your your neural network um you
know uh it's always going to be the case
that you can run into a problem that
that neural problem you care about okay
that that neural network just isn't
going to be able to solve because it
doesn't have enough you know memory
basically and people say oh it doesn't
matter you know we'll just build them
like huge we'll make them huge man like
super massive the size of the entire
State of California and then at that
point like it'll be able to solve like
every problem human beings will ever
care about when he's talking about
making these llms bigger I guess he's
talking about bigger in the sense of
let's have llms with more parameters
like more hundreds of billions or
trillions of parameters and let's have
bigger context windows so they can
handle bigger chunks of input and these
days context windows are already hitting
100, or a million plus tokens like
people are finding hacks how to have
really big context Windows uh but you
know the the obvious problem with making
them bigger is just that like you're not
necessarily going to nail the next token
on your first try that's a strong
intuition we can get when we look at how
humans think like you could have the
smartest human in the world but if you
make them come up with a first word of
the answer on their first try you're
really going to impair their thinking
compared to giving them a pen and paper
and be like hey just think think for a
while write down your thoughts edit your
thoughts right that it feels like it's
more powerful and just be like you had
to nail that first sword and don't get
me wrong I think as we get to gp5 gp6
gpt7 I think we're going to get better
and better at having AIS that nail the
first word of their thoughts but I think
even a super intelligence is going to
get value out of being like here's a
really good candidate first word but let
me also use my 100th word to reflect
back on it and maybe change my mind
right let me not have a perfect stream
of Consciousness let me have a branching
stream of Consciousness that I can edit
so of course we're not just only going
to solve everything on the first shot by
just having more parameters and a larger
context window so you know I'll Grant
him that much but let's see where he's
going with it when you try to take a
touring machine program one that has
like a
variable amount of sort of compute space
and stuff like that and you try to
instead convert it into like a single
forward pass sort of neural network that
doesn't understand iteration and read
write memory and things like that what
happen s is you get this kind of
exponential blow up of the size of the
program um and this is like nothing new
folks we can do a fun example let's see
what happens when I ask GPT to factor a
number because on a classical computer
we are pretty sure that factoring takes
exponential time and these neural
networks really are a fixed amount of
time to generate each token so Keith is
right that you can't just use this
neural network with this very fixed
amount of layers and have it start
spitting out the correct
first numbers in the prime factorization
of this large number that's not going to
work because you really do need to just
iterate you need to have more steps than
this fixed amount of neural network
layers so totally on point there it'll
be funny to see how GPT 40 handles the
request to factor this number so I said
please Factor this number
58293335
and 137 and I'm like wait a minute what
how did that happen but I go and I check
the answer I multiply together those
numbers and of course it doesn't
multiply out to
58231 it actually multiplies out to
let's see
585,000 out of 582,000 so the classic
hallucination right it gives you an
answer that without sitting down and
entering into this kind of looping
process this multi-step operation just
by like does this sound right it
absolutely sounds right it's like as a
human bser right with my limited human
brain if you came to me and you're like
hey Leon Factor 58293335
you about what happened when I used GPT
40 I didn't tell you something else that
happened the first time that I typed
into GPT 40 when I said Factor this
number 58293335
had to specifically tell it hey don't
write code just magically give me the
answer on your first guess so I put it
in a position which is very similar to
you walking up to me and pointing a gun
at me and saying hey Leon factors of
about where it's like yes Keith is right
you're not going to have a turing
complete genius get things right on the
first token always especially with these
kind of math problems or other kind of
problems that we know that their
complexity theoretic minimum is a
certain number of operations that's not
going to fit in the feed forward passive
neural network like that is for sure
there there's no questioning that claim
like it's provably true but the actual
nature of thinking has to do with
spitting out lots of tokens and then
looking back on the tokens you've spit
out before and doing editing passes and
using branching logic right so let's not
talk about this impossible ideal of
being a genius one on your first token
let's just talk about the reality where
you get to spit out a lot of tokens
while you think let's talk about that
let's analyze the computational
complexity of that or the computability
of that let's have that discussion let's
just start with your definition of
reasoning what is it for you hey let me
take a stab at defining reasoning this
is actually something I've never done
I've only just looked at other people's
definition of reasoning and scoffed at
it it and poke Tools in it but here's my
own definition reasoning is the process
of taking in an input and then doing
valid stepbystep operations on that
input and then getting to the right
output as a result of those operations
simple epistemically this is a
definition of reasoning where it's kind
of results oriented so if I have a
process that claims it's doing reasoning
but it just keeps getting the wrong
answer and it's not useful I might argue
that whatever it thinks our valid
operations aren't valid in a useful
sense so we can always argue over the
different components of my definition we
can still disagree on individual
examples whether it's reasoning or not
but I think it gives you a good
framework where if you accept my
definition of reasoning then we can
reduce the argument from an argument
about what does reasoning mean to an
argument about like hey is this input
well defined do we agree on the
definition of a valid step in the
reasoning right so I think that I'm
adding value by letting us argue over
smaller pieces I think I've applied
uction ISM into this concept of
reasoning so anyway that's my definition
of reasoning process that takes an input
does valid operations on it gives you an
output Let me Give an example if you ask
an AI to factor a prime number and it
writes a Python program and factors it
and gives you the answer it's done
reasoning to realize that python is
going to give you the right answer and
the Python program has done hardcoded
reasoning about what it's it's gone
through the operations that you need to
do to factor the number now you may be
like reasoning that's all hardcoded the
the human who wrote the program was the
one doing the reasoning man and I'm like
okay sure so now we're applying
causality to to trace back what other
causal pieces were involved in the
reasoning that's fine we can do that but
I I would still say that it qualifies
under my big umbrella definition of
reasoning at the end of the day slap
fights these semantic slap fights about
what's reasoning and what's not the
reason I tend to just scoff at them is
because I think they will all ultimately
just take you to the optimization
question or or the analysis of like
where where is optimization happening
where is the future of the universe
being squeezed which part of the
universe
foresaw the the future of the universe
what it can do to alter the future
trajectory of the universe and got it to
a point that was a priori improbable
which puzzal process overcame the
improbability barrier that's where I
think the discussion is going to end up
when you start having a slap fight over
reasoning and you saw it now in the
example of factoring like ah the human
who wrote The Python program that human
was actually the one who was envisioning
a future where programs would factor
numbers it's like okay fine yeah the
human brain was the optimization engine
responsible for this sure but that's
different from defining what reasoning
is and I think that if you only focus on
what reasoning is and what's doing
reasoning and what's not then you're
just coming at it with a less powerful
mental model a less a a concept that you
think is foundational that you're
actually going to discover is just not
as useful in foundational as
optimization but let's take examples
let's see what they have to say so I
will give that before before I give that
I'm not I understand the word is used to
mean you know lots of different things
okay but if you if you literally just go
and look in in the dictionary or the
wiki or or whatever you're going to see
that almost all of those kind of Common
Sense definitions okay they they have
they have two com you know components in
common um one is that they all refer to
a
process a process okay so we have to
think about kind of iteration that kind
of unfolds here a
process yeah and then the other thing is
it's a process that
applies logic okay and sort of rational
you know sets of rules hey this turned
out similar to my definition we both
talked about a process he said the
process has to apply rational sets of
rules I said it has to apply valid sets
of rules I think that's basically the
same idea I think we've got the same
definition great okay so to me okay to
me the most you know General process
that applies logic is an effective
computation in other words it's the
computations that Turing machines can
perform that's you know the another word
for all the possible computations that
Turing machines can do is is effective
computation um so to me reasoning is an
effective
computation in pursuit of a goal
or inference of knowledge what he's
saying here is hey we talked about a
process that can apply valid steps to
get you some output but did you know
that every type of process that can be
well defined is a process that can be
done by a turing machine ever heard of
the church Turing thesis process is
synonymous with thing that a turing
machine can do for all intents and
purposes in the real world certainly so
watch me mix the church Turing thesis
into my definition of reasoning so that
instead of just saying valid process
process of applying valid steps I can
also say effectively computable process
of applying valid steps I don't see why
it was necessary to bring up the church
Shing thesis in the current context of
talking about llms but let's see where
he's going with it okay but but many of
the computations that neural networks do
of course are a subass of the types of
computation you can do on a touring
machine so they are still effective
computations and they are in many cases
still in pursuit of der knowledge or
achieving a goal so why is that not
reasoning yes excellent question and
this can even be asked on the first
token like if you ask the AI a pretty
complex question like hey is nuclear
power a clean energy policy for the
United States to support and it writes a
long answer but the first token in the
answer is yes comma so it's done under
the hood within the matrices of even
just outputting the first token it's
done some reasoning to come up with the
output yes and probably the explanation
it's going to write after yes is going
to reflect the same kind of reasoning it
did so you do get some interpretability
of where the yes came from even though
it could always be hallucinating like it
could just be randomly outputting yes
having no idea what it's talking about
and then just making up a
rationalization why it said yes but
point is you're going to have the word
yes come up more likely when you ask it
questions that are more obviously yes so
if you ask it something like hey would
it be a good policy to let anybody shoot
anybody I'm going to guess that the
first word is no here let's test it
right now
now would it be a good policy to let
anybody shoot
anybody no it would not be a good policy
to let anyone shoot anyone such a policy
would lead to chaos dot do dot so good
answer but the very first token that it
outputed before it even knew what it was
going to say next the first thing that
it said is no it didn't say yes it said
no how did it know to say no well I
would argue that the mathematical
operation that happened under the hood
between taking in my input and giving
that first output token no I would argue
that that was reasoning reasoning
happened not perfect reasoning not the
kind of powerful reasoning that also
lets you factor numbers because that's
not going to be enough computation steps
for that but still an amount of
reasoning that can get you to the next
word in your stream of Consciousness as
a thinker in this case no was just the
first word because you already just knew
enough to say no
the way that it did Matrix math under
the hood in those self attention blocks
and those feed forward neural networks
all of those mathematical operations in
the context of this particular input
about whether everybody should get to
shoot everybody in this context the way
that it got to its answer was valid it
happened to be a system that was good at
one-shotting a valid answer to that kind
of relatively easy yes or no policy
question I think a good question is how
robust is it you always trust it to say
yes or no even when the problem got
complex and then I think the answer is
it's the same as asking a human and
giving the human one second to answer I
think you're going to get a very similar
experience right you can have the
smartest human in the world the best
policy thinker in the world but if you
ask that person hey here's a policy
question you have to answer in one
second and then take the first word that
they say you're probably going to get a
similar validity to the reasoning just
like the person has like a decent first
word but they could really use more time
to think you're getting the same
situation with these a where they give
you a very decent first token but it
would help if you give them more time to
think when you observe a situation like
this and you just point your finger and
you're like Ah that's not reasoning I
think you're doing it wrong you're using
the wrong mental model arguing over the
definition of reasoning like I said I
don't think that's where the productive
meat of the argument is I think you can
just go ahead and call it reasoning and
you can just get to the question of okay
well how powerful is this reasoning
because there's a spectrum there's no
hard boundary where we can say aha today
they're not reasoning maybe tomorrow
they'll reason but today they're not no
they're clearly reasoning to a degree
and this is before we even talk about
using multiple token outputs to reason
and having the tokens loop back and edit
previous tokens the way that humans do
when they reason that's before we even
talk about that I claim that it's worth
using the word reasoning to describe
what an llm does even when it just
outputs the first token or one token at
a time but let's see what Keith thinks I
would say these people are kind of like
being the the technical you know the the
kind of arguing technical points but
kind of missing the the forest for the
trees it's like you know sure um the
drink vending machine at the gy okay is
performing a subclass of effective
computations namely it's a finite State
automata and with the goal of collecting
money and and and dispensing a drink
is that
reasoning yes it's a simple toy case of
reasoning you put in a quarter you put
in another quarter it has conditional
logic under the hood to say when you
then press that Gatorade button is it
going to dispense the Gatorade or is it
going to wait for more money right that
is a simple toy case of reasoning and
when you look at more sophisticated
reasoning it's built out of the same
kind of building blocks that that simple
case is right there's just a whole
spectrum of complexity of reasoning but
the simple cases of reasoning are also
worth being called
reasoning now what's the point of even
having this discussion besides just a
semantics slap fight what's the point of
all this well we're trying to predict
how powerful AIS are going to be that's
what everybody's interested in that's
what's economically valuable that's what
threatens the destruction of the human
species in our lifetimes how powerful AI
is going to be at optimizing the
Universe I think we can all agree okay a
vending machine is doing a trivial case
of reasoning on some spectrum and a
vending machine doesn't have enough
scope in its input output mapping to
pose an existential threat to humanity
right I don't think that there's
anything substantive that we're arguing
about here but if we're having the
semantic slap fight about what is
reasoning I still think I should win and
you just admit that that's a case of
simple reasoning but whatever what we
don't want is an anything Go's
definition of of reasoning right right
so I said reasoning is knowledge
acquisition the new open AO models don't
reason they simply memorize reasoning
trajectories A Gifted from humans boom
there it is okay that's the kind of
claim that llms aren't reasoning that I
like to scoff at here we go I'm going to
scoff memorize trajectories from gifted
humans it's one thing to say hey part of
their training data includes examples of
good reasoning in various domains that's
obviously valid
and we know that if you give them a
prompt that's very similar to something
that they've seen in the data that is
going to drastically increase the
likelihood that they're going to get it
right fair enough fair enough but then
to say that they don't reason because
they only memorize trajectories is just
minimizing the accomplishment that you
can observe like when you give them a
new medical case study when you talk to
them about your unique set of symptoms
and your unique history and then on the
Fly they put it together they logic
connect what you're telling them with
some other knowledge that they have when
they do that you don't want to call that
reasoning because there's other quote
unquote trajectories there's other
things in their textbook that by some
generous metric are similar to what
you're saying and again this similarity
metric as I've mentioned in past
episodes it's not so obvious that these
things are similar that before a few
years ago we could ever make an AI that
could do this right so this notion of
similarity is a notion that we've only
just discovered now with these high
dimensional Vector spaces and this
attention mechanism and this exact type
of training and inference process to
just brush off the Pinnacle of
achievement in how AIS can map inputs to
outputs in these huge search spaces to
brush that all off and be like well it
doesn't count as reasoning because
there's some textbook that they ingested
just like a do a human doctor does
there's some textbook that they read
that describes other case studies so
they're just applying what what they
read about other people's medical case
studies when they're giving you advice
that's relevant to your case study even
though they've never seen your case
study before it doesn't count as
reasoning I'm sorry generalizing a
textbook in something like a medical
case study and applying it to you that
is traditionally called reasoning and a
human who can't reason is not going to
be able to pass a medical exam that has
case studies memorizing the textbook is
not enough to handle case studies you
have to apply the knowledge the
application process is called reasoning
you can't retroactively pretend like
that's not reasoning anymore like maybe
you need a new term right you're clearly
in the market for a new term to describe
something that humans can do that AIS
can't and that's fine go make up a new
term but to repurpose the term reasoning
as if passing a medical exam or doing a
medical case study just to take an
example as if that's not
reasoning you've really gone and lost
the semantics of reasoning that that
have been accepted in every generation
until the last 5 years I would scoff a
lot less if you just made your point
using more appropriate terminology if
you just said look it's doing a lot of
things right that are displaying an
impressive degree of reasoning but it
doesn't have the highest degree of
reasoning where it can robustly pick and
choose which parts of its knowledge are
relevant when and not make mistakes and
notice when it does make a mistake and
correct its exact right mistakes and
also come up with new knowledge that's
really different from anything it's ever
seen before right if those are your
beefs then you're making sense if you
talk in that language but to go all the
way to say that it's not reasoning just
because it's not fully matching a human
in every way you're just going way too
far and everything that you've said to
this point about computability and
effectively computable functions has
been a logical non-sec you haven't
actually defended a point using logic
which is why you've just been stopped by
logic
C so a lot of a lot of reasoning is just
kind of knowing what to do because I
know you're saying it's an effective
computation some of the people responded
to this saying oh you know Tim what are
you talking about reasoning is is just
know mapping an input to an output in
data space yes by applying a sequence of
valid steps to get there so I would say
that uh the the drink machine you know
is is not performing reasoning okay or
if we have to really be if we really
have to be uh you know technical about
it it's it's per it's performing
reasoning at at level you know um
0.0 you know something another okay yes
we absolutely have to be technical about
it okay so you just admitted that it is
a low point on the reasoning Spectrum
great so instead of saying those stupid
Vending machines they have nothing to do
with Advanced reasoning you could also
look at it and be like oh wow it did one
or two or three valid steps to just tell
you that you can't have a soda because
you don't have enough money and valid
steps chain together valid steps compose
big things emerge out of little building
blocks here is a system that only uses a
few little building blocks instead of
just being like Oh well this is so
different from Human reasoning you can
look at it and be like this this is a
microcosm of human reasoning and
composition of building blocks is an
arbitrarily powerful thing that you
could do you can be surprised by how
much power emerges from this kind of
composition so I think your whole point
has been undermined about what's
reasoning and what's not when you just
point to an example of simple reasoning
and just dismiss it and previously you
called it not reasoning there's a
spectrum of
reasoning and again moving beyond the
semantic slap fight the upshot of this
doomers like me are looking at AIS and
we're noticing oh they're reasoning
better they're reasoning better they're
reasoning better in some senses they're
reasoning better than humans they're
reasoning better than doctors at the
medical exam already right they're
they're reasoning better than a lot of
math students at the math Olympiad so
I'm just noticing oh okay the reasoning
is getting better the optimization is
getting better people like Tim and Keith
are looking at the same observations and
they're saying H there's no reasoning
here yet they have not cracked the
reasoning problem we've just gone a
million years of human history and the
only thing that's ever been able to
reason is the human brain everything
looks good everything looks safe thanks
to my questionable semantic distinction
this black and white distinction where
I've spent most of the podcast saying
that AIS and vending machines don't
reason and now I've admitted that
vending machines reason a little bit but
I haven't you know updated the
implications about AI like what are you
guys doing this isn't productive the way
you're analyzing
this and then and then an effective
computation that sits there and does um
let's say uh some geometry calculations
and some World modeling and and some
image recognition and and some some
other turning complete you know
computations that may not hold and
whatever then that's doing reasoning at
you know
level8 something and another I mean we
could have a whole scale here if we
needed to right yes there's a reasoning
scale in the same sense that you can
have a scale of math problem difficulty
there is easier math problems and harder
math problems and when you see a child
getting better and better at harder and
harder math problems you probably want
to extrapolate that the child will keep
getting better at even harder math
problems that's what the doomers are
saying with AI reasoning but if we're
just going to kind of break things down
into some binary the soda machine's not
reasoning and neither is a dictionary
okay if I go and like and I look up my
problem in a massive dictionary and it
has the answer and I spit it out that's
not reasoning you know it's not
performing the computation that led from
the problem to to the result it's it's
just doing a lookup yes if you have a
system like a lookup table where it's
mapping inputs and outputs just by doing
a single lookup so it's not stepbystep
valid operations You could argue it's a
trivial step I mean look up as one step
but let's just say that counts as
reasoning at a level of zero no
reasoning fine that's fine but when you
look at an AI when it's using things
that it learned from the medical
textbook to diagnose your case that's
not a lookup right if it's a lookup then
we could have done it 50 years ago right
we we didn't need to wait until 2020 to
be able to start doing these kinds of
quote unquote lookups they're not
lookups right they're reasoning they're
Advanced reasoning why would you even
compare the
two okay now finally we're going to get
into a discussion of open a I's new AI
01 what Keith is about to say I think is
a good plausible candidate explanation
of how 01 was postt trained or
fine-tuned to be better at long Chain of
Thought reasoning I think it's kind of
important to emphasize what you said
about what 01 is doing and how it was
trained right which is when they trained
it they took a whole bunch of problems
um ran a bunch of you know passes in
very creative mode so like temperature
one or whatever where it could kind of
come up with um you know all kinds of
fantasies about like you know
justifications for an answer and then
they selected those which had the
correct answer okay so we've got like
all these trajectories of reasoning and
by the way those are all by some
definition reasoning lots of them were
complete nonsense and led to the wrong
answer okay so then they elected those
which were which gave the right answers
and then used those to to retrain
retrain the model so it was more likely
to give patterns of quote
reasoning um that uh that gave right
answers he's saying they used an llm to
generate a bunch of chains of reasoning
some of them were right some of them
were wrong they focused on the right
ones but hold on wait a minute wait a
minute that situation where some of them
are right some of them are wrong that's
a lot better better than it sounds I
want to dwell on this point because
people don't get this typically when you
have a problem getting to the right
answer of the problem it's not just like
oh let me take 10 guesses or 100 guesses
and then I'll just keep the right one if
you've already narrowed it down to
generating a 100 Solutions and one of
them is Right typically on a lot of
these problems you've already crossed
most of the distance to the right answer
if you're trying to compete in a math
Olympiad and you've already narrowed it
down to a 100 chains of reasoning one of
which is correct one of which wins you
the math Olympiad then all you have to
do is Loop through the 100 candidates
and verify which one has the most robust
reasoning that's a lot easier than
searching the space of all possible
arguments to even generate 100
candidates in the first place where one
out of the 100 is correct this is a very
very key Point hard problems have
exponentially huge search spaces key
point this is what people don't get it's
I'll try to think of an analogy imagine
that you're in the Sahara Desert and a
leprechaun comes up to you and he says I
have a bow and arrow that I can shoot
this and I can shoot it at a piece of
gold and you're like gold where are we
going to find gold all I see is sand
everywhere and he's like well don't you
think that somewhere in this entire
desert buried somewhere randomly is a
small piece of gold and you're like well
I guess statistically that might be true
for some geological reason I guess the
Sahara Desert is a big place and you is
like okay do you mind if I take 100
tries and you look around and you're
like yeah take 100 tries I don't see how
that's going to help we're in the Sahara
Desert so he takes 100 tries and you go
through all the places where his arrows
landed and it's like okay no gold no
gold no gold and suddenly on Arrow
number 57 it's stuck on a piece of gold
you'll be like how the hell did that
happen how did he fire an arrow at a
piece of gold in the Sahara desert right
so that that's what these AIS are doing
yes not every arrow is hitting but
they're finding gold in the Sahara
Desert solving a math Olympiad problem
that's narrowing down a giant search Bas
into you know within 100 tries at a
piece of gold there's a reason why a few
years ago this had never been done and
people were saying oh yeah math Olympia
that could be decades away right there
there's a reason it's so crazy to me
that people are now looking back and
they're like oh they made it generate 99
wrong answers of course they just took
the right one right they don't have that
sense of perspective they don't get how
crazy it is that it came up with one
correct answer yes some wrong ones too
but the fact that it was within Striking
Distance it was within 99 rejects or
whatever it was right all you have to do
is sift through 99 Rejects and then you
get a correct answer to a hard problem
that is incredibly powerful and scary
you can't write that off by just being
like it didn't know which of the hundred
top tries was the best okay yeah we're
getting there we just have to narrow
down from 100 to one which we literally
just did with 01 right we made another
incremental step of progress of helping
narrow down the last 100 that as I
understand it is the specialty of 01
it's attacking the problem of oh wow we
just shot 100 arrows and one of them is
hitting the gold great let's narrow down
the 100 and as far as I can tell they're
succeeding right not perfectly but maybe
it's like they took the 100 and now
they've narrowed it down to like 30 or
something right like that's the nature
of the type of progress that they've
made and at some point they're going to
be more more human level where they just
narrow it down to one or two right
they're going to just keep narrowing
it's like how do you not see this trend
why are you seeing a boundary on this
trend can't you see that to even have
candidate solutions that you have the
privilege of narrowing means that we're
most of the way toward solving the
problem and so like you said it's sort
of building up this
dictionary of of kind of um
specific program specific rationals
let's call them rationals it's building
up like a a massive database of
rationals okay here was a problem here
was my rationale it gave the right
answer this massive database of kind of
little rationals and then when you give
it a new problem it sort of does this
context sensitive hashing and matches
the rationale that's kind of closest to
it and then sort of fills in the blanks
like an ad lib you know let me just
stick in all this stuff in there and
then hopefully it works
well sometimes it works sometimes it
gives complete nonsense and is that
reasoning
yes you're just using the most
dismissive terms to describe reasoning
let's look at what Albert Einstein does
okay he wasn't happy with some paradoxes
in Newtonian physics he asked himself
what happens if we just make the speed
of light constant in every reference
frame what does that logically imply
okay well Albert Einstein already
studied the history of Science and and
he knew that asking these kinds of big
intuitive questions was a Playbook that
scientists can do and then as soon as he
asked the question he just used regular
logical reasoning that everybody studies
in school to reason through the
implications of what happens when light
is constant in every reference frame so
there's no actual reasoning happening in
Albert Einstein's mind he just pulled
out a Playbook and he basically just
step by step did what the Playbook told
him to do he was just following a
trajectory of past human scientists he
never did any reasoning you know worder
Von Bron designing Saturn rocket he was
just looking at engineering textbooks
and physical calculations that had
already been done he was basically just
ingesting an understanding of physics
from some textbook and then just
applying it to figuring out how to
design where all the pieces should go I
don't see any reasoning here it's like
give me a break man your distinctions
are
non-existent I don't not in my opinion
this is not what we're talking about
when we talk about reasoning you know
we're we're more about the application
of the set of first principles to some
inputs in a process of applying logic to
derive the answer right yeah and to be a
bit more specific just in case the the
the folks haven't played around with
this yet it's doing the generation of
Chain of Thought templates I see how it
is every time an AI does something
impressive like diagnosing you based on
your specific case study you always
trace it back to Something in its input
like hey look this combination of three
case studies or three sections from this
textbook that was in its training data
they're all kind of similar so all it's
doing is really just pulling from these
using these as templates for various
pieces of what you're telling it it's
just applying templates so it's not
really reasoning but again it's like if
you're getting to that point how do you
compare that to humans aren't humans
also just referencing things that
they're hearing to templates and
applying similarity comparisons I mean
these are the building blocks of how
somebody reasons finding the closest
template or the closest logical rule is
how you reason it's rules all the way
down knowing which rule to apply when
and then applying it at the right level
of abstraction that's all there is man
that's all Einstein does that's all wner
V Brown does that's all Elon Musk does
when he's the CEO of a company right
that's all you do when you play the
universe as a video game you use the
same reasoning skills as when you play a
board game or a video game on a computer
it's all reasoning it's all degrees of
reasoning it's it's all optimization
it's all
search how long are you going to
rationalize your observation of
increasing degrees of competence and
skill at these General search problems
you don't have a boundary that you can
draw now there's a specific test that
can be used to prove that people like
Tim and Keith aren't making a coherent
distinction when they make these kinds
of claims they're just kind of
retroactively dissing everything that
they're saying as not reasoning but
they're not having a useful model that
has any predictive power or that even
compresses their observation to be
useful in that sense the test that you
can give them is to ask them to predict
what they think that AI won't be able to
achieve in the next year tell us the
least impressive thing that AI
definitely can't achieve in the next
year after all it can't reason right so
give me a sample input output that
requires true reasoning and therefore
you think a I won't achieve now they
might say something like well I bet it
can't medically diagnose you without
also having at least two or three
similar things in its training data and
it's like okay well how do you define
similar should we go through the Corpus
of text and scrub out things that are
similar like to what degree right like
the same body part the same muscle the
same combination of things so even
defining similar is already going to be
fraught I don't think that they have a
rigorous definition of similar here but
even if they said okay yeah let's wipe
certain similar case studies from the
training data let's make it have to do
kind of a bigger leap if it wanted to
diagnose you like it would have to do it
would have to make some multi-step
inferences based on other data that it
has that's less similar so they might
put up a challenge like that and then
I'd say great so you really think that
AI can't do what you consider a leap in
one year like tell me what the leap is
that'll convince you like okay this
particular mapping of input output
required a leap that's true reasoning
they are not providing a test that when
it passes is it's true reasoning they're
not saying that they're not saying aha
my test you know like Archy vals that's
my test once it passes that then that's
proof that it's reasoning right their
wholeo is to look backwards and always
dismiss the latest thing that just
happening as like still not reasoning
still not reasoning right they don't
have a test that can be passed to prove
that they're wrong like when are they
going to change their mind you have to
let your mind be changed using a
definite Milestone otherwise your claim
is meaningless if it's not clear how
somebody like me can provide you the
easiest evidence that I can to change
your mind right you got to give me the
easiest possible way to change your mind
you have to specify what that is and if
you can't then you're simply a pundit
who's retroactively calling everything
not reasoning because that's just your
Mo right you you get some kind of
benefit from being like that but you're
not thinking productively or using
productive distinctions or making
meaningful
claims and for those that don't know
about Chain of Thought it's a type of in
context learning so a lot of models
sometimes do better or they generalize
better if you kind of say you need to do
this and then you need to do this you
need to think about this so it's kind of
decomposing a problem into a rationale
that could be applied in many future
situations so you kind of say to the
model um think out loud or apply this
thinking protocol to it and then the
model works better but the thing is with
prompt engineering and Chain of Thought
the human supervisor always had to put
this into the model so what they did was
they came up with quite a clever idea
they said well why don't we get the
model to prompt itself with Chain of
Thought right so what they've done is
and as I understand I mean I'm just
guessing what their architecture here I
think it's really simple I think that
you have the base model which is just
trained to match the statistical
distribution of text this just basically
a autocomplete oh come on that's a slur
man autocomplete it's like stochastic
parrot you got to stop with that man
thinking of the next word that's likely
to come in the context of a piece of
text is not doing statistics on the
distribution of words in a text we
already knew how to do statistics right
we have a totally different architecture
where we're training the AI to predict
something that has no statistical
precedent right you can tell it a novel
situation and it somehow is mapping it
to stuff that it does know it's not
statistically judging what you're going
to say next it's doing something deeper
that we don't even fully understand but
it has to do with making sense out of
all the words that you're saying right
embedding them in this High dimensional
Vector space solving the symbol
grounding problem in my opinion and then
yes reasoning or applying steps taking
steps right doing little mini
Transformations like oh you asked the
question like this so I have to make
sure to give the answer like this like
that's not statistics these are
syntactic transformations semantic
Transformations like reasoning is the
normal word that we use to describe this
kind of thing sorry you set me off
because you said autocomplete and you
said match the statistical distribution
of text I I just can't believe that you
guys run a prominent podcast and you're
using that terminology to describe the
miracle the advance that is
llms you have the base model which is
just trained to match the statistical
distribution of text it's just basically
a autocomplete and then they do this
next thing called rlf which is where
they basically shape the distribution by
supervising it with examples of
conversation trajectories that are good
so saying this is good this is bad so
you know go here don't go there I think
all they've done is the same thing with
Chain of Thought trajectories now of
course the question is where did they
get the Chain of Thought trajectories
I'm guessing that they've done a bunch
of synthesized data and they've hired a
whole load of people to reflect on what
these cognitive templates are yeah I
heard that speculation too that open AI
hired a bunch of people to solve a bunch
of problems and type out what they were
thinking when they solved it so that
they would have training data about what
people think which okay yeah I mean that
is how llm seem to work is when you give
them data that's similar to what you
want to do that does help I just have to
keep pointing out that the similarity
metric is quite generous right like oh
this guy thought through this problem so
therefore the AI is going to learn how
to Think Through problems okay it's just
the radius of that similarity Circle
right all the different points that
you're going to consider similar to this
other point it's a pretty big circle
it's a pretty big circle right it's
carving out a pretty large region of a
space that's supposed to be this huge
exponentially size space and suddenly
the space of things that are similar to
this template is somehow some huge
fraction of this huge space I just want
to point that out but yeah let's keep
listening to How gbt 40 works the
interesting thing is this is all in one
model now it gets a bit more complicated
than that so the model has a mode a lot
of people my original intuition was that
there was more than one model there was
like an adversarial Chain of Thought
model which was injecting prompts and
doing this big SE I don't think it's
it's that it's really simple so it goes
into a thinking mode it starts
generating these chains of thought and
they hide the Chain of Thought but it's
all going into the context for this one
model and the UI is quite interesting
because it says I'm thinking about this
I'm thinking about this I'm checking the
policies like I'm checking this I'm
refactoring the answer I'm doing this
and then it just gives you the result
and I think that's how it works that
sounds like a very plausible description
to me that it's all one model so
basically if you're training an old
school model like GPT 4 you just train
it on like hey you gave the answer I
just wanted the answer great but in the
case of 01 it's like you're training it
to always output answers that show your
work so if somebody says Hey should we
outlaw guns and you're just like yes
that's a good policy because of ABC
that's not going to get a good score for
0 one's training process because they'd
prefer an answer that's like should guns
be out Outlaw here are 10 considerations
we should think about 1 2 3 4 6 7 8 9 10
okay let's weigh this like that blah
blah blah and therefore the answer is
this so they basically train a model to
Output chains of thought like that and
then in the interface they hide the
Chain of Thought and they they separate
it out and they don't even let the user
see the details of it uh so I think
that's what Tim and Keith are saying
that that's how 01 was trained and that
sounds very plausible to me like we've
basically never brought to bear the full
power of fine tuning an rhf on good
trains of thought Chain of Thought was
always something that we would do at
prompting time once we had a model that
was fine-tuned on you know something
other than Chain of Thought So this all
makes sense to me like this is a pretty
satisfying explanation for how they got
this incremental Improvement of 01 now I
personally you know I'm not an expert
and I don't even know what they did
right but I I'm I'm on the same page as
this plausible speculation like that's
my guess
too well I mean we don't know because
because
you know they don't exact open AI isn't
exactly that open about uh you know what
they're doing behind the scenes and how
how things function um so so I don't
know I'm going by like you know blogs
and whatever else they've they've kind
of published published on the site um so
I think it's something along those lines
well I think it's good that AI companies
are keeping things closed Source because
as I occasionally say the only thing
that's keeping us alive right now
literally the only thing that's going to
make tomorrow a living day for you and
me is that humans haven't figured out
these last conceptual insights standing
between us and super intelligence and
maybe open AI has one or two more than
everybody else and if they were to make
those public that would definitely burn
down the most precious remaining
resource that's keeping us all alive so
I'm all for cl Source I don't think it's
going to help us that much but it's
better than nothing so I'm glad that we
have to speculate on how 01 works right
maybe that'll buy us another couple
years of slow people down um and I mean
it's kind of like of course it's more
the same you and I have talked about
this so much on the show this idea that
really you know neural networks are
doing this kind of um interpolation the
sort of locality you know locality
sensitive um lookup tables lookup tables
and this guy's hitting all the slurs
statistical pattern matching he hasn't
said stochastic parot I feel like that
term labels you as ignorant now so he
doesn't say that but he's definitely
saying all the other slurs the important
thing that's happening in front of your
eyes that you're unable or unwilling to
characterize is unprecedented
optimization power asking they I to
write an essay about a topic that
nobody's written an essay before about
that topic nobody used to be able to
generate essays on that topic now
suddenly we can generate thoughtful
essays on that topic making connections
to The Prompt making connections between
between paragraphs what some of us like
to call reasoning this important thing
that's happening before your eyes and
you're just labeling it with a slur of
statistical
interpolation and not acknowledging what
a breakthrough it is I'm sure if you ask
him to say something positive about the
AI he probably could but I'm curious to
know what kind of language he would use
that allows him to complement something
that's just statistical interpolation
and explain how much power it's wielding
in terms of practical applications that
we never were able to unlock before
right he'd probably say something like
oh yeah it's great how we found the
statistical properties of essays right
it's just like word salad at that point
there's a disconnect between new
capabilities that you yourself would not
have predicted or about to be unlocked
right you were not going around in 2020
or 2018 or whatever saying man if only
we had better statistical interpolation
then we could have an AI pass the Turing
test right you would not have predicted
that I'm pretty sure right that's that's
my guess and so now you're just using
these slurs you're just retroactively
dismissing everything I don't think
you've earned the right to use that
language unless you're going out on a
limb making a prediction of what the AI
can't do a year from now otherwise it's
too easy to fool yourself always
retroactively saying that everything is
just statistical interpolation when it's
not um and then and they've just gotten
so massive now they have these massive
tables of of of um lookups that they can
do I'm just going to keep repeating this
we're not using lookup tables to
navigate a
combinatorially large exponentially
large search bace if I write a prompt
and the prompt has 20 words the number
of prompts is something like 100,000 you
know the number of words to the power of
20 which is more than the number of
atoms in the universe so if you have a
couple trillion parameters in gbt 4 that
doesn't let you navigate a Google worth
of possible 20-word prompts it just
doesn't it's a much smaller number it's
like infinite decimal right so when
you're out here saying like oh my God
that lookup table is so big it's in the
trillions but there's way more than that
in terms of just 20w prompts right not
to mention like a longer prompt not to
mention like a million word context
window where I can feed it like an
entire book as the prompt right so you
guys are just not bringing the intuition
of how you use intelligence to navigate
an exponentially sized space of possible
inputs and of possible outputs you don't
get it you're dismissing this idea of
like oh yeah it has two trillion
parameters it's pretty much just looking
up the answer in a table really that's
how you pass the touring test with a
lookip
table and then if at runtime you're kind
of allowing it to do that a few thousand
or hundreds of times or whatever over
some some period of time time and then
kind of selecting you know the best one
so I don't know I don't know why people
get so excited about about this kind of
stuff in the long term folks this is not
the path to
hii yeah come on guys all you did was
increase your measured IQ by 10 or 20
points but you did it by using a
thousand trial and error attempts so
that doesn't count come on a thousand
that's cheating man sure maybe the next
thing you'll do is you'll put human
scientists out of business but that
might take you 10,000 different attempts
and you have to pick the best one out of
10,000 so that doesn't count that's
easy sorry I'm being mean but I do want
to just highlight the discrepancy
between numbers like 100 or 10,000 like
these tiny numbers of like oh yeah you
know I I threw 10,000 arrows trying to
get toward the piece of gold and one of
them hit compared to the Sahara desert
right these huge huge search spaces that
these AIS are now coming in and now
we're getting criticisms from these
professors saying like yeah cuz you took
10,000 tries of then you pick the best
what of course it's going to get better
at finding gold in the Sahara when you
train it 10,000 times come on give me a
break it's like this is a freaking
Miracle how close it's getting to the
gold like we are circling around the
gold the gold bar right like we're
getting so scary close and
quantitatively you can see that when you
simply contrast the size of the space of
possible answers with the amount of
tries that we have to narrow down by
like can't you see most of the
improbability barrier appears to have
already been crossed and now we're
closing in on the last few orders of
magnitude on the last few conceptual
insights like whatever it is we seem to
be in the end game it doesn't seem like
there's that much work ahead of us when
you have that quantitative perspective
when you can see the orders of magnitude
when you can see you know the the
bullseye in the Sahara right like the
small Target the distance of the misses
within the Sahara when you have that
perspective then you're not as
dismissive you're not you're not willing
to oh yeah this is not the path of AGI
because from an outside Observer just
looking at the improbability barrier
that's behind us compared to the
improbability barrier that's in front of
us it looks to mostly be behind us
meaning AGI looks to be near which may
explain why literally prediction markets
are saying that it's near like maybe
they're not just making that up maybe
they actually have the right intuitions
to predict that first of all you were
talking about the the supervision so
when you talk with the language model
you know you're generating some code
but that can be self-supervision or like
in other words you can you can set up a
system where you just ask a large
language model for some code and you've
decided ahead of time you're going to
feed it back to itself five times each
time telling it it's buggy try to
improve it right so it's it's there
doesn't necessarily have to be any human
supervision it can be it can be a kind
of iterative self-supervised finite
thing right oh we can but it it it my
point is that that never works well so
the reason why language models work so
well for generating code is it is a a
didactic exchange of knowledge Discovery
right so I tell it to do something it
gets it usually it gets it wrong and I
say no I actually wanted this and then
it might come back with something
interesting and I'll say no you just
accidentally deleted a load of my code
but I quite like this thing um why don't
you keep that thing but go back to that
thing the reason why having this tight
supervision process is important is
because the model will diverge so having
a reasoning system that actually goes
many many steps and what about this and
what about this and what about this it's
actually not that useful right because
it's going to be wasting loads of Cycles
which you're paying for by the way doing
things that you didn't even want it to
do in the first
place um I so I I I can't go that far I
agree with you that it's very
inefficient but I think if we if we
really knew how to
train um neural networks train them
knowing that they're going to run for
some unbounded amount of time and kind
of looking for a stopping condition once
we get to that point I think the
training process can can kind of grind
away some of that that
efficiency I agree with Keith the
interesting question to ask is just do
we have a process where an AI can
generate a bunch of these candidate
answers like candidate code suggestions
to improve your code and then run some
other process where it can evaluate
itself and be like okay you generated
100 things this one is the best this is
a good suggestion let's keep it because
if we had that it's pretty easy to be
like okay now let's train on the output
of that right so it's not about
optimizing it and fine-tuning on it it's
just about can we even do it at all if
we had just a ton of resources we don't
care if your CPU gets really hot just
can the AI generate candidates and then
find a good one because if it could then
it now has a series of predictable steps
that it can do to be more intelligent
right to raise its IQ and that's kind of
what 01 has done right it's raised its
IQ because to some degree it's able to
think and then the end of the thought
process ends up selecting something
better right so we have kind of made
progress on that front but I think
that's where we're bottlenecked right
like that's the interesting bottleneck
right now is that even 01 is still not
perfectly robust even 01 is capable of
thinking for 200 seconds and then
essentially giving up or like you know
it went down the rabbit hole so it
wasn't fully robust but the level of
robustness from what I'm seeing from
what I'm reading about people's
experiments with it and benchmarks the
level of robustness keeps increasing and
we don't know if there's some kind of
threshold where it's like oh wow it's
really robust it's as robust as a human
it can look back on itself with the same
degree of robustness that humans look
back on themselves and judge whether
they've made a mistake and now whenever
it Narrows things down to like a 100 or
a thousand options it's really good at
picking the best one and not going down
a rabbit hole you know a rabbit hole is
when it picks like a bad option and then
it builds on that because it tries to
stay consistent with all the bad options
that it's picked so it's now like too
far gone it's never going to get back on
track so it's just like kind of useless
as a robust human coworker so that's the
interesting question for me is can AIS
pick out which of the 100 candidate
Solutions they've generated is the right
one on a problem that doesn't have an
automatically verifiable right answer
like writing code usually a code base is
so big and the functionality is so
complex that you can't just easily prove
aha I've written the correct block of
code this one satisfies the proof of
correctness no cuz you have to like run
it in production and you have to see if
that's like the thing that's the right
user Behavior which is like too
complicated to fully specify formally so
I think that's what's bottlenecking the
feedback loop right now right like we
can't have a perfect fine tun because we
can't generate synthetic data of what
was the ideal code in every code
environment but I think that's what open
AI was probably hiring those people to
come and do exercises and come generate
samples of the kind of internal
monologue that they're doing in order to
make these kind of judgments right like
that's how open AI is presumably trying
to solve the problem of like reflecting
on yourself and becoming more robust I'm
just speculating but to me the
interesting question right now is now
that we're in Striking Distance of
having these correct answers now that
the AI is narrowing the Sahara Desert
into like a thousand candidate places to
strike and we just have to pick one out
of the Thousand that's all we have to do
now that we're there what's the next
step like how do we achieve that kind of
picking and it seems like opening ey's
answer right now is like well we'll F
tune on like a programmer's internal
monologue like how they think through
things and that seems to be making
progress right like maybe that is kind
of the path forward maybe you need to
mix in some other new insights right but
it just it seems like that's now the
problem that's where the battlefield is
which is what I would describe as the
end game that's what I call it I feel
like the opening game has just been to
get this far to narrow down the Sahara
down to a relatively small number of
candidates that make for good next steps
in the agents action chain you know
that's that's what I feel like and I
could be way off base right I mean for
all I know there's going to be another
50-year AI winter right I don't know for
sure but that's my sense of where things
are I mean a lot of this is is sort of
like this this philosophical debate
right which is if you're you know if
you're trying to um walk across town and
get from from the bar where you were
just you know drinking too much you know
back home okay if 10,000 drunks perform
a drunkard's random walk um some of them
will make it
home okay you know most of them won't
would you say that the ones who made it
home were reasoning and they you know
were reasoning their path home like no I
mean it's just by chance some of them
made there so anytime where you've set
up a problem um wherein it's very easy
to check if you have the the right
solution of course you can randomly
generate Solutions and if you generate
enough of them and you can check if it's
the right one you eventually you'll find
a right one like that's that's just not
what we're talking about when people are
talking about you know
reasoning without using modern AI if we
go back to 2018 for example there's no
piece of code we can write of the form
oh yeah just generate a th candidate
Solutions and then evaluate which one's
correct it doesn't work like that
there's no code you can write saying
like oh yeah just give me a grammatical
English sentence that responds to this
other grammatical English sentence we
couldn't do it we couldn't have a
natural language conversation for God's
sake right so something else is going on
here to go back to your analogy of the
drunkard's Walk yeah if it's just a few
turns to get home and you've got a
thousand drunkards taking random turns
one of them is going to get home no
doubt but if you have to make a hundred
turns and it's kind of a weird branching
structure where you can end up at two to
the power of 100 different end points
which is like a tril trillion trillion
different end points and you've got
10,000 drunkards navigating that map
none of the 10,000 drunkards are going
to get home none of them are going to
find home when there's a trillion
trillion trillion possibilities right
when a when 10,000 drunkards are all
shooting their bow and arrow in the
Sahara none of them are going to find
the one piece of gold that's sitting in
the Sahara it's all about the orders of
magnitude involved these professors
clearly just have the wrong intuition
for how many orders of magnitude are in
the rear view mirror with these kinds of
AIS the fact that these kind of AIS
teleport you from these huge search
spaces down to a few plausible
candidates that is the miracle that's
what we're achieving right now that's
what's bringing us to Striking Distance
of AGI and they just seem unaware of
this counting argument right it's like a
simple order of magnitude perspective
type of argument that they're not
acknowledging in fact they're explicitly
acting like it doesn't exist when they
think 10,000 drunkards taking wild
random gu
is enough to pull off the same Miracle
of narrowing the Sahara down to 10,000
candidate end
points to be fair there are situations
where the claim they're making is
correct if there's a problem that you
can as they say efficiently check the
answer to and the space of answers is
limited then yes by definition or by
simple logical implication you just
enumerate all the possible solutions and
then you check every single one and then
you go with the one that checked out
right I mean yes that that's true that's
trivially true but you're just
describing something that could have
been done decades ago right these are
not the interesting problems that llms
have helped to solve the only thing
that's magical about llms is
specifically that fact that we can point
them at problems with huge output spaces
and or outputs that aren't easy to
rigorously check that's what we're
talking about here that's what's
intelligent about
them and so then neural networks take
you a step kind of closer right which is
because well we can do better than a
random walk we can do a whole bunch of
training and so then the kind of
sampling the generation is much better
than random you think you know it's it's
heavily skewed towards a population
where one out of 10,000 is the correct
solution instead of one out of 100
million you know and then is it
reasoning holy sh it's not 10,000 versus
100 million it's 10,000 versus a Google
it's 10,000 versus more atoms than they
are in the universe that many orders of
magnitude change that's the magic that's
happening it's not just a little
optimization it's a GameChanger it's why
this only works in the last few years
it's a new breakthrough it's not a
lookup table it's not statistics look at
the numbers look at the orders of
magnitude statistics don't help you take
a search space with a Google points in
it and narrow it down to a th that's not
statistics there aren't enough data
points in that big of a space to to do
statistics on it's intelligent search
it's understanding it's abstraction it's
reasoning it's everything humans do it's
simple grounding there's stuff happening
man you can't just dismiss the stuff
that's happening when the proof is in
narrowing down a search space from
something like 10 to the 100 right these
exponential numbers these astronomical
numbers large astronomical numbers
literally more than number of atoms in
the universe down to a thousand right
that's quantitative proof that something
is being optimized before your eyes you
got to see the proof you're proving your
lack of appreciation for that truth when
you compare 1,000 to 100 million you're
making it sound like an optimization
when really it's a fundamental
breakthrough it's the signature of true
intelligence you know you're interacting
with a true intelligence when you have
this exponentially size search space
that you yourself are kind of hopeless
to navigate through to find a great
solution and then in the blink of an eye
the superior intelligence presents you
with a Sol solution or presents you with
a mere 1,000 candidate Solutions out of
the search base that to you was just
hopeless right to me trying to write an
essay answering about an obscure
scientific Topic in the year 2018 before
we had GPT I couldn't do it but then L
and behold GPT comes to me and gives me
a thousand candidate essays and a bunch
of sources some of which are real some
of which are hallucinated and I just
have to do a step where I go check its
work and I pick the one out of 1,000 and
then I turn it in and then I get my PhD
right I can pass the PHD exit exam
thanks to the help of GPT that's not
going from 100 million to going from a
thousand it's going from impossibly huge
to a thousand okay major
distinction this gets to to the next
point which is that people say well
humans don't reason either and and I'm
I'm amenable to this view right because
I I I think that all intelligence is
collective now of course our brain is a
collect Ive our bodies are a collective
you know we're made up of all of these
little mitochondria I was listening to
Richard Dawkins on Sam Harris's podcast
the other day apparently they they they
come from bacteria mitochondria is
basically a bacteria that's one Theory
Yeah Yeah well yeah okay but you know we
we we're just made up of of all of these
little you know autonomous things that
have their own agendas but you know the
the bacterium actually want to pass
their their genetic material onto future
Generations through the female line
actually not the M line Dawkins was
saying but um but but but anyway you
know it also happens outside of the
brain and the body the way that we
discover knowledge and we do reasoning a
lot of it is just Serendipity and chance
and because we have this amazing power
of mtic information sharing trying not
to use big words here basically we
discover something interesting we share
it with the collective and that
information survives after I die so as a
collective we are accumulating more and
more and more knowledge and it survives
after we die it's a beautiful thing
logic cop here everything is a
collective okay well the brain is a
collective of neurons every cell in the
brain is a collective of genes so you've
already got levels of being a collective
oh and every cell has mitochondria okay
great everything's a collective who
cares the interesting observation is
that a single human's brain is able to
map inputs to outputs in a way that no
other objects can not even AI today so
when the topic of conversation is do llm
reason what are llms like gp1 what are
they still missing and you say hey
intelligence is a collective man it's
just it's really not helping the
argument if you really think that it has
to be a collective then you need to be
specific about an input output that can
only be done collectively otherwise
you're just hand
waving I I don't have any problem with
the idea
that um that a robot can one day reason
okay that for that matter like I don't
have any problem that a computer program
like just you know can can perform
reasoning I don't have any problems with
this idea it can be 100% deterministic a
computer it can be in silico perform
reasoning I also don't have a problem
whatsoever as you know with the idea
that I'm a machine I am I'm I'm a biom
machine okay I'm a bio machine I consist
of a um wetwear and software and you
know whatever else like but I can still
perform reasoning so I'm not saying
machines can't reason what I'm saying is
the the types of neural networks that we
can train
today um are not doing the kind of
reasoning the effective computation kind
of reasoning that we need for general
intelligence okay for to solve problems
in general in general problems require
turning machines okay like they they
cannot be just finite State automat or
you know you know fsas with another
finite amount of computer or whatever
and come on people you all know that
you're reasoning you know if you're
watching the show you're probably at
least the kind of person who spends some
of your life thinking deeply over time
about a problem applying rules and heris
STS that's reasoning that's what we're
talking about right is this unfolding
process yeah this is basically Keith's
Central claim that he's been making
throughout the podcast he's basically
saying AIS can't reason because if you
want to reason you need an architecture
that's Turing complete but a feed
forward neural network like what we have
in the llms that's just a finite number
of steps right like you can't factor a
number when you just ask an llm to
immediately tell you what the factors
are without using python airG go llms
can't reason right getting back to that
argument and he's saying look I get that
machines can reason but I need to see
something that has loops and
branching okay it's the obvious response
is let's see what comes out of the token
stream the same thing as what comes out
of your stream of Consciousness right he
hasn't been very clear about where the
analogy breaks I guess he did discuss
that in a previous section when he was
discussing a Divergence so he basically
just doesn't trust that llms can have a
token stream that stays robust stays on
the problem and just Narrows down you
know the last thousand solutions to one
solution or whatever it is right like he
just doesn't trust that they can solve
problems in general and I think he is
correct about today's llms right it's
just not a fundamental distinction of
reasoning versus not reasoning right
it's totally a matter of degree but yeah
if his whole claim is that llms can
reason but not as well as humans I think
that's still largely true for today's
llms and for today's smarter crop of
humans but I don't get why he's not
making a Apples to Apples comparison of
llms outputting a large number of tokens
where some of those tokens are allowed
to go back and say hey edit my previous
tokens like why isn't he comparing that
model of computation to the model of
computation that is a human because if
he were to do that then he would realize
that both of these models of computation
are Turing Universal so the distinction
he's making about oh an llm is just a
finite State automaton which is a more
primitive model of computation than a
computer because it has finite memory a
fixed amount of memory like a fixed
amount of layers in the network he keeps
saying look this is a finite State
automaton I a human brain I'm Turing
complete but it's just so easy to turn
it from a finite SATA tomod into a
turing complete computer when you just
let it have a stream of thought and the
stream of thought can be arbitrarily
long right like when you use chat GPT it
just has a hard-coded restriction like
hey you're not supposed to Output more
than this number of tokens if you take
out the hard-coded restriction and you
just let it say it gets to correct
itself it gets to tell you to go back
and change some of the previous tokens
that it says to be other tokens in order
to fix its answer once you make those
two simple tweaks suddenly you have a
model of computation that's turning
complete so at best he's just
oversimplifying things when he's saying
we need a turn complete model of
computation we totally do have it so I
don't think that that's his real point
or if it is it's just like not a good
point right I think his real point is
more like hey it's not working well it's
not robust right but I do think that
there is a major problem here of him not
analyzing the situation well also
remember he's doing this podcast because
gp1 just came out and gp1 does have
better performance on a bunch of
benchmarks how did he say that was
possible he said because gp01 knows how
to pattern off of some other reasoning
that it saw somewhere and use that to
get better performance on these
benchmarks okay fair enough let's even
Grant let's say hypothetically he's
right it's just patterning it's just
lookup tables it's just statistics let's
say that he's right but the
computational power of being able to
pattern off of other templates to an
arbitrarily long degree the way gp1 can
do it that power is not a finite State
automaton right it's a turn complete
model of computation so try to get your
story straight is it a finite State
automaton is it just pattern matching
statistical similarity like you can't be
both at the same time right you have to
pick where your line really is what your
beef really is but of course the the
true answer is that there is no line
like these things really are break
taking down every barrier that we can
think of on a less than yearly basis
right like the the the alarms are
sounding here if you have eyes to see a
lot of important stuff is happening on a
fundamental level it's not as simple as
finite State at Tom up statistics it's
really really not that simple and the
only way that you can get a clear
picture of what's really happening of
how significant the progress really is
is by setting your goal posts on the
level of input output that's the most
subjective way that you can see
Milestones that are being surpassed is
if you say I'm impressed with this input
output I'm impressed when it can solve
this problem and then you'll see yep
more and more problems keep getting
solved more and more benchmarks keep
getting slayed then you'll see what's
really happening so I encourage them to
try to be objective to try to make
predictions to try to Define benchmarks
instead of just retroactively saying oh
yeah that was just statistics that was
just pattern matching because their
excuse of just statistics just pattern
matching at this rate is going to work
all the way until super intelligence
when they literally get killed by super
intelligent AI until a day before then
they'll be they'll just be saying oh
yeah that wasn't that great that wasn't
that great right so they they have to
stop the slippery slope by making a
prediction or defining a benchmark
that'll convince
them for instance the arc Benchmark
right the arc Benchmark is designed to
be one way to define true reasoning in
the simplest possible way and prediction
markets are saying that the arc
Benchmark is going to be surpassed in
the next one or maybe two years that's
what the prediction markets are
currently betting so do Tim and Keith
want to sign on to the arc Benchmark
saying oh hey it can TR truly re it now
because it passed the arc Benchmark I
suspect that they won't I suspect that
their Position will be to not predict
anything and then when the arc Benchmark
gets passed to be like okay yeah I just
got passed but it just did it using
templates and pattern matching it's
still not true reasoning right I feel
like that's their MO but they can
pleasantly surprise me if they lay down
a concrete prediction of something that
can happen in the next 2 years pla
that'll change their minds and be like
oh yeah hey cool it's true reasoning
okay we were wrong it turns out that the
llms we have now were actually getting
close they were just a small ingredient
away from doing true reasoning right
that would be great if they said that
okay now Tim pleasantly surprises me and
challenges Keith with basically the
question that I want to ask you said
before that the reductio ad absurdum is
we memorize everything and that's not
reasoning right and then on on on the
other Spectrum we do pure reasoning so
we build something that can solve the
arc challenge that can you know from a
base set of knowledge it can
automatically compos together through
meta learning or knowledge acquisition a
new model to solve every single problem
efficiently because challe says that the
efficiency of of the reasoning process
is is intelligence and then we've got
all of this gray area between the two
poles right so the gray area is now I've
got these reasoning trajectories um and
I've you know do rhf from my language
model and the the language model has
intuition has creative intuition so oh
this is an interesting problem um maybe
I should use this um reasoning Motif
maybe I should use this reasoning Motif
and and it's internally doing a tree
search right and it's it's a valuating
all of these different paths and it's
it's finding those motifs through
something akin to a locality sensitive
hashing table lookup right so it's kind
of like pulling these things from its
memory and if it works
why is that not reasoning how is that
any different my answer is to the degree
that you can take on hard inputs hard
input output maps with some reliable
procedure that you're doing to whatever
degree you can achieve that you must be
reasoning whatever you're doing should
qualify as reasoning if it's a set of
predictable repeatable algorithmic steps
that get you from the input to the
output that's my definition that's how I
would semantically carve things out
why is that why do I have such a
different approach than these guys well
I have a rationalist eyes view of the
situation I've been trained by the less
wrong sequences by Alazar owski how to
navigate these semantic Battles by
keeping in mind what actually matters in
the world remember what I said before
the semantic slap fight that we're
having over the definition of reasoning
is ultimately not important compared to
the real question of optimization power
how far long are these AIS on a spectrum
of optimizing over a broader domain like
the whole universe and deeper
optimization like more powerful
optimization superhuman level
optimization on the same kind of
problems that normally the humans are
best at right so that's the key question
for me that's the question that's going
to determine whether we're doomed or not
reasoning versus not reasoning that
semantic slap fight is more like a side
quest of just whether you have good
mental models to answer the important
question of whether we're going to be
doomed by superhuman optimizers just to
emphasize that point
consider a scenario where you just have
a really easy input output like if the
game is tic-tac-toe I input the current
state of a tic TCT toe board you output
the optimal next move it's such an easy
input output like the total number of
tic tac toe configurations is
it doesn't matter it doesn't matter how
reasoning is your reasoning was in that
particular example the nature of the
problem the fact that it's such an easy
optimization problem with only
255,000 possible inputs it makes it so
that it doesn't matter whether you're
using a lookup table or not because the
fact that you could use a lookup table
and the lookup table would fit in a
pretty small amount of memory just that
fact makes it not even interesting
question of whether you're reasoning or
not at the end of the day the only
interesting question is optimization how
big of a search problem can I give you
and expect you to still somehow find a
really good solution within that space
right like a search space that's larger
than the number of atoms in the universe
and yet you're finding really good
nuggets of gold within that search Bas
that's the dangerous thing that's the
power that's the source of power that
lets humans put the other animals in
cages instead of Vice Versa that's the
source of power that makes us
potentially end up in the cage pretty
soon it's optimization power so this
whole discussion of reasoning only
matters in so far as it lets us measure
progress on the optimization power
Dimension I think it's kind of funny
that they're confusing themselves just
talking about what counts as true
reasoning without having this
perspective of why we're asking the
question and why it matters and what are
the stakes in this semantic game I think
it would improve their quality of
discourse if they had this kind of
concrete external motivation for their
game of semantics
and by the way if you read elaz owski if
you read the less wrong sequences this
is actually one of the lessons that he's
explicitly written down this idea that
like yes we can play semantics there's
ways to have good definitions and bad
definitions But ultimately you want to
replace the question of semantics with
the more important question of like what
are you trying to do how do you want
your brain to have power over the world
and from there that'll give you good
perspective on your game of semantics
well I mean it's again maybe maybe we
need like a a level a level of of
reasoning it's performing a very
shallow a very shallow form right like
it's doing this kind of lookup in its
library of templates and applying the
closest matches and then you know
selecting uh selecting the winner or
whatever the one that the one that
scores the best okay that's so it has a
very very shallow depth let's say in in
kind of computation time what Keith
seems to be doing here is he doesn't
think that what current llms can do is
that impressive like ah they can't even
factor a number in the first token and
so he wants to reason backwards to why
their internal operations shouldn't
count as reasoning because he's not
impressed by them so they must not be
reasoning they must be lacking some
Essence that humans can do and their
architecture can't do yet and we need a
different architect
he really wants to make that claim so
he's kind of ignoring the power of their
input output he's ignoring the fact that
they're navigating these exponential
spaces in a way that statistics can do
and he's kind of fitting everything to
this claim he wants to make of like this
isn't reasoning yet and it's probably
because uh the model of computation is
not powerful enough which is like I said
it's not even accurate unless you
intentionally go narrow to like okay the
first token if you're not right if
you're not allowed to edit your tokens
then it's a finite automaton like he he
seems to be straining to come up with an
explanation of why it's not reasoning
which potentially could work but
remember the problem with it is that
he's about to get put into a position
where the next version of the llm if he
decides to make a prediction of what it
can do is very likely going to break
through his position to contradict his
prediction and then he'll be in an
awkward position where he has to explain
how is this non- Reasoner delivering
better and better results that's
actually what Tim asked him to explain
he says imagine a scenario where all of
the things that are going on under the
hood that you're trying to dismiss as
non- reasoning imagine that those things
keep delivering better and better
performance what would you say would you
then Grant the RW at that point would
you apply the label and he's like no
it's got a shallow still right he's
getting put in an awkward position at
that point so it has a very very shallow
depth let's say in in kind of
computation time it has a very wide like
a lot of this maybe one way to think
about this that that may be a little bit
better um is in the good old SpaceTime
sort of tradeoff right like I talked
about earlier with with circuitry so you
have you have a computational problem
and you're trying to solve it well you
can solve it with a small circuit that
runs over some let's say linear amount
of time okay or you can solve it with an
exponentially large circuit that that
solves it in one
step right now what I'm saying is um we
call the former reasoning so we when
it's something that happens over some
like linear amount of amount of time
let's say or kind of like unfolds over
time we call that reasoning whereas we
don't typically call the exponentially
blown out circuit that does it in
one-step you know reasoning again coming
in with this incorrect mental model that
an llm is like an exponentially blown
out circuit he's thinking oh we're here
in the Sahara yeah of course this
algorithm can find me a little piece of
gold inside the Sahara because it's a
map and the size of the map is half the
size of the Sahara so of course I can
look up where the gold is on the map but
I'm like Keith your map fits in your
hand it's actually what you call a blown
up map it unfolds and it still fits in
your hand the map is not the size of the
Sahara so this idea that it's a time
space trade-off and it's just a big
lookup table because it's 2 trillion
parameters relative to a Google size
search space it doesn't work like that
something else is going
on if you go the route of building
exponentially large circuits that can do
things in one step the problem is that
you have to know ahead of time the the
range of the inputs right because I have
to build the circuit large enough that
it can handle all possible inputs that I
will see in the future and in general
you don't know that like I don't know
that okay whereas if you go the route of
building the reasoning C circuits the
things that can perform this this
computation over time we do have the
luxury of things taking longer than we
expected like time as it happens is
potentially infinite it so far it keeps
unfolding you know we always get another
minute another minute another minute and
that's the biggest difference right is
either you have to know ahead of time
all the input sizes you're ever going to
see um or if you come across an input
that oh crap it's too big well let's go
back and retrain it let's create GPT 76
you know because we we hit an input that
was too big now we got to spend um you
know uh5 trillion dollar to um retrain
it and then we can go solve this problem
well instead if you'd have just spent
aund million building a machine that
could you know do iteration for some
unknown amount of time you wouldn't need
to go back and do that again you just
would maybe go buy some more tapes so it
has like a very practical important
difference right so there is a very
important mental model from complexity
theory that Keith is talking about right
the time space tradeoff where often
times you can take an algorithm and you
can make it run faster if you're willing
to make like a big cache if you're
willing to use up space strategically
and do stuff with the space that's very
true and conversely if you have an
algorithm that takes up a lot of space
but you're okay taking up a lot of time
instead there's ways to make it use less
space so he's absolutely right that
there are time space trade-offs in
complexity Theory there's a lot of
interesting things to study on that
front but funny enough when it comes to
LMS his mental model is clearly wrong
just imagine the year is 2018 and I say
hey Keith you know neural networks
imagine a deep neural network that also
has this other part called the attention
mechanism imagine that I run that in
finite time per token that it generates
so it is kind of a finite State
automaton per token because we're we not
talking about in context learning and
the idea of using tokens to edit
previous tokens we're not talking any
about that we're just talking about like
Hey we're going to write natural
language essays and we're going to
answer essay questions on tests using a
neural network architecture with some
Secret Sauce thrown in right attention
we're going to do all that and it's
actually not going to have a time space
trade-off because we're going to fit it
on a thumb drive like some of these
compact open source llms today you can
fit it on like a few gigaby thumb drive
it's like a compressed model and even
that model is going to be like GPT 3.5
level it's going to be able to write you
good essays but it's going to operate
quickly predictably like a finite sodon
and it's going to fit on a thumb drive
what do you think of that Keith how does
that navigate the time space trade-off
it just gives you everything it just
outputs essays magically from a thumb
drive running quickly how do you explain
that I think that if I were asking Keith
that in the year 2018 he would probably
think that the trade-offs are more
drastic than what we're really seeing
he'd probably be like well surely you
need like a ton of space or a ton of
time like you can't just have both of
those things and write an essay because
think about how many different possible
essays they are and they all have to be
grammatical and they all have to connect
ideas together imagine the time space
tradeoff on that but we already know
through you know we already have the
advantage of retrospect where we can see
like oh nope the thumb drives just
somehow accomplished these amazing
essays and again we live in a time now
2024 where you can just dismiss like oh
yeah writing an essay that's just
pattern matching that's just like a type
of fuzzy lookup table that's nothing but
in 2018 his eyes would be wide at the
suggestion that you could have something
running in a 01 amount of time on a
thumb drive or you know o of n
proportional to how many output tokens
you want his eyes would be wide like how
is that possible how does that comport
with the AXS of computability theory
right he would be confused but I'm just
pointing out that the way he's trying to
shoehorn that time space model now in
2024 is equally
misguided now Tim is going to try yet
again to poke at Keith's reasoning
versus non- reasoning distinction and
try to get him to be like come on don't
you think the llms under this
circumstance are doing reasoning and
Keith gives one of the clearest answers
so far which I think is misguided but
it's pretty clear let's listen a model
is just it's just a high fidelity
representation of the world that allows
you to perform powerful inferences about
the world because the model is correct
in in some sense if you actually compose
the models correctly and you have uh you
know um an efficient accurate inference
about the state of the world then you've
done
reasoning I'm fine with that as long as
people understand the core point that
that is a shadow a shadow of the kind of
reasoning that your machine up here can
do because your machine up here can do
this process for a very very long time
okay you can be Andrew WS in your atct
for however many years it took him to
solve fats Last Theorem and nobody said
up front you're going to be up in the
attic for only three months like it
turned out it was a lot longer than that
right bro you're recording this podcast
right after gp1 came out there's reports
in the news that it's solving crossword
puzzles by thinking for 115 seconds and
solving like a 7x7 cross word better
than any of the other AIS because of
that long Chain of Thought how is that
fundamentally different from what you're
saying about a human like Andrew WS
thinking for many months in order to
solve his proof it's using more
computation steps it's using more tokens
it's using more of its memory tape right
it's using more time and space in other
words it's doing computation like it is
the most advanced computational system
ever built so no surprise that it's
doing computation Beyond just a finite
automaton Beyond just a stochastic
parrot like this is in fact an advanced
computational technology I don't know
why that's so hard to grant that right I
don't know why you won't grant that this
100 second thinking crossword solving AI
didn't too reasoning to solve a freaking
crossword you don't think thinking for
115 seconds about what could be a
solution to a 7x7 crossword puzzle
requires reasoning because it's not
enough like how Andrew WS also thought
for a long time like do you not see the
connection between these two
observations like I'm not saying that
the AI is literally Andrew Wilds today
but where do you think this is going
what do you think the bottleneck is
because when you get asked that question
you point to models of computation you
point to time space trade-offs and those
are clearly not the right distinctions
to point to those do not get the job
done of distinguishing
gp01 with Andrew WS you're getting at
the wrong
distinction what do I think the
distinction is again I don't think
there's a distinction I think it's just
a matter of optimization degree and yes
I do think there's other architectural
pieces that are going to be Unearthed
that you can mix into the current llm
architecture I don't think we've
discovered every key piece I think we're
going to have much more efficient
smarter better architectures but I still
think that we're close I still think
that the architecture we have is tearing
down almost any kind of input output
that we needed to take a crack at like I
think think we are breaking down the
last walls separating current technology
from Super
intelligence the last part of Tim and
Keith's podcast that I'm going to review
is this metaphor that an ai's knowledge
is like a bunch of slices of Swiss
cheese where yeah it covers a lot of
territory but there's still holes I do
think that metaphor is somewhat apt to
describe how AI is so much better today
when you're asking it things that are
similar to what it's done before for
like I definitely think there's a big
kernel of truth to this idea that okay
it's just layers of Swiss cheese and so
Keith is asking the question okay well
is it going to be swiss cheese forever
how big are these holes what is the
topology of the Swiss cheese as the AI
keeps getting bigger and smarter what's
going to happen how much of it is Swiss
cheese let's let's give the Fanboys you
know uh the the uh benefit of the doubt
everything we care about you know it's
going to um solve like IAL problems and
science problems and you know political
problems and like all kinds of things
everything we care about but there's
going to be tons and tons and tons of
holes in there okay because because open
AI or whoever AI just yeah we didn't
quite run into that situation yet and so
we didn't patch the fact that it's
missing a template that uh that covers
that one because we have to remember
these templates they're so like fine
grain and specific like think back to
our episode where where we where you and
I learned right from bisri that it's
creating this Patchwork honeycomb like
you have to think about it as just this
massive honeycomb of all these little
cells where it's learned to kind of fit
everything right and and lots of
situations are going to arise where when
it gets when it gets pinpointed into a
particular cell it's broken so that's
the disadvantage to reasoning based like
reasoning that's
very um spatially in the computational
sense wide so it has like very very very
many many many many many many templates
but they're all like really stupid and
sort of shallow you know and then it
just tries to find the right one and
applies it that's the disadvantage
versus that versus versus a small set of
Highly reliable first principles that
when
iterated give you an answer like one is
much more parsimonious right so like the
what humans develop is a knowledge
Corpus that's highly parsimonious it
contains a small set I mean small using
you know versus a versus a neural
network it's it's there's obviously you
know lots of detail in physics and and
biology and Science and whatever but
what science does is it finds
parsimonious principles um that when
applied in sequence and iteration and
combination um
um reason out you know the answer and
and there's a sense in which that's much
more reliable and doesn't come as a
giant ball of Swiss cheese with all
kinds of holes in it right that we don't
even know they're there like we don't
know the holes there until something
blows up and then like we figure out
there's a hole because nobody bothered
to look I'm willing to accept that
framing that AI today and its weaknesses
do feel like you're uncovering a hole in
the Swiss cheese just because if you had
asked something that is really similar
to something that was in its training
data then it probably would have gotten
it right so to the degree that you've
chosen a prompt that's really unique and
that's why you're confusing it I think
it's fair to apply the metaphor of like
ah now you're in a hole within the Swiss
cheese I think that's fair so it's
totally valid to ask okay how small can
we make the holes will they ever just
get really small and we'll just have
like a solid block of cheese or will
there just always be holes he seems
pretty convinced that there will just
always be holes like it's always going
to be this big honeycomb and that's why
we're never going to get to general
intelligence without a fundamentally new
architecture that's his perspective my
perspective is like I think the holes
are going to get smaller I think that's
the trend we're seeing I mean look at
the trend from gpt2 to gpt3 to gbt 4 if
you go back and use gbt2 today it would
be very appealing to be like wow this is
the biggest swiss cheese I've ever see
and there's so many holes how are these
holes ever going to get filled I
definitely think that as you use gbd3
and for you do get the sense that the
holes are smaller like it's not making
as obvious mistakes between the gaps in
its knowledge that's the sense that I
get I could be wrong right like look he
may be right that there's always going
to be holes no matter what you do
without like a fundamental change in the
architecture but I can give you some
reasons why I don't see it playing out
that way why I see the holes getting
filled pretty
soon number one is by analy to humans
he's treding to draw a distinction where
he's saying well reason parsimoniously
we just understand the world as like
emerging from like simple physical
principles but do we really I mean our
brains are pretty big we have like a 100
billion neurons in there and they're
doing a lot of caching as far as we
understand like when you see something
in the world a lot of times what your
brain is doing is just matching it to
shallow patterns and just trying to
activate cache sub routines because we
don't have that many serial operations
that we can do and we do have a lot of
space so our brain is very much a big
swiss cheese right I mean if you look at
people you know especially people whose
IQ is below 100 a lot of what they're
doing they'll leave it admitted as like
okay this is what we do in the situation
this is the pattern that matches in this
situation so when you talk about
somebody being swiss cheese somebody
thinking in terms of like pattern
matching to the nearest template I think
you're very much describing all of us
humans especially the ones who aren't
brilliant original thinkers right think
about how many humans just go with the
flow just want to fit in just want to do
what they're supposed to just want to do
what they learned right not all of us
are so like inventive and original first
principles thinkers right first
principles thinkers are kind of rare
okay so that's one reason why I think
the Swiss cheese the holes are going to
get filled at least to a human level
degree and Beyond another reason I think
the Swiss cheese is going to be filled
is because I think that when you get
really good at certain domains like when
you get really good at language when you
get really good at strategy games like
there's certain challenges that when you
put your mind to this Challenge and you
just get really good at this one skill
it's such a generalizable skill that it
just kind of covers up the whole block
of Swiss cheese just that one domain
it's like a slice of Swiss cheese that
like turns into a block of Swiss cheese
so it covers the holes just by being
such a general slice you know like Sora
video generation if I get so good at
generating video frames if I really have
the video frame generation skill in full
generality then I can tell you what's
going to happen in a physics simulation
because I can use heris to predict
physics with a high accuracy without
simulating it in detail or I can tell
you what's going to happen in a social
interaction because I can simulate the
social interaction or I can even tell
you what's going to happen with like a a
geometric puzzle right because I can
like somehow use the video generation to
simulate geometry of course the big
question to all this is like how good is
the video generator really which is the
same question we can ask about llms like
how good is it at predicting the next
token really so when we're talking about
slices of Swiss cheese being layered
down the nature of the slice of Swiss
cheese it depends on the algorithmic
implementation of the AI learning this
knowledge but it also depends on the
nature of the knowledge so it's a
question that we haven't conclusively
answered about the nature of these
slices of knowledge which is like how
thick and wholef free does a slice of
knowledge get like does the universe
just have these slices of knowledge that
can just cover up a lot of holes I
suspect the answer is yes that you just
have these slices of knowledge that
inherently tend to cover up holes
another part of the argument I would
make is if you go back to like 1600 and
talk to humans about like how physics
Works what you're going to get is like
swiss cheese right like they can give
you partial answers to different
questions but they can't go super deep
with you right like they can't really
tell you what's going on with stars
they're just like well I think I have a
good approximation for how big the star
is I have good guess as to how far away
the sun is I think it's fire I don't
know what kind of chemical energy is in
that thing right they don't really know
about nuclear energy so their knowledge
about physics is kind of like swiss
cheese right because they didn't have a
textbook that they could open up and
like grab more layers of cheese they
just had you know fragments and of
course that's still true about us today
right like we don't have the theory of
everything so we still kind of have a
Swiss cheese understanding of physics I
think the average person's brain trying
to reason about different things is
going to look like swiss cheese right
everybody has their own fragments of
knowledge you know just as an example
crack like we knew before in 1600 they
they might have had a good estimate as
to how big the sun is how far away the
Sun is but they couldn't tell you that
the sun burns because of nuclear
reactions right that was more of a 20th
century thing so that was a hole in the
Swiss cheese and it would have gotten
answers wrong about that particular area
of knowledge unless it happened to be a
really good Reasoner and draw
connections and invent the concept of
eal MC s and nuclear reactions I mean
those are all Concepts that you don't
even necessarily need an experiment to
invent the experiment is just super
helpful because it kind of points you in
the right direction but if you're super
intelligent you could have just reasoned
your way to that in the first place so
you could Point your finger at Humanity
in the late 1600s when Newton was alive
and you could say look Humanity you guys
are such a Swiss cheese civilization
like you can't even invent Nuclear
Physics even though you still have
enough scraps of evidence that if you
were sufficiently intelligent you'd be
able to put together Nuclear Physics
what's wrong with you guys why is your
knowledge just a bunch of Swiss cheese
so the Swiss Cheese model I agree it's a
fair accusation to make about AIS but I
also think that you have to make it
about humans humans through history
humans today art humans versus dumb
humans where the Dumber humans tend to
have like thinner layers of Swiss trees
with bigger holes it's a good metaphor
but I just don't think it fundamentally
differentiates between AI reasoning and
human reasoning I think it's a Continuum
very much like when we busted out the
concept of reasoning and when Keith
acted like true reasoning was this
dividing line between a and humans I
pointed out no it really just looks like
a Continuum I don't really see a line
and it's the same thing with the Swiss
cheese metaphor I think the Swiss cheese
metaphor is also a Continuum how thin
the slices are how big the holes are and
I agree that humans tend to be on the
smaller hole thicker cheese side of the
Continuum but I also think that the Gap
is closing right so I don't think that
this metaphor is succeeding at drawing a
boundary between AIS and humans that
seems to be the recurring Crux of
disagreement between me and Keith and I
guess Tim Tim seems to be kind of
following Keith's lead on all the stuff
so I'll just say Keith the Crux of
disagreement seems to be that Keith is
convinced that these AI have a long way
to go to unlock true humanlike reasoning
he thinks he's seeing distinctions
distinctions in the computational
complexity trade-offs of Time Versus
space or even a distinction in the model
of computation right finite automaton
versus turing machine which I think is
just really not the case once you start
streaming multiple tokens and letting
the tokens make edits to previous tokens
which is not hard for like an in context
learning Chain of Thought situation you
he sees a big gap between
gb1 coming up with 115 seconds of
thought and solving a crossroad puzzle
compared to Andrew WS proving for ma
Last Theorem because he got to think for
a really long time right like he sees
like a huge boundary between those two
scenarios he sees a huge boundary
between gbd4 answering Knowledge
Questions based on templates and facts
that it's memorized compared to a human
who's answering questions based on stuff
that it knows but like more original
first principles thought right like he's
making all these distinctions which to
me all feel like continuous distinctions
distinctions where if you draw a trend
line from gbt2 to three to four to 01 if
you draw a trend line it just seems like
the trend line is headed toward human
level and Beyond and in many tests it's
already proving to be super human like I
personally would not want to face off
against gpt1 in a math Olympiad at this
point right or I wouldn't want to face
off against deep Minds AIS and various
video games so I see continum he sees
some kind of hard distinction and that
seems to be an irreconcilable Crux
between us I understand why if I had his
position I would be less doomy because
I'd be like well we must have time maybe
we have like a century before AI learn
how to truly reason so we're probably
not doomed in the near future I still
don't know why he wouldn't be a Doomer
in the longer term like I don't know why
he's not a 100-year Doomer probably the
same reason why AI Doom wasn't a
prominent movement before people
realized that timelines were short short
timelines do have a way of focusing on
Doom just ask Jeffrey Hinton and yosua
Benjo who basically explicitly admitted
that now that the timeline is short they
had to rethink their approach to things
they're like oops yep I probably should
have thought of this earlier I just
didn't think I would have the urgency to
do
it all right so that is my analysis of
this video I got to say it turned into a
deeper and more elaborate analysis than
I was expecting there was actually a
good amount of meat on this video I was
pleasantly surprised this was a
substantive video I came away with a
pretty good impression of these two guys
Tim and Keith they actually are clearly
very thoughtful people they get a good
back and forth going they're making
coherent points like they're not bsing
they're not rambling you'd be surprised
how many people are like clueless
Ramblers like that's not them for sure
they're making a ton of sense it's just
they have these cruxes of disagreement
with me that seem pretty major and seems
like they should be able to do a little
bit better like stop making analogies to
computability Theory and time space
trade-offs like it looks like they're
barking up the wrong tree for some of
these mental models that they're using
but besides that object level
disagreement I like them I think they're
putting out quality content for sure and
it makes me want to listen to more
machine learning Street Talk which is
our podcast because I have a higher
opinion of just their quality of thought
despite this like massive disagreement I
thought like I said I thought they're
putting out good content here so I do
recommend checking out this episode I
clipped a lot of it but there are some
parts I didn't especially a big chunk at
the end like the last quarter or the
last third they give gp01 a very
interesting puzzle that got me thinking
for a while uh they give it this puzzle
and I won't spoil it for you but they
see how it does with their puzzle and
it's quite an interesting example of
where the current limits of 01 are like
what the current state-ofthe-art is when
you compare like smart human reasoning
versus state-of-the-art AI reasoning
anyway overall great contribution to the
discourse by Tim and Keith hope I wasn't
too mean to them in my reaction honestly
I think people should listen to both I
think we got a good back and forth going
I would love if they want to come on my
podcast or just give me a response to
this episode you know I appreciate the
engagement also I really appreciate that
their podcast gave me a chance to weigh
in on gp01 because frankly I'm not good
at just like analyzing random New pieces
of AI news that's not really my
specialty if you want somebody who's
good at that the number one person is
going to be TV mosho witz head over to
THV tzv i.w wordpress.com if you want to
Read's he's got a huge post on gp1 that
I read myself today it's great stuff as
usual it's a great Roundup so follow its
V for that kind of content today I
thought maybe there's something I can do
on the 01 news maybe I can find a
podcast of people whose Viewpoint I tend
to strongly disagree with and then by
reacting to their podcast and explaining
why I disagree with them I'll also have
a chance to give you a few of my takes
on gpd1 so I think that worked perfectly
in this case because they gave such a
meaty podcast to react to I think
mission accomplished but let me know in
the comments what do you think what do
you think of the machine learning Street
Talk guys what do you think of the Crux
of disagreement that I have with them
what do you think of
gp01 I want to hear it that's it for
today I'm going to put another plug for
my substack the people who come and read
the bonus content on my substack and get
on my free email list those people are
my true committed fans because that
takes a couple extra steps compared to
just watching on YouTube I really
appreciate it did you know that there is
an exclusive free subscriber chat where
I type some extra stuff that you can
read if you go to my substack bet you
didn't know that got to get on my
substack highly recommend it and like I
said before you know me getting your
email it just lets me email you content
even if for some reason you're not
coming over and watching me on YouTube
right it's like another point of
connection and it's something you can do
to be part of the Doom debates Army the
army of people who are trying to raise
society's temperature
create a fever fearmonger up a fever to
fight impending AI Doom so that we don't
just sleep walk straight into the
whirling razor blades so I'm recruiting
you to go to Doom debates.com type your
email in that popup join the Doom
debates Army see how you can contribute
or just enjoy the bonus content thanks
for watching and I look forward to
seeing you on the next episode of Doom
debates