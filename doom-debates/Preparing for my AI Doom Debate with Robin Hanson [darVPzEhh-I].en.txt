welcome to Doom
debates I'm here on the night before
Robin Hansen is scheduled to come and
debate me on AI Doom in preparation I've
gone over the entire 2008 Hansen owski
fum debate I've gone over every post I
could find where Robin blogs about AI
Doom I've gone over a bunch of tweets
I've gone gone over a bunch of Eleazar
posts I've gone over hours and hours of
conversations that Robin had last year
on YouTube you can find him just search
Robin Hansen 2023 there's like a dozen
conversations that he has with a bunch
of smart people comparing their views on
AI Doom in my opinion the most
enlightening thoughtful debate that
Robin has done is that 2008 one with
alas rudowski I don't think that any
follow-ups to that have been comparable
I would like to think that the one I'm
about to do that you're going to enjoy
on this channel is going to become the
best followup to the 2008 debate the
best round two That's My Hope because I
personally am not satisfied with all the
other follow-ups he's done so we're
going to try to kick it up a notch we're
going to try to get to the Crux of the
disagreement of why he thinks P Doom is
much less than 1% while I think P Doom
is like 50% that's a huge huge
disagreement and of course it hinges on
my own belief that AI is quite close to
getting very smart and then ending the
world within our own lifetime or maybe
our children's Lifetime and he doesn't
think that's the case and he thinks that
whenever AI does get super intelligent
it's just going to be fine it's just
going to grow the same way that the
economy grows not some huge scary
discontinuity so this is a massive
difference in worldview and after 15
years we still haven't been able to
reconcile it right he hasn't been able
to reconcile with alzar and we're at an
impass it's crazy right so let's at
least go over the disagreement let's try
to keep it focused on the difference
between P Dooms and see if we can meet
at 5% 10% right at least come on Robin
you got to give me 1% because he's not
even giving
1% which is mind-blowing in my opinion
but at the same time I respect Robin
Hansen as much as anybody in the world
the guy is brilliant he's a huge
intellectual influence so I'm looking
forward to having that debate tomorrow
in the meantime I have another prep
episode from you if you remember I
already did a debate prep episode a few
days ago it was with David Zoo a fellow
AI Doomer who has an amazing Twitter
really smart guy I encourage you to go
watch that episode if you're interested
in how I'm prepping to debate Robin
Hansen because I took on the role of
Robin Hansen and he debated me and he
was the Doomer and I was the non- Doomer
so I think it was a great time it was a
good example of how to do an ideological
touring test um so go check that out if
you haven't already if you're looking
for a doom debates episode that's really
in the weeds you've come to the right
place because this is just a monologue
this is just my own preparation it's me
going over all the things that I've
written down in my outline over the the
last couple weeks that I'm hoping to ask
Robin Hansen I probably won't get to
everything but it's been helpful for me
to just prepare this and organize my
thoughts and it's been helpful for me to
record this episode and organize my
thoughts as audible speech for you so
the central question that I'm going to
be debating with Robin Hansen is Will
uncontrollable AGI make us go extinct
will it do that with high probability
High P Doom of like 50% or will it do
that with very low probability of P less
than 1% that's what we're going to be
debating
so now without further Ado let me launch
into my list of topics and questions
that I'm planning to ask Robin Hansen in
our debate for my opening statement my
position is that in the near future
there will be an uncontrollable super
intelligent AI whose values don't align
with Humanity's values and it leads to
human extinction in our lifetime or our
children's lifetime my argument breaks
into two parts part one super
intelligent AI is coming soon part two
we aren't close to knowing how to
control super intelligent AI so when it
comes it's going to be unaligned and
uncontrollable simple as that it's quite
a simple position so I'll be looking
forward to drilling down and seeing
where the Crux of disagreement is and
then I'll let Robin say his opening
statement I'm not sure I know his
position exactly but I did my best
attempt to simulate it if you watch the
last episode where I debated David Zoo
as Robin Hansen I took the ideological
tring test so definitely check that out
if you're interested in the robin Hansen
debate prep okay so that's opening
statements then the first thing I want
to do is reject Robin's framing that AI
is just our descendants and what you
don't like about AI is really the same
thing that people in the past didn't
like about people in the present it's
just a value shift and you need to
really think about which tribe you're
part of and maybe you should expand your
tribe and don't be so biased against AIS
Robin likes to think about values and
that's a great discussion to have but I
think we're so far on the other side of
the spectrum where it's not even close
we're just getting slaughtered right
like getting slaughtered is not a good
value maybe if it's replaced by
something really great sure but then the
other part of my claim is that we're
getting replaced by something closer to
Grey Goo so the AI that replaces us
isn't going to have all the things we
like it's just likely to have some crazy
optimized function that gets us like
nothing that we like and it's going to
happen in our lifetimes it's going to be
crazy right so even Robin who is very
Cosmopolitan very broad about what
values he's open to I don't think is
going to be happy with that scenario and
so I think we can move past the
discussion of values and t a discussion
of How likely are we to be replaced with
this slaughterbot type of species next I
want to go to the topic of future AI
capabilities so I'm going to ask Robin
do you agree that there's a lot of
headro above human level intelligence
you know like look at the progress from
ape IQ to human IQ can that dial be
turned much farther right much Beyond
human IQ and I think he'll probably say
yes eventually right it's only a
question of how long it'll take and then
I'll move on and I'll say okay is Nick
bostrom's vulnerable World hypothesis is
true the idea that eventually we're
going to have a technology which we
can't really handle because it's too
dangerous and so far we've been kind of
lucky that we pull out technologies that
aren't quite dangerous enough to extinct
us and in particular we got lucky with
nukes we got lucky that nukes are hard
to manufacture and we got lucky that
when a single nuke explodes yes it's a
big explosion but it does run out of
fuel it doesn't ignite the whole
atmosphere it doesn't keep sourcing fuel
as it's exploding and those bits of luck
AR AR necessarily going to hold when we
pull the next technology out of the
bucket those Strokes of luck aren't
necessarily going to hold when we have
super intelligent AI right a new kind of
technology that could be unprecedentedly
dangerous so I'm just going to ask if
Robin agrees with that overall Framing
and then it's just a question of okay
dangerous technology is possible to draw
at one point does AI meet the criteria
for being such a technology I'm going to
get us thoughts on what Jeffrey Hinton
said he said AI will be a master
manipulator I'm going to see if Robin
Hansen agrees again it's just how big of
a threat is this given that hopefully
you agree that some technology will pose
some kind of huge threat right is this
the big one and relatedly I want to ask
him is there a steep gradient of
intelligence increase near humans if you
look at the past and present rate of
human AI progress it seems like we keep
making our AI smarter quite rapidly
doesn't that mean that there may be a
steep gradient right around us and you
might say okay there's a steep gradient
that gets to us but does it get Beyond
us and I would argue well look at the
evidence from the difficulty of human
child birth right it seems like the
human head really got as big as it could
be before it would have prevented human
adult women from walking right so the
pelvis is really as big as it can be in
human females before walking is kind of
a lost cause and walking is pretty
important so you had to put down major
considerations major trade-offs before
natural selection was like okay let me
take a rest let me not try to increase
intelligence more but it sure would be
valuable if I could right so to me
that's telling us that there's probably
a steep gradient of intelligence
increase a gradient that we're about to
see surpassed by our AIS but I want to
see what Robin thinks does he consider
this evidence at all does he think the
gradient is just going to be shallow
past the human level does he have an
excuse does he think that AI progress
only seems rapid because we had it catch
up to humans and we helped it along
using our content I want to see what he
says okay and then the next topic I want
to ask Robin about is AI
timelines so I've done some reseearch to
claims Robin's made in the past in 2012
he estimated how long it would take to
make human level Ai and his estimate was
at least a century and maybe a great
many centuries so today that estimate
seems super long right it's rare to find
somebody who thinks AI is so far away it
was more common to find people talking
like that in 2012 so it wasn't as crazy
at the time but you could argue that the
doomers should get some base points for
always acting like AI was probably
closer than a century right that's a
pretty key prediction difference between
Robin ansen and the doomers back in 2012
then again in 2014 Robin offered to bet
at 20 to1 odds pretty huge odds ratio he
offered to bet against computer and
electronics as a percent of US GDP
Crossing 5% by 2025 now this bet is a
tricky one because I actually just
tweeted at him and he replied and he's
using a certain government statistic and
he's looking at like I said computer and
electronics which is like a weird
category it doesn't seem to be capturing
the crazy explosion of gpus for some
reason it doesn't seem to capture the AI
software boom like I don't think it's
aware of opening eyes Revenue even
though they just announced that they're
at like 3.4 billion a year run rate it
doesn't seem to be trickling into there
and even if it is we still need it to
increase by like hundreds of billions so
open AI is not going to be a huge
fraction but point is Robin Hansen had
this bet at crazy confident odds that it
seems like he should be losing but
technically because he used a weird
definition he might actually be winning
so I guess I'll you know we'll put a pin
in this one potentially even I'm losing
right I'm not sure um but I just wanted
to flag it and maybe he has something
else to say about it in the
interview because the point he was
trying to make is that he just doesn't
think AI is going to be that
discontinuous that significant on a
short time scale and it seems like it's
another prediction that's kind of
getting
falsified and then once again third data
point in 2020 Robin bet Alex tabarok
from George Mas University his colleague
he bet that gpt's line of language
models will generate less than 1 billion
in customer Revenue before 2025 so that
one we know for sure is falsified by
open ai's declared Revenue at least
their quarterly run rate right it's like
99% likely and actually I checked on
metaculus the prediction market and
they're literally saying that Robin's
chance of winning is 1% so it's pretty
much closed out at this point so once
again we do have a disconnect between
Robin's way of thinking about the world
and the way it's been empirically
playing out of course this isn't the end
all Beall right there's a lot of
predictions you can make and a lot of
little points you can argue but it's
just you know this is a point right you
got to take your base points where you
can get him so that's the part where I
follow up on predictions that he's made
and then I'll just ask him what he
thinks about Jeffrey Hinton yosua Benjo
Ilia satk all of these experts saying
hey AGI is probably in less than 20
years right Elon Musk saying it's in 3
to six years when he used to say that it
was centuries away and he's always kind
of skeptical he keeps referring back to
times in his life when he thought it was
close and he was wrong but Robin like
it's sounds like it really is close this
time like I know there's been false
alarms but it sounds like this is the
real one so I'm just going to ask him if
he wants to defer to Modern experts or
if he'd rather defer to like his own
experience I guess I'm just curious and
then I'll ask him a little bit about the
outside view which is a way that he
tries to reason he tries to not pretend
to understand little details he tries to
zoom out and say look what are the big
trends or what are like the biggest most
robust data points that we can use to
make predictions that's kind of what he
calls the outside View and I'm going to
argue with him I'm going to say look yes
the outside view might tell you that AI
progress is many decades away if you're
trying to extrapolate like other
predictions that people have made in the
past and they've been too optimistic
about when AI is coming so I get that
there's some sort of outside view that
can tell us AI progress is still going
to take many decades but isn't there an
equally valid outside view where you can
say something like biological functions
get rapidly surpassed by human Tech
isn't that also an outside view for
example how rapidly we watched AI
surpass humans on airplanes and Rockets
So if you look at how airplanes
surpassed Birds didn't that happen very
rapidly isn't that a type of outside
view can't we expect the human mind to
get rapidly surpassed by
AI I just want to see what he thinks
right but this isn't like a knockdown
argument because you can play reference
class tennis there's no one right
reference class but I just want to
bounce it off him and he might bring up
the anecdote of like hey one time they
underestimated AI progress because there
was this professor who assigned a
graduate student the task of working on
Vision in like the 1950s he's like yeah
take a summer go solve Vision come back
and of course Vision has taken like 70
years to solve but we are solving it
like Today's Vision AIS are finally
surpassing human level Vision on many
different types of tests right so I'm
not saying it's going to take one summer
but it's probably not going to take much
more than 70 years right so like these
timelines are very finite I don't know
how you get to multiple centuries like
he said in the past right so I'm just
curious how he thinks about timelines
next topic I want to ask Robin about is
what can't current AI do right because
current AI you probably shouldn't call
it AGI or you shouldn't call it human
level in every sense right it's
obviously lacking some kind of important
skills but I want to ask Robin okay so
what do you think it's lacking right how
do you see the limitations of AI I want
to see what he'll say about that like
how would he characterize what's missing
because a lot of people will be like ah
what's missing is that it never does
anything new it's just a stochastic
parrot it's just interpolating it's just
completing patterns I think that's a
pretty crude and naive way to talk I
wonder if Robin will say that or I
wonder if he just says I don't know
right like something is off and I don't
know what it is and I'm just humble and
I'll just wait to see what else comes
right I'm just curious if he has a
particular mental model uh and then I'll
ask him follow up to that do you agree
with David Deutsch that current AIS are
lacking creativity and they can't quote
unquote create new knowledge do you
think that that makes sense I'm just
curious to compare Robin Hansen versus
David Deutsch and then I'll challenge
him I'll say okay what do you think is
the least impressive thing that you're
90% sure that AI can't do in the next
two years I'm very curious to see what
he says about that because like I
personally have no idea right I mean I
think it can't quite take over the world
in 2 years but short of that it's very
hard for me to name any particular
achievement that I don't think it can do
in 2 years and then I'll ask him do you
think the $1 million Ark prize by Fran
Chet and Mike noop will be claimed soon
and then I'll explain a little bit about
the arc prize it's basically it shows
you like a grid of large pixels where
the pixels can represent crude drawings
like maybe it'll show you like a Tetris
shape made out of these large squares
and then like the next frame will have
the tetris shape which is like fall to
the ground and you won't know in advance
that that's what you're going to see
like you can see anything you can see
one set of squares and you can see
another set of squares and you have to
figure out ah the rule is that there's
like gravity and the gravity is making
the squares fall to the ground to the
bottom of the screen so you have to
figure out the rule and then your
challenge is that you get another sample
input and you have to apply the rule and
you have to generate an output so that
kind of human reasoning where we're like
aha these look like Tetris blocks
falling or in another case it's like ah
these look like shapes that are moving
to the right or these look like shapes
that are imploding or like whatever
you're seeing in the squares like it
could be an arbitrary pattern and that's
the nature of the challenge and
apparently llms are struggling with this
although they keep doing better and
better so I'll just ask Robin like do
you agree with Fran Chalet that this
represents like a fundamental limitation
of llm Architecture is it is it a
meaningful price to you I'm just curious
the next topic I want to ask him about
is architecture SL algorithms versus
content like the two pieces of the
puzzle of intelligence at least the way
he seems to see it I'm going to ask him
do you tend to think content is more
important than architecture and
algorithms because that seems to be his
position over the years I'm going to ask
him why and whether he's updated that
and then I'm going to give him a little
gotcha question I'm going to say Robin
you're a polymath meaning you spent your
whole life gaining broad content did you
experience firsthand the importance of
content over algorithms and I'll answer
and I'll be like well your most
well-known content your most well-known
concept is the great filter which you
wrote at age 37 now you're 64 you have
73% more content since he wrote that are
you producing 73% better research than
great filter or how do you think about
that and you know it's obviously a
gotcha question but I am curious if he
thinks it's relevant to his own
experience and then I'll say let's talk
about parameter counts how do you think
about the interplay of these three
ingredients architecture parameter count
and
content and it'll say I don't know maybe
parameter account helps you absorb
content I'm not sure what he's going to
say so it's interesting you know this
isn't just like me dunking on Robin
right I'm also just curious to learn
what his latest position is about Ai and
help you learn what it is right I'm
helping Robin disseminate his mental
model then I want to ask him okay
hypothetically what if you keep the same
content but you substitute a better
architecture like if somebody took the
same training data as gpt3 but just got
to add more parameters does gbt 3's data
set contain enough content to bootstrap
taking over the
world in my mind it probably does I mean
the internet just has a lot of content
right if you just really understand how
the internet was generated if you run
enough algorithms on it and figure out
the Deep patterns and how physics works
and the levers of power I think there's
just enough data you're not going to be
like oh I just need a little bit more
data like I think you got it you you
know which Universe you're living
in and then I'm going to ask him about
Psych cyc the project from 1984 that has
been going for about 40 years kind of
fizzled
out Psych is interesting because he has
stated that he's a fan of it and it's a
project to basically collect as much
content as possible as much structure
structured content as much logical
relationships between pieces of content
and kind of note them down took a lot of
human efforts to write down all these
different facts about the world and how
things logically connect and I'm going
to ask Robin why were you optimistic
about psych as a path forward toward AGI
how else have you updated about AGI in
light of the whole psych situation does
elazer get a base point for predicting
in advance that psych would be useless
for AGI and then moving on past psych
does the internet contain enough content
to fuel an AI with the right
architecture to F or is there no process
that can rapidly generate content fast
enough to F I'll ask him does your
content Centric view of intelligence
make you think there's a low speed limit
on recursive
self-improvement because if you keep
just having to find more content and
you've never got a crazy powerful
algorithm maybe you kind of top out at a
linear rate of improvement right is that
kind of how Robin sees the world I think
it is but maybe he can flesh out some
detail and then we get to the next
section where I want to ask him is
elligence many different things or one
thing for example the quote that scale
is all you need do you agree or disagree
with that sentiment scale is all you
need it means we basically figured out
how to be intelligent it's just a matter
of substituting the right values into
all of these parameters in this fully
General architecture that we figured out
maybe with some optimizations but just
like whatever we figured it out keep
throwing in more data the system will
take it from here do you basically agree
with that approach or do you feel
strongly that there's some more Secret
Sauce besides the Transformer
architecture that's fundamentally
different as a way to get intelligence
then I'm going to touch on goal
completeness do you think goal
completeness is an important property
that future agis will have goal
completeness is a term that I coined by
analogy toering completeness it means
the same AI engine being easily
adaptable to achieve any arbitrary goal
so do you think that that's going to be
what we can expect from AI engines yes
or no I think there's a good chance that
he'll say yes but he might think no
there's going to be a bunch of narrow AI
engines they're never necessarily going
to converge I'm curious to hear what he
thinks about that I'll ask him do you
buy the analogy toering completeness do
you think it's a useful concept that has
high explanatory power or no because I
personally think goal completeness
really is how we should think about AI
the same way that we think about
computer chips as just being General
computer chips right you don't buy a
separate type of device to do different
applications you're pretty much just
always buying an arm computer chip right
an Intel computer chip there's not that
many computer chip makers but there is a
wide range of electronic devices and a
wide range of software programs right so
that same convergence that we saw with
Turing completeness I think we're going
to see with goal completeness in super
intelligent AI okay and then I'm going
to bring up
aixi which I can't really explain right
now but it's the theoretical ideal of a
goal complete algorithm so it can just
optimize any function using any data
even though it's uncomputable it's it's
a theoretical ideal and my question to
Robin is does aii hint that intelligence
is one thing because we have this ideal
description of it when we know an ideal
algorithm and we try to optimize it the
optimization doesn't typically work with
a bag of heris sixs usually we are able
to narrow it down to a pretty simple
algorithm so the fact that we're able to
specify what aixi is to me is a strong
hint that somebody is going to build
some actual working system that
approximates Ai and the approximation
doesn't have to be perfect but it's
going to be far super human because a
human is an axi approximator and a human
is just barfed out by Evolution right a
human is not the ideal the same way that
I could make an ideal of Transportation
I could be like hey there's a vehicle
that moves really really fast in an
arbitrary direction and you could be
like aha well birds are already close to
that ideal I'm like what are you talking
about there's so many other things we
could build that are closer to the ideal
of something transporting itself very
quickly right and that's how I see axi
I'm like we know the direction we know
what intelligence is there's some very
abstract sense in which it's one thing
the sense of axi it's just mind-blowing
to me to not think that somebody is
going to far surpass Humanity on this
Dimension but that's just me I want to
hear what Robin thinks and then I want
to ask him what do you make of The
observed convergence happening in AI
systems so for instance in the domain of
the board game go we had Alpha go and
then we had alpha0 and then we had mu0
and in each case we're generalizing the
AI algorithm to accomplish the same
thing so alpha0 didn't have to study
human playbooks and human games of go it
started from just the rules of Go and
mu0 is an algorithm that can play
multiple board games so you don't even
need to have fundamentally different
architecture for the particular board
game can you extrapolate the domain
broadness that we're seeing with AI
we're seeing increasingly broad domains
that AI can handle and of course the end
game of this is that the whole universe
is the domain right why would the size
of the domain stop at some point I don't
get it so I want to see what Robin makes
of this trend and basically where he
gets off the extrapolation of
convergence in AI
systems the next section I want to get
to with Robin is fum of course that was
the subject of the famous 2008 Hansen
yudkowsky fum debates let's define fume
as a positive feedback loop that rapidly
disempowers
Humanity how confident are you Robin
that a f scenario won't happen and he's
probably going to say I am pretty
confident that's my whole claim I you
know I give it a 1% or whatever and then
I'm just going to have him rehash some
of the Articles he's written like he's
written an article about an Uber tool
that's supposed to be a toy model of AUM
like a company made a tool which is so
powerful that the company uses the tool
to make other tools and there's an
explosion within this one company of
this Uber Uber tool where the tool
improved the tool and no other company
can catch up that's Robin's toy model
where he's pointing out that this is
like a ridiculous situation it would
never happen in the economy and he's got
another toy model which he calls the
betterness explosion which is like
imagine that we figured out how to make
things better and if everything got
better then that would let us figure out
how to make even more things better and
there would be a betterness explosion so
he's using these toy models or these
hypotheticals to show you by analogy how
ridiculous he thinks the F claim is so
maybe I'll have him just reash these
arguments in his
words which is basically that
competition prevents a single Uber tool
and then my question for him is wasn't
the human brain an Uber tool or a
goodness explosion that other species
couldn't adapt to in evolutionary time
what do you make of that claim Robin
because humans really did use our brain
to build up a bunch of infrastructure to
support our brains right so I'm using my
brain to get a lot of what I want in the
world and make pretty big effects you
know get projects built for myself get
people to do what I want by employing
them in my business over the internet
right so I'm pretty effective over the
world using my intelligence but at the
same time I'm kind of helpless if I
didn't have all the support right like
I'm not good at uh you know building
stuff with my hands right um so that's
part of the human F right compared to
these other animals right I'm able to
like go hunting and take down a bunch of
other animals because my species has fed
and they can't do the same to me and we
didn't bring the whole ecosystem along
with us right it was just isolated to
humanity when when we did this F so I'm
curious what Robin thinks he thinks that
a f is kind of impossible under the
economic analysis that he likes to do
but what about looking at the data of
species Level FS on an evolutionary time
scale right so the human food maybe took
a million years maybe took tens of
thousands of years however you want to
analyze it but the point is that the
other species didn't follow along and it
doesn't seem like they're on track to
follow along because they didn't hit
into this attractor this part of the
genetic landscape where you make general
intelligence there's no other species
that has hit the Steep gradient hit the
Cascade of intelligence improvements
where one intelligence Improvement then
makes it higher Fitness to have the next
intelligence Improvement and they build
on each other in various ways now we get
to a big section where I want to ask
Robin about the outside View and how we
extrapolates what he thinks are robust
Trends so I'll start with a question
Robin how would you describe your
methodology for reasoning about super
intelligent AI is it basically you
document and extrapolate robust Trends
in the owski and inside view we focus a
lot on a timeline of optimization
process discontinuities
so we're talking about specifically the
dawn of natural selection that was an
optimization process that led to life on
Earth because it optimizes Generations
that survive and replicate and mutate
right so it was the first real
optimization process in the universe and
it did a lot right I mean the animal
species the different Kingdoms of Life I
mean that's impressive stuff compared to
what came before and there was another
discontinuity next with the dawn of
human brains right that's the next
optimization process discontinuity and
then finally the upcoming dawn of AGI
it's an intelligence built by an
intelligence it's like Intelligence
Squared if you want to call it or
intelligence of the power of
intelligence so these are super Salient
events in a three-point trend line that
dwarfs everything else from the
perspective of an AI
Doomer but in the hansoni quote unquote
outside view Salient events timeline aka
the major human economic transitions
timeline it seems like you prefer to
focus on the forager era I.E the dawn of
human brains and then the farming era
and then the industry era right so to
you those are massive changes in the
economy the three economic modes that
we've lived through as a species you
think that those are more Salient and
there's other timelines that you've
pointed to but I think that this is your
favorite so in your own words what do
these particular events tell us that's
related to AGI you've written about how
you can extrapolate that economic
doubling time will enter a new mode
where it doubles even faster because
that's how you extrapolate from foraging
to farming to Industry each time you saw
a major shortening of economic doubling
times so can you go back over how you
expect economic doubling time to shorten
again soon with as much detail as you
think is relevant for this discussion
now if I remember correctly Robin's
going to say I expect economic doubling
time in the next few years or decades
probably not more than a century from
now we shift into this new gear of the
economy where I expect the doubling time
will take about 1 month because that's
how I would extrapolate previous
economic Trends so I think that's what
Rob will say and to me it's kind of
interesting that he really thinks that
with all the different inside view
factors involved with everything we know
about like super intelligence and fum
and Robin's just like no I mean there's
been previous economic Transitions and I
want to extrapolate them so to me that's
a very bold move very much like Moore's
Law like it's kind of like a shallow
Trend where sure it might hold but like
look at all these other major Dynamics
at play right so it's a little shocking
to me that this is how Robin reasons but
I want to hear him tell it his way uh
and then I'll ask him okay do you still
predict that the age of M is coming next
a brief period of like a year or two
where emulated human Minds kind of take
over the economy is that going to be a
period coming next and he might still
say yeah I think it's like kind of
likely but not for sure you might say
something like that and I'll say okay
and what do you still think comes after
the age of M or what do you think comes
next like in your own words your own
Mainline scenario what do you think is
likely another thing I want to ask him
is like okay you have these predictions
of economic doubling time but why are
you using time as your x-axis when you
extrapolate this trend you know time as
your independent variable in the
function why not the rate of
optimization being applied so all of the
human economic transitions have happened
when the optimizer the underlying
optimization has been still pretty low
like yes it's been growing it's arguably
been growing exponentially because
there's been more humans and there's
been more technology that humans have
built but the underlying human brain has
still been a pretty weak Optimizer
compared to how smart these AIS are
about to be so we've never modeled a
curve where the input of optimization is
about to go up so drastically right so
why are you so fixated on time as the
x-axis if you model it based on
optimization power and then map it back
to time aren't you going to get a fum
and this is an argument of course I
stole from Alazar udas I'd like to hear
Robin respond to it and then I'll ask
him okay Robin you have a certain way of
reasoning let's imagine going back to
the dawn of
humans what abstraction what way of
reasoning would have let us correctly
predict hum's present level of power and
control what could you have extrapolated
back then is there any sort of view that
could have tipped you off that this is
coming and maybe Robin will say look
there's just no way to do it we just
don't have that kind of powerful
reasoning we just have to throw up our
hands and be like let's wait for a trend
it'll be interesting to hear if Robin
thinks he has any good answer to that I
mean maybe Robin will say fine at that
point when we have no economic data
because it's 4 billion years ago at that
point I would turn to the owski and
inside view model just because I don't
have a better alternative is Robin going
to say that I don't know let's wait and
find out soon my next question to is
kind of a deep cut from the 2008 Hansen
yowy fum
debate Robin elzra claims that
capability growth rates are determined
by three levels level one the dominant
optimization process like natural
selection human brains AGI level two
metal level improvements to that process
so like cells sex writing science those
are metal level improvements to the
dominant optimization process and then
level three object level Innovations
like light bulb automobile so again
elazer claims that capability growth
rates are determined by those three
levels your timeline of sailing events
undermines elzar's inside view elzar's
claim there your timeline says that
farming elzar's level three an object
level Innovation was more important than
writing Alazar level two right so
writing is a metal level Innovation
farming is an object level Innovation
but in your timeline of economic growth
mode transitions farming is actually the
big one farming mostly came before
writing farming enabled writing more
than vice versa so you're kind of
undermining elzar's description of three
levels right um do you think that that
is like a strong argument that elzar has
it wrong with his focus on optimization
processes so I'm curious what Robin
would say there and then I'd be like
okay and let's review you also think
another way that you can undermine
elzar's levels is that you can point out
that scientific thinking Alazar level
two a meta Innovation scientific
thinking wasn't the key to to Industry
the key was probably sufficient economic
growth to enable a density of
Specialists industry was not enabled by
a meta level Innovation so it's like
Alazar has this way of viewing the world
and you're looking at actual data actual
Trends and you're saying look the data
is not supporting eleazar's favorite
inside view model right so am I doing a
good job of like representing your world
view Robin and your argument in the 2008
fum debate so you might say yes you're
doing a good job and be like okay well
you know I am more of on elazar side
where I just don't find your outside
view super compelling even if you are
able to poke holes when elazar tries to
Define these levels in this one case
like sure those are interesting holes
but let me try to poke holes in in your
version and a hole I'm going to try to
point to is leaf cutter ants because
leaf cutter ants have farming too so is
farming really that causally powerful
because are these leaf cutter ants you
know they they cut the leaves they grow
a fungus and then they eat the fungus so
they're farming the fungus
are they on track to get industry next
are they on track to get writing next no
right so it's not like farming itself is
the super powerful thing so just you're
you're doing a hard project here right
trying to look at the record and be like
Oh farming is really powerful uh because
like why not right why didn't leaf
cutter ants progress to writing or any
of these next stages once they had
farming how do you Analyze That Robin
why was farming only a major transition
for humans okay and then let me bust out
my claim my claim is that high op
optimization power like AGI trumps
everything and trying to use outside
view data or quote unquote Trends can be
highly misleading let's take this
example spotting a level two
breakthrough like cells and farming is
only something we can do in the
goldilock zone of optimization power
it's a meaningful milestone for
exponential progress driven by natural
selection or human brains because those
two optimization processes are in the
goldilock zone but farming
would not be a major exponential driver
for leaf cutter ants or for something
like AGI we shouldn't expect to notice
level two and three wins for AGI
watching a human amateur play chess if
they capture or lose a rook piece we
update that they've had a booster a
setback watching stockfish the chess AI
play chess against the human if
stockfish loses a rook we just assume
that it's a worthwhile Gambit we are not
documenting level two and three wins
when we see an AGI and similarly if see
a Lea cutter ant okay they figured out
farming they're not going to grow
exponentially right so there's this
goldilock Zone where you can look at
this trail of object level improvements
that they've done where some of them
triggered a faster growth rate but
you're looking at shallow
patterns if you zoom out my inside view
of optimization power is going to matter
a lot more for the future that's my
claim next question for Robin aren't
there many other possible outside views
so you kind of have an outside view that
you keep returning to right the economic
outside View foragers Farmers
industry but what about let's say
Moore's Law is that an outside view you
endorse do you think it'll just keep
holding does it hold past Super
intelligence I don't know what Robin
thinks about that but that's interesting
to have him challenge his outside view
which I consider surface level like yes
Mo's law is a surface level Trend it
doesn't have to hold into 5 centuries
from now or into super intelligent
optimization power it doesn't have to
hold with time as the x-axis so I'm just
curious to poke around how Robin sees
these so-called robust Trends and how
robust really are they compared to
forces that I consider deep because I
guess I have a certain type of inside
view as Robin would call it right I have
the optimization view another outside
view that I'll throw at you Robin is
what I call game over
buttons it's a button that you press it
in a video game and you just created a
game over situation right you just ended
the game like a suicide button when we
invented nukes we invented a single
button you can press that SLS 100
million humans just like that right
point and shoot if you extrapolate that
outside view of humans having more and
more powerful Slaughter buttons doesn't
that extrapolate all the way to a button
that says okay you kill a billion humans
you kill 10 billion humans you kill
everybody right it's a single button
that ends the game isn't that an outside
view that we can exra extrapolate
doesn't that have support where weapons
kept getting larger and larger can you
make a timeline predicting when the 10
billion human extinction button is
coming I'm just trying to do outside
Robin just like you like to
do okay so that's my outside view
section okay next topic is alignment
feasible so I'd ask Robin is your own
alignment strategy to make sure that AI
respects the rule of law because I think
you've talked about that or something
like that and you might say like yes
that's why you trust other humans to be
nice not because they necessarily share
your values but because they've agreed
to respect the rule of law among
themselves and that makes you confident
that they'll keep respecting the rule of
law for you he might say something like
that because I think I've heard him say
that kind of thing in the past so I have
a follow-up like okay will AI let humans
have money and property in their economy
when does a weaker demographic tend to
get cut out of the rule of law is there
going to be a Jews in Germany situation
right so I'm Jewish right and my grandpa
thought he was in good standing in Nazi
Germany right he thought he was a Jew in
good standing and then he was not in
good standing anymore right so they
changed the rules on him so under what
circumstances does the so-called robust
trend of everybody's property rights get
honored under what circumstance you say
you know what you're not part of my
faction and so you get excluded from the
property rights because I definitely
think that we as a human species if
we're weaker than AI as a species I
definitely think the abstraction of rule
of law can easily stop applying to us if
the AI is the one who runs
Society next subtopic remember the topic
is is alignment feasible so the next
subtopic I want to ask him about is what
do you think of rhf that's our current
tool the way we use to align AI that
stands for reinforcement learning with
human feedback rhf is useful now but is
it useful for super intelligence I want
to see what Robin says he'll probably
say no because that's the standard
position that the AI labs are saying
they're admitting like yeah it's not
useful for super intelligence so I'll
ask him okay what do you make of AI Labs
admitting that it's an open problem what
do you make of quote unquote super
alignment what do you make of open AI
strategy to build an AI that helps help
with their AI alignment elzas called it
oh we'll have the AI do our alignment
homework what do you think do you think
that sounds like a good prudent
strategy how about this idea of debate
or iterated amplification with debate
will debate be a scalable AI alignment
strategy do you think that humans can
judge ai's arguments if you search my
podcast feed I did a whole episode where
I oine on why I don't think that humans
can be the judge of a debate between AIS
and my argument is like look humans have
a lot of trouble judging simple debates
among other humans right like think
about Israel Palestine think about
blockchain applications web 3 think
about all these topics which are
objectively quite simple and yet humans
have a very very hard time converging on
the right conclusion for simple
arguments so that's why I personally am
not optimistic about the idea of humans
judging debates between super
intelligent eyes and yes I know that
complexity Theory tells us that there
can be interactive proofs where the
verifier is a lot dumber than the prover
I get that but still I'm afraid
computational complexity theory is not
giving us the right intuition and
debates that you see on Twitter I think
are giving you the right intuition on
how incompetent humans are as judges of
debates so anyway it'll be interesting
to see what Robin says about
that next I'd like to ask Robin about a
warning shot so I'll say Robin what's
the event or the threshold or the
warning shot that would make you
concerned about rapid human extinction
from AI is there any kind of event like
that
or will you just never be concerned no
matter
what you've previously predicted that AI
advances like gbt wouldn't be
responsible for over a billion dollars a
year of economic impact so the fact that
this seems to be falsified pretty
rapidly right we're kind of breaking
through what you thought was the ceiling
is that a warning shot for you at all
does that mean something to you do you
have a new threshold to watch out for so
maybe you're not that concerned about
this particular ceiling but maybe you
think something like 10% of world GDP
attributable to recent AI technology if
that ever happened would you start
freaking out or just like when do you
see a warning shot if
ever is there a particular thing AI
could do that would make you reconsider
f for instance if AI started earning
money for itself would that make you
think uh oh this process could f it
could grow exponentially inside of one
company what if AI would rewrite itself
to be smarter is there anything or would
you always just say no fum is impossible
I just never expect to see evidence that
would change my mind or rather I don't
expect to see evidence but I can't even
name what kind of evidence I could see
to change my mind even improbable
evidence I can't even name that right so
what exactly is your position on your
ability to change your mind about
fum and then I want to ask Robin an
ideological tring question about us
doomers I want to check if Robin
understands do doomers like me so tell
me Robin why do doomers like me continue
to believe that fum is coming when there
hasn't been much direct evidence that
fum is starting to happen
now why haven't doomers updated P fum
downwards after seeing continuous AI
progress with no fum so I'm curious if
Robin can kind of pass the ideological
tur test how much does he know about
what I think and what alaz owski
thinks the answer of why I believe that
for the record is because I think that
there's a critical threshold when the
fum starts right F doesn't just always
happen when AI gets better the threshold
for recursive self- Improvement that
feeds on itself at an unprecedented
level is roughly when AI becomes more
powerful at goal optimization in the
domain of the physical Universe than
humans are so the same way that
stockfish has become superhuman at
optimizing chess winds if you had a
version of stockfish but it was an AI if
it could optimize nend states in the
universe if it treated the physical
Universe like a game of chess and it
could outmaneuver and it could outplay
human strategists at the domain of
achieving outcomes if it could run a
company better than Elon Musk for
instance right Elon Musk basically plays
physical Universe chess right he has a
space program if AI could have a space
program the way Elon Musk does and it
still doesn't fum at that point I do a
major update I say wow something is
preventing fume even though the power to
beat Elon Musk at having a space program
where you're the leader of it and you
know you you get all the chips in place
to beat him economically have a better
space company than Elon Musk manage the
employees better get the suppliers get
the technology if you can do all of that
and you still can't edit your own AI
code you still can't do a Manhattan
Project to improve yourself and have
that improve yourself at that point I
will absolutely admit as a dmer like wow
something is wrong with the fum
hypothesis because the preconditions to
F are being met and it's not fing and I
don't know why so I'll I'll take a
massive update at that point
unfortunately I'm not expecting to take
an update I'm expecting to die Mr Bond
right that's so but at least I can tell
you when I would be like okay I give up
I'm just going to be happy now I'm not
going to be a Doomer because it seems
like I am just clueless there should be
a f but there's not so I give up I'm
going to hand over the Reigns of Doom
debates to somebody else who is much
better at knowing whether we're doomed
or not I thought we were and we're not
right so I can tell you a coherent
condition where we can potentially still
live to see it I mean open AI is
predicting that this condition will
happen in potentially in less than 10
years it literally says on open ai's
website it's saying AI might become more
powerful than a human CEO at running a
business in less than 10 years so open
AI is literally specifying a condition
that to me means that we should see
massive doomy consequences like we're
doomed or we're close to doomed you know
we should start to see warning shots all
over the place so it's kind of funny the
thing that open AI is saying yeah this
could totally happen is also the thing
that I'm admitting should falsify my
prediction if it happens without having
warning shots all over the
place so anyway I want to see if Robin
knows that I think like this or how he
thinks that I think I also want to ask
Robin an open-ended question how else
have your views changed in the last
decade I mean you've been thinking about
this for a while you've been writing
posts what's changing what are you
learning and finally I want to talk
about other related thinkers like Robin
where do you see yourself among these
other related thinkers let's compare and
contrast starting with franois
cholet he thinks llms are a dead end he
has this fascinating called The Arc
prize I can explain the arc prize a
little bit I explained the arc prize
earlier in this podcast what do you
think Robin do you agree with franet
thinking that llms are kind of a dead
end and they need a massive
architectural Improvement to solve
things like the ark
challenge okay what about Yan Leon he's
a huge name in AI anti- dumer ISM you
and Yan Leon share a very low P
Doom do you agree with Yan that a badly
built AI could be extremely dangerous
but Humanity will only build a version
that's safe is that how you see the
world too or do you not even think that
a badly built AI could be extremely
dangerous like do you compare yourself
to Y Queen at
all and I'm not sure if Robin does I
don't know if Robin spends much time
learning what other people in the aid
Doom discourse are saying I don't know
but it'll be interesting if he has
opinions like that let's talk about
David Deutsch do you agree that current
AIS are lacking creativity that's a
David Deutch claim do you agree that
current AIS can't quote unquote create
new knowledge you know I'd love to get a
David deut Defender to come on this
podcast and and argue that with me
because I think it's nonsensical but I'm
curious to know what does Robin think
right because I disagree with David
Deutch I disagree with Robin but is it
the same disagreement are they on a team
are they on two separate teams that's
what I want to find
out Stephen Pinker what does Robin think
about stepen Pinker by the way there's
an episode of this podcast from a couple
weeks ago you can go listen it's my
takedown of Steph pinker's a I non- Doom
claims check that out if you're curious
what I think about stepen Pinker but
what does Robin think about Stephen
Pinker that's what I'm going to ask now
Robin do you expect that AI will know
how to balance multiple goals to align
with Humanity's goals is that kind of a
key to the problem is to not go too
hardcore on one goal and I asked that
because stepen Pinker keeps making the
point that it's like stupid to optimize
one goal but if you're balancing
multiple goals then everything is going
to be fine which I think I answered in
the previous podcast I did about Stephen
Binger I don't think that makes any
sense but I wonder if Robin agrees with
it
Ray Kurtz what does Robin think of Ray
Kurtz maybe we talked earlier in the
podcast about Moore's Law so maybe we
already covered Ray Kurtz but just in
general do you agree with Kurtz's
methods of
extrapolation do you think that Kurtz is
predicting the singularity accurately in
just a decade or two or do you think you
have a better prediction that AI will
take a century or
more and by the way Robin we've gone
over some names but who else do you know
who agrees with your reference to
forager farming industrial era timings
because that seems like a very unique
Robin point I don't really see other
people adopting and repeating that point
what do you think about
that okay so these are all the topics I
want to ask Robin I've been talking for
an hour so that means when Robin and I
debate it could easily take two three
four hours so we're probably not going
to get to everything that I just covered
now especially because Robin is actually
going to come in swinging asking me a
bunch of questions he specifically said
he wants to ask me questions too and
we're going to exchange questions so
this is a massive outline maybe Robin
and I should do a part two I hope you've
enjoyed it going through it with you has
helped me prepare you may notice this
podcast has edits like most of my
podcasts so I was thinking through stuff
as I was telling it to you thanks for
being part of this journey and I look
forward to having a productive mutually
enlightening discourse elevating debate
tomorrow with one of my heroes Professor
Robin Hansen stay tuned for that very
special upcoming episode of Doom debates