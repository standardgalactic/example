so yeah just uh two aspies here two ASP
owski debating P Doom right I have a
99.999 maybe per P Doom over the next 40
years we've practically already lost
this game unless something genuinely
super unlikely happens
welcome to Du debates today my guest
goes by the pseudonym nephus I
originally met him when he commented on
my YouTube channel I've never seen him
before in any other Forum but he's been
unusually well informed and insightful
in my comments so I invited him to come
on the show walk us through his thinking
even though he prefers to remain
anonymous I think he has a very
interesting perspective and some
insights that you guys are going to
like hey NE welome welcome to the show
great to be here
luron so tell us where are you from I'm
from the US born and
raised fair enough don't want to get
more detailed than that and really
quickly what's kind of your life story
and how did you first realize the
concept of AI do what was your exposure
to
it uh gifted Kid read a lot went to
boarding school started rambling about
how everyone on Earth was is doomed for
various reasons because in the next
hundred years technological progress
increased power
scale I intuited that Society wasn't
ready for this kind of stuff I pointed
my finger at AI as being one source of
this kind of problem though at the time
I would have also said you know nanotech
asteroids uh you know some sort of
technological or just random accident
that we weren't ready for um so I had a
sense that the world was vulnerable and
a set of crises were
approaching I started talking about AI
more and more and someone said hey have
you ever read this Harry Potter
fanfiction Harry Potter and the methods
of
rationality and I opened it up I had a
sort of religious experience reading it
for the first time or not a religious
experience I think I just stayed up for
like two nights in a row trying to cram
as much of it into my head in the first
you know 48 hours that I was exposed to
this thing and since then I've been a
owski fan broadly less wrong all that
kind of stuff um and have just drilled
more and more deeply into this topic
of AI
AI sourced existential
risk yep neth you're kind of giving away
the game when you say that you had a
religious experience because we often
get the accusations that rationalists
and doomers are a religious cult but I
definitely know what you mean because
when I personally started reading less
wrong I didn't have a religious
experience but I was just like wow this
is like better quality thinking than
other rational people right like it
reminds me of like Richard feeman and
other like rationalist
greats yeah apologize for using
contaminated language there I would like
to uh would like to say like I
I have never been anything but an
atheist as far as I remember I mean
there are those childhood moments where
you wonder if you know
something's organizing all of this stuff
something above you and you know maybe
you pray or something like that but I
mean even in elementary school I was
debating people about whether or not God
is real and uh I am firmly materialistic
I I think we live in a universe with
a regular causal structure with a bunch
of hard edges down at the at at every
level it's what happens just
happens I agree and I think that was
Einstein's position right even when he
made reference to God I think he was on
the same page of like look this just the
universe like whatever ever the law say
happen is going to
happen yeah I
think I don't know specifically what was
going on in Einstein's head when he
thought about cosmology but I I think he
had
a a vision of the universe as a
mathematical
object yep so back to your life story it
sounds like what I'm hearing is you kind
of independently invented Nick bostrom's
vulnerable World hypothesis because you
independently had the intuition of like
hey we keep pulling balls out of the N
right we keep pulling new technologies
out and they're really powerful and
eventually we're just going to hit on
one that we can't handle and did you
independently think of that without
reading Bostrom I hadn't read Bostrom at
the time I don't know the full process
like I had encountered Sam Harris before
that and he has read Bostrom and
probably had read Bostrom before I
encountered him so there's probably some
contamination of other ideas it's it's
really hard to pick apart you know where
things originated from but it wasn't
something where I was reading someone
arguing for it and then came to that
conclusion based on their arguments it
was very much looking at the world and
seeing a picture of it as sort of in
Carl San terms like we just a little
rock revolving in I think he called it a
a Sunbeam or something like that in his
a pale blue dot in his pale blue dot
speech where on this rock with this very
thin blanket of atmosphere it's like if
you have an apple the Flesh of the Apple
would be the Rock and the skin of the
Apple would be the atmosphere and within
this atmosphere and just like laid over
the surface of the rock is this threy
connection of chemical molecules that
are doing very complex self-replicating
things and there's this like slime on
the surface of the Rock and it's just
hurdling through the void hurdling
through hard vacuum with radiation and
all this kind of stuff and it seemed
fairly obvious that what Humanity was
doing was out of proportion to how
vulnerable we were in that position I I
this is also called a spaceship earth
right like we're all just living on the
spaceship and like have a life support
system but if that falls down you know
we're just in this inhospitable universe
right yeah exactly so you're in your 20s
right now right yep mid 20s okay um and
in terms of your Dune perspective would
you say you're mostly aligned with
elazar Cy yeah I think eler has a much
clearer grasp of the intri intricacies
of this stuff than I do um he's got a
broader range of specific technical
theories that he uses to generate the
argumentation that he uses um I wouldn't
say that I'm just paring stuff that
elaser says but if you want a better
perspective on this stuff listen to laer
uh listen to him read him go really deep
into it and even if stylistically
something doesn't click uh just keep
reading and try to extract the
regularities in his arguments the simple
theories behind them that are that he's
using to generate all the other stuff
that he
says absolutely when you say neurod
Divergent would you describe yourself as
Aspergers maybe I don't know um I've
never
felt much problem social wiing like it
never became a uh self-consciousness an
anxiety sort of thing
I you know have in the past had moments
of social anxiety like hesitating to
engage with social interaction but it
was never
like I was entirely ill suited to it or
didn't become a sticking point in my
psychology but I'm not normal I talk
like this basically all the time I write
the way I do basically all of the time I
run out ideas into these huge long
chains of like explanation and
explanations for the explanation and
qualifiers and all this kind of stuff
and uh it's not a normal way that people
relate to each other and can you know
yeah sort of narrow down the number of
situations you can insert yourself into
easily it seems pretty common for like
the less wrong rationality community
have those attributes so yeah whatever
it is it's not uh it's not unheard of so
yeah just uh two aspies here to ASP
owski and debating P Doom right I would
like to say that I don't think this
stuff is as complicated as it looks on
the surface I think it takes time to
onboard but I think the upside is when
you actually get out into the world and
try to apply it it makes life simpler
all right you ready for the big question
yeah are you going to do the
drop hit
[Music]
it neth what is your P
Doom all right
I thought about coming up with a
specific number for this but I figured I
might as well weigh it on the spot um I
have a
99.99 n maybe per P Doom over the next
40 years I think actually if we go out
to 40
years maybe add another one or two nines
in there I I think okay so greater than
99% P Doom in the next 40 years is that
right yeah I think substantially greater
wow okay why do you have such a high P
Doom I don't see a good reason to not
have such a high PE
Doom I there's a lot of additional
details there but I am firmly in a camp
that burden of proof is on the other
side uh that's not quite how I'd phrase
it but that's sort of the common
language about this kind of stuff I I
think okay sure what about the fact that
we've survived all the new technologies
so far so how can you be so confident
don't you think at least it should be
more like
5050 uh it depends
on
okay the 5050 thing is an anchoring
point that is not
well not well defined by the nature of
this problem like if you is the natural
anchoring Point more like 99% right
where where do we start Where is the uh
the reference class or the background
estimate that we update
from so the background estimate the the
place we start out estimate like
estimating from here
is in the space of search processes that
have a particular kind of character as
being
superum as being better at steering the
future than humans
how much do you need to specify the
details of what those search processes
are looking for in order for them to be
aligned to humanity in order for humans
to be a step towards them or human
values to be the thing that they're
aiming
for so I think you're basically giving
me what some people are accusing doers
of doing like it's become pretty popular
to accuse an argument like yours of
being just a counting argument meaning
you're kind of counting the possible AIS
that you like and then you're counting
like all possible AIS and you're being
like look it's a small fraction
therefore our chance of a good future is
low would you say you're just using a
counting argument or how is what you're
doing better than that my argument is
not a counting argument my initial prob
probability distribution does involve
counting in that I'm trying to determine
what space of outcomes over which I
begin to apply updates too so like yes
the actual picture of what's going to
happen in the future is going to be much
narrower than just a spread over all
possible configurations of computing
Hardware um because we're going to apply
an optimization process to that
Computing Hardware I'm trying to put my
finger on how much optimization power we
need to use
to configure that computer hardware in a
way that gives us good
outcome now to give you guys more
context on neth's P Doom let's go to the
YouTube comment that he wrote on my
channel that originally made me want to
invite him on the show Okay so this was
on a community post of yours where you
said overall how aligned are you with
learon on AI Doom
views and I voted on that poll saying
agree with Lon greater than 70% of the
time and I further commented about my
disagreement with you or the
disagreement that stood out to me at the
time I said I think your P Doom is
unjustifiably low I know you leave
yourself a bit of a range in there but
frankly it feels a bit like we're in the
stands discussing a home team game where
our side is down 100 is zero with 2
minutes left on the clock and you're
saying well I agree we have less than a
50% chance of
winning more specifically reroll the
dying with Dignity Post in your head but
swap out the April Fool's tone and
content for a straightforward message
about how to reason correctly about
being in a dire situation and just for
some context here dying with Dignity or
death With Dignity or something like
that was an April Fool's post by elazer
owski where he sort of I think first
came out and said things look pretty
grim and some of that post isn't him
kidding he says real things about how to
reason under conditions where you have a
really low probability of success okay I
continue this isn't about defeatism or
appearances it's about getting a correct
strategic view so you can actually have
a world model that's as close to reality
as possible with which you can then make
the correct decisions under some
plausible but seemingly unlikely Black
Swan events which open up some narrow
path to
Victory this doesn't mean the universe
is going to give us a black swallen or
10 which open up narrow paths to Victory
the main line still looks like getting
nanom machined into fuel SLS spare parts
for near black body solar
collectors but you're not fighting for
the mainline at this point in
history even if everyone gred it and we
got an indefinite moratorium we'd still
have an uphill battle you still lose
more uphill battles than you win
fighting uphill as a
disadvantage but the world where
everyone wakes up tomorrow with the
correct strategic picture on ASI is
still a fictional
World never mind that I didn't specify
additionally in the hypothetical that
everyone also has a correct view of the
way their own values interact with this
magically downloaded strategic picture
you just don't get any magic that turns
around our culture at this
point surviving worlds seem a lot more
like World War I didn't happen nor World
War II more smart people were alive in
the 20th century Vibes were better
Turing and others got the time and space
to map out the correct
strategic picture on AI development and
then we had a 100 years Head Start Over
Our universe's Hardware scaleup to spend
on becoming a species that can handle
this and even that doesn't seem insta
win if you think it through with a
historically appropriate amount of
pessimism parts of the hor in World War
II might have been highly contingent on
individuals making unique choices
because of their own
idiosyncrasies but all the other hores
seemed pretty systemic in nature we
already weren't a species that was
growing up quickly in a direction that
would be sufficient for this
stuff you know maybe this world that
diverged from ours in the 20th century
also just happened to do better with
lead contamination in the biosphere so
at an extra billion IQ points and less
lead damage people by the time they got
around to their 1970s
1980s even that doesn't seem fully
sufficient this is a really long mental
exercise but it's the method I found
helpful for how to start seeing the game
State we're in
correctly we don't live in that easier
world we have a pretty awful
civilization almost no one really gets
how bad of a situation we're in this
place is low trust low wisdom low
happiness low sec security etc etc etc
etc and anytime you stumble into an
uncertainty here you have to perform the
correct mental motion for integrating
that uncertainty into your world model
so you don't make the uncertain on
winner lose therefore 50% chance I win
the lottery
mistake that we are unsure of how things
will specifically play out in the future
fut doesn't invalidate having a bunch of
Nines in our P Doom in this specific
situation most uncertainties are over
spaces where
99.9 dot dot dot
99% of outcomes are
doom like uncertainty about how the AI
don't kill everyone policy battle is
going to turn out there are vanishingly
few PS into the future where we get a
sufficient amount of sane regulation
back by a sufficient amount of effort
since our Global policy loading systems
don't take neat instructions like don't
build ASI and then spin those up into
Global Order that does that
effectively even the much easier vanon
noyman challenge of having a stable
civilization where greater than one
group has nuclear weapons we are not
doing well enough at that our
civilization has a widespread and easily
digestible meme about no winners in
nuclear war and we've yet to manage to
create an equilibrium where nuclear war
seems like something that can't happen a
slowly ex escalating or accidental
nuclear war is a continuing risk because
there are few International Arrangements
that drive that probability to near zero
and our civilization doesn't take
correct strategic pictures as input and
then out output a process that steers us
into those narrow stable
regions so yeah I disagree with your
pdom I haven't found a compelling reason
to move the conversation about
reasonable pdom values away from debates
about how many nines you have in your
99.9 dot dot dot 99% probability of Doom
we've practically already lost this game
unless something genuinely super
unlikely happen
happens and yes I could just be wrong
about what is true about the universe
such that some part of my world model is
wrong in a way that makes Doom way less
likely and I should do a meta adjustment
for
humility except the precise amount of
humility you take here is actually
dependent on the plausibility of a truth
about the universe that makes this easy
which has the same nature is here for
you problem of expecting faster than
light travel to be out there and be a
physics feature that we can utilize in
space travel the universe we're in
doesn't look like that it doesn't look
like FTL is going to work out that way
and it doesn't look like the alignment
of an ASI is going to be easy for us I'd
be comfortable with a lot of Nines in
both
propositions nice man yeah that's that's
me just talking to you trying to explain
why I uh I feel a little weird about
saying 50% P Doom at this point or the
start of the reasons why I feel a little
weird about that to me the easiest
reason why I think P Doom is lower than
99% it's more like 50% maybe 60 70 80 90
once you push pom above 90 it just feels
crazy to me because like don't you think
first of all we might just not even
build AGI don't you think there's like a
few per chance of that for a few decades
we just don't have ai
yet
I
am more tempted to go into what I would
call sort
of optimistic visualizations of the
future like internally I'm I'm not
accusing you of this I'm saying that as
no no and by the way I'm just arguing
for like a few per here right I'm not
saying you got you got to give me like a
huge probability but like you got to
give me more than like 0.1%
yeah maybe at 40 years there's greater
than 0.1% chance that we'll still be
around just because we haven't built
something like this yet this isn't how
it feels to me and I think I have good
enough reasons to convince me that
things are going to happen sooner than
that but I also have much
more humility in this area than other
stuff because you know there's
maybe I I don't know what the actual
fearure is from people who try to
seriously estimate this stuff but maybe
a 0.1% chance that in that span of time
we go into total nuclear war and I think
total nuclear war would genuinely
disrupt the progress of
AGI um right which I personally think
it's something like 1% a year to go into
major nuclear war that might be the
figure that's fairly alarming if it is
but you know I haven't seen John Von
nyman's worry about how stable this
equilibria is been it hasn't been
falsified yet it doesn't look stable to
me even you know 100 years out or yeah
roughly 100 years out for when he was
talking about this
stuff um right so so when you say 99.99
it sounds like you've already
conditioned on the assumption that we're
going to have AGI and also we you know
Tech won't be disrupted by something
like a nuclear war right so you don't
even want to talk about okay because of
those probabilities maybe that makes pii
Doom only 90% but you're just saying
let's condition on all that it's more
interesting to say if we have AGI then
what's the be Doom right I think that's
fair um and I
think yeah I think I think me trying to
sort of Wiggle that through is a fair
call out um and it isn't what elaser
says and I'm trying to to say different
things than just what elazer says elazer
says we're going to get there eventually
that's not necessary to say different
things than what elazer says it's okay
yeah and I think uh a sensible person in
my position would just you know pull up
my favorite elaser blog post and freed
it for batim
um right um you know Roman yampolsky is
the notorious research scientist who
gives like a 99.999% I think he he's
willing to extend the nines out as far
as you like M um so you're coming in a
little bit lower than him but I think in
his case he's not conditioning the way
you are I think he's just saying that's
the probability that we'll eventually
get doom and he doesn't really talk
about a timeline yeah I I think what I
was doing with putting that timeline
there
was
I giving more of the flavor of what I
anticipate happening and what I strongly
anticipate happening and maybe adding a
bunch of nines to what I strongly
anticipate happening you know within my
lifetime is not the right approach here
uh because I am a silly little mortal
and cannot cannot reason over you
know amount of
evidence or cannot reason precisely
enough with the amount of evidence that
I have to give very confident answers
about that let's talk about uh living
with a high PE Doom what has been your
experience since you realize uh you came
to the confident conclusion that fedom
high it sucks uh it it doesn't suck all
the time but there's a lot of ways that
people look at the world and feel like
it's inadequate and feel like it's kind
of doomed but they have ways
of lessening that emotional burden on
themselves by hypothesizing a very
distant future where everything's okay I
think some number of people who
intellectually grasp that this is a very
dangerous place that humanity is headed
and we're headed there pretty quickly
and it doesn't look good for us in our
current
shape I think they still managed to keep
that pretty abstract in their own heads
and not really feel it impact their
anticipations of future experience or
things that were part of their emotional
core and I'm not a mind reader I I can't
quite say this confidently but it seems
like there's a sense in which people
aren't reacting strongly enough to this
that they don't really feel in their
bones that this is a thing that's
actually going to happen to us almost
certainly
and you know I think even if you feel a
20% chance in your bones like you
know it it's well I'm I'm one of those
people right because I I just I just
don't really feel it on a gut level
right so intellectually I mean I always
say my P Doom is high because I think
the arguments make a lot of sense right
I don't think we're building AI that we
can control
I think intelligence is the most
powerful thing right it's the key to
power you can put the animal in the cage
purely based on intelligence right it
can't put you in the cage and similarly
I do think that an AI running loose on
the internet that's smarter than us can
essentially imprison us do away with us
I I totally intellectually buy all that
and yet intuitively I'm just expecting
like the next Tech release right like oh
open AI has another announcement oh cool
they can do voice now right like
intuitively I'm like yeah we're just
going to keep getting these features and
yeah some of them will be kind of
dangerous but maybe it'll like happen
slow enough and they'll like somehow all
be kept under control and it'll be fine
like that's my intuition and but I think
my inion is
wrong yeah and I think this is an
important question because I think when
people make decisions about what they're
going to do in the future a big part of
the place they draw from is their sort
of wordless emotional anticipations
about what sort of value they're going
to experience in the future like you
know do I pick this career path or this
career path do I visualize myself in a
big beautiful house with a white fence
and three kids do I visualize my
grandchildren getting to do interesting
things with their lives um do I think
the future of humanity is cool or do I
think it's
scary um and I think in order to
motivate the correct decisionmaking in a
situation like this you really have to
feel that we
are you know walking towards an abyss
and a pretty Dreadful
Abyss there's a a digression here
about the game board the game board that
we're on what we're playing for what the
very far future could look like in
different
configurations I don't know if you want
to get into that but if you do prompt me
yeah sure tell me about the game
board okay so there's a bostom paper and
a paper by a guy named Toby or that
talks about this talks about the first
concept I'm going to invoke here the
bostom paper is called astronomical
waste the opportunity cost of delaying
technological
development and the Toby or paper which
references the Bostrom paper um but goes
into more of the calculations and and
models used to arrive at this picture of
you know civilization in the very far
future is called the edges of the
universe and the one part of both of
these papers is
is what is the size of the effable
Universe from our perspective
like if we got our act together and
started on just sort of the very basic
trajectory of building infrastructure in
outer space colonizing the solar system
colonizing other solar systems Von
noyman probes which is a space probe
that
contains within it all of the
prerequisites for building additional
space probes given some resources so you
send it off to another star system it
lands somewhere it builds more space
probes those
launch and the upshot of this is for
every year we delay leaving our solar
system on a trajectory to colonize
things as quickly as
feasible we are losing in our distant
future light
cone roughly three galaxies so every
year of delay roughly three galaxies
this isn't in terms of percentages a lot
of the galaxies we could affect in the
very distant
future um I think it's one part in 50
billion every
year yeah one part a small percentage so
you're kind of making be transhumanist
or even the accelerationist case of like
hey we we really got to get a move on of
building more powerful technology and
the other famous argument that even
transhumanists believe which I count
myself as a transhumanist is just like
hey a lot of people are dying like let's
try to prevent death let's try to solve
aging and better technology can help us
do that so are you on the same page of
like yes it is great to increase
technology it's just that some
particular Technologies are just going
to run out of control yeah I I don't
think these things are opposed at all I
think I don't agree with the
accelerationists in every domain but
when they point out a true thing you can
just say that's true and then you can
say here are some additional details
about the specific consequences of
developing this technology in this way
um but the the game board that I see and
the the thing that I try to tie my
emotions to as
tightly as I can is what is the fate of
those very distant stars is do we get
there do we you know delay a million
years and lose
0.002% of them um and just to be clear I
I'm talking about a delay of a million
years and only losing that very tiny
percentage of the it's three galaxies
per year it's thousands of stars every
second if you make a decision that
delays us getting there by even a second
you are losing Humanity thousands of
stars right and so by the way when we
have these kinds of conversations about
like oh my God we're losing thousands of
stars but it's still like less than one
in a billion of all the value in the
universe and even something that's more
closer to home which is like hey people
are dying like we know people who are
dying right and that's actually a much
more significant fraction of the Earth's
population are are dying right so like
150,000 a day
when we have these kinds of discussions
and they're often brought up by the
accelerationists being like look you got
to develop technology like the stakes
are too high of course the
counterargument is to bring in the
entire expected value equation and
there's a term in that equation saying
the probability that you're going to
save those lives as opposed to kill
everybody yeah and suddenly you see ah
yeah the probability that we're going to
kill everybody is very very high and as
nice as it is to save all those lives
killing everybody forever with high
probability really dominates every other
term in the calculation correct yeah
yeah I I think you just try to do the
expected value calculation and you try
to apply the best principles you have
for each term in that expected value
calculation I'm referencing one term
here which is the time it takes us to
launch those space probes outside of our
solar system but that is a separate
question than the question of what
number roots to that outcome end up
going through dangerous territory or
lethal territory and another way you can
lose all those stars in the future is by
going through leil territory and you
know spaceship earth we there are a lot
of things that could kill all of us and
then you lose all the
stars exactly okay so like when you talk
about AGI what do you think is like a
good concrete way to describe AGI
when you talk about definitions of
things like AGI we're trying to map
something that's out in
reality um and one of the stumbling
blocks you run into is the different
ways people interpret the semantics of
the definition that you come up with so
a lot of people talk about AGI being a
system such that that it can do every
economically valuable task that humans
can do and that could mean a bunch of
things and does mean a bunch of things
to different people I think when you say
that to people and I think this is true
of a lot of people currently attempting
to build
AGI they
imagine something like
a cluster of computers or even just a
singular laptop that you could plug into
a robot and have that robot you know
type on a typewriter or pull a
lever in some kind of like as part of
the normal coordinated process that
humans do to optimize our environment
that an AGI is something that fits into
any of the roles that humans are a
puzzle piece
for and I don't think there's a reason
to to imagine that that sort of system
is very distinguished in the space of
all possible Minds that we could build
with the hardware we have on Earth I
think if you try to aim for that system
you end up with something very different
that has the ability to do those things
not because it has you know a shape such
that it can fit into any of the puzzle
piece shapes that humans make up to do
their economically valuable work I think
it has a shape such that it could solve
a vastly greater array of
problems um right I mean I think we're
we're definitely seeing that convergence
right I mean we're seeing people take
like the same engines the same training
I mean I think the most recent example I
heard was you know physical intelligence
right so they're taking these robots and
they're being like hey look we can just
in English talk to the robot being like
hey I want you to pick up that shirt and
fold it and like that English is
actually helpful to translate that into
command somehow because it's like this
one unified engine that's just able to
process and predict just like all data
in general right we're seeing this kind
unification I would
be I would be somewhat surprised if they
did no fine tuning for that at all like
if you just took a base model and
somewhere in that base bottle training
process it had run into the semantics
for like manipulating actuator
and then you gave it an English language
problem with some additional in the
context Windows saying these are the
like these are the actuators we have
plugged into you these are the different
commands you can use to um to uh tune
those actuators and then it just output
an action policy that was able to Pilot
a robot yeah yeah no and I hear you
right and I'm sure that at this stage
they've got a lot of focus training
going on and they probably have like a
different data set for like feedback on
different actions but I'm just saying
more generally right we're seeing a
convergence like look the same chatbot
that is translating languages for you is
also able to answer your math questions
right answer your questions about
Plumbing right and it's the same chatbot
they didn't even train separate chat
Bots they're just like yep throw more
data in like will it blend kind of like
that um and then and you're seeing
multimodal models right so the fact that
even just images right the fact that you
can tell it's something to draw in
English give it feedback in English and
it can convert what you're saying in
English to the pixels that that it's
drawing like we're we're definitely just
seeing everything kind of falling into
this black hole like yeah the models
just want to learn like this this
architecture where we're just like
seeing you know unsupervised learning
we're just seeing a bunch of examples
predicting the next example feeding it
back into all these weights and then
there you go like it's just a giant
brain that understands everything right
like we're just seeing this unified
architecture yeah and I think the the
thing is that
there are many things you could imagine
happening um as you train these things
and I think a lot of this was discussed
with things like neural networks you
have things like
overfitting and all that kind of stuff
where it learns very specific slices of
these problems as they're represented in
the training data but what we are
observing as these things are being you
know refined by these Laboratories is
that there does seem to be
a deep pattern here of the things the
internal algorithms discovered by these
Labs through training these things fall
into an attractor Basin of
very simple um algorithms that
generalize to a broad variety of
problems
and if you push that hard enough such
that the thing can do every economically
valuable task I think youve created
something vastly
superhuman um not just you know a
general purpose you know human imitator
I think you have something that is a
general purpose Problem Solver such that
its problem domain goes Way Beyond what
even human civilization is collectively
able to optimize
yeah I agree with that okay why don't
you think the AI company's alignment
efforts will keep working as AI becomes
more
intelligent
okay this gets
into how much you need to
constrain
the different outputs that these systems
could have like what do do we mean by
alignment
and what sort
of inner mechanisms go into something
like that I think from a human
perspective sort of from a naive human
perspective
we have an intuitive sense that the
stuff we do is simple
that there's this sort of
anthropomorphizing kind of
intuition inside of us that somehow what
we're doing is very distinguished in
reality that our values form a natural
abstraction that other things that have
characteristics like us like
intelligence are going to also have
other features that we have
and I think there's a lot more detail
than that I think there's a lot more
specificity that you need to Target the
thing that humans are doing to the world
the thing that we're optimizing for in
the very far
future um I think things like launch a
solar probe beyond the solar system as
quickly as possible are not very
distinguished those are pretty
overdetermined by the shape of the
universe like the way these distant
galaxies are receding from our affable
Horizon means implies that you want to
send part of the material in a very
ordered
condition such that you can begin
optimizing things further and further
away in order to get more matter and
energy I think all that requires is
having some use of that much matter and
energy the thing that you do with those
galaxies the things that you do with
that matter and energy what you do along
the way in getting to those galaxies is
not very
determined um there are
some instrumental considerations that
are probably going to play a big part in
the shape of whatever process claims
those very distant galaxies
like there are efficiencies in the way
you can use matter and energy
that would probably be arrived at by a
very wide array of search processes like
things like
superconductors or things
like particular ways of transferring
heat energy that
are you know there's some boundary of
efficiency there where a wide variety of
search processes whether it be humans
working at science for a long time or a
vast array of different super
intelligences trying to optimize their
own particular value systems over those
very distinct galaxies like the space
probes might look pretty similar similar
from you know in these many worlds of
different types of super intelligences
that we build but the thing the space
probes are trying to accomplish is a
much larger configuration space and
right human value are complicated
there's complicated Machinery behind why
we value things the way that we do
caring about other people involves
complicated brain
Machinery
um
and that doesn't seem overdetermined as
a place a lot of different intelligences
would end up I'm rephrasing some things
here that other people have said but
hopefully there's you know some value
and just hearing me say it as
well yeah I think so one thing that
strikes me about you n this is you don't
have traditional credentials so you know
neither does elaz rowski you're just a
smart guy writing on the internet who I
feel like is making good points and you
actually have some views being in that
position of why it makes sense that
people like you should still have
something to contribute so tell me
more well I'm not going to speak for
everyone's decision-making process here
you can use theistic you use there is a
valid argument for gatekeeping
intellectual
conversations I think the way I would
push back at this is saying you have to
have an exception handling
procedure and the reason
is the specific characteristics that
you're using when you're deploying
credentials M or those kind of things
are lossy compared to the actual things
that you
value at as consequences of letting
people participate in a
conversation if you notice someone
saying something that sounds like it
makes sense to you that sounds like it
has a coherent thread of argumentation
valid argument steps even if they
haven't listed sources haven't listed
any of that kind of stuff if it sounds
reasonable to
you either one of two things is
happening either they've delved far
enough into this topic to good heart
past good heart's curse you know when a
Target becomes a measure it ceases to be
a good Target either theyve good-hearted
pasted your charistics about what
arguments sound like they make sense or
they're actually making an argument that
makesense sense and if you notice a
random Youtube commenter doing something
like that you can skip over the step of
asking whether they have the credentials
that kind of stuff and just investigate
your
own internal ability
to decide which arguments make sense
because either there's a flaw in that or
someone is saying something that you
should listen to I think in the computer
programming Fields one of the sort of
Innovations of the corporate culture was
to stop caring about credentials and
instead just try to directly measure the
merits of each individual contributor
yeah and I I think listeners to my
podcast respect owski even though he
doesn't even have a high school degree
for God's sakes and I think they respect
Goran I don't even think Goron has any
degrees uh I don't think gorin would
tell you I gor he may or may not yeah
yeah he doesn't flat whatever degrees he
has yeah and just just I I've read a bit
of gor I haven't read um enough of gor
to even guess how much he's written but
you know I go on gor.com like once a
month or something and just check out
you know an article from there all right
NE this so to recap you basically have a
very owski in position which is similar
to my own you're projecting forward you
think we're going to get get AGI you see
a lot of different outcomes where the
AGI just optimizes for some future
that's not what we want you're not
optimistic about
alignment and so you just have a very
high P doom and it sucks is that a good
recap or what would you add I'm
additionally not optimistic about the
societal response I don't think we have
mechanisms in culture governance all
that kind of stuff that are going to
respond to this in the very very precise
way that would let you escape almost
certain
Doom okay so last question what do you
think viewers of Doom debates can do
that's
productive um read more about the topic
I think that's a good way to start on
any path to being more productive about
this kind of stuff is just saying fewer
things that are nonsense saying fewer
things that have been refuted before say
fewer things that are based on invalid
argumentative steps making it so that
less people have to retread ground in
these kind of conversations what about
the puse AI
movement I think they're doing great
first of all I I not just saying this is
like turning on my political brain and
you know these people are my all can we
just do this again then how can we do it
again let's do it tighter how about I
ask you the question again and what's
productive and then you say well first
of all you know get informed like read
Al Kowski make sure you're contributing
the conversation and then also check out
P are you willing to give an answer like
maybe okay I I I I have I struggle to be
a joiner I I struggle to um I I do want
to get behind the thing that's getting
the closest which I think is pause AI
but when I imag myself doing something
like that it feels like a betrayal of
some part of my personality that like
has refused to call myself you know
Democrat Republican Socialist Communist
any of these kind of everybody you know
as I remember our Our Kind can't
cooperate remember the Alazar article I
do that was a whole part of the
sequences it was a it was a a whole um
subthread in the sequences about
matching the output of
Catholicism um and all this kind of
stuff yeah I I've read that it it
resonated with me but also like there's
something there's something that just
tries to drive me towards pointing at
that and saying great but do a few
things
differently um right well I I thought
that this was kind of a funny turnaround
where when I asked you what do you think
of P you seemed very positive on it and
then when I'm like like okay so how
about just tighten up your answer and
say check out POS the eye and you're
like whoa whoa you're like very
resistant yeah I think it's easier to
say something nice
when I think part of what I was
imagining there was me just coming out
and saying hey check out puse AI as
like implying that I had any involvement
in it implying that I had looked into it
deeply I I have read very little about
Pai I have heard some of their slogans
I've um seeing some of their protests
that they've organized not in person but
my main reaction to it
is they are doing something good and
that they are
directionally better than any other like
political movement on the topic that
I've
seen
but but there needs to be there needs to
be an inclusion of the actual points
that we are on a very dangerous
course
um the the concrete details of the
dangerous course that we're on like how
to get people in government that
actually grasp the Strategic picture of
the situation that we're in not just
feeding government a slogan like when
you feed government a slogan like
women's
suffrage there are
established conceptual Frameworks for
how to do something like suffrage they
know how to write those bills they know
how to push them forward they know what
Clauses they need to include what the
actual mechanisms of creating that
policy will do I think if you're going
to have a movement with a slogan about
pausing AI there needs to be quite a lot
of strategically informed detail in the
tactics that you use to get that
movement forward and you can't just give
government a slogan can't just give them
a visible push for taking the stuff more
seriously you have to have a very
specific thing that you want them to do
as a result of that and
that specific thing that they need to do
is just too long to just send them that
you need to send them a more compressed
version that involves you know what
situation are we really
in that makes sense and I understand
what you mean about how like you haven't
looked deeply into P AI you don't know
all the details so it's possible that
you wouldn't like some of the details so
that's why jump into being like join B
AI yeah it feels like you're not ready
to do that even though from what you've
seen you like it right so that's kind of
how we can reconcile the two responses
yeah I I'd rather respond to a question
about you know what do you think of
Paul's Ai and say looks good from what
I've seen so far but just having pause
AI come out of my mouth as something
that um as something I'm presenting as
coming from a internal well-reasoned you
know pros and cons sort of perspective
on pause AI is uh is a little weird for
me but all right viewers well if you
want to join pause aai or just check it
out see what kind of projects we're
doing what kind of topics we're
discussing you can go to pa.info click
the link to join the Discord worst case
you just get to join a really cool
Discord community and just have cool
people to chat with I recommend it
there's even a channel called # Doom Das
debates and I'm in there too so that's
actually the official Doom debates
Discord is actually Channel within puse
so there's another reason to just check
it out and get
involved cool all right yeah so nethys I
just want to thank you for being a
really good YouTube commenter really
distinguishing yourself with that
quality comment and then agreeing to
come on and chat with me in more detail
it's been great to have you yeah no
problem it was fun thank you for having
me all right viewers thanks for watching
Doom debates now stay tuned I want to
share with you another clip from this
video it's made by my friend Michael he
calls himself of lethal intelligence and
he has this amazing video that he spent
more than a thousand hours on to really
lay out the entire AI Doom argument
complete with really good animations
this clip is an overview of the concept
of agency it's talking about how once we
make chat Bots a little bit smarter so
they can just get a little bit better at
answering questions about what's a good
plan of action to do next then it's
pretty simple to go from Smart planning
chatbots to agents so it's making the
good point that agency isn't this
fundamental barrier the dangerous part
of an agent AI isn't the fact that it's
an agent it's the fact that it knows
what to do once you know what to do it's
just a trivial step to be like hey you
can you help me do this just do exactly
what I say go that part is actually
relatively easy so this video does a
good job pointing that out to the Layman
check it out currently AI commercial
projects of today mainly operate as
conversational chat Bots they are tools
waiting for a user input in order to
produce an output they do not have any
agency or long-term memory they can be
used for example as a creative device to
make art a business tool to generate
professional content or an efficient
replacement for a search engine an
oracle that can provide an answer to any
question the fast approaching AGI which
will be seriously worried about though
will not be a reactive tool waiting for
the next prompt it will be operating as
a deliberative
agent such an agent will be bootstrapped
with a set of primary objectives and
will be released to interact with our
environment in real life figure out sub
goals on its own break them down in
steps and use realtime feedback of the
impact its actions make in the real
world to update the inner workings of
such an agent will be using something
like the AI chatboard of today as a
building block it's a surprisingly
simple design that combines prompt
responses from a chpt like tool with a
feedback loop where the responses are
stored processed as instructions outside
and the result is fed back to it for the
next question one can visualize it as a
machine with two parts one is the CH GPT
like Oracle and the other functions as a
management
component the manager takes the primary
objective from the human as an input for
example make me
reach the manager part then prompts the
Oracle give me a step by-step plan for
how to get rich the Oracle responds
there is a plan with 2,000
steps the manager connects with the apis
executes the first step and returns to
the Oracle my goal is to become rich and
I have this plan of 2,000 steps I
executed the first step and I got these
results give me the new
plan the Oracle processes the planet
gave before together with the current
status of things and response here is an
updated plan with
1,982
executes The Next Step feeds the results
back to the Oracle prompt and
repeats this cycle can keep going on and
on for as long as it needs until the
goal is achieved if you zoom out a very
capable chat tool combined with a
feedback loop becomes a very powerful
agent potentially able to achieve
anything it can self- prompt and produce
every necessary step required to
complete any
task to turn the Oracle a reactive tool
into an autonomous agent is not a
complicated architecture in fact it
already happened multiple times almost
immediately after the release of the GPT
open- Source projects like Auto GPT baby
AGI and kpt spawn fast within weeks and
started running in the wild the only
reason these first prototypes have not
disrupted everything and aren dominating
the headlines yet is because this combo
is just very early stage the Oracle part
is still early version and the actions
it can take the hooks it has into the
internet and the real world are are also
very new and currently under
development but things are moving with
break next
speed and most experts agree that it's
quite probable that the real thing the
really clever General AI agent that's
executing its goals in the world
autonomously will arrive to
Earth very very
soon if you're just listening to the
audio of this you really should go on
YouTube and check out the lethal
intelligence guide the video version
because the animations are so
painstakingly crafted and there's so
much good stuff in there they tell a lot
of the story visually and again this is
really just the highest quality AI Doom
explainer I've ever seen so Props to
Michael from lethal intelligence
everybody click the link in the show
notes let's get lethal intelligence some
new subscribers that's it for today's
episode remember to head over to YouTube
and smack that subscribe button if you
haven't already also leave us a comment
let me know what you thought of the
episode and I'll see you right back here
very shortly on doomed to bakes