Ral is one of the foremost voices making
the claim that llms can't truly reason
these are essentially engram models the
old stochastic parrot claim in general
the creativity part is something that
llms are extremely good at we can make
like a ven diagram right like new
knowledge reasoning creativity and like
different experts who are granting that
AI do or don't have all of these magical
terms
welcome to Dun debates today I'm
reacting to an episode of machine
learning Street talk with Professor
subar Cati this is from a few months ago
I've been meaning to get to it for a
while because Ral is one of the foremost
voices making the claim that llms can't
truly reason so it'll be interesting to
dive into exactly why and what kind of
examples he can give and how strong I
think his AR is I'm looking forward to
listening to the podcast and making a
judgment on that some background info on
Professor kumati he is a professor of
computer science at Arizona State
University he studies fundamental
problems in planning and decision-making
motivated in particular by the
challenges of human aware AI systems uh
he has a PHD in computer science from
the University of Maryland and he's
pretty active on Twitter
r2z all right let's Dive In
okay so um you you've said that large
language models are engr models on
steroids approximate retrieval systems
or maybe even databases and they can't
reason or verify anything that they do
out of the box oh boy here it comes tell
us more so so first of all I think the
large language models they are trained
essentially in this autor regressive
fashion to be able to complete the next
word you know guess the next word that's
obviously correct that llms are trained
to complete the next word but now let's
watch ra make this huge logical leap
from llms are trained to complete the
next word to llms are basically just
statistical engram models here we go uh
n these are essentially engram models
they've been around since the time of
cloud Shannon the difference really is
those engram models for n equal to one
or two or three so we have pretty good
understanding of Byram models trigram
models if I say left and you tend to
think of right and that's sort of the
trigram model sort of a thing uh what
actually is happening here is that we
really even the lowly GPT
3.5 happens to be you know sort of a
3,000 G model so that means given last
3,000 words what's the most likely next
word it's the same idea except on huge
steroids a yes the old stochastic parrot
claim he's basically saying you know how
naive AI from 50 years ago is able to
just catalog the frequencies of
different pairs of words in a text well
llms also catalog the frequency of like
3,000 word phrases and then somehow
match them to other 3,000 were long
story short it's just word frequencies
okay it's just stochastic parrot so this
is super handwavy and he even kind of
admits why it's handwavy in this next
part the the the tricky part about this
is the number of 3,000 word
sequences if you have about a 50,000
word vocabulary is 50,000 power 3,000
and so if you are trying to keep
occurrent statistics which is the normal
way to do engram models you have we run
into two problems one of course is this
a humongous you know um set of rows for
which you need to have you know
conditional probability table and what's
the next word that's likely to come and
secondly the other problem is the
possibility that the same 3,000 words
will occur more than once is essentially
zero right exactly so the two problems
you just point it out are exactly the
reason why you're totally wrong that
these aren't engr models right so just
your review Rouses two problems because
he got them correct the first problem is
like he said nobody could realistically
build a lookup table where every Row in
the table has a string of 3,000 words in
it like a 3,000 G
because 10 the power of 3,000 or some
exponential like that is way too big to
fit in the memory of any computer so
that's the first problem and then the
second problem which is an even bigger
problem from my perspective is you don't
see the same 3,000 G again right I'm
just repeating what he said so he's
talking about doing statistics in a
space where it's not about frequency
it's not about statistics right so he
just undermined his own claim but now
supposedly he's going to rescue own
claim and make it make sense again make
it make sense Ralph so the thing that's
surprising about llms basically is we
essentially have you know are able to
train this really large Bas um and
there's huge amount of compression that
goes on so I tell students that you know
when you think of GPD 3.5 right people
will be surprised it has 176 billion
parameters and I think no you should
actually be so happy that it's only 176
billion
because if you really are trying to do
it in the full first principle sere you
would wind up having 50,000 power 3,000
that's actually Infinity you know in
some sense okay so it's somehow a big NR
statistical model that achieves a
compression Factor that's larger than
the number of atoms in the universe that
seems like such a big compression factor
that you're no longer talking about
statistics but go on explain how it
makes sense so because there's this huge
compression going on interestingly any
compression corresponds to some
generalization because you know you
compress so some number of rows for
which there would be zeros before now
they might be nonzeros um and so for the
some words and so this what we found
empirically is that with when you
complete with these trained models
they're giving completions that have
very interesting properties in
particular it's a generative you know
it's a sort of a generative a system and
so they capture distrib bution extremely
well and what that means is they capture
the style of the data on which they have
been trained on and as a um as English
as a second language speaker you know I
learned English with grammar rules which
kind of almost makes no sense but that's
you know your first language you don't
learn grammar you just learn to speak
language second language you tend to
follow some rules and exceptions and
rules and exceptions and that's kind of
pretty hard way to learn a language and
I was continually impressed when gpt3
and GPD 3.5 came around that they can't
make a grammatically incorrect sentence
okay hold on I feel like we're getting
lost here Ral hasn't Justified the claim
that gbd 3.5 or gbd4 is only a
statistical model that's really not much
more than an NR model he's talking about
compression and he's saying wow it's so
impressive that they've like compressed
it down to somehow a smaller statistical
model and it speaks English with like
the right statistical properties but
hold on a second my competing theory is
that it speaks English with good cical
properties because it actually
understands English because it's not an
engram model right that's my competing
hypothesis so if all he's doing is
pointing out hey it has good statistical
properties when you look at the output
right any good solution is going to have
good statistical properties what
happened to your claim that it's only an
endr model justify that so what the
point is grammar essentially is a sort
of a distributional characteristic and
they capture that grammar is a
distributional characteristic and they
capture that or what if grammar is not a
just actually capture the rules and the
underlying structure of grammar and then
they output it and then the output has a
distribution because it's correct have
you considered that alternative when I
was speaking English as a second
language I knew what I wanted to say I
knew the content of what I wanted to say
it was the damn language putting it in
the right grammar that was getting in
the way right and so I would tend to
assume that if something can actually
speak the language maybe the content is
easy because content was always natural
to me that is where people start
thinking llms because they can speak you
know write good high know grammatically
correct English know the good Farm
language of any kind you know
programming languages English etc etc
maybe they have already got the content
fine too because we actually find
content easier and the style harder okay
and that's actually the place from which
all these reasoning claims for llms and
the factuality claims for llms come and
and just they don't hold for angra
models it sounds like ral's
justification so far is just begging the
question he just went on a pretty long
monologue saying people think that they
understand the content and then they get
it grammatically correct and they just
assume that they understand underlying
content like they understand the meaning
of words so they assume that they're
reasoning but they're not they're engram
models okay we get your claim but can
you please justify your claim why are
they engram statistical models I'd like
to know people out there are convinced
that language model is reason I mean for
example I had Raphael Miler on the show
and and he was citing the example of um
orell and there was that paper where
this it's a board game you know kind of
similar to go and they said that it was
learning this abstract World model and
and it was generalizing why do people
believe so deeply that these things
reason so that's a very interesting
question um the tricky part about
reasoning is if you ask me a question
that requires reasoning and I gave an
answer to
you on the face of it you can never tell
whether I memorized the answer and gave
it to you or I actually reason from
first principles my favorite example is
in the old days when Microsoft started
having these interviews they apparently
used to ask this question as to why are
manhole covers round right and the very
first sucker who had to answer this
question basically had to do it from
first principles basically they had to
realize that pretty much any other shape
you can manipulate it such that the
cover falls through and you know round
is the only place where actually you
know if the keep that if you can play
with the covers and they fall through
then all these holes would be you know
coverless and this is a neat way of
thinking
but the first few people had to do this
right now when you know anybody ask this
question in
interview the only thing you will know
based on the answer of the candidate is
whether or not they prepared for the
interview prepared by looking up the web
and looking at what are the usual
Microsoft questions and of course humans
being humans they will also try to say
even you know that people want to know
whether you can reason and so if you
actually happen to know the answer if
you blurt it out that would give it away
so you would act take some time and then
blurt out the answer and so you can't
tell just by the end part whether I was
able to actually I was reasoning are
retrieving okay that whole example that
Ral just gave from my perspective is is
dripping with irony so in case you
missed it he's talking about that famous
interview question from like 20 years
ago where Microsoft would ask why are
manhole covers round and the answer that
everybody memorized is well if you had a
square manhole cover on a square manhole
passage maybe somebody would lift the
cover and rotate it and then the side of
the square would line up with like the
diagonal of the manhole and then it
would fall in and that's incredibly
dangerous that's kind of like the cach
answer that everybody's memorized part
of the irony is that there's actually
like five or six good answers for
example if you have a round manhole
cover you can roll the manhole cover so
you can transport it more easily also
when you have a truck driving over a
manhole if you have a circular manhole
cover it's Distributing the pressure
evenly which is good because you don't
want cracks to form around the corners
like it's it's just a more robust shape
the circle also the manufacturing
process for circular cover is simpler
because you don't need to line up the
corners of something like a square
during manufacturing and then when
you're placing the cover on top of the
manhole it's easier to align and also
when you're cutting the opening of the
manhole it's easier to do it with just a
single cutting tool that specialized for
just cutting a certain round radius just
all the way around the hole as opposed
to needing to make like four separate
Cuts right so the cutting can be simpler
and then there's reasons why the manhole
shaft is better off being round compared
to square like maybe that's easier to
cut maybe that is a more robust shape
that distributes pressure more evenly so
all of these different considerations
that I listed these are all dimensions
of the manhole cover shape design
problem right it's a multi-dimensional
problem it has connections to a lot of
other factors it's a kind of interesting
problem the irony is that all of these
things that I listed to that make the
problem kind of Rich they all came from
Claude I just typed the problem into
Claude and Claude gave me all of these
answers and it seems like Ral himself
didn't realize that the problem had
quite so many dimensions right he just
acted like it was a single input and a
single output and the reason Ralph
thinks like that is because he didn't
really do much reasoning when he thought
through this example right he just heard
the problem and he heard the solution
and he filed it away in his head as an
example of a problem with a single
solution that everybody memorized and
ironically there was a way to reason
through it and Ral didn't opt to do any
reasoning or he didn't even opt to go
and do like a broader web search to see
if other people had other ideas so
there's a little bit of irony there that
Ral himself without even thinking about
it is being the non- Reasoner here but
anyway regardless of the irony right
because you can disagree with me you
could be like no no no it's not ironic R
knows about the other reasons he's just
making an example right my point doesn't
look how ironic this is my real point is
yeah it's a bad interview question
because it's on Google because it's in
textbooks about how to interview and
because it's now at the point where
you're able to answer it without
reasoning if you want right if you care
that much about memorizing interview
questions then you don't need to reason
during the interview and so if Ral
himself were conducting an interview I
think he would know better than to use
that question he would use other
questions that actually test the
candidate's reasoning in real time of
course I think everybody can agree that
some questions don't require reasoning
and it's our job as people designing
questions to ask questions that do
require reasoning yes input output
question answer the question answer
relationship is one that sometimes can
be solved with reasoning and sometimes
not it has to do with the space of
possible answers that need to be
searched if a question appears in a
memorizable place like an interview
study hide then it may have no degrees
of freedom right you may just have to
regurgitate the memorized answer you can
do that in a single computation step
right a single memory lookup and you
don't need to reason now if it had a
bunch of degrees of freedom like if he
sketched out if the interviewer sketched
out like a complicated manhole shape and
said give me the pros and cons of this
complicated shape now maybe the only way
to answer about that exact manhole cover
shape might be to instead of memorizing
one answer you might have to memorize 10
different criteria of how you go about
analyzing a manhole cover shape right
how you go about analyzing the
manufacturing difficulty how you go
about analyzing the pressure
distribution once it's in place right so
now we've taken o of one or a single
ciation step now maybe we turned it into
like well now there's 10 different
checklists you go down and so it
requires more reasoning right there's no
shortcut the entire field of complexity
theory in computer science is all about
putting lower bounds on how many
computation steps you need to answer a
question and it doesn't really matter if
it's a human
answering a certain problem if it's you
know NP hard it's a certain complexity
Theory designation it doesn't matter if
it's a human attempting to solve an NB
hard problem or if it's a computer
attempting to solve an NB hard problem
you don't have to look inside what the
algorithm is doing you're just able to
say hey this problem anybody who solves
this problem must be doing some amount
of computation so similarly you can just
make an analogy from lower bounds on
required computation analogize that to
lower bounds on required
reasoning right and and that's what I'm
pushing back against Ral about he's
acting like question answer pairs aren't
sufficient to tell whether something is
reasoning or not no you can embed a lot
of evidence whether something is
reasoning just by noticing that they've
mapped a question to a good answer that
is something you could do and if you
take his own example of the interview
question yes it's a bad question don't
ask that question ask another question
like hey here's a piece of code you've
never seen before help me debug this
code right that's a question with many
degrees of freedom there's no way to
cheat on that question so ask good
questions that's my little rant but now
I think he's going to take the other
direction he's going to be like look you
can't ask a question to determine if
something's reasoning let's listen many
of these reasoning claims come from
Facts like llms are extremely good on
the standardized tests right and the
thing that people forget about
standardized tests is standardized tests
have standardized question Banks okay I
see where he's going with this he's
saying you get overly impressed seeing
AIS pass your benchmarks because you
don't realize that every question on The
Benchmark is similar to a Microsoft
interview asking you about a manhole
cover they're just going memorized
answer memorized answer memorized answer
so your whole set of questions is
actually these o of one questions every
single question is actually memorizable
because you aren't varying the questions
enough or you're not giving the
questions enough degrees of freedom
you're always asking the exact same
question you're always asking why
manhole covers round you're not giving a
dynamic variable type of manhole shape
when you ask the question and that's why
your test sources acceptible to
memorization I think that's his point
let's listen further and I keep telling
my students you think that the hard part
is answering the exam questions really
the hard part is coming up with
interesting exam questions that haven't
already been listed on the course hero
which is like this website of the kinds
of the websites that exist now where
they take every possible question that
has ever been asked on the exam or put
on a question bank with the answer and
so students can pretty much try to
prepare to the test rather than prepare
to understand the material and then
answer the questions sometime hold on
that's a different point his point right
now isn't that the round manhole cover
shape question should really be a
question about arbitrary manhole shapes
no no no his point right now apparently
is that writing test questions is harder
than answering test questions which is
crazy right I mean that's the inverse of
the usual P versus NP claim that people
make right usually P versus NP is the
observation that answering a question is
exponentially harder than asking it in a
lot of General cases right that's why we
have the P doesn't equal NP conjecture
right he's saying no no no no no asking
the question is so hard if you can ask
the question then answering it is maybe
not as hard like it's I don't know if
I'm putting words in his mouth but he's
really emphasizing right now the
difficulty of coming up with a new
question to ask which traditionally
isn't thought of as a high computational
complexity thing to be able to ask a
question and again this isn't the point
I expected to hear from him so I do feel
like he's shifting between multiple
points I'm not sure he's focusing on the
same point for long enough for me to
understand what his justification is
let's continue to listen so this
ultimately winds up being in many cases
where people people think llms are
reasoning it winds up being the case
that they're actually doing approximate
retrieval so his claim is show me a
really impressive example of an llm
answering a question that looks like
it's using reasoning for instance maybe
you give it what you think is a novel
medical case study like here's my
situation doctor help me about this
unique combination of symptoms and the
llm outputs well based on this
combination of symptoms I think this and
this condition may be my two top
hypothesis for what might be wrong with
you and this kind of problem was
unsolved until very recently right it's
only very recently that AIS are getting
better scores or similar scores on
medical exams than humans but it sounds
like Rouse's claim right now is okay you
take those examples that you're so
impressed by there's actually going to
be something in their training Corpus
like some sample exam or something that
they actually memorized and you actually
just asked a question that was very
similar to the thing they memorized
right that's basically his claim right
count now how do you show that the usual
way you get to show that is by a
diagonalization argument you almost for
example I'll give you an examples I'm
very interested to see this example of
proving by diagonalization that what
looks like llm reasoning is actually
llms pulling up some memorized test
answer let's see so I am interested in
planning capabilities of large language
models planning is a form of reasoning
it's a form of reasoning involving time
and action um so one very simple kind of
planning problems that people in AI
planning have been looking at are things
like block stacking that is you have a
bunch of blocks in some configuration in
the initial State they could be just
named blocks or colored blocks and you
want to have a different configuration
in the goal State and there's a bunch of
actions such as picking up putting down
stack unstack right given these actions
given this blocks world problem can you
come up with a sequence of actions that
actually will lead to the goal State
that's the planning problem block
stacking is typically a straightforward
reasoning problem like early computers
in the 1950s could follow a pretty
simple set of rules to derive what
series of actions could stack blocks a
certain way right typically the hard
part of getting a robot to stack blocks
turns out to be like looking at the
blocks and identifying where to place
its hand to pick up the block right it's
kind of like the fuzzy part of the
blocks not like the rigid logic of which
block to place where but okay let's run
with this example
interestingly Enough original llms like
gpt3 Etc were suck were very bad at this
GPT 3.5 in fact and GP gpt3 in fact I
think in 2022 we wrote this thing uh
when everybody is making claims about
planning capabilities that no they can't
in fact they get close to 6% accuracy
they're just guessing and then the the
action sequence won't lead them to go
yeah so there's always been kind of an
impedance mismatch between llms and
these kind of pure logic problems or
pure math problems you know like
factoring a number if you give gpt2 or
gpt3 one of these earlier word
completion AIS and you said Hey how do I
stack these blocks or how do I multiply
these numbers or how do I factor this
number it would kind of be coasting
right the same as like a human who
doesn't really understand math or logic
and you're just like hey what's 52 * 43
and they're just like ah you carry the
five and then you get 3 1 n like they're
kind of like bsing it right they're kind
of just like trying to trick you by
spitting something out that sounds
convincing right that's normally the mo
of llms but with these smarter llms they
have like a big enough Corpus where they
start to get trains to to predict the
answer better like an answer that makes
more sense so they can't just like BS
some random digits at you because
they've seen enough examples in their
training Corpus where somebody's
factoring a number and the answer has
more justification like no no no the
factors are this because there's this
internal structure like they do a little
bit of actual multiplication or actual
addition or actual representation of
block stacking right so so things come
online that are more than like shallow
word statistics because that's the whole
scaling hypothesis right you scale these
AIS to be larger and they stop being so
surface level like they make the
prediction more accurate because they
actually have a deeper understanding and
thereby they get a better score right
their loss function gets a lower value
right because they're doing better by
actually understanding stuff so I'm not
surprised that he's using block stacking
as an example of how the earlier gpts
didn't do well but as we approach the
limit of smarter gpts they seem to be
using true reasoning to solve it or
aren't they let's see his claim that
they still aren't interestingly when GPT
4 came and you might remember that
Sebastian buck and coote this the
notorious Sparks paper and so we were
interested seeing maybe are there the
Sparks going to help the planning
capability so we checked GPD 4
interestingly it increased in accuracy
it's still nowhere near 100% but it went
from something like 6% to close to 30%
you know so you can argue that wow if if
GPT 4 is 30% GPT 5 would be maybe 70%
yes I think we should all make that
argument as something that has a very
significant probability I think we
should all wonder if the next scaled up
llm will have higher than 30% accuracy
on this particular type of test problem
and I don't think anybody can conf
defitely say that it won't and by GPT 10
you can get 150% accuracy of some sort
right I get that that's supposed to be
funny to talk about 150% accuracy but
he's also being very dismissive of a
perfectly reasonable
hypothesis the question that you we
asked ourselves is can you explain this
potentially in terms of just the
retrieval as again as actually reasoning
if you are doing reasoning then if I
change I take the blocks world and
change the names things like instead of
Stack I will say fist instead of you
know unstack I will say slap something
like that so if you know if you have any
background in logic you realize that the
predicate names don't change the
Dynamics of the system and so any
planner which can solve the original
domain we can solve this with exactly
the same efficiency and
accuracy you give this to um jp4 it dies
and in fact we wrote this paper called
plan bench paper for new rips 2023 which
essentially shows both the blocks World
Logistics kinds of planning problems as
well as these obfuscated versions of
them and it's almost like a fun sideline
for us that Karthik walikum who is the
lead author on that paper he keeps
running every new s um llm GPT 4 and um
you know Claude and and um you know um
Gemini and they all basically are stuck
close to
zero
because they are essentially guessing
the plans for blocks world because they
have the word you know they have the the
usual words associated with blocks World
which are stacking unstacking Etc and
there's enough of the data for that in
the in the web scale carpa but you
change the words they complete Ely lost
so that is a great way of showing
essentially that they are not doing
reasoning because if you are doing
reasoning you should be able to solve
this just as easily as the original one
so in
general I can understand why people
easily fall for this because typically
as I said um when people answer a
question that requires reasoning you
can't tell just on the face of it
whether they just happen to have heard
the question on the answer before they
came into the room and they just blurted
it out or did they actually start from
that question and answer it by you know
reasoning okay there is actually some
empirical substance to the argument he's
making right now I haven't looked into
his blocks World paper that he's talking
about but I have played around with some
other examples that he's given in the
past like a year ago I have seen some
examples where if you go ask the AI a
question sometimes you can change the
words a little bit and it will screw it
up when it's kind of surprising that
it's screwing it up like wait a minute
if it got the original question
shouldn't it still get this small
variation so he is onto an empirical
phenomenon here right like sometimes
that is a real thing but it's not always
that easy to srup AI like we do have
many examples where you ask it a
question it's a pretty hard question the
AI Nails it you ask it a variation of
the question that it's never seen before
it Nails it the variation is still hard
but it Nails it you ask another
variation it Nails it you ask another
variation oh it messed up that variation
it didn't get perfect you ask another
variation it Nails it right so that's
the behavior we see it's like yes
sometimes you screw it up and it's like
wow why did that screw it up and when it
screws up it kind of makes it makes you
suspect like the whole illusion got
shattered and you're like oh man you
know you you just you betrayed me right
now you betrayed me I trusted you like
you were truly reasoning but then I
modified the problem and suddenly you
had no idea what I was talking about and
that proves to me that you were faking
it when I thought you were reasoning I
feel like that is r's argu right now
right like the fact that it doesn't
robustly move with you to every
variation that seems like a small
variation shatters the whole illusion
and Rouse mind right that's kind of my
attempt to paraphrase his point but
here's the thing it's not a requirement
to nail every variation it's already
impressive to have even a decent rate
like a 30% rate of getting a bunch of
different hard questions right you can't
dismiss that getting those hard
questions right is ridiculously hard
that doesn't prove that it memor it
doesn't shatter the illusion like the
fact that you changed and then it got it
wrong you can't then go revisit the
instance that it got right and dismiss
it no no no no no those instances that
it's getting right like you know in the
case of like the medical exam question
let's say it got one medical case study
correct and then I slightly change the
words I obate a little bit and then it
gets it wrong that doesn't let me
dismiss the medical case study that it
got right because the one that it got
right it wasn't like a multiple choice
question where you guess a b or c no to
come up with the correct hypothesis
there's like a million different answers
right so it was doing the one in a
million guesss and it hadn't seen it
before it wasn't literally copied out of
a book it was a big variation right
there were a lot of degrees of freedom
changes from anything it had ever seen
in an example set of test questions
before so that's really where I diverge
from Ral I remain impressed at the
instances of difficult problems that
it's getting right even if I see trivial
modifications then leading to it getting
it wrong that does not shatter Theus
that it's actually reasoning for the
ones that it gets right and then you can
ask well what if it mostly gets it wrong
and it's rare for it to get it right and
I think that is the heart of the matter
is how often can it get things right and
the ones that it gets right how
impressive is it that it got to the
right answer there's obviously questions
that are very hard that if you'd asked
me five or seven or 10 years ago I'd be
like no that's way harder than anything
AI can do like we don't have a way to
shortcut getting this answer right we
don't have a way to memorize these test
questions and get them right even with
hundreds of billions parameters that's
not going to get you the right answer
right so they're clearly doing
impressive things and it does not negate
that just because some variations of it
they're not getting right I will give
him this I do think that it does
represent a limitation an imperfection
right the fact that you can change some
keywords and then have it some of the
times screw up the ai's answer I do
think that indicates that they're not
perfect yet right I don't think we have
super intelligence yet I don't think we
have ai that can beat even a a moderate
human at like like skills right I don't
think we're in the singularity yet so
he's raising a good point there but to
treat it as in his words a
diagonalization argument or some kind of
proof that they're not reasoning is
going way too far it might be
interesting to have a small digression
on on what reasoning is now I I'll give
you my my definition so I think it is
performing an effective computation to
derive knowledge or achieve a goal using
a world model in all existing semantics
so you know you can think of it very
Loosely as you know deriving uh kind of
truths that we don't currently know if
that makes sense I feel like Tim is kind
of throwing the kitchen sink into this
definition so first of all he says
derive knowledge or achieve a goal those
things are kind of different are they
both sufficient conditions so if I just
derive some knowledge and I don't
achieve a goal I'm reasoning if I
achieve a goal but I don't derive any
knowledge I'm reasoning can the goal be
really simple what if the goal is to
just say anything right and then I
successfully said anything so that I
just do reasoning did I do a trivial
form of reasoning is there a scale of
reasoning if there's a scale of
reasoning shouldn't we just Define the
metric instead of try to use like a
black and white definition so I'm not
really vibing here with Tim's definition
of reasoning it does kind of seem like
buzz word Bingo from my perspective I
haven't even finished reading his
definition he says uh Drive knowledge or
achieve a goal using a world model and
all existing semantics okay but again it
could be like a trivial goal like I'm
not clear on where his definition really
starts and ends and the last thing Tim
said is you can think of it as very
Loosely deriving truth that we don't
currently know so you really have to
drive truth so we don't currently know
just to do reasoning like it doesn't
count as reasoning when I said hey I
better check my email so I can find the
coupon code for my instacart grocery
order like I wasn't reasoning I was just
completing a meaningless pattern I don't
know it felt like reasoning it felt like
I was chaining together some inferential
steps in order to do something useful in
my life
right the kind of thing that an AI
wouldn't even be super robust at doing
today because it's kind of hard like
that doesn't count as reasoning or maybe
it's like an easy form of reasoning I
don't know I just I'm not getting a lot
of productive value out of Tim's
definition and of course if you watch
the channel you know my preferred
definition where it's really just
whatever you do in the middle to answer
sufficiently hard questions so what I do
is I punt all the meaty goodness of
reasoning I punt it all to to the
mapping between the question and answer
I think that if you have a trivial
question it's not interesting to ask how
much reasoning it takes to answer it so
if you have an interview question like
the manhole cover and you know that the
correct answer from the Microsoft
interviewer's perspective is that you
don't want the manhole cover to
accidentally fall into the manhole and
that's why it has to be round then
that's a a single computational step
right that's a single lookup and so
asking that question in a context where
the exact answer is in the data dat it
doesn't require reasoning because
there's this trivial shortcut so you
have to ask a question that doesn't have
any shortcut if you want to claim that
reasoning isn't required to solve the
question I'm pretty adamant about this
like I will kind of die on this hill
that reasoning is only an interesting
concept when you have a non-trivial
input output map okay and now once you
do have a non-trivial input output map
and there's no cheating in the
environment like the environment doesn't
have like some ready-made solution that
you just grab so that the input output
map actually does become trivial right
so for example if I ask you like what is
the prime factorization of and then I
give you like this thousand digigit
number Well normally I'd be very
impressed if you could figure it out
because factoring is thought to be a
very hard problem but if it turns out
that I asked you the question because I
multiplied two large primes and then you
found my Post-It note where I wrote down
the two numbers I multiplied and you
just repeat back to me what you saw in
the Post-it note like you're basically
cheating if you do that then I'll be
like okay well the question I asked you
just happen to be a lot easier because
this extra context made it easier so
again I'm only impressed by the amount
of reasoning that it takes to answer a
question if something is fundamentally
hard about the question like if it has a
high computational complexity right like
if it has like a known lower bound or a
conjectured lower Bound in the case of
factoring it's kind of funny to me how
like the Tim scarf of the world or Keith
Dugger if you go and watch my debate
with Keith a lot of these guys will just
not adopt my definition where it matters
so much about the input and output of
what you're doing they really insist on
trying to open the black box and being
like okay if you did the right actions
while you were solving the problem then
I'm going to give you credit for
reasoning even without referring to what
the problem was like that that's more of
their approach right they're more
focused on the process whereas I'm more
focused on the relationship between the
starting point in the destination right
like people who say like oh it's not
about the destination it's about the
journey I'm really much more about the
difficulty of the destination regardless
of the details of the journey it seems
like R is on the uh Tim scarf and Keith
Dugger side of things where Ral I think
really wants to open the black box and
look into the process before he's going
to be willing to count something as true
reasoning but let's hear what he has to
say yeah so I would go back to I mean
first of all reasoning is pretty well
defined from a logical perspective I
mean that's one particular kind of
reasoning that we have understood from
the Greeks on right and you know that I
always find that it's useful to just go
back to that even though you know
there's all sorts of reasoning logical
reasoning is certainly one very
important kind of reasoning and as you
said in the case of logic given a set of
Base facts you are able you're trying to
see whether some new facts will follow
in the sense is the the deductive
closure of the knowledge that you have
are these other things present okay but
if you're just talking about about
deductive reasoning we've had computers
that can perfectly deductively reason
since the 1950s right so what is the
missing piece here like you want to say
that today's llms can't reason they can
certainly just call functions that can
deductively reason perfectly right they
can punt over to algebra solvers or all
kinds of deductive reasoners so why are
we talking about this as the definition
of the secret sauce that AIS don't have
when again if you just focus on the
difficulty of the problem you're just
talking about problems that were solved
in the 1950s right like logically
deducing things is something that we all
know computers are much better than
humans at right computers tend to be
better than humans at logic they tend to
be better than humans at stacking blocks
right better than humans at finding the
shortest route better than humans at
playing a heavily logical game like
chess so where are you going with this
Ral this is actually very important
distinction so everybody in AI used to
know uh logical background um you know
but nowadays most of the grad students
actually start with just deep learning
courses and machine learning courses
this is very important technology I'm
not questioning that at all but you need
to understand that the in versus out of
distribution the usual terminology that
people use in machine learning that's
for databases for reasoning you need to
think in terms of deductive databases at
the minimum which is you're not just
looking to see where whether the stuff
in the database you know basically
whether you are whether the query that
you're asking is the same distribution
as the kind of queries that you already
presented previously I don't accept that
premise that what llms are doing is
checking whether their inputs are in
distribution of stuff that they've seen
before I think they're getting novel
inputs all the time it's Rouse premise
that they're not it's Rous premise that
there's some sort of statistical
relationship between novel queries that
you and I are asking to our AIS Ral
claims that they somehow fall inside of
a statistical distribution of inputs
that these LMS are saying I don't find
that to be a meaningful claim I haven't
heard R justifying it but he's kind of
moving on taking it as an assumption
that there's some sort of statistical
thing happening here and that and then
he's going to contrast that with it
sounds like he's talking about uh
prologue or data log right these kind of
deductive databases and and he's drawing
a contrast but I'm just not following
his original premise about lm's being
statisical but let's continue instead
you're looking to see whether you
whether the system is able to compute
things in the deductive closure okay
that's what winds up requiring logical
reasoning and there's no reason to
believe that LMS can do that um and one
of the very interesting first of all
there are actually multiple increasingly
more and more evidence showing that this
definitely can't do it you know we've
been saying that um as a Cory of the
fact that they can't do planning they
clearly must be failing in doing um you
know um deductive closure but there
people have even shown recently there
was a paper which talks about um for a
transitive closure transitive closure is
a poor cousin of deductive closure which
is if you let say a P1 B P2 C okay you
should be able to say a P1 P2 C if you
can't do that you certainly can't do
deductive closure because transitive
closure is a really small part of
deductive closure and and so that's one
way of realizing that llms cannot do
reasoning okay here's another example
where there is some empirical weight to
this like I've seen examples of trying
to tell llms to follow computer program
instructions step by step and what I see
is that the llm kind of starts doing it
kind of gets it but then pretty quickly
messes up and then once it messes up it
tends to kind of spiral into messing up
more and more right it doesn't really
fix itself so what he's saying about hey
they can't even do transitive closure
they can't do deductive closure it
sounds very similar to what I've
witnessed where it's like hey it pretty
quickly screws up a problem that seems
like it should be able to solv because
of how smartly it's talking but you know
it was all Just an Illusion because it
can't even know that like the law of
transitivity applies to a few variables
right like it messes up something that
seems kind of simple and then it's like
game over and then that proves that it
like never knew anything because it
can't even do the simple building block
and yet once in a while you give it like
a harder problem and it somehow tells
you the answer to the harder problem
right even though you kind of prove
that it doesn't even understand the
basics well somehow it got the answer
right to a harder problem okay somehow
it happened I always talk about how I
don't see a fundamental separation in
the kind of reasoning that llms can do
versus can't do I don't see a
fundamental barrier but I do see them
not being robust and in my mind what
would Advance the next generation of
llms is to just like mess up less often
which is what I would describe as the
step from GPT 3.5 to GPD 4 I would
describe that step as like yeah it's
doing a lot of the same stuff and
messing up less often right I do see
that progression so from my perspective
when R is pointing out like look at this
test we did where it messed up so many
of these problems yeah I agree it
sometimes messes up but again it's also
impressive that sometimes you do give it
instances of the same problem and it
does get it right so it's not like you
have a simple problem that it never gets
right right I'm certain I haven't seen
the paper but I'm quite certain that
you're just talking about a fraction of
the times it's getting it wrong right
it's not always getting it right but
it's not always getting it wrong either
so when R gives examples of things llms
can do it's the point I said before that
doesn't disprove that doesn't say ah so
they were never reasoning all along I
feel like that is the Crux of our
disagreement because when I see certain
inputs novel inputs complex inputs
somehow getting mapped to correct
outputs I'm pretty much ready to say
okay it was reasoning like that's pretty
much all reasoning is in my book because
you can't fake that you can't fake your
way to An Answer train novel medical
case study try it yourself right go try
to take the medical exam and and have
like the biggest lookup table in the
world and try to pass a medical exam
like it's not easy because again if it
was easy we would just have used a giant
lookup table in the year 2010 right it
wouldn't have taken us until the 2020s
to do this lookup table hack so again
the Crux of disagreement between me and
Ral seems like Ral throws the idea of
reasoning out the window when he sees
examples that look easy to him that
aren't being solved whereas I focus more
on the miracle that some hard novel
problems are able to get solved and I
say the strong points tell you that
reasoning must have happened even if
they go together with weak points a
passing score on the bar exam or a
medical licensing exam those kind of
passing scores you never get those by
chance you can take the bar exam
multiple choice every day of your life
and you're never going to pass the bar
exam right so the fact that the AIS are
passing on these kind of hard exams that
to me is so much basy evidence right
it's so much impossible to fake
legitimate evidence that they're doing
complex reasoning that if they then go
on to choke on a bunch of really easy
problems that's still okay I'm still
convinced that they're reasoning I don't
throw out the evidence that they're
reasoning the way ra seems to be doing
now people say but no there are things
that llms are doing that are not
considered base facts right like I claim
passing a medical licensing exam full of
case studies is more than regurgitating
base facts right that's why we couldn't
do it until recently but that actually
is because we don't really understand
the training data the training data is
not something that we put together it is
the entire web and everybody thinks that
they know what's on the web nobody
really understands how many things are
on the web the answer key to every
Year's medical licensing exams are all
on the web and the training data when
there's a new medical license exam the
questions on that exam are all just
copied from exact questions on the web
so I keep remembering this thing that
when Palm the old Google llm came about
one of its great claims to fame is it
could explain jokes why is that like an
AI task is beyond me because it requires
semantic understanding of Concepts that
are being referenced by words in the
joke usually with a ton of ambiguity
where the only way to dreference the
pointer of what the joke is talking
about is to actually have a web of
interconnected knowledge about a bunch
of different concepts what many of us
call solving the symbol grounding
problem so again this is incredibly
impressive that AIS in the year 2024 can
explain arbitrary jokes right like you
can make up a new joke you can have the
AI explain it I've seen it explain some
pretty crazy jokes that even humans
might have trouble explaining and it
gets it like we have AIS that get things
so maybe that answers Rouse's question
as to why we test AIS on their ability
to explain jokes seems pretty clear to
me why we do that but it is surprising
that it could kind of explain jokes and
so you'll be very surprised wow it is
actually able to do some sort of thing
that's beyond the data but really I
don't know whether you know this but
there are humor challenged people in the
world and there are websites that
explain jokes and these websites are
part of the entire web crawl that the
system has been trained on and because
of which it's not that surprising so
reality is in normal logical terms you
would think that I give base facts and
the system says something that's not
typically a base fact then it must have
done some kind of a reasoning but often
times the web
corpora contain both what we would
consider base facts and what we would
consider parts of the deductive closure
in in interesting you know mishmash way
and so when once in a while it actually
winds up retrieving the stuff from the
deductive closure you think it's
actually reasoning because you think it
it couldn't have probably been in the
web that's because you don't know
actually what is on the web it's like
it's very hard to show that what is e
and what is not on the web Ral is making
a very bold claim here he's saying AI
explain jokes because they've been
crawling various joke explainer websites
ites and they're really just
regurgitating explanations of jokes that
some human has written okay R just to
prove you wrong I took a joke that I
thought up myself it's actually a joke
that I tweeted a couple months ago and
it actually went viral so it's a pretty
good joke it's very dark humor I joked
about the October 7th Nova Music
Festival Massacre so it's kind of
gallows humor um the joke is October 7th
changed everything Billy mcfar
just changes LinkedIn to say he's the
founder of the second worst music
festival okay it worked better as a
tweet like I said it actually went viral
so I didn't delete it because a bunch of
Jews were liking it I'm Jewish myself so
it's okay to joke about it uh but it's
obviously a huge tragedy right so I
don't know maybe it is too soon uh but
anyway listen the point of this is that
it's a novel joke okay it's proven to be
entertaining to humans based on the
Twitter virality it also has no result
on Google explaining it which isn't
surprising because I personally thought
of it so I type it into Claude I say
Claude explain this joke October 7th
change everything Billy McFarland just
changes Lincoln to say he's the founder
of the second worst music festival what
do you say Claude and Claude says this
joke references two separate events one
October 7th 2023 marked the Hamas attack
on Israel a tragic event that had major
geopolitical ramifications two Billy
McFarland was the organizer of the
infamous fire f Festival in 2017 which
became known as one of the biggest
Festival failures in history the event
promised a luxury Music Festival
experience in the Bahamas but descended
into chaos with inadequate
accommodations food and infrastructure
it resulted in multiple lawsuits and
McFarland's imprisonment for fraud the
joke implies that McFarland is making a
dark self-deprecating reference on
LinkedIn by suggesting that his
disastrous Festival is no longer the
worst music festival in light of the
October 7 attack which occurred during
the Supernova Music Festival in Israel
right so that's what I was comparing it
to the humor comes from the
inappropriate and tasteless nature of
McFarland attempting to make light of
his own scandal in relation to a tragic
event the joke points out the stark
contrast between what was merely a
fraudulent business failure fire and a
devastating humanitarian tragedy right
exactly Claude completely understood my
joke and it's never seen that example
before in the training data what do you
say to that r i mean to me this answer
is wildly impressive and to be honest I
typed it into gbt 4 too and gbt 4 didn't
quite get it it was just saying well
you're just supposing the fire Festival
was some kind of event that happened on
October 7th and I'm like do you know
what happened on October 7th and it was
a little bit confused it didn't make the
connection to the Supernova Music
Festival but again it doesn't matter
what the AI sometimes can't do it
matters that once in a while they have
this kind of stroke of Genius I mean Cod
got it on the first try and this is
brilliant right this is a brilliant
inference this is the kind of thing that
the people on Twitter read it and they
got the inference that I was making and
they thought it was worth retweeting
right and and and laughing at I mean
this is like the essence of humor right
this is like part of what makes us human
and here's Claude just nailing it on the
first try right within like seconds of
spitting out this output right Claud 3.5
son it so I I I just don't see what Ral
is getting at he's kind of taking the
Pinnacle the Highlight the crowning
achievement of what today's llms are
capable of doing and then he's going to
downplay it by claiming that it's coming
from some existing knowledge base that a
human wrote and by the way when I posted
that joke on Twitter I I didn't just
write it out I made like a fake
screenshot where you're looking at Billy
McFarland's LinkedIn and you're seeing
that he's like changed his bio to talk
about the second worst music festival
anyway too soon right I mean Israeli
hostages are still being held like you
know I feel bad even bringing this up
now but it is a really good example of
Novel joke so I'm going to keep it in
let's continue um I actually have an
experiment that this Tom Griffiths group
did that I really repeat to a lot of
people that again makes the same kind of
a point which is one of the Sparks that
um the original gp4 paper talks about is
that gp4 can do Cipher text decoding
right and Caesar Cipher decoding and so
what these guys basically did is check
you know because Cesar Cipher decoding
just basically based on like an offet
right so you take a plus four then you
take the know four letters off from a
and then replace a by that that's the
usual thing so for each offset there's a
code and so they try to see how well
does the system gp4 decode for different
offsets 1 2 3 4 all the way to 25 and
surprisingly they find there's a huge
Peak at 13 and it's basically close to
zero anywhere else and why is that
interesting if you remember if you're
old enough you'll remember that in Unix
there used to be this there is still a
command called R 13 rotate by 13 which
is while Caesar Cipher is a general idea
13 is the special case you know that is
rotating Letters by 13 is a special case
that Caesar Cipher is more famous for
and this tons and tons of data of normal
text and rotated by 13 text on the web
so that's the part that LM does well and
it doesn't do well for 2 three four all
the way and any other number but okay so
the empirical observation is that GPT 4
is going to be better at spitting out
Caesar Cipher decodings when it's rough
13 when it's shifted by exactly 13
letters as opposed to like four or five
yeah this is totally plausible it maps
to what's called system one in humans
right so in humans there are certain
things that we just kind of process
natively with more ease and fluency
there's things where there's more
shallow patterns that we're just used to
seeing over and over again and so they
become intuitive for us like if you're a
human if you're used to reading a lot of
English have you ever done one of those
tests where you see a bunch of words and
the letters in the words are scrambled
around but you just kind of recognize
that bag of words and so you can kind of
read like it's no problem well imagine
you're learning Spanish right and you're
not as familiar with the words you'd
probably do a lot worse at the bag of
words test but if you had more time to
unscramble the words you'd probably
still do okay right you'd just take like
10 times more time it's very much the
same thing with an llm and rot 13 so it
just has kind of an intuition a shallow
pattern matching for a bunch of words
that it's already seen R 13 before so
they kind of flash into its next
character right it can it can kind of
get it as part of its like Smooth stream
of Consciousness it can kind of Coast
the way you as a human might Coast if
you're just reading these jumbled up
letters in a language that you're
familiar with like it's no problem the
answer just comes to you right I don't
think that there's a profound Insight of
like wow gp4 doesn't truly understand
the Caesar Cipher it only pattern
matches rot 13 and it's totally confused
about a Caesar Cipher with a step size
of five I really don't think that's the
case because you can dialogue with gp4
with Claud you can dialogue about what
is the Caesar Cipher how do I apply the
Caesar Cipher what is the step by step
let's think step by step and let's get
the Caesar Cipher for this word so if it
brings system to online right this kind
of explicit understanding of what the
Caesar cyer is it's going to nail it and
it's also going to be able to write
python code to do the Caesar Cipher the
same way that a human might essentially
write code for themselves right give
themselves instructions like okay 1 2 3
4 five I'm moving the letter by five I
know how to do this right so to me this
isn't damning evidence of like oh my God
a a can't truly reason because they have
like a a better pattern match for rot 13
than rot four or five like this actually
it's almost like a human thing that
they're doing right like I identify as
having a human brain that can sometimes
Coast that can sometimes do things
comfortably right like if I see piano
sheet music I play the piano so if I see
like a chord that I'm familiar with my
fingers will just kind of snap to where
they need to be and if you show me piano
music that's in like a really weird
style that I've never played before
suddenly I'm like really slow and I have
a harder time with it am I just pattern
matching no I'm reasoning about the
music right I'm I'm a fully General
musical no player it's just that some
configurations just load right into my
fingers right because the whole end to
end the all the levels of the hierarchy
are are just like tuned to process this
particularly efficiently but now Ral is
going to take this evidence about the
Caesar Cipher in rot 13 and he's going
to try to generalize a point which gets
back to the Crux of our disagreement I
think he is drawing the wrong lesson
from this kind of thing let's listen and
I think if I might be so bold as to say
what's somewhat wrong about the kind of
empirical studies uh nms is that people
tend to stop at the first sign where
they get interesting results and I think
you have to be very skeptical you have
to assume by thinking that the system
does not have you know certain
capabilities and then actually try to
you know poke holes rather than write a
paper right away no sorry I disagree
there's certain individual tests that if
you see an llm passing it that's a
Eureka moment when I wrote a viral tweet
making light of the October 7 Massacre
and humanitarian crisis that's still
ongoing when I joked around about that
and then CLA 3.5 son it instantly
unpacked why it was funny that is a
Eureka moment you can write a research
paper about just that if I then go and
tweak the joke and Cloud 3.5 son it
suddenly doesn't understand it anymore
it fails to get the reference it doesn't
matter it doesn't matter that single
example is incredible there's nothing
that can dampen the sheer excitement of
getting that one particular example
right it's like if I hide a bar of gold
in the Sara desert and I have a friend I
don't tell the friend where it is the
friend flies into the Sahara Desert at
some random location somehow goes to the
right location where the gold is and
grabs it that is a very interesting
thing that you can write a research
paper about like how the hell did that
happen that begs for an explanation that
is not a phenomenon that you can expect
just by having that friend you know they
studied a bunch of maps before like what
no something begs for an explanation so
just to reiterate the Crux between me
and Professor Ral is that Ral thinks
that you can undo this kind of
achievement he thinks that as long as
you can find variations that it doesn't
get then it shatters the illusion of the
original achievement but I'm telling you
nothing can shatter how impressed I am
with the fact that it just unpacked my
novel joke nothing can shatter that that
is a real reasoning achievement R um and
currently we are going through a hype
cycle where essentially both in research
and in you know commercial terms where
people are just happy to write llms are
zero shot XXX where XXX is reasoning
planning something else and mental
modeling etc etc and it turns out for
most of these you can find that really
you can explain the results from an
approximate retrieval point of view
because by diagonalizing in fact as I
mentioned the planning where you change
the names that's a diagonalization
argument what these guys did in checking
whether it can do Cipher text decoding
for something other than 13 is another
diagonalization argument because if you
know the general principle you should be
able to do it if on the other hand
you're memorizing you won't be able to
do it and so that's a classical way of
finding but you know showing that it's
not doing reasoning but I agree that
typically in general it's easier to be
impressed because coming up with these
diagonalization arguments is hard but
from a scientific perspective we need to
see you know whether or not they
actually have these capabilities so yeah
what Ral just said is exactly what I'm
saying is the Crux of disagreement
between me and him so I think we've
nailed it I'm just going to beat a dead
horse right now I'm just going to reread
you the last thing Claude 3.5 Sonet said
when I gave it my joke and just keep in
mind Ral is telling you that this is not
real reasoning that this is just
memorization here's that last sentence
from quad the joke points out the stark
fraudulent business failure the fire
festival and a devastating humanitarian
tragedy the Supernova Music Festival
Claud figured out the deep meaning of
the joke Ral when Ral talks about a
diagonalization argument if I understand
correctly he's saying ah yes you can
take an example like this but then you
can build on it in a way that cloud will
no longer work but how does that
undermine the achievement that it just
did coming up with such and a good
perfectly OnPoint analysis there
literally nothing that I would add to
this analysis it completely understood
every little Nuance of this joke the
fact that Billy McFarland posting it on
LinkedIn probably represents a
light-hearted self-awareness by
McFarland himself and that that's
inappropriate like all of those layers
are as correct as they could possibly be
and you're telling me that this is just
some lookup on the internet some what
let me go use his term what do you call
it he calls it approximate retrieval
like oh yeah everything that CLA just
explained about that joke that was just
approximate retrieval I've diagonalized
away the idea that it was doing
reasoning guys don't don't worry
nothing's going on here just approximate
retrieval just statistics are you
kidding me the statistics of comparing
the Nova music festival in Israel to
Billy McFarland's fire Festival that's
in your statistical distribution that's
just using engrams to figure out what
the correct analysis of that joke is
come on man look at the actual example
there's many like this right that when
it passes benchmarks it's doing this
kind of stuff this is deep up this is
all there is when it comes to being
intelligent it seems like this kind of
stuff is pretty much all there is like
it's not clear what else there is
besides just like doing it more robustly
that's it I don't know what else there
is yeah I mean a few thoughts on that I
mean um it's interesting that people are
not more skeptical I mean my my religion
is skepticism yeah I think so science is
supposed to be skeptical in general this
part's annoying because they just keep
going under the assumption that Ral is
correct at downplaying the the reasoning
achievement of gp4 and Claud and then
under the assumption that Ral is right
and we're wrong now you have to
psychoanalyze Lon like why can't Lon see
how much of a stochastic parrot gp4 is
let's psycho analyze Lon in general
humans are not good at giving credit to
anything in general they would like to
say it is their idea except strangely
enough with llms they're willing to give
credit to llm than to themselves because
I think my cynical way of saying looking
at it is if you say llm did it you get a
newps paper if you say you did it who
cares you know you it's not supposed to
be impressive yeah so I'm just some guy
who likes giving credit to llms whether
they're output is a slightly modified
version of some joke explanation that
they saw on the web or a totally novel
right on the nose zero shot unpacking of
the exact joke that I personally came up
with yeah whatever I just like giving
the llms credit that's just my
psychology that's just my desire to
write a Europe's paper you got me R to
some extent people are suspending their
disbelief too easily and that is not a
good thing for science I mean you know
if you're doing it if I'm a startup
clearly yeah the whole point is let's
hope that the world suspends their
disbelief while I make my Millions right
but science it's a different thing and
so one of the interesting questions is
what's going to be the half-life period
of many of these llms are zero shot XXX
kind of papers I have this in tongue
full full on tongue and cheek I have
said on Twitter that when some of these
papers are shown to be kind of wrong
they should lose their citations right
now what happens is is the bad papers
which actually made claims that actually
were proven to be wrong they still keep
gaining citations because people say
this said this and it was wrong I think
one way of Designing mechanism is that
if I prove your idea to be wrong I
should get all your citations for the
bad that's to the extent for us the
citations is the currency I mean I mean
obviously I'm being factious but in
general that reduces the incentive for
people to rush to print and uh and hope
that maybe you know basically they may
be right and even if it's not right they
will get some number of some amount of
um attention you want to talk about
accountability for people's research
papers look at your papers ra I've
actually tweeted to ra a couple times
about this which is I don't see Ral
making a falsifiable prediction in any
of these papers like when he says hey
these can't really reason when they
explain jokes they're just using
memorized answers I'm not seeing him put
a stake in the ground being like hey
here is my own Benchmark here are some
questions that have degrees of freedom
that aren't on the internet and I will
privately test the next Ai and if it
passes these questions then that'll
actually falsify my research because I
predict that it won't answer these
questions and of course you know show
those kind of questions to an
independent auditor right so he can't
cheat or you know maybe publish them
online but ask the AI companies not to
use them like whatever it is I don't see
him making a falsifiable prediction I
see I may be wrong but from my
perspective I see Rouse Mo as being to
always look backwards to always
retroactively turn around and be like
hey look I found situations where
current AIS aren't answering something
right and all the way until super
intelligence you're always going to be
able to turn around and be like aha
there's still a gap between the current
Ai and super intelligence it's more
inconsistent at zero shotting these
particular versions of Caesar Cipher
right like you can always find something
but BR like let's put a Stak on the
ground I would love to see you predict
something falsifiable I had the same
problem on Twitter with Martin cassado
he also is somebody who thinks he knows
a lot of limitations about AI but it's
very hard to get him to predict
something confidently that AI won't be
able to do in the next couple years in
the case of Martin cassado he engaged
with me enough to give me an excuse why
he can't do it he said I can't tell you
what AI is going to do in two years
because I don't know what data it's
going to be basing its answers off so I
have no answers for you and I try to
keep the conversation going I try to say
well can you make a statement where
conditioned on the AI being able to do
something we should expect to find a
particular type of data or can you say
hey assuming that it doesn't have this
type of data that I can give you some
description of assuming that that
doesn't exist like this novel source of
data then I bet that it still won't be
able to solve this like I bet it won't
be able to win gold and math Olympiad
unless this other new type of data set
gets created can you at least say that
but he stopped responding at that point
so I see both of these individuals as
very much not participating in in the
kind of productive constraints on on how
humans need to do science right because
humans we're all biased we all want to
be right we all want to have the ego
trip of being like look I knew this was
going to happen I call it right I don't
fought you for for having that ego I
mean I feel the same way where like I
like to be right I like to be credited
for predicting things in advance I hear
you but like R the spotlight really
should be on you right now to tell us
what will the next AI what will the next
generation of AI not be able to do the
reason I think this is so important is
because I think that if R had only seen
GPT 3.5 which couldn't quite unpack the
joke that I told I think or or maybe
gbd3 couldn't quite do it I don't
actually know exactly where the
threshold is for gpds being able to
unpack the the the joke I told all I
know is that Claude can do it really
easily right now if you'd only shown Ral
gp3 I strongly suspect R would be like
ah yes understanding this kind of novel
joke is infinitely beyond the
capabilities of llms because it's not in
their training Corpus to me that sounds
very much in character for R to say that
I don't want to tell you for sure that I
know that he wouldn't say that even he
doesn't know if he would have said that
or not because it's too late right like
we all already know that Claude can do
it but like I think R is very much just
using confirmation bias and just
retroactively acting like he knows
something about what AI can't do but
he's cheating and looking at the results
of what they can and can't do I don't
think he ever makes predictions about
about what they can and can't do and so
people like him unfortunately are always
going to be around from the present all
the way up until the time of super
intelligence you know recursively
self-improving super intelligence you're
always going to have people like route
at every stage being like aha I knew
that it can't do this after peing at
what it can and can't do I want to bring
in creativity so first of all you know
when we do deduction we are applying
these rules that could work in any
situation but if you think about it we
can compose these deductive rules into
this big kind of tree into a big graph
it's similar to in the physics world we
talk of the physics World being causally
closed but we're talking about like a a
deductive closure and what is the role
of creativity when we Traverse this
graph that's that's first of all that's
an an excellent question in in general
this actually so
despite what the tenor of our
conversation until now I actually think
llms are brilliant it's just they're
brilliant for what they can do and just
I don't complain that they can't do
reason use them for what they are good
at which is
unconstrained idea generation okay I
reject that characterization of what
llms are good at I mean yeah they are
good at idea generation but if you look
at my example of explaining my joke that
was highly constrained and yet it nailed
it right so it's perfectly good at
constrained other person's idea
explanation and understanding but okay
unconstrained idea generation what do
you have to say about that in pretty
much every
human field you know fields of endeavor
you have this interesting G position
between the creative idea generation and
the
laborious deductive idea checking okay
even in math this is true Forma came up
with this conjecture and of course also
claimed that he proved it and and R
while spent 17 years of his life 200
years later to actually prove that what
Fara was saying is actually you know
provable with respect to the
mathematical XMS you know with some math
that was invented much later than forat
right this is kind of funny it seems
like Ral is doing a kind of reversal
that I pointed out before if I give you
two different scenarios scenario number
one some intelligent agent says hey look
at this equation a the N plus b the Nal
C the N I wonder if I could conjecture
something about it having integer
Solutions with arbitrarily large values
of that would be kind of interesting hm
I wonder and then scenario two some
other intelligent agent is like okay I'm
supposed to prove that a the N plus b
the N equals c to the N doesn't have any
solutions under certain values of n all
right let me just crunch through this
let me see what it's going to take to
prove it so you have these two phases of
the prove right coming up with what to
prove and then crunching through and
proving it I feel like most people would
want to say ah yes the crunching part is
more of the domain of computers and AIS
but look at the creative part that is
uniquely human so there's a little bit
of a reversal now when R is saying look
AI can help you generate this idea that
a to the N plus b the N equals c to the
N might be interesting to try to prove
but you need a human to Crunch through
what the proof is going to be a little
bit of a reversal right now of course
he's going to argue well no Andrew WS he
had to invent new fields of math to
prove it so it's you know it's not a
great example but I am detecting a
little bit of a funny reversal anyway
math requires both the creativity as
well as deductive thing and sometimes
the same mathematician has both of them
sometimes it's also the case that some
mathematicians are better at the actual
proof part and some are better at seeing
the creative connection ramanujan the
guy from India is like the most famous
of his lot because he seemed to have had
a creative gen RoR that almost had a
much higher density of being correct and
so he would just write these huge crazy
uh Know series and write the answer with
no proof and you know GH Hardy basically
famously said they must be true because
if there weren't nobody would have the
imagination to write this he was
obviously being fous actually no he
wasn't being factious what he just said
about Raman jang's conjectures the some
of them being so brilliantly inspired
that how could they not be correct
that's largely true that's actually an
important Insight uh it applies to
science too uh when Einstein came up
with the special and general theories of
Relativity his attitude was like look
this math fits so well to the
constraints that I needed it to fit it's
very likely right you guys can go do
your experiments but like I know this is
right because it's so over constrained
like when I found something that fits my
constraints this was so hard to find and
so unque among what it could possibly be
that I think I got it guys I think this
is it and it's the same thing presumably
with the search process that happened
inside of ramanujan's brilliant
subconscious brain like he's plucking
out these conjectures that the amount of
constraints they had to satisfy to get
promoted to something ramanujan would
conjecture he's already done a lot of
the work he's already traversed a lot of
the bits of basy and evidence that you
need promoting the hypothesis locating
it in the space of possible hypotheses
a lot of times you've collected enough
bits of evidence just to know what
hypothesis to say that the extra bits of
evidence you then need from your
environment to promote it all the way to
being super sure about it those last few
bits yeah those are important but
there's still a small fraction of all
the evidence you've already collected to
even know that you should name that
hypothesis so in the case of Einstein
coming up with theory of relativity most
of the basian evidence that he needed to
believe that the theory of relativity
was a true description of observation he
already had those bits even before the
first experiment was conducted this is
actually a correct description of how
super intelligence's reason it's okay to
have this much evidence before you
collect empirical data so that part of
what GH Hardy is saying seems to have
gone over Rouse's head because he called
it being factious I really think that
he's missing something here and it's the
same thing with Claude explaining my
joke right to the the fact that it
nailed this one problem to me gives me
very strong evidence that it has what it
takes to nail many problems and if you
vary the problem and show it getting it
wrong like I keep repeating that does
not undo the amount of evidence that I
gained by seeing it do one problem of
this level of complexity right you do
not get one problem of this level of
complexity right by coincidence again
that's the Crux of our disagreement R in
general the creativity part is something
that llms are extremely good at there's
that funny IAL again Ral is so intent on
telling us that AI can't reason that
he's giving away like okay yeah so
they're great at this creativity thing
okay so now they're just creative and
actually T use his precise language
before he said unconstrained idea
generation is the sense in which they're
creative right so it has to be
unconstrained and it has to be
generation okay but then if he's
comparing it to the example of like
ramanujan and FMA coming up with
conjectures of things to prove is that
really uncon strained because these guys
have a high hit rate right so it's not
just like hey dump out any mathematical
proposition no it's dump out a
mathematical proposition that seems
sufficiently high probability of being
true even though how interesting it is
and that is quite a Showcase of what you
call unconstrained idea generation I
would argue that having a high
likelihood of being true is a huge
constraint that you're calling un
constraint idea generation right and now
you're you're saying like Okay well it's
it's creatively coming up with these
propositions it's like um and you're
telling me this isn't reasoning like
it's just popping out these propositions
that have a super high probability of
being true and aren't found in any
previous textbooks like this is your
boundary that you're saying llms can do
versus things that they can't do they
can just be Geniuses at telling you
mathematical propositions that have an
unusually high chance of being true I'm
pretty confused there's nothing wrong
with having ideas the only thing that
Civilization needs is that when you get
the idea you check whether the idea
works that's the robustness aspect if it
doesn't work you give up on this idea go
for something else and we typically find
that verification of an idea is easier
having the idea is harder okay what Ral
is saying now I think he is actually
confusing himself because he's invoking
this idea of P versus NP the idea that
often times finding a solution to a
problem can be really hard but then once
you've got the solution verifying it is
easy and he's making a connection he
made it earlier to like hey look at
forma's Last Theorem it was relatively
easy for FMA to kind of have the brain
wave of like oh this would be fun to
prove but then Andrew WS had to spend
like 17 years and invent new math and it
was actually really hard for him to
prove and originally Ralph's point was
that AI could have helped do stuff like
what FMA did it could have helped
generate these ideas for cool things to
prove and then it takes a real human
genius who can reason to search through
the space of possible proofs and prove
that theorem because stating a theorem
that's not the hard search Problem the
hard search problem is actually cing
through the space of proofs and finding
a valid proof and then you get an easy
verification problem which is to go step
by step through the proof as written and
verify that the found proof is a correct
proof right so there's actually there's
three different steps right there's
coming up with a conjecture that you
think would be cool to try to prove and
then there's doing all that hard work
and Labor uh a creative labor of finding
a proof if you can or maybe proving its
negation and then the third step is the
easy step of just reading through the
proof and verifying that like yes this
is aov I think he's getting himself
confused because Ral what exactly is the
part that AI can easily do versus the
part that AI can't do because back when
we were talking about forma's Last
Theorem you made it sound like ah yes AI
can help generate the theorem AI cannot
search the space of possible proofs but
what are you saying now about what AI
can do and what humans can do in fact in
our lifetime in our lives we would have
friends sometimes who are F of ideas so
you get stuck you ask you know hey Tom
do you have an idea for on how to D this
Tom may not have a big stake in this but
they are good at generating ideas
they'll give an idea and it's your
responsibility to check if it actually
works and we when does when it does work
we tend to give much more credit to Tom
uh because Tom gave the idea but Tom
didn't prove that the idea was right
okay you proved it and it's easy for us
to kind of verify and we think idea
generation is the more important thing
llms are actually good for the idea
generation and they're actually bad for
the thing that is easy for us to some
extent which is
verification yeah I'm still confused in
the example with Tom okay Tom threw out
a conjecture and then
for you to find a proof of Tom's
conjecture in general is a very hard
search problem so you put in that Sweat
Right you solve this potentially
exponentially sized search based problem
and then yeah once you solved it you can
show your proof to people like Tom and
they can easily verify it so maybe Tom
had some way to easily throw out
potential candidate things you could
prove and then Tom had no problem
verifying your proof because proofs are
easily verifiable okay but is the
hardest thing in this whole interaction
your own sweat your own search for a
proof I think Ral is getting confused
and he's starting to use the term
verification in the context of like oh
yeah you verified Tom's conjecture by
searching for a proof of it right so
that I feel like he's confusing himself
verification I don't think he meant to
say llms are bad at verification I think
he meant to say LMS are bad at doing The
Logical search work right at searching
the exponentially branching tree of
possible logical proofs okay llms are
bad at that I think that was his point
and then he started getting confused and
saying llms are bad at verification
which is something that should be easy
for everybody and and so in in general
that's sort of the creativity part is
making the inductive leaps to cut down
the search you know um so that once you
if you just make an idea and you don't
actually check if it works then it's
just an idea okay and but then once you
kind if it if you have hopefully a uh a
generator of ideas that has a higher
density of good ideas it can reduce the
search quite significantly in terms of
and so that's sort of the way creativity
and the reasoning come together so
inductive versus deductive leaps right
and inductive leap versus dedu inductive
closure okay the inductive leaps
basically are all the imagination all
the creativity part and llms are
actually quite good at some of those
things because not not it's no skin of
their nose they're giving you ideas you
ask and they will give you ideas but you
have to check whether that idea actually
makes sense in the context of all the
constraints of the of the problem if I
have a problem I want to solve and
there's a lot of different constraints
as part of the problem and somebody
through out an idea that I want to check
and I just take that idea and I just put
it into an llm prompt I write the idea I
write all the different constraints that
it has to solve I write the problem and
I asked the LM hey what do you think do
you think this idea might be plausible
with these
constraints why shouldn't we expect the
llm to give us a pretty good answer why
shouldn't we expect the llm to work with
all of these constraints I mean when it
analyzed my joke it was working under a
lot of constraints I was like hey I
wrote something I think it's funny
there's some connection here find the
connection tell me what it is tell me
why it's funny right I mean what's
fundamentally the problem here I think
before R was trying to say well the
problem is that if there's a logical
deductive search involved llms aren't
good at that but he's kind of talked
himself into this other cran where it's
like oh I bet it's going to have a hard
time verifying if a candidate creative
solution obeys enough constraints like
what are you talking about like what is
the fundamental distinction here I'm
completely confused you know in general
people in like more um more creative
Fields like things like design and like
engineering design which straddle the
creativity aspect as well as the
realizability aspect architecture is
another one they tend to almost say that
during the ideation phase you need to be
unconstrained and then then once you get
good ideas then you have to make sure
that actually the building which looks
like that can be supported
with the kind of strengths of materials
that we currently have otherwise it's
just a beautiful looking building that
doesn't you cannot build it and and so
this second part is equally important
and you know one of the hope is when
people you know uh do architecture
courses let's say they tend to constrain
their generator their ideation phase
such that they're more likely to come up
with you know ideas which are likely to
be realizable
okay I still have no idea how he would
analyze the creativity and the reasoning
involved in suggesting something like
the conjecture of fma's Last Theorem
because these kind of conjectures are
only interesting when your hit rate
isn't just 50% right the thing about
forma's Last Theorem is that the
statement of the theorem you know a to
the N plus b to the N equals c to the n
i I mean the way the formula looks it
looks like it's taken out of God's book
of formulas that might be true uh you
know the mouth Edition Paul Eros talks
about like God's book of elegant proofs
it's such an elegant statement and it
seems like you're really learning
something about math if you learn that
that is true a typical proposition might
sound more like this hey I bet there are
infinitely many primes P such that p +
one is divisible by the sum of the
digits of P that's kind of cool but
whether you learn the answers one way or
the other it just doesn't feel like
you've learned quite as much about
numbers as you have when you look at
forma's Last Theorem because like some
of their digits that might be kind of
contingent on base 10 it might just be
like some random trick it's probably
going to be easy to prove and that's the
thing about for Maas theorem the very
fact that it turned out to be really
hard to prove that actually is evidence
of like how important it is right like
how much underlying structure it must
have that it's kind of like on the
borderline of like you can imagine that
number theory is almost willing to let
it be false right it's not hellbent on
making it true I don't know there there
just it feels like there's something
very significant about it like if it's a
candidate for the GH hearty reaction of
like wow the fact that you even thought
to scroll this in the margin to me makes
me feel like you're on to something
right there's something a little bit
more special about that than the uh Su
of the Prime digits thing that I just
said where I really just pulled it out
of my butt I'm like okay yeah adding
digits together whatever anyway back to
my confusion about Ral Ral keeps talking
about how cool it is to creatively
generate uh candidate
hypotheses but he's not EXP explicitly
addressing the hit rate that we need out
of these hypotheses right because if you
have a very very low hit rate what uses
the creativity the creativity is useful
to the degree that the hit rate is high
imagine that you're trying to write a
research paper and you're trying to get
your PhD or you're trying to get
published and you've got this helpful
assistant the job of the assistant is to
do what Ralph says the AI assistant is
good at which is to generate candidate
hypotheses that you can then prove and
you can verify the proof it job is to be
the creative one starting but the
problem is every time it spits something
out for you you start working on it and
you realize it's a dud and this happens
like a thousand times either it's like
trivial to prove true or false or maybe
it's like too hard to prove true and if
you do manage to prove true it's just
like an ugly proof that doesn't deserve
to make it in the research paper like
the point is in some sense you've got
this creative generator but the hit rate
of what you need is low I feel like that
undermines what Ral is claiming that
these creative systems are useful so I
think there's hidden assumption in what
Ralph's saying which is that like okay
sure yeah AI can be creative as long as
it's unconstrained but wait no it is
constrained like there's some useful
constraint that you're giving it that
it's hitting it's hitting a Target right
and then is he claiming that hitting a
Target is merely creative but it's not
reasoning like you don't think there's
some underlying reasoning going on
that's enabling the AI to consistently
hit a Target in a scenario where it's
hitting a Target right like I just this
important kind of discussion seems to be
totally missing from Rous and 's right
he seems to be leaning on this
distinction where you can have a system
that's just unconstrained idea
generation but he's not acknowledging
wait a minute if it's fully
unconstrained it's not useful if it's
useful then it must be operating under
some constraints right he's not
addressing that that's my way of looking
at where the creativity and the
reasoning part come in and it's very
relevant with llms because I would use
llms for their creative things because
not mostly because ideas require
knowledge it's like ideation requires
shallow knowledge a shallow knowledge of
very wide
scope idea generation requires shallow
knowledge of a wide scope so when
Einstein generated the theory of
relativity that was shallow knowledge of
a wide scope seems like he had to
research pretty deeply into what
constraints he was trying to solve like
invent the constraints
explore a bunch of math to figure out
which math was able to fit you know like
meowski space right like the particular
set of equations that got him all the
properties he wanted you're telling me
that that was a shallow knowledge of a
very wide space it sounds like you're
just thinking about certain examples
right like okay fine maybe if I think of
a joke I'm just making connections where
I have a shallow understanding of the
different things involved fine I agree
sometimes creativity is a search where
the knowledge of the different things
you're searching is shallow of course
but but that's not a defining property
that's not a characteristic of
creativity right it just seems like a
convenient thing that is supporting your
handwavy model but isn't fundamental to
intelligence so why would you say that R
like how confident are you about this
assertion because if it's a loadbearing
assertion of your whole mental model of
what creativity is what reasoning is if
you're that confident that idea
generation always makes shallow
connections then we need a new term to
Define what it is that made Einstein's
breakthrough possible because I don't
think you've accurately described it
analogies are a great case where llms
actually do better than normal you know
people a man on the street and to do
analogy you need to have at least
significant amount of knowledge about
the world so if you want to sort of
analogize analogize how things in
Finland will work knowing how things in
India will work you need to know
something about these countries and not
everybody might know them so they may
not be able to construct those analogies
whereas llms actually have been trained
on the entire world's data you know and
and so they can make these kinds of
shallow
unguaranteed um leaps and then you have
to pick up and check whether or not that
thing holds so that's one great place
for them okay clearly analogies
themselves aren't shallow because the
deepest thinking is performed by
analogies right we just just call them
deep analogies it sounds like the
substantive claim Ral is making now is
the claim that llm thinking is always
shallow like llms never produce output
based on a deep thinking of any kind
which if you ask me it was somewhat deep
the way that it interpreted my joke or
like it's somewhat deep the way AI can
process a medical case study or even
just start following the instructions of
the factoring algorithm like not
robustly but the fact that they're
starting to follow it I would call that
somewhat deep right like the ability to
go step by step I'd call that somewhat
deep so the only new piece of what R is
saying here is he's saying well of
course llms are going to be good at some
analogies the shallow ones because
they're shallow pattern matching they're
doing something with Statistics and
engrams that are suddenly letting them
make analogies work I think Ral is
probably implicitly granting that using
Vector embeddings is doing a lot of
heavy lifting right so like can Rouse
model of the world sure
yeah okay we've grounded these symbols
and we're making analogies but it's just
because Vector eddings are really
powerful right so like sure they kind of
locate the true meaning of the word in
Vector space but they don't do much
beyond that I feel like his Rouse
position right I would be surprised if
he didn't grant that they at least do
that much so that's kind of interesting
that he's not explicitly acknowledging
how amazing that part is right he's just
skipping over to being able to criticize
how little analogical reasoning they do
past that point anyway if you if you
don't mind I want to just gently push on
on this um creativity thing so um you
said that um what was the term you Ed
yeah that uh in in order to generate
ideas we we need knowledge but reasoning
is about creating new knowledge and if
you think about it we could just be
completely open-ended when we generate
our ideas and that wouldn't be very good
or it can be guided by Intuition or
existing knowledge but I think there's a
difference between inventive creative
ity and combinatorial creativity so I
think there should be some form of
reasoning even in the creativity um
itself yeah I mean if you want to
characterize llms as only being useful
for idea generation you really have to
explain why the hit rate of their idea
generation is high right I didn't ask
Claude to generate ideas for what my
joke could mean I just asked it to give
me the best guess and the best guess was
right and I feel like the best guess is
often going to be right on a lot of
these jokes similarly with medical
diagnosis if if you ask it hey give me
the top five things that might be wrong
with me you're going to get a high hit
rate right at least as good as any
doctor right that's what the Benchmark
is showing that in a lot of these case
studies they're matching the human
doctor at listing things that are
correct to diagnose you with right so is
that unconstrained creativity is that
shallow analogies is that pattern
matching or is that
reasoning right the ultimate thing that
he doesn't want to admit that and can do
right is there really such a difference
because he's carving out things that
he's willing to say that they can do
right he's willing to say that they can
do this unconstrained creativity like he
literally used the word creativity right
whereas if you look at somebody like
David deuts he typically says AIS don't
create any knowledge and they're not
showing true creativity right now I
guess David Deutsch doesn't really use
the term reasoning if I remember he's
more about creating new knowledge and
creativity so we can make like a ven
diagram right like new knowledge
reasoning creativity and like different
experts who are granting that AI do or
don't have all of these magical terms
whereas from my perspective or the alasi
perspective it's really just optimizing
right you take an input you try to map
it to an output the output represents a
small Target in a big search space if
you can do that mapping correctly and
it's not just a trivial algorithm like
the search Bas does indeed seem very
vast and yet you're navigating It Anyway
then
you're doing reasoning you're doing
creativity you're doing intelligence
like all of it just falls out right
you're basically just doing magic you're
achieving something that is going to
give you power over the universe so
that's my preferred framework it seems
really crisp there's a lot of useful
math you can do around it but no right
people like Ral have to insist that okay
it can do creativity but it can't do
reasoning and Tim scarf is pushing back
in a really useful way he's saying well
kind of like what I said he's like okay
but if the creativity is useful you got
to figure maybe there's some reasoning
happening in inside the creativity to
power the creativity and Tim scarf is
suggesting what if we can carve out two
types of creativity and we can say that
AIS can only do combinatorial creativity
but they can't do inventive creativity
that sounds to me like David do is
create new knowledge right so Tim scarf
is just trying to draw a boundary and
maybe the same place David deuts would
draw the boundary right which again I
wouldn't do but it's an interesting
question to put Force to rout and and
that leads me to think that language
models are surely Limited in the types
of things that they can um create
because they're bounded by the training
data I I I agree but my point is that
compared to you and me they have been
trained on lot more data that even if
they're doing shallow almost pattern
match across their vast knowledge to you
it looks very impressive and it's a very
useful ability as you keep listening to
ral's answer keep in mind Tim scarf is
asking him what type of creativity do
they have and what type do they not have
and keep in mind I showed you an example
where Claude I asked Claude to explain
the joke and Claude said the joke
implies that McFarland is making a dark
self-deprecating reference on linkton by
suggesting that his disastrous Festival
is no longer the worst music festival in
light of the October 7th attack which
occurred during the Supernova music
festival in Israel remember cla's
response to my question and try to
connect that try to see if that squares
with what R is saying here about what
llms can and can't do there's this very
interesting uh point
about whether in fact the creativity
that a machine creates is considered as
you said combinatorial versus something
Beyond but that becomes almost a second
order question if you don't even have
you know a tool which actually generates
this combinatorial um creativity over
vast knowledge you know that's where
they really shine and and for that
perspective I am willing to use them for
that creativity aspect I would almost
never use them for reasoning aspect
because reasoning requires some kind of
a guarantee that what you said is
actually true and uh I can't believe
that I they are not going to be able to
do that for at least the usual
correctness um uh considerations they
can do it better for style in general
that means they can create an essay that
looks more like a well-written English
essay as again as making sure that the
content content of that essay is
actually factual and that there is and
then and maybe even making interesting
deductive closure claims uh which is
actually going from the premises towards
you know more interesting uh that's the
part that I don't believe LMS can do but
they can write and also check the style
of another essay say because that's what
they're good at and to some extent
actually this is a I mean autor
regressive LMS are a specific form of
generative Ai and generative AI learns
properties of a distribution style is a
distributional property
correctness uh and factuality is a
instance level property and llms do very
well on the distributional properties
because they're distribution Learners
and they are not going to give you any
guarantees about instance level uh
correctness and and so that we do have
other tools for obviously and we can put
them together I didn't detect any
connection whatsoever between what Ral
just said and the example of explaining
my joke for example when he talks about
distributional properties how does
correctly getting all the references to
my joke and explaining them and
explaining why the joke is funny what's
the distributional property of that or
Rouse point about how llms can mimic
style really well but don't get factual
content reliably okay well all of the
factual content and its answer to
explaining my joke was perfectly on
point right there were no factual
inaccuracies and what did its response
have to do with style it just wrote it
in a good informative English style so
again Ral just seems to be talking past
this amazing achievement that llms are
doing right I gave you one example I
could give you other examples like if
you if you see it give a really good
diagnosis right another one of my
favorite examples medical Di agnosis I
don't think what he's saying about style
and statistical distribution are going
to apply to some of the inferences that
it's going to print out when you ask it
to make a medical diagnosis it seems
like he's not substantiating some of his
claims about these terms and these
limitations he's just not substantiating
them with good examples I'm getting a
lot of mileage out of telling you my one
example that I was impressed by so I
wish people like Ral would refer to more
examples to make their point because
examples of what clad can do just tend
to be quite amazing quite meaty quite
interesting to dive into so examples
would be helpful yeah interesting so so
it sounds like we're saying there is a
verification and reasoning Gap and
potentially a creativity Gap but the
reasoning Gap is much bigger we we
should focus on that but the other thing
of course is that because we we're
talking about building this mtic system
where humans are engaged so we're always
doing creative things we produce data
the data goes into the language model so
cating this big collective intelligence
and and in a sense um let's say you you
do something creative tomorrow it will
be in gp4 yeah exactly Kim is just
repeating back Rouse's worldview of like
yeah humans are creative and then gp4
just has a database of all the times
when humans were creative I see this as
a shortcoming on Tim's part to not try
to think of a counter example like I
just did right just to bring it back to
the example of my joke like which
previous example of human creativity is
is allowing gbd4 or Claude to understand
my novel joke right a single counter
example would demolish from my
perspective demolish R's claims but
instead of reaching for a counter
example which I don't even think is that
hard Tim is just say he's just repeating
back what R is saying and moving on
right it's like let's let's have some
push back here Tim many people don't
realize as you very well put just now
that they are scaffolded over our
Collective knowledge so I keep asking
people to do this thought experiment
imagine Sam Alman got Satya nadala to
already pay for training GPT I mean you
know gpdk right and he just didn't have
web right and he had to create the web
and so he comes and says guys why don't
you just put everything you ever wanted
to talk about on the web so that I can
use this as a data that would be dead on
arrival because nobody would do that we
made web for each others to actually you
know kind of communicate with each other
and that then got became fer for these
systems so in a way the fact that these
things are doing well is very much based
on the fact that they are doing shallow
um pattern matching sorts of things over
this vast knowledge that we have put on
the web it's obviously true that gp4 has
a deep knowledge of the web like it
understands what every web page is about
right it can spit out a summary any kind
of that you want it to make it uses an
actual deep understanding of the meaning
of what's on that web page to create the
summary right there's no shortcut to
create the summary there's no such thing
as a shallow way to create the summary
it's not going to create a summary that
matches the statistical patterns that
you expect from a summary no it's going
to create if you say hey summarize this
in 15 words it's actually going to
project the embedding right do whatever
it is that a deep understanding would
actually do like there's no way to do
something deeper than projecting
embedding in a high dimensional space
like we as humans don't really do
something fancier than that that as far
as I can tell that seems to be what true
understanding looks like is representing
things in these high dimensional space
where like the dimensions represent
properties at different levels of
organization like I think they're onto
the magic that the human brain is doing
I don't see a reason to believe that
there's extra magic the extra magic that
the human brain is doing that the AI
can't follow seems to be in our like
system to you know there is some secret
sauce with human reasoning that makes
human more robust right that's where I
see potential secret sauce I find it
interesting that Ral is working in this
term shallow as if being shallow is
somehow connected to having this big
knowledge base yeah it has a knowledge
base that it deeply understands right
and then when you ask it a question like
asking to explain my joke or ask it to
explain any joke the way that it
explains the joke is by actually
understanding the joke right there's no
shortcut if you look at how it explained
my joke where's the shortcut what did it
do differently and how it explained my
joke compared to what a human would do a
human would also think what are the
references that I'm making in my joke
out of everything I know what seems like
the reference and you can call it like a
a shallow statistical match or whatever
but we as humans a big part of what we
do is we use this embedding and we try
to search the embeddings we know right
we also do a similar match right and
then we try to find a connection we
search our space of possible deep
connections to say what connection is
this joke making right so everything
that the AI has done to explain the joke
to me seems very deeply connected to
what a human would do to explain a joke
and the only way you can write this off
is you can be like well maybe joke
explanation is a fundamentally easy
problem which reminds me of something
Steven Wolfram said when he saw alms
he's like well I guess using language
was just an easier problem than we
realized which to me seems like a cop
out like I don't think it's an easy
problem I think it's a hard problem and
I think we solve it and I think there's
not that many hard problems left right
that's my perspective but anyway in
terms of what Ral is saying right now he
seems to be smuggling in this judgmental
term shallow and he's trying to connect
it with the evidence of like yeah it
just has this big knowledge base whereas
my point is like it's not a point
against you to just understand knowledge
as long as you can successfully apply it
the same way a human applies it it's
very important to remember that they not
creating this in fact if you make them
create knowledge he said creating new
knowledge so now he's got all three
terms on my AGI skeptic bingo card right
he talked about reasoning he talked
about creativity now he's talking about
creating new knowledge let's listen if
you make them create knowledge create
kind of text you know and my biggest
worry is I actually said this that you
know in Norway there is this um seed wal
and they basically have a wult in
somewhere high up in Northern Norway
where they kept all the seeds of the you
know the things just in case there's
some nuclear this thing Etc and we need
to restart the civilization you have
seeds there and I always felt that we
need to take a snapshot of a coverent
web and put it in that seed Vault
because as llms generate more and more
completions and they become part of the
web essentially you are now have a much
more noisier version uh than things like
Wikipedia and New York Times Etc which
are lot more curated kinds of sources
and when it's all combined it becomes
much harder okay false alarm when he
said make the llms create new knowledge
I think he was just saying when you make
the llms output whatever text they're
going to Output it's going to pollute
our Corpus of human generated text so I
don't actually think he was explaining a
scenario where he thinks the llms are
creating new knowledge so I think he
just never really addresses what does
that mean to create new knowledge I
think he just sticks to talking about
reasoning and talking about creativity
he just happened to say the words create
new knowledge but he's back to saying
well anything that an llm could ever
produce is just going to be statistical
slop it's not going to add value for
instance if an llm explains my joke he
doesn't think that can add value to add
that into the internet because it's
probably just going to be redundant with
other knowledge that's already on the
Internet or maybe it could be equally
likely to be wrong is right so it's like
too risky to put on the internet this
kind of implies that this whole idea of
synthetic data is worthless in his mind
right I'm putting words in his mouth but
if you take his worldview you might
conclude that there's no point in having
llms generate a bunch of training data
for other llms even if they took a bunch
of computational time to come up with
their data like oh here's a corpus of
joke explanations that I've written
based on unexplained jokes I've seen on
the internet from R's perspective that
would just be like too sloppy right too
too much like AI slop to be productive
and training the Next Generation
something like that I mean I'm just
trying to extrapolate what he's saying
let's keep listening so whenever when
people talk about we will you know you
can somehow you don't need human data I
not too many smart people say that but a
few um you know not well informed people
think that somehow why can't llm just
cre data on train themselves that's
essentially blind leading the blind Yep
looks like I passed the ideological
Turing test for Ral I correctly
predicted that somebody with his
worldview would be very down on
synthetic data it wasn't really hard to
predict but let's see what else he says
about synthetic data and it's actually
becomes much worse because you know the
completions are only the distribution
they're not really have any kind of an
accuracy and and so the factuality goes
even more down so killing me that he's
continuing to make statements like the
ai's completions are only the
statistical distribution of the data
that you gave it like okay explaining my
joke where I put together these two
music festivals and you know found
Gallows humor in it that was just a
Staal completion really and so it
doesn't add any value when you show a
version of the joke and then the
explained version of the joke that's
just worthless as additional data like
no reasoning took place just a crazy
worldview and when people talk about
synthetic data
typically what they'll wind up doing is
they'll depend on some external solver
which actually produces the synthetic
data with guarantees so if I want to
kind of do better planning I will create
a huge number of planning problems use
an existing outside planner to solve
them and then now have the problem and
the plan I find tune my llm so that it
sort of becomes slightly better at
generating Solutions uh for this
distribution of planning problems if it
just generates plans itself and trains
itself that would be you know blind
reading the blind that makes sense Ral
and I both agree that if you take a non
llm type of AI like some sort of logical
solver right like something that
computes the best chess position not
just using llm internals but using more
of a game tree lookup or maybe something
like a mathematical proof Finder right
things that you don't traditionally
think of as llm strengths even a python
interpreter right things that are like
more oldfashioned neat structured AI
maybe the outputs of that kind of
structured AI can then feed into the
llms quote unquote intuition right it
can feed into its system one that's a
familiar experience for us humans right
so for example I'm an amateur Pano
player like I mentioned if I'm playing a
bunch of music that was written by the
greats where I could never hope to
compose something like a Bok Fugue right
that's Way Beyond My Level of
understanding why that sounds good but
I'm playing so many of these Bo pieces
that my fingers are just starting to get
the pattern right something in my layers
of organization of system one where even
though I'm not architected like the
genius that can solve how to build this
kind of music I can still get the hang
of playing it and even subconsciously
predict what kind of chords are coming
next because I'm so used to it I'm so
used to matching shallower patterns even
without deeply understanding how to
generate the music so I think R and I
agree that this is a pretty good
description of how synthetic data by
some kind of non M structured AI can
still be helpful to making the llm
perform better the only difference
between me and Ral is that Ral kind of
doesn't give the llms enough credit he's
like yeah they're just always coasting
right they never have like that extra
flash of insight that extra reasoning
step where they go oh wait a minute wait
a minute I'm not just going to say like
this totally trivial obvious thing I'm
actually going to like make a little bit
of a leap right that's where Ral kind of
gets off the train he kind of thinks
like nope they never do that they're
always just like smoothing you know
kernel smoothing right it's just all
about statistical distributions that's
where we diverge in this next section
Tim asks ra about the arc prize which
stands for abstraction in reasoning
Corpus it's a very interesting set of
challenges where it shows the AI like a
grid of pixels and there's like a
certain pattern and then it sees another
grid where the first pattern kind of
evolves into another pattern and the
challenge is for the AI to predict the
transformation rule to kind of figure
out why these grids are transforming
based on some examples without knowing
in advance what kind of Transformations
are allowed so like each going to be a
novel transformation like one time I saw
the transformation was that the colored
blocks in the first grade were kind of
falling down as if they're under the
influence of gravity but that's just one
rule right they could be changing color
they could be doing anything and we as
humans can look at it and be like hm I
wonder if this is the rule right it's
kind of like an IQ test but it's been
given to the AIS the inventor of the arc
prize franois cholet he's pretty
confident that current llms can't solve
it because you need something more
humanlike to solve the arc prize and of
course r is going to be kind of on the
same page being like you're not going to
solve the arc prize with statistical
stochastic Parry you're going to need
something else let's listen are you
familiar with um Chet's Arc challenge
yes yes yes because that's a great
example so I mean I'm I'm friends with
Jack Cole and the winning team and
they've done data set generation and you
know like um test time inference and
fine-tuning and so on and and they've
kind of they they've um they've made a
whole bunch of transform rules you know
so the reflections and symmetries and
stuff like that and they're they're kind
of like building up the data
distribution and then they're
fine-tuning a language model on it and
then another approach is like Ryan green
blats one where you use it as an idea
generator just like you Advocate exactly
Ryan green blats one is actually very
much the sort of llm modulo version
because they have python interpreter so
they generate a huge number of code
Snippets and then check whether or not
those Snippets could have generated the
right result on essentially like the arc
training data and that kind of gives
them a huge advantage and it's very
reasonable thing to do uh but it's
basically the the guarantee is coming
from this interpreter as we were talking
earlier formal languages have
interpreters natural languages don't so
by actually converting the whole thing
into I will guess the python code for
solving the arc challenge right you now
can depend on the help of the Python
interpreter uh to make sense of whatever
you guessed and you can at least compare
the expected output with you know what
actually output you know this particular
piece of code is generating in in a in a
at a level it is still extremely Brute
Force way because they're generating
huge numbers of you know huge numbers of
combinatorial possibilities and you have
to make sure that it is diverse enough
so in general this is again similar to
the llm modulo stuff that I talk about
where these are sort of generate test
approaches where tester is an external
thing which actually can give guarantees
yeah Ral thinks the arc challenge is a
great example of how he talks about llms
just being these unconstrained idea
generators and you really just need this
different type of logical intelligence
like the python interpreter to help make
sense to help kind of verify if their
candidate solution is to work okay Ralph
fair enough but this combination you're
describing right the generative creative
part that gives you a bunch of ideas and
the ideas have a surprisingly High
chance of being correct and then the
part that lets you check if they're
correct isn't that how the human brain
works I mean when I personally look at
an arc challenge I can kind of feel
myself doing the same thing I look at
the different squares and I'm like hm I
wonder what's going on with these
squares what if they're doing this would
that work and the only way that I can
answer my own question and check if they
work is I activate a part of my brain
that's more just like stepbystep
crunching through rules kind of like the
python interpreter part of my brain but
it's interpreting my own guess as to
what the rule is like if the rule is
okay they just always move down or okay
the right the top rightmost square pixel
always changes color first like whatever
kind of rules I invent for myself in the
generation stage I need another step
where I'm like okay what would that mean
right if if this is the rule what
happens in the next frame so I'm kind of
python interpreting my own idea and I
don't know the answer it's not like I
have a part of my brain that skips to
the end maybe John Von noyman does right
certain Geniuses might subconsciously
somehow do this entire process but I
don't feel like I subconsciously do the
process I feel like there's a
consciously accessible step like an
introspect step where I'm like okay
crunch through run the Python program
that you just wrote for yourself really
quick and tell me if it actually outputs
what you're seeing in the example and if
it does then you've kind of solved this
puzzle and you're ready to use that same
program to predict the output right
that's like my own inner monologue when
I'm solving one of these Arc challenges
so when Ral was saying hey look llms are
just idea generators it seems like Ral
is willing to entertain the idea that
we're getting close to human
capabilities here because what is a
human brain but a generator that happens
to always be really good at generating
ideas that actually have a chance of
being correct even when the idea space
is huge plus yes a fire a sanity check a
system two a robustness thing right I
think we're all on the same page that
the system one part right the the
pattern matching part is getting really
good maybe the disagreement between me
and Ral is just that I think system one
is more than quote unquote statistical
pattern matching right I think that what
LMS are doing I think the human system
one right the intuition like a fireman
running into a building but knowing
exactly when something in the building
is going to make it collapse right this
is an example of like a deep intuition
something that's hard to describe with
simple rules
but like the aggregate of all your
observations somehow goes into this
trained model right goes into this
function the system one and somehow
screams at you like uh I'm worried the
building is going to collapse
probability of building collapse is high
right these kind of models what ilas has
like the model wants to learn right this
kind of magical system that we've kind
of cracked the code of right now that
we're now scaling up right we're now
benefiting from scaling laws and like
the the mystery is deepening right they
just keep getting better somehow and
like yes it doesn't include the python
interpreter like you also have to The
Interpreter step but don't you see where
this is leading again Ral is just like
oh no no no the scaling laws isn't doing
anything because it's still statistical
I think that is where R is like refusing
to to see observations here right
because he has these research papers and
they're telling him like oh no no no we
were able to like modify the input and
get it to get the answer wrong so Ral is
still entertaining the idea of like
don't worry these machines that you're
scaling up they're like so limited
because the python interpreter part is
helping so much
in this particular example sure the
python interpreter is helping but I
think once again he's neglecting how
much the generation part has already
narrowed the search he is acting like oh
yeah it just spits out every possible
Python program right those aren't
literally his words but that's his
attitude like he's not impressed with
how good it is at only spitting out a
relatively small handful of plausible
python programs like that's the magic
man it's not the interpreting the python
isn't that impressive right the
impressive part is that you even spit
out you enough candidate programs that
we can then go and check those programs
One does not simply spit out a few
possible candidates in an exponential
search space you got to appreciate
What's Happening Here let me replay this
part for you because Ral literally
referred to what the LM is doing as
Brute Force at a level it is still
extremely Brute Force way because
they're generating huge numbers of you
know huge numbers of combinal
possibilities I have haven't even looked
into this to tell you how many programs
are being generated I don't know if it's
hundreds thousands tens of thousands
even hundreds of thousands what I do
know is that the total number of
possible short python programs is 10 to
the power of thousands right so much
larger than the size of the universe and
so the narrowing that's happening is
thousands of orders of magnitude so when
that's happening and then you're calling
it Brute Force how did they narrow it
what did they do to narrow the program
space right you're kind of glossing over
that and being like Oh Yeah you just
kind of try everything you don't try
everything that's the Breakthrough right
that's why we couldn't do this in 2020
that's why this is The Cutting Edge it's
because it's not Brute Force it's this
new magic right these giant brains that
just scale up the number of neurons
scale up the number of parameters and
somehow they learn they learn something
they learn levels they learn
relationships they don't learn freaking
statistics they don't learn freaking
linear relationships like this is
ridiculous and you have to make sure
that it is diverse enough and and the
soundness is guaranteed by the Tester
the completeness depends on the
generator and as as impressive as llms
are there are no guarantees that they're
complete there's no guarantee that an
llm is complete meaning it can't
necessarily ever output in response to
any prompt the perfect solution because
the space of Perfect Solutions is really
big and it doesn't map the entire prompt
space to the entire solution space right
it maps to certain clusters and solution
space and those clusters don't cover the
entire space yeah I mean that's true
about everything right I mean me as a
human if I had my whole life to think
about all the different theories of
physics my brain probably isn't going to
be complete with respect to finding the
correct Theory of physics if you give me
my whole life to try to brainstorm how
quantum theory and un sense relativity
can be unified I will probably never
output a research paper that's covering
the actual space where the answer lies
the range of what brain can plausibly
output given my level of math skill and
just intelligence overall you're just
not going to see complete coverage of
idea space right that's true about
pretty much every brain you can use a
counting argument to say look even in
principle the only way that something
can be complete that something can cover
the entire range of possible outputs is
if it's highly sensitive to the prompts
right because you have to give it like a
huge range of possible prompts you can't
have more outputs and there are prompts
anyway the reason I'm getting into the
weeds here is because I'm kind of
frustrated to hear talk about
completeness as if this is like a
desirable property like oh yeah you need
completeness like wait a minute what are
we really talking about here like this
is kind of a difficult framing to work
with in contrast the framing I like to
work with is are we optimizing toward
the target are we getting closer is the
arrow getting closer to the bullseye is
that happening yes or no and then it's
pretty obvious the answer is yes right
the arc prize using a combination of
llms and python interpreters something
that wasn't possible a few years ago
every week every month I'm watching
Twitter and I'm seeing tweets oh it got
closer it got closer to the arc prize
and this has never been possible before
so even if you don't understand how it's
happening even if you want to hand wve
it away and say it's not truly
intelligent the arrow keeps getting
closer to the bullseye and it couldn't
be and the range of possible bulls eyes
that the arrow is able to get closer to
that range keeps growing we keep hitting
more bulls eyes so again you're always
going to have the RS of the world
turning around and saying ah yes it's
not complete it's not truly intelligent
and needs to use a python interpreter
okay but the world is getting swallowed
R do you have anything to say about that
is is do you have a prediction as to
when it's going to stop a falsifiable
prediction anything are you just going
to keep skitting and skitting in this
game of tug ofar right you're just going
to keep getting tugged you're going to
keep skitting and keep pulling on the
rope until you just fall into the ditch
that's what's going to happen this is
sort of goes back to this issue of yes
they're creative but can they create
every possible thing and in in general
that almost requires a prompt
diversification strategies you know
where you bring in extra outside
knowledge to tell them okay now stop
generating this kind of code generate
this other kind of code because you then
introducing some sort of an inductive
bias saying I believe that you know you
should also consider this other kind of
code because maybe one of them might
actually pass the verifier test that is
this extra knowledge that you're
bringing in this old idea this this idea
of tree of thoughts that you know Bally
people that that that's become very
popular actually is best understood in
that way because it's really a prompt
diversification strategy which brings in
external knowledge uh to push the llm
okay Ral um good commentary so does this
mean that the arc prize will be solved
in 2025 or not what are you implying I
feel like R's answer would be like well
if you did enough prompt diversification
then you would hack your way to a
solution right so it I feel like I'm
speculating here but I feel like Rous MO
is just going to be to brace himself to
just explain whatever outcome happens in
terms of how he likes to frame the world
so he loves framing the world as if llms
are just statistical just pattern
matching and so to the extent that they
will actually win the ark prize in 2025
because they keep getting closer I'm
sure Ral will have some reason why that
is consistent with his worldview because
they just used prompt diversification
like they automated The Prompt
diversification piece and then they
automated ated the checking piece and
then the llm didn't really do much it
was just brute forcing everything and
then the arc prize got solved and the
arc prize has been unsolvable until now
right so that's that kind of Rouses
worldview so like how is this helpful R
like this doesn't seem to have much
predictive power right this doesn't seem
to actually be drawing a barrier between
what AIS can and can't do like maybe
he'd be like oh there is a barrier
there's a barrier between what the llm
module can do and what the other modules
around it can do maybe there's something
to that I mean system one system two
like I I agree there's some useful
things that you can say about that but
again the important question is where is
AI progress going right I feel like
there's a Mot and Bailey going on here
where the M the part he says that's well
defended is like look llms aren't that
good at logic right now so they need to
be augmented with systems that can check
their logic similar to human system one
and system two right I feel like I turn
on the logical part of my brain to check
the pattern matching part of my brain
that kind of the m and then the Bailey
is AI is not going to take over you're
not going to have systems that can
overpower Humanity because it's all
statistical but wait a minute no no you
didn't say when when we were in the M
you didn't say that it was all
statistical you said that part of it
isn't capable of doing logic very well
and needs to be augmented but then you
can draw a box around the system as a
whole and suddenly you're getting the
solution to the arc prize right you're
getting self-driving cars you're just
getting everything you're getting really
good insightful joke explanations you're
getting them doing an entire job you're
getting them being the CEO of a company
right like where does it end rout what
predictive power can you tell us about
where it ends right the daily of people
like Ralph seems to be that you know
Mark Andre and Martine Cado go on a
podcast and say everything is going to
be great you don't have to worry about
AI right that's their Bailey and then
they go to the m and they say hey look
here's a research paper that says all
llms do is Kernel smoothing and they act
like those two things are connected the
mod and the Bailey yeah so um I I
interviewed Ryan greenblat and
unfortunately he still thinks that I
mean because obviously I asserted that
it's uh you know you got a system one a
system two you've got a neuros symbolic
model it's only working because you have
the python in
you know verifying and and he was
adamant that no no no you know just wait
until the next GPT model comes along
it's going to you know and but the the
the beautiful segue is that he did a few
things right he did um Chain of Thought
so you know he kind of like he carved up
the space he did self-reflection so
refinement of the solutions and he
believes deep in his bones that the llm
can autonomously verify itself and come
up with the right solution okay you're
talking about the guy who pushed the
frontier of the highest score of the arc
challenge right this guy's a winner he
has empirical validation for his
worldview and he's telling you that he
thinks the llms are really grasping the
challenge right they're doing most of
the work of doing the challenge they're
not just brute for sync right they're
not just unconstrained generating a
bunch of random statistical python
programs no they're just understanding
what would be a good guess the same
thing that I describe my own brain is
doing so you know people always say oh
if you truly understand AI you won't be
worried about where it's going here's a
guy who has empirical proof he's getting
the results right so people like R will
speculate about how oh these are so
limited these are so limited and then
people like Ryan greenblat will be like
okay I just passed the next challenge
what's next give me another challenge
give me another challenge so Ral is in
this position where he has to look at
the Ryan green blats of the world who
are just charging forward and
demonstrating empirical results even on
challenges specifically designed to show
what they can't do and they're plowing
those challenges anyway right the arc
prize is now predicted to to fall in
about a year a year and a half like
that's what people are predicting even
though it's supposed to be this
challenge that shows what today's AI
fundamentally can't do and yet one way
or the other the Ryan green bots of the
world are going to take it down so it
keeps supporting what I'm saying about R
which is he's the guy holding the rope
in a tug ofar who's getting tugged into
a ditch but he's still you know he's
still holding on he's still facing the
other way being like no no no this is
it's not tugging like the Rope is
actually not getting tugged very hard
like other things are tugging the Rope
but he's getting pulled into the ditch
and once he's in the ditch once AI
overpowers Humanity it's going to be too
late to be like you know what I was
probably not saying useful stuff on all
those podcasts I really should have
focused on how my results are like
limited in scope and they don't really
give you the accurate impression of what
to expect from AI next right when
humanity is overtaken it's going to be
too late for R to take the blame for the
approach he's taking now in this next
part R makes a distinction between
correct
and style and he argues that llms aren't
good at getting things correct and
checking themselves whether they've
gotten something correct to make sure
it's robustly correct which I kind of
agree with right I'm always saying hey
they're not robust enough that's what's
missing with llms today and then he goes
on to say oh but on style they're great
so they've like mastered style and they
suck at correctness I don't like the
distinction but let's listen to how he
says it there is no simple formal system
which can take an essay and say it is
correct right because you're tending to
basically look at the kind of a style
characteristics rather than um in fact
if you have a beautifully written essay
which at some point of time has a fatal
factual flaw they're not going to be
able to find
it right and yet if in fact then
basically what they'll do in these cases
they will do human subject studies and
you ask humans are these better essays
versus worse essays and they might find
that somehow the essay quality has
improved with this llm critiquing itself
but that's the style Improvement not the
correctness Improvement one of the
interesting things is the correct you
could have a correct plan that doesn't
have the right style so I give this
example of a correct travel plan to come
to Vienna from India where I started is
walk one mile run for another mile then
bike for another mile etc etc by the end
of which you know indefinite number of
these actions I will be in Vienna that
is sort of correct but it is highly bad
style most people would not consider
that as a reasonable travel plan because
they tend to stick to like like an
airline flying or some other kind of you
know standardized versions so the style
is something that llms are actually
better at critiquing the correctness is
something that they cannot and you want
the external verifiers and it's a
complimentary thing because classical AI
systems are actually much better at
correctness than style if you actually
think about Rouse scenario run one mile
bike one mile walk one mile run one mile
is that really a style issue that seems
like a profoundly wrong answer right I
mean if biking 10 miles is a fast and
usable option then why would you stop
and walk that's going to be a slower
Journey uh what Criterion are you
optimizing for where that kind of switch
of medium is helpful if there's a bridge
where you have to walk your bike and as
a result it says bike one mile walk one
mile cuz there's a bridge bike one mile
again then suddenly there's no style
issue right that's it's just correct it
found a good answer so I find it
interesting whenever somebody gives an
example and then their own example
doesn't seem to be making a strong point
here right I mean I get what he's trying
to say that it's kind of a shallower
property right if you're switching
between Transportation methods and
people don't really expect it llms can
notice shallower properties I feel cuz
that's that's always is Mo is to be like
oh yeah llms always just do things that
you can notice in the blink of an eye
because they're shallow and they're
statistical I get what he's trying to
say but the fundamental distinction
between correctness and style there is
no fundamental distinction an essay with
a good style that's part of having a
correct essay like however you want to
Define what constraints make a good
essay one of those constraints is for
instance being readable to humans being
logically organized what he's calling
style from my perspective are
constraints that make something correct
in own domain right there there are some
domains where you characterize the
correctness constraint as being style
constraints but like I could design a
computer chip which you normally don't
think of as a style thing you normally
think about it more as a correctness
thing but I can always just arbitrarily
characterize oh that layout is so
beautiful it has a nice style right
we're we're not talking about a
fundamental distinction here like it's
correctness all the way down right it's
constraint satisfaction all the way down
it's scoring metrics all the way down so
when raal takes his correctness versus
style distinction and he goes on to say
llms are never going to get you
correctness they just get you style and
you need a different system to get you
correctness he's neglecting what llms
are doing which is they're outputting
things that not only have the correct
style but they have a good shot at being
correct right they're not just brute
forcing every possible output they're
actually narrowing down things that are
both the right style and likely to be
correct right that's the optimization
work being done that he's just not
acknowledging right it's like there's
this amazing feet of magic happening
right there's complex joke explanation
happening in front of his eyes and he's
not putting his finger on why it's
amazing so in my mind he doesn't have
the credibility to be dissing llms
constantly the way he's doing when he
can't even tell you how big of a miracle
it is and why it's a miracle right what
it's doing that's amazing he he hasn't
put it into words yeah I mean I I agree
with you so I'm I'm kind of like a
philosophical rationalist you know I I
think we should be reasoning in the
domain of certainty you know it's either
correct or it's not correct I guess I'm
not a philosophical then I guess I may
basian rationalist but go on so we agree
with that but devil's advocate for a
second um there are folks who think we
can do endtoend predictive models and we
can do this active inference and we can
just keep leaning into the test instance
and fine-tuning and fine-tuning and
fine-tuning and then you know we can't
verify it we don't know that it's
correct but we can build systems that
work you know reasonably robustly and
produce often the correct answers I like
that Tim's asking this question he's
basically asking Ral look can't we just
keep improving the LL and keep making
them do useful things until they consume
every task and like realistically when
you put them in a job can't they just do
the job because like sure occasionally
you'll find some flaw that they can't do
perfectly but they'll just keep eating
more and more of your job and in
practice their accuracy rate will just
keep getting higher and higher and it's
going to be so low that it's going to
get below like the human accident rate
right which is already happening with
like self-driving in San Francisco
righto I can nitpick how Tim is
describing the situation Tim is saying
they'll never never know for sure that
they're doing the right thing but
they'll just do it anyway and it'll be
good enough I would nitpick that and I'd
be like well they're going to just
develop strategies for using external
tools to check on themselves to the
degree that humans can right I mean if
if you look at most humans in most jobs
it's not like they're that good at
standing back and reflecting on
themselves and being like did I really
do a great job today did I make all good
decisions I mean they can do it to some
degree but it's not like they're
Geniuses at that right like they have
like a bag of tools they have certain
procedures that they use to reflect on
their own work and I think the system
one of the AI are like whatever magic
it's doing it's just going to get
magically good at copying humans at the
same kind of procedures that they use to
convince themselves that they've done a
good job right and the only way to do
better might be to have like a genius
reflect on like the whole organization
like have Elon Musk you know fire 80% of
the team and like rethink everything
from first principles right but that's
just not what we normally expect from
humans right that that's like doing
something extra so anyway maybe a small
nitpick in Tim's question but let's hear
R's response do you think that that's a
reasonable thing yeah I think the point
there is first of all the end to end
correctness you can use the world itself
as a verifier yeah that's actually a
really great point the reason why we
just have these big pattern matching
system ones in our head and we can just
be really effective even without doing
that much reasoning is because we
constantly get feedback when we like do
stuff and then we like look at what we
just did and that gives us a lot of
signal about the The Logical causal
consequences of what we just did and
then we can iterate right so that's
actually a powerful feedback loop a
powerful supplement between the logic of
external physics and the pattern
matching that we're doing in our pattern
matching brain right and that is an idea
that works only in erodic domains where
in fact the agent doesn't die you know
when it's actually trying its bad idea
yep another great point when we think of
something as oh it's just this iterative
thing we're going to work on in the real
world uh sometimes what we're actually
doing is around and we're going
to find out so good point so even when
you're doing end to endend verification
there needs to be a signal as to whether
the output is actually correct where is
that signals coming from in general it
you would like to have an advice taker
to whom you can give very high level
advice and they will follow it right in
general and Chain of Thought makes it
look like LMS can do that but what they
really wind up doing is if you for
example tell them how to solve like a
three four block stacking
problems they actually improve their
performance on the three four block
stacking problems but you increase the
number of blocks the the principle
Remains the Same in particular for
example one Chain of Thought idea is in
the blocks world you can always un put
all the blocks on the table and then
just construct the stacks as you need in
the gold state if I sell this to a kid
they understand this and then they can
do it for one one block or 15 blocks or
200 blocks until they get bored but llms
basically don't get the procedure they
only do better for the three four stacks
and then as you increase the number of
blocks they die nope sorry I don't think
that's the case I'm sure Ral is
referring to some kind of research paper
by himself or somebody else and I'm sure
that within that paper is some contrived
example where they you know tweaked the
lm's input and the llm didn't do the
thing that seems obvious to humans I get
it but I refuse to believe that today's
state-of-the-art llms you can't just put
an instruction in their memory like hey
when you're thinking through how to do
this problem if it's a block world
problem just make sure you get all the
blocks on the table first before
proceeding I refuse to believe that
there's no reasonable configuration of
such an llm that's you know maybe
slightly tweaked but not like super
contrived I refuse to believe that I
can't set up something reasonable where
the llm won't follow my instruction to
first put the virtual blocks on the
table I refuse to believe it if you want
to convince me you know pull up chat GBC
pull up a reasonable system and show me
how hard it's choking I get that they
sometimes choke but they don't choke
that hard and I think it's leveraging
these kind of contrived examples that
I'm I'm claiming is contrived right go
ahead and prove me wrong viewers if you
think I'm misrepresenting him but I
think he's leveraging contrived examples
to take away these huge achievements of
these llms now I also know a type of
push back that he likes to give is he
likes to say oh sure yeah the prompter
is the really intelligent one because
the prompter will just keep changing the
prompt until he gets what he wants I get
it I'm still just willing to die on this
hill of saying nope llms can in fact
listen to the advice of putting the
blocks on the table first like it is
more accurate to say they can then to
say they can't so open challenge anybody
can come fight me or prove me wrong but
that's my claim the reason I'm so
confident is because I see how Claude
has mastered the basic operations of
this type of problem so here is Claude
3.5 on it I just typed hey Claude when I
asked you block problems the first step
should be to move all the blocks to be
flat on the table and then I continue
there's a tower on the left a triangle
block is sitting on a rectangle block on
a circle block there's a tower on the
right A red block is sitting on a Green
Block on a Blue Block explain
step-by-step operations to swap the
position of the rectangle block and the
green block so basically the two middle
blocks so Cloud answers let me first
quote and understand the initial setup
and constraints Two Towers left and
right left Tower bottom is top Circle
rectangle triangle that's correct right
tower bottom to top blue green red again
that's correct goal swap the rectangle
block and the r and the green block
correct constraint first move all the
blocks to the table so it perfectly took
the constraint that I mentioned and put
it under the label constraint so like
it's totally just understanding what I'm
saying right like there's no trick here
this is what real understanding looks
like the ability to do this right this
is not statistically somebody writes the
word constraint with a Colin and then
talks about what they do first next to
it this isn't freaking statistics okay
all right so quad continues let me solve
the step by step deconstruct the left
Tower remove triangle from rectangle
remove rectangle from Circle correct
deconstruct the right tower remove red
from Green remove Green from Blue
correct now all blocks are flat on the
table in order Circle rectangle triangle
blue green red correct build a new left
Tower Place Green on Circle Place
triangle on green correct build the new
right Tower Place rectangle on Blue
Place red on rectangle yep correct final
result left Tower bottom to top Circle
green triangle correct right tower
bottom to top blue rectangle red correct
the swap is complete would you like me
to clarify any of these steps no Claud
I'm I'm happy with your answer right I
just correctly nailed everything when
you look at Claude performing something
like this perfectly it's natural and
accurate to say hey okay look yeah it's
good at this like toy problem but the
moment the problem starts getting bigger
or more complicated the moment you tell
it to hey simulate this Python program
without using python it starts to become
less robust right it starts to break
down it starts to not notice its
mistakes fine but if you look at you and
me as humans right I don't know about
you but when I was saying correct
correct correct I already had to read
the problem beforehand and like spend a
minute actually checking if it was
correct right it it took me a little
brain power of like green on circle
triangle on green right like this took
some mental effort for me to check that
it even was correct right and Claude
barfed this out very quickly right in
seconds so I'm already starting to reach
the limit of what my brain can
realistically do like yes I'm robust
than clae I can check over stuff but
it's just like it's a strain right my
brain is not going to stretch that much
compared to a problem like this so when
I see Claude just walking on past having
apparently no difficulty with his
problem moving on to more and more
advanced problems I just don't see that
many more notches that it can surpass me
like I I think that it's it's very easy
for me to imagine in a year or two that
I could be talking to Claud and it could
just robustly get like huge problems
right right and then I'd be
painstakingly checking and be like yep
as far as I can tell it's correct I am
no better at checking Claude in two
years let's say than Claude is at
checking Claude right and so when I hear
R be like oh no when we told it to put
blocks on the table and there's four
blocks it couldn't do that I mean I'm
not saying I proved him wrong like I'm
sure he has some research paper where
the data supports it but like he's
clearly missing the point it hit flick
whatever it is like that research paper
I'm just going to go go on a Lim and say
it's a bad paper ril whatever it is
without even looking at detail I don't
like it and it turns out it doesn't even
need the planning it actually can be
doing like the last letter concatenation
is this much simpler problem where given
a couple of words um you know dog and
cat you want LMS to say G and T because
G is the last letter of dog and T is the
last letter of cat and original coot
paper basically said look you know out
of the off the shelf they can't do it
but if I give you know this advice about
how to do this they seem to do much
better and we basically did this
experiment where we increase the number
of words having given examples for two
three word problems we increase the
number of words and that should be very
reasonable if you want an AGI it should
at least understand that last letter
concatenate is a simple problem where
the number of words you just basically
repeat the same thing it dies so I keep
telling that you know this is a sort of
the advice taking in general is sort of
reminds me of this old you know
capitalist proverb that teach a man how
to give a man fish and you will feed
them for a day and teach a man how to
fish you will feed him for life and
Chain of Thought acts as if it's a
second but it's it's actually a weird
version of the second where you have to
tell llm how to fish one fish how to
tell how to fish two fish how to fish
three fish Etc at which point you will
basically lose patience because it's
never learning the ACC ual
underlying procedure it is basically
just taking the examples and that of the
length that you gave and you're almost
it's doing sort of a um more or less um
pattern transfer there as against any
sort of procedure learning okay so again
I'm not going to bother reading Ras
paper but I am going to have a
conversation with Claud so I message CLA
I say consider this example input dog
cat wolf out put gtf the last letter in
the words now process the following
inputs input one apple banana pear input
two sweet sour salty in my conversation
with CLA I didn't even tell clae we're
looking at the last letter I literally
just gave clae an example right dog cat
wolf gtf okay so Cloud responds let me
understand the pattern from the example
it appears we need to extract the last
letter of each word let me process the
inputs input one apple banana pair
output one e a r yep that's correct
input two sweet sour salty output TR r y
yep perfectly correct and then I replied
I said input three Ros tarianism
supercalifragilistic Expialidocious and
Claude said let me extract the last
letter of each word input three rosarian
ISM supercal fragilistic expad doas
output 3
Ms okay so Claude just nailed everything
like better than you know as good as the
smartest human could do like 100%
perfect so I'm supposed to believe that
there's some paper that says hey we
tried to make these llms better at
getting the last letter of the word and
they couldn't and they didn't follow our
advice so I'm sure that you took gpt3 or
Pap for gpt2 or some weak llm or you set
up some uh some pathological conditions
right like you tweak things so that it
failed I I get that right I mean we're
both on the same page that this isn't
robust now you might be like you know
what Lon you're this is your fault here
you're you're not being intellectually
curious why don't you just go read the
research paper so that you know exactly
what R's even claiming no I will turn
the tables and I'll say this isn't my
fault for being intellectually lazy this
is actually R's fault because when he
goes on a podcast and says this stuff
okay both people aren't going to read
his paper and he is taking the results
of his paper and generalizing it to be
like oh yeah these llms you know they're
so fragile they can't do anything and
that contradicts my everyday experience
playing with CLA so it's really Ral
who's just misleading people like he's
the one who's not addressing the
elephant in the room of how is Claude
this freaking Savvy this freaking on the
ball and maybe you know this is from
like 6 months ago so maybe the answer is
just oh well llms got better since Ral
had this conversation fine but in that
case Ral is kind of misleading Everybody
by not acknowledging the fact that LMS
might be about to get better right I I I
just consider Ral to be almost
Distributing like disinformation here I
mean again I'm sure he's technically
right about his paper but he's just
misleading people about what llms are so
clearly capable of he's giving the wrong
impression even if he's technically
correct correct so a couple of things I
mean first of all many of these
techniques they they brutalize the model
in the sense that they make them quite
domain specific even if we do external
verification but also things like Rag
and Tool use and and Chain of Thought
you know these things kind of make make
the the models quite domain specific but
the bigger problem I wanted to point out
is that we want to teach the models how
to fish but the models don't know how to
fish in principle because they're not
touring machines you gave the example in
your paper here of you know like a semi-
decidable problem you know and maybe you
should bring that in but these these
aren't solvable in principle in language
models this sounds a lot like a claim
that Tim's co-host Keith Dugger likes to
make uh if you're interested in that
type of claim then I recommend listening
to the debate I had on my channel uh me
versus Keith Dugger because I really did
my best trying to point out that that is
like not a good argument that because
llms are like finite State automa
instead of thring machines in like a
very technical sense therefore that's
going to constrain what they can do uh
from my perspective I totally counter
argued against that and and basically
called it nonsensical but if you're in
the Keith Camp you weren't convinced
right so there's a faction of people so
maybe you want to join that faction and
like continue arguing against me but if
you think the topic is interesting go
watch uh me versus Keith but let's
continue here with Ralph so actually
there is an interesting point there that
people many of the bad ideas in llms
come because of the
wrong connections people make between
computational complexity and what llms
do okay in fact for example the very
reason why people thought LMS can be
better at
verification is generation is typically
computationally harder than verification
so they just assumed that maybe they
can't do the harder problem they can do
the easier problem I'm glad he's self
aware about the funny reversal that I
mentioned earlier right because I
actually caught that earlier I was
saying um Ral is saying that that
creativity like solution generation of
solutions that actually have a decent
chance of being correct he's acting like
that's an easier problem than verifying
the solution is correct like he's just
letting llms have the honor of being
like truly creative because he's so
intent on making his case that they
can't truly reason I thought it was an
unusual choice so it's nice that he's
going to address head-on right now so
tell us R why can llms solve search
problems when they struggle with
verification the reality is there not
Computing the solution in any which way
so in fact as you know many people have
pointed out
um if I give like consider the following
thing if I have like five different
prompt let's say the one prompt
basically implicitly for those of us who
understand English it's implicitly
asking uh for a solution for a constant
time computation problem another one is
linear time another one is polinomial
time and then all the way to another one
is undecidable problem and for all of
these the answer is yes or no let's say
except if you want to give the guarantee
you need to actually do the computation
ask yourself is llm going to take more
time coming up with the the token yes or
no depending on the prompt no it
basically just takes constant time to
come up with the answer and so people
try to say let's just actually people
basically try to say then that since
it's anyway going to give it in constant
time let's make it put up out more
tokens so that all together it'll take
more time yes obviously you get a weaker
model of computation when you only let
the llm output the first token if I give
the llm a huge number and I say hey is
this Prime and it's like 1 thousand
digits long and the llm has never seen
it before uh primality testing is
actually known to be in P so it's
actually not going to take like longer
than the end of the world it can
actually be done pretty quickly but it
probably can't be done in a single pass
of the llm like you're going to need
more steps than that there's a lower
bound on how difficult this problem is
and so there's just no way that in the
general case the llm is going to
correctly output prime or composite when
you give it a huge Prime like that right
and in worst case you can always just
make the prime like ridiculously big and
it gets so big and it overwhelms the the
minimum number of computation steps
right so however many computation steps
the llm has during a forward pass you
just make the prime big enough that you
can prove that primality testing takes
more steps than that okay anyway I'm
making this a little bit complicated but
my whole point is just blurting out a
single yes or in your first token
without having more tokens to think is
obviously not always going to get you a
good answer right if you go up to
Einstein and you ask Einstein like this
really complicated math question or
policy question whatever it is and
you're like okay Einstein you have three
seconds to answer 3 two one you're going
to get like a better than random answer
right like whatever Einstein blurts out
at least he's injected his intuition
into it but you're not going to get a
thoughtful answer right Einstein could
never figure out the theory of
relativity if you keep resetting his
brain to a certain State before he done
a bunch of work on it and he said okay
Einstein figure out uh how the speed of
light can be constant in every reference
frame 3 2 1 oh 's up right like he needs
time to think right even if you're like
hey Einstein give me give me the first
token that's going to be in your
research paper of the theory of
relativity 3 two 1 go no I mean clearly
the nature of thinking is that it
reflects back on previous thoughts right
so you need to create a structure of
thought as you as you think so it's a
priori obvious that llms aren't going to
be super insightful in just their first
token right the much more interesting
question is hey if we give them a couple
thousand tokens to think right that's
kind of the equivalent of giving a human
like an hour of time and a piece of
scratch paper right and yes the piece of
scratch paper is finite I know this is
like a big sticking point for Keith
Dugger like yes you don't get more
scratch paper than exists in the pages
of the books in the Library of Congress
right like you just have to use a
realistic amount of scratch paper which
has never stopped a human before the
need to use a realistic amount of
scratch rer but yeah so if you just give
the llm a realistic amount of scratch
RoR and tell it to and tell us it take
an hour uh that's where the interesting
thinking happens right I I can tell you
speaking personally there's very few
moments in my life where I've needed
more than an hour with scratch paper of
like hard thinking to come up with an
Insight right I'm sure it's happened
like a few times but it's very rare I
don't think it's happened in the last
year so if we're just talking about
computational properties right and
provable lower bounds of models of
computation it's just a non-issue right
like an llm just has enough tokens
during Chain of Thought prompting or
whatever it is like there's enough
computation here that's not going to be
the ceiling that's stops llms from being
smart they may have a different ceiling
like you know their ability to reflect
on themselves there may be some other
limitation that prevents them from
thinking effectively but it's not lower
bound on computation steps okay anyway
what is Ralph saying here there's
actually a paper I forget uh who other
were who said let's pause let's actually
put pause tokens in llm hoping that then
that way it will start reasoning because
constant time it can't possibly be
reasoning so you just make it pause and
maybe somehow reasoning will occur like
totally magical thinking what's wrong
with that logic how's it magical
thinking when you output a pause token
yeah you don't have the advantage of
outputting Chain of Thought so you don't
get to read over your own thoughts and
use that to build up to the next thought
okay so you're you're not getting that
part of the benefit of Chain of Thought
but you're still getting the part of the
benefit where you get to make another
pass through all the layers of your
neural network right the way each token
comes one after the other and every time
you make a pass you're propagating the
different weights of these vectors of
various stages right you've got a lot of
matrices I mean look I'm not an expert
on these details right but you get to
run like the attention blocks all over
again and the current state of your
vectors like where they've landed in
this High dimensional space they get to
influence each other and they get to
make these different types of queries on
each other and take the result of the
query and like project that into some uh
result space right I mean I'm just
regurgitating what I've learned about
LMS but the point is computation is
happening like you're letting more
computation happen when you're injecting
a BOS token so Ralph is just kind of
dismissing that logic but it just the
way he explain the logic sounds like
perfectly good logic there is a very
different question of can Transformer
networks are Transformer networks
shuring complete I tend to argue that
that's an orthogonal question you can
make it in fact remember there were
neural touring machines beforehand and
you can actually Transformers with
external memory can be made touring
complete right and this is all trivial
like this question about whether they're
train complete or whether they can deal
with arbitrary large pieces of memory
like the answer is obviously yes this
isn't interesting sorry Kei Dugger this
is like totally barking up the wrong
tree if you're looking for a way to
analyze these in a productive way but
the real question there are variations
that people have talked about but the
real question is I think that's not
actually impacting how llms
are you know generating the next token
essentially it's not like they're
actually doing touring computation uh of
any kind they just basically picking the
next token in constant time they andram
models wait
[Music]
what logic cop here sir can I please
take a look at your podcast we got a
report of somebody saying that large
language models are not actually doing
Turing computation of any
kind can I see how he elaborated on that
he said they're basically picking the
next token in constant time and their NR
models what's the logical connection
here
sir yeah doesn't look good I mean when
you've got this big algorithm the large
language model inference algorithm and
it's passing through one token at a time
and each time it's outputting a token of
Chain of Thought and it can keep going
and going and until it reaches a stop
token but there's no fixed constraint to
how long it can reason right and then it
can have a memory and it can explain
jokes or like solve problems or propose
candidate proofs or candidate
conjectures and it's doing all this
stuff you don't get to say that it's not
actually doing touring computation like
it is very much doing touring
computation that's not a high bar to
clear right that's a ridiculous
accusation to make against it with
really no logical argument to back it up
and then to add icing on the cake he
goes and adds that they're just engr
models when he himself established at
the beginning that no it would be way
too big of a lookup table to actually
have an endr model where n equals a th
and also how can you use an engr model
when the engrs in the prompt are things
you've never seen before so how are you
going to do statistics on things you've
never seen before right so he disproved
his own claim at the beginning of the
podcast and he never resuscitated it and
for that re this has been logic
cop in this next segment Ral gives a
useful clarification about what he's
claiming about what llms can do versus
what AI systems as a whole can or can do
do I do question this issue of later
models will do everything the real
question is if you are saying the later
models of a different architecture than
autor regressive llms I have no reason
to disagree with you because the world
is wide I mean there's just so many
different things you can do and we never
said AI cannot do reasoning AI systems
do exist that do reasoning like you know
for example um you know Alpha godas
reason it's an RL systems do reasoning
planning systems do reasoning Etc but
llms are a shallow Broad and shallow
type of AI systems they're much better
for creativity than for reasoning tasks
and and if you just keep increasing that
parameters and the size of the system I
have no rational reason to believe that
they can do reasoning they may be able
to convert reasoning into retrieval for
larger subclass and that may be practic
Al enough this is a useful clarification
because Ral is setting himself up into a
position where he can explain pretty
much any capability that's going to come
down the line so if scaling up an llm
lets it just solve any puzzle really
quickly without using python right it
can just be its own python interpreter
like you can imagine like oh here's my
Chain of Thought I'm just executing my
own Python program step by step without
literally using python I'm just using my
brain as python right so any workaround
that larger llms do to just
single-handedly solve any problem R is
already getting himself set up to
explain that because he said well maybe
they can convert reasoning into
retrieval so like they're just doing
retrieval that tells them like what they
should do next and it's not really
reasoning but they've converted it into
reasoning so they've solved the problem
and then they've destroyed the world
right so Ral is already prepared to
explain that he maybe doesn't fully
expect it but he's not denying that that
might happen and he's also been very
clear that he's not denying okay yeah
maybe you can attach an llm to some
other system like a python interpreter
and yes the llm will do a lot of the
heavy lifting even though he's just you
know downplaying it it's just doing
unconstrained generation according to
him but like like I said Ral is
basically allowing any possible outcome
of AI getting extremely powerful and
ending the world Ral would still be like
I'm not wrong I didn't say they couldn't
do that I just said that they couldn't
reason so at this point in the
conversation I'm not really interested
to have a semantic slap fight with Ral
about what reasoning is I mean I told
you my preferred definition which is to
just look at optimization power look at
what kind of input output mappings it's
capable of manifesting it's capable of
navigating right look at what kind of
power it has to make things happen or or
to know what needs to happen in order to
make things happen right that that's my
own preferred definition and of course
R's preferred definition I guess is more
like well it has to look more like what
I'm used to from good old fashioned I
don't even know what his definition is I
guess and I don't really care anymore
because any input output behavior is is
something that falls within what Ralph
thinks might happen in the future and my
motivation here is to just point out hey
the increasing power of AI is what makes
us doomed right and so I don't even
think at this point Ral is claiming to
be like don't worry we're not doomed
because AI can't reason I think he is
retreating into semantics being like
yeah okay we might be doomed but AI
still can't reason I don't want to speak
for him right this is just my guess I'm
not entirely sure how he would respond
to to this kind of line of questioning
but I'm ready to say okay let's put to
bed this question of can's reason I
don't think we're going to squeeze any
more juice out of it between me and well
let's see what other points R is going
to make to finish this up if they're
doing reasoning then I shouldn't be able
to come up with any diagonalization
arguments of the kind that I was talking
about earlier such as you know changing
the names of the predicates or looking
at other offsets of for SE cyer test Etc
I don't see that going to change just by
increasing the size of the models
actually this is a claim where we can
make it falsifiable and we could
probably find a prediction where and I
disagree on what's about to happen when
we scale up the models because I'm
seeing a trend where if you go from gpt3
to gp4 or Cloud 3.5 Sonet you are
actually seeing these models become more
robust so I can tweak the input in more
ways and they don't get fooled they
still do get fooled by some tweaks but
it's pretty clearly fewer tweaks I'm
pretty convinced of that so I'm ready to
extrapolate and be like hey I bet
they'll be even more robust and you know
and fewer things will throw them off but
R is basically saying no I'm pretty
convinced that there will always be
small tweaks I can make that throw them
off and that's an interesting prediction
I mean it doesn't necessarily prevent
them from taking over the world I me you
can have a mind that's like kind of
glitchy that can't fully fix itself but
is getting enough reinforcement from
like the surrounding environment or from
like having some other system check its
work right like a a domain specialized
math solver system or or whatever it is
so even if R's correct that llms can
never like robustly check their own work
and they can always kind of be fooled I
think and I both agree well there's
still some other llm plus X system which
is very powerful and dangerous but I
would love to have Ral come down with
the prediction of like okay what do you
think literally the gbt 5 chap Bo can't
do can you write down right now you know
can you give us a hash give us a
cryptographic hash so you can save the
time stamp of your prediction of a
specific prompt that you just think is
really simple and yet GPT 5 is going to
choke on it or some next scale of lolm
is going to choke on it I would love to
see that prediction and I might
personally want to take the opposite
side of that bet I think that's a great
separation of Me versus Ral and it would
honestly be the first time I've ever
seen Ral put a Line in the Sand where
he's not just willing to retroactively
explain anything that's happening this
would be the first time that Ral is
sticking his neck out and doing The
Honorable thing of being like I know
something about what llms can do and
that's why I feel like I'm entitled to
make this prediction right same
challenge I've issued to Martine cassado
that he managed to rigle his way out of
by saying that he can't tell you which
data it's going to have access to
therefore he doesn't OS any predictions
I would love for R to find a way to not
rle out of that and to actually make a
prediction because he seems to come off
like somebody with a knowledge state
that should enable him to make a
prediction so balls in your court R
let's make a prediction let's bet let's
disagree I think that would be a
highquality discourse that would be a
great way for gentlemen to actually make
progress toward resolving disagreement
we actually play this game where
whenever there's a new bigger and uh
bigger llm that that out we do the same
plan bench experiments on them and as of
like I we haven't yet done the Lama 3.1
that just came yesterday uh but pretty
much everything basically dies on the on
the upos skated blockx world problem for
example okay great so he has this
measure called plan bench I see there's
a research paper for it it has examples
and every time a new llm comes out he
tests it and I guess he's implying that
he predicts that llms because of their
nature as engr models according to him
they're just never going to solve Clan
bench I think is the claim he's making
or that's that's the vibe I'm getting
from him so that's great if he wants to
formalize that as a falsifiable
prediction like hey my plan bench test
you're just never going to have
something whose architecture looks like
an llm that's going to solve plan bench
now I took a look at plan bench and it
has some props that are like medium
length like I don't know a couple pages
of prompting they seem like the kind of
thing that Claude 3.5 Sonet is quite
good at like the kind of thing that I'd
expected it to get right but it's it's
it's going to be interesting to for me
to understand exactly why is it not
getting it right like they just seem
like maybe larger versions of the simple
Blox world problem that I showed you in
this episode um it would be great if R
could just post a walk through of like
hey I tried plan bench and this is where
it choked because I would love to debug
why is it choking right it seems if it
does choke on these kind of problems it
seems like it's really on the border of
getting it right so maybe we can just
put a stake in the ground where I say
like it's so close to getting this right
like I bet the next one will get it
right and says oh no no no no it's not
close to getting this right at all it's
not robust so to his credit he does have
a paper on it although I wasn't able to
ascertain from skimming um I just I
didn't come away from my skim with good
insight about where it's failing on plan
bench so I'd love to get more guidance
from him a quick Google I'm just not
seeing a lot of results on plan bench so
maybe the viewers could educate me like
teach me why clae 3.5 Sonic can't do
Blox World okay cuz from my perspective
it seems like it's got a pretty good
handle on easier versions of block so
educate me let's make a falsifiable
prediction let's let's have something
productive come out of this podcast and
I'm open-minded that maybe I'll even
learn something maybe I'll even change
my own prediction based on things some
evidence let's see finally we have ra
opining on agentic systems on the
agentic systems first of all I I kind of
I'm like bewildered by the whole agentic
hype because people
confuse acting versus with planning okay
and my my comeback to that is you leave
a gun in a house with a toddler the
toddler would act but they're not
planning necessarily so that's why we
shouldn't have guns hm that's actually
not a planning versus agency distinction
that's actually more of a statement
about where the optimization power is so
if you learn that a mom got shot in the
arm most of the optimization of that
event happened in the design of the gun
right the improbability of being able to
get a bullet that quickly into her arm
it mostly just happened when somebody
was architecting you know the gunpowder
and the Chamber and only a little bit of
optimization and really was
unintentional optimization unless the
toddler had a scheme but there's pretty
much zero optimization happening when
the toddler picked up the gun and fired
it so from an optimization analysis yeah
the gun maker did the deed but from a
planning versus agency analysis nobody
had a plan right the plan is
non-existent so I I don't know if this
is supporting the point he wants to make
where planning does more work than
agency it's interesting that he chose
this particular example conversely the
fact that you can pick up something or
you can press any little button or you
can make function calls doesn't
guarantee that those function calls will
actually lead to desirable outcomes so
most people in agentic systems
essentially are just somehow thinking
that if you can call the function
everything will be fine that's only true
in highly erodic worlds where pretty
much any sequence will kind of none of
the sequences will fail so it's the only
kinds of cases where planning almost is
not needed that's when you can basically
get by but otherwise agentic systems
require in addition to planning the
ability to call functions I thought Ral
was going to make a similar kind of
point that I make about planning versus
agency the point that I make is that if
you want Power then it comes down to
optimization optimization comes down to
planning an agency is like a little
detail if you've got the plan you're
going to figure out somebody to go do
the plan for you right like if you're a
super intelligent uh AI living inside
the internet you're going to bribe
somebody to go do what you need them to
do like the agency part is Trivial right
coming up with the idea of of who needs
to get bribed when okay there's one part
where your code says you know go call up
this API to send an HTTP packet over to
this Facebook chat or whatever right to
tell somebody to do something like is
like a minor detail right the
interesting part is what happened in the
ai's brain to hatch the scheme and then
you know simultaneously operate all the
different tentacle arms of the scheme
you know lightning speed and playing
people off against each other and
building power like that's the
interesting optimization that's
happening like the agency part isn't
really a separate part but anyway that's
not what R is saying right R is saying
hey look you can have agency without
having planning and agency just means
like picking up a gun if you see it and
also you can have planning uh but you
might not get enough Agency for your
planning if the world isn't erotic
enough world isn't erotic enough just go
bribe somebody man you can execute your
plans when you bribe somebody to do what
you tell them to do like it's not that
hard so uh yeah R just has separate
points that he wants to make I guess
people I I mean obviously would like to
have agentic systems but to the extent
since llms can't do planning you know I
don't expect that anything is going to
change unless you can have llm modulo
agentic systems where the plan is kind
of guarant there and then you do
function calls you know that's sort of a
a a possible uh thing uh so that's
that's how I look at this particular you
know generalist versus Agent systems I
disagree with the claim that LMS can't
do planning I mean when you ask an llm
to do all kinds of tasks like hey write
me an essay arguing for some conclusion
X the ability to structure an essay and
have it meet the constraints of like Yep
this essay is in fact an organized
argument for that conclusion X that is
what you plan to do like you might not
call it planning you might just call it
oh it's statistical magic but the thing
that it performs the type of work that
it performs is normally referred to as
planning work you normally think of a
planning system being needed to perform
it so if you want to dismiss the type of
you know feed forward Vector linear
algebra that the llms are doing you want
to just dismiss it and count it as not
planning okay but it just did something
that you would normally do planning for
so you might as well just call it
planning so I really object to him
saying llms don't do planning I get that
he can argue why the type of planning
they do isn't always robust you can
perturb it and then it stops working
fine but the kind of problems they help
us solve I strongly believe is
accurately describable as planning so
anyway we'll have to agree to disagree
on that because we're coming up to the
end
here okay that is my comprehensive
review of superar out com um Patty on
machine learning Street talk talking
about his research and why llms can't
reason according to him overall the part
that really sticks out to me the most is
the way he doubles down on his claim
that llms are just doing engrams with
really no justification whatsoever I
mean the one time when he made a clear
attempt at justification was when he
talks about diagonalization which I
think means hey we perturb these inputs
and we don't get a robust result I think
that's good research to do those are
good experiments to do but that's your
justification for saying that they're
freaking engrams like that's not
rigorous that's extremely concerning
that he's talking like that about lm's
right maybe he can just eliminate that
particular turn of phrase from his
vocabulary I mean it's as bad of a slur
as calling him stochastic parrots like
really really we're approaching the
singularity and you're saying stochastic
parrot you're doubling down on that
language it just makes me question is
credibility about other claims that he
makes uh now to be fair I guess the the
strongest point that he brings brings up
I guess is when he talks about specific
research he did right so it does sound
like he's actually kind of an expert as
to how you go perturb and llm to make it
say the wrong thing like he's talking
about having this Benchmark plan bench
and he thinks of himself as being like
on the Forefront of detecting what the
next version of the Ln can't do I would
love to actually get more detail about
that I specifically marked that as an
area where I can have my beliefs updated
a little bit I have actually in the past
a few months ago gone and looked at some
of the examples in some of his Point
presentation and I was to be fair a
little bit surprised at how tweaking
certain things about the llm input made
it choke it it was to be fair worse than
I realized so I had to update on that uh
but I still maintain that its ability to
get things right like the joke analysis
example like I don't care if tweaking
that example makes a choke or even if
the next three jokes I give it it fails
to explain just the the fact that it was
so good with that one particular joke is
incredible right so and I and I just
don't see enough appreciation when he
uses terms like statistical engram I
just don't see a proper understanding
and appreciation of how this is possible
and then of course when he goes on to
claim that like they're not doing real
Turing computation I mean he just raised
some major major red flags with me right
and of course the general attitude where
he just doesn't seem to expect much from
llms but you know what that whole thing
the tug war game where he's like
retroactively explaining everything I
feel like he kind of owned it when he
said look I'm actually kind of
optimistic about AI in general I don't
want to restrain what I might expect to
see from him next so it seems like he
kind of owns it that he's not
particularly predicting an AI winter or
anything so if you want to get to a
falsifiable prediction I really would I
guess I would dive into plan bench right
I'd be like let's talk about plan bench
what exactly does plan bench imply that
gb5 probably can't do let's talk about
that and let's try to make predictions
you know on opposite size let's try to
make a bet the other thing I'll say
about Ral is I think he's a smart guy I
like listening to him talk I find him
pretty easy to understand he's somebody
that I could have a conversation with so
shout out to Ralph if you want to come
on Doom debates if you want to Hash this
out here in the lines then if if you
don't mind a little bit of a debate
please come on I I'd love to have you I
think it'd be a productive discussion
and it's like yet another episode of
machine learning Street Talk where I
disagree with a guest but there's no
denying that they've made a quality
piece of content that's it for this
episode of Dune debates please do your
part if you support the mission of
raising awareness about AI existential
risk and raising the quality of
discourse you know how you can help
share the show smack Smack That
subscribe button go to Doom debates.com
subscribe to my substack and I look
forward to seeing everybody next time on
Doom debates